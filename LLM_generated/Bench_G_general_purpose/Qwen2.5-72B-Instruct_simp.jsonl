{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton code implements a custom attention mechanism with forward and backward kernels for computing gradients. There are two main functions, `LightningAttention2NoDecay.forward` for the forward pass and `LightningAttention2NoDecay.backward` for the backward pass. In the forward pass, it computes the output using queries (Q), keys (K), and values (V) tensors. The backward pass computes gradients with respect to Q, K, and V.\n\n            The `_fwd_kernel` function computes the attention output using block processing. It loads blocks of Q, K, and V, computes the QK product, and then computes the output using matrix multiplication.\n\n            The `_bwd_intra_kernel` function calculates gradients within each block by backpropagating the error from the output to the inputs. It updates gradients of Q, K, and V.\n\n            The `_bwd_inter_kernel` handles inter-block computations for gradients. It iterates over blocks to compute and accumulate gradients for K and V.\n\n            The block size for forward and backward computations is 64, while the compute block size (CBLOCK) in the backward intra part is 32.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, Out,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_ob, stride_oh, stride_om,\n    N_CTX, N_HEAD, N_EMBD,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (N_CTX // BLOCK_SIZE)\n    pid = pid % (N_CTX // BLOCK_SIZE)\n    q_offset = bid * stride_qb + pid * BLOCK_SIZE\n    k_offset = bid * stride_kb\n    v_offset = bid * stride_vb\n    o_offset = bid * stride_ob + pid * BLOCK_SIZE\n\n    Q_block = tl.load(Q + q_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_qm + tl.arange(0, N_EMBD)[None, :] * stride_qh)\n    K_block = tl.load(K + k_offset + tl.arange(0, N_CTX)[:, None] * stride_km + tl.arange(0, N_EMBD)[None, :] * stride_kh)\n    V_block = tl.load(V + v_offset + tl.arange(0, N_CTX)[:, None] * stride_vm + tl.arange(0, N_EMBD)[None, :] * stride_vh)\n\n    QK = tl.dot(Q_block, K_block, trans_b=True)\n    QK = tl.softmax(QK, axis=1)\n    Out_block = tl.dot(QK, V_block)\n\n    tl.store(Out + o_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_om + tl.arange(0, N_EMBD)[None, :] * stride_oh, Out_block)\n\n@triton.jit\ndef _bwd_intra_kernel(\n    Q, K, V, Out, GradOut, GradQ, GradK, GradV,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_ob, stride_oh, stride_om,\n    stride_gob, stride_goh, stride_gom,\n    stride_gqb, stride_gqh, stride_gqm,\n    stride_gkb, stride_gkh, stride_gkm,\n    stride_gvb, stride_gvh, stride_gvm,\n    N_CTX, N_HEAD, N_EMBD,\n    BLOCK_SIZE: tl.constexpr,\n    CBLOCK: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (N_CTX // BLOCK_SIZE)\n    pid = pid % (N_CTX // BLOCK_SIZE)\n    q_offset = bid * stride_qb + pid * BLOCK_SIZE\n    k_offset = bid * stride_kb\n    v_offset = bid * stride_vb\n    o_offset = bid * stride_ob + pid * BLOCK_SIZE\n    gob_offset = bid * stride_gob + pid * BLOCK_SIZE\n    gq_offset = bid * stride_gqb + pid * BLOCK_SIZE\n    gk_offset = bid * stride_gkb\n    gv_offset = bid * stride_gvb\n\n    Q_block = tl.load(Q + q_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_qm + tl.arange(0, N_EMBD)[None, :] * stride_qh)\n    K_block = tl.load(K + k_offset + tl.arange(0, N_CTX)[:, None] * stride_km + tl.arange(0, N_EMBD)[None, :] * stride_kh)\n    V_block = tl.load(V + v_offset + tl.arange(0, N_CTX)[:, None] * stride_vm + tl.arange(0, N_EMBD)[None, :] * stride_vh)\n    Out_block = tl.load(Out + o_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_om + tl.arange(0, N_EMBD)[None, :] * stride_oh)\n    GradOut_block = tl.load(GradOut + gob_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_gom + tl.arange(0, N_EMBD)[None, :] * stride_goh)\n\n    QK = tl.dot(Q_block, K_block, trans_b=True)\n    QK = tl.softmax(QK, axis=1)\n    GradQK = tl.dot(GradOut_block, V_block, trans_b=True)\n    GradQ = tl.dot(GradQK, K_block)\n    GradK = tl.dot(Q_block, GradQK, trans_a=True)\n    GradV = tl.dot(QK, GradOut_block, trans_a=True)\n\n    tl.store(GradQ + gq_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_gqm + tl.arange(0, N_EMBD)[None, :] * stride_gqh, GradQ)\n    tl.store(GradK + gk_offset + tl.arange(0, N_CTX)[:, None] * stride_gkm + tl.arange(0, N_EMBD)[None, :] * stride_gkh, GradK)\n    tl.store(GradV + gv_offset + tl.arange(0, N_CTX)[:, None] * stride_gvm + tl.arange(0, N_EMBD)[None, :] * stride_gvh, GradV)\n\n@triton.jit\ndef _bwd_inter_kernel(\n    GradK, GradV,\n    stride_gkb, stride_gkh, stride_gkm,\n    stride_gvb, stride_gvh, stride_gvm,\n    N_CTX, N_HEAD, N_EMBD,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (N_CTX // BLOCK_SIZE)\n    pid = pid % (N_CTX // BLOCK_SIZE)\n    gk_offset = bid * stride_gkb + pid * BLOCK_SIZE\n    gv_offset = bid * stride_gvb + pid * BLOCK_SIZE\n\n    GradK_block = tl.load(GradK + gk_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_gkm + tl.arange(0, N_EMBD)[None, :] * stride_gkh)\n    GradV_block = tl.load(GradV + gv_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_gvm + tl.arange(0, N_EMBD)[None, :] * stride_gvh)\n\n    GradK_sum = tl.sum(GradK_block, axis=0)\n    GradV_sum = tl.sum(GradV_block, axis=0)\n\n    tl.store(GradK + gk_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_gkm + tl.arange(0, N_EMBD)[None, :] * stride_gkh, GradK_sum)\n    tl.store(GradV + gv_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_gvm + tl.arange(0, N_EMBD)[None, :] * stride_gvh, GradV_sum)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    Out,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    BLOCK_MODEL: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_bh % h\n    off_e = tl.program_id(1)\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    # channel offset\n    e_offset = off_e * BLOCK_MODEL\n\n    ##### get block ptr\n    Q_block_ptr = Q + qk_offset + tl.arange(0, d)[None, :]\n    K_trans_block_ptr = K + qk_offset + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    O_block_ptr = Out + o_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n\n    ##### init diag decay(Lambda); q, k decay; kv\n    # q, k decay\n    off_block = tl.arange(\n        0, BLOCK\n    )  # Not bug, this is a bit different from algorithm 1, but is mathematically equivalent\n    # diag decay\n    index = off_block[:, None] - off_block[None, :]\n    kv = tl.zeros([d, BLOCK_MODEL], dtype=tl.float32)\n\n    ##### compute\n    for i in range(NUM_BLOCK):\n        # load\n        q = tl.load(\n            Q_block_ptr + off_block[:, None] * d, mask=off_block[:, None] < n, other=0.0\n        ).to(tl.float32)\n        k_trans = tl.load(\n            K_trans_block_ptr + off_block[None, :] * d,\n            mask=off_block[None, :] < n,\n            other=0.0,\n        ).to(tl.float32)\n        v = tl.load(\n            V_block_ptr + off_block[:, None] * e, mask=off_block[:, None] < n, other=0.0\n        ).to(tl.float32)\n\n        # compute\n        qk = tl.dot(q, k_trans)\n        qk = tl.where(index >= 0, qk, 0)\n        o_intra = tl.dot(qk, v)\n        o_inter = tl.dot(q, kv)\n        o = o_intra + o_inter\n\n        # save and update\n        tl.store(\n            O_block_ptr + off_block[:, None] * e,\n            o.to(O_block_ptr.dtype.element_ty),\n            mask=off_block[:, None] < n,\n        )\n        kv += tl.dot(k_trans, v)\n        off_block += BLOCK\n\n\n@triton.jit\ndef _bwd_intra_kernel(\n    Q,\n    K,\n    V,\n    DO,\n    DQ,\n    DK,\n    DV,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    CBLOCK: tl.constexpr,\n    NUM_CBLOCK: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK + tl.arange(0, BLOCK)\n\n    ##### get block ptr\n    Q_trans_block_ptr = (\n        Q + qk_offset + block_offset[None, :] * d + tl.arange(0, d)[:, None]\n    )\n    K_block_ptr = K + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = (\n        V + v_offset + block_offset[None, :] * e + tl.arange(0, e)[:, None]\n    )\n\n    DQ_block_ptr = DQ + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[None, :]\n    DK_trans_block_ptr = (\n        DK + qk_offset + block_offset[None, :] * d + tl.arange(0, d)[:, None]\n    )\n    DV_block_ptr = DV + v_offset + block_offset[:, None] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + block_offset[:, None] * e + tl.arange(0, e)[None, :]\n\n    ##### init diag decay(Lambda)\n    array = tl.arange(0, BLOCK).to(tl.float32)\n    # diag\n    index = array[:, None] - array[None, :]\n\n    ##### load block\n    k = tl.load(K_block_ptr, mask=block_offset[:, None] < n, other=0.0).to(tl.float32)\n    v_trans = tl.load(V_trans_block_ptr, mask=block_offset[None, :] < n, other=0.0).to(\n        tl.float32\n    )\n    do = tl.load(DO_block_ptr, mask=block_offset[:, None] < n, other=0.0).to(tl.float32)\n    q_trans = tl.load(Q_trans_block_ptr, mask=block_offset[None, :] < n, other=0.0).to(\n        tl.float32\n    )\n\n    ##### compute\n    dqk = tl.dot(do, v_trans)\n    dqk = tl.where(index >= 0, dqk, 0)\n    dq_intra = tl.dot(dqk, k)\n\n    dk_intra_trans = tl.dot(q_trans, dqk)\n\n    qk_trans = tl.dot(k, q_trans)\n    qk_trans = tl.where(index <= 0, qk_trans, 0)\n    dv_intra = tl.dot(qk_trans, do)\n\n    dq = dq_intra\n    dk_trans = dk_intra_trans\n    dv = dv_intra\n\n    # save\n    tl.store(\n        DQ_block_ptr,\n        dq.to(DQ_block_ptr.dtype.element_ty),\n        mask=block_offset[:, None] < n,\n    )\n    tl.store(\n        DK_trans_block_ptr,\n        dk_trans.to(DK_trans_block_ptr.dtype.element_ty),\n        mask=block_offset[None, :] < n,\n    )\n    tl.store(\n        DV_block_ptr,\n        dv.to(DV_block_ptr.dtype.element_ty),\n        mask=block_offset[:, None] < n,\n    )\n\n\n@triton.jit\ndef _bwd_inter_kernel(\n    Q,\n    K,\n    V,\n    DO,\n    DQ,\n    DK,\n    DV,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    CBLOCK: tl.constexpr,\n    NUM_CBLOCK: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_bh % h\n\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n\n    ##### get block ptr\n    DQ_block_ptr = (\n        DQ + qk_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :]\n    )\n    K_block_ptr = (\n        K + qk_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :]\n    )\n    V_trans_block_ptr = (\n        V + v_offset + tl.arange(0, CBLOCK)[None, :] * e + tl.arange(0, e)[:, None]\n    )\n    DO_block_ptr = (\n        DO + o_offset + tl.arange(0, CBLOCK)[:, None] * e + tl.arange(0, e)[None, :]\n    )\n    # mask\n    off_block1 = tl.arange(0, CBLOCK)\n    off_block2 = tl.arange(0, CBLOCK)\n\n    ##### init lambda; kv\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n\n    ##### compute dq inter\n    for i in range(NUM_BLOCK):\n        # compute in subblock\n        for j in range(NUM_CBLOCK):\n            if i > 0:  # if not add this, may have bug\n                do = tl.load(DO_block_ptr, mask=off_block1[:, None] < n, other=0.0).to(\n                    tl.float32\n                )\n                dq_inter = tl.dot(do, kv_trans)\n                dq = dq_inter + tl.load(\n                    DQ_block_ptr, mask=off_block1[:, None] < n, other=0.0\n                )\n                tl.store(\n                    DQ_block_ptr,\n                    dq.to(DQ_block_ptr.dtype.element_ty),\n                    mask=off_block1[:, None] < n,\n                )\n\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n            off_block1 += CBLOCK\n\n        # update kv in subblock\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(\n                V_trans_block_ptr, mask=off_block2[None, :] < n, other=0.0\n            ).to(tl.float32)\n            k = tl.load(K_block_ptr, mask=off_block2[:, None] < n, other=0.0).to(\n                tl.float32\n            )\n            kv_trans_current += tl.dot(v_trans, k)\n\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n            off_block2 += CBLOCK\n\n        kv_trans += kv_trans_current\n\n    ##### get block ptr\n    m = NUM_BLOCK * BLOCK\n    off_block1 = m + tl.arange(0, CBLOCK)\n    off_block2 = m + tl.arange(0, CBLOCK)\n\n    Q_trans_block_ptr = (\n        Q\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[None, :] * d\n        + tl.arange(0, d)[:, None]\n    )\n    K_block_ptr = (\n        K\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[:, None] * d\n        + tl.arange(0, d)[None, :]\n    )\n    V_trans_block_ptr = (\n        V\n        + v_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[None, :] * e\n        + tl.arange(0, e)[:, None]\n    )\n\n    DK_trans_block_ptr = (\n        DK\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[None, :] * d\n        + tl.arange(0, d)[:, None]\n    )\n    DV_block_ptr = (\n        DV\n        + v_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[:, None] * e\n        + tl.arange(0, e)[None, :]\n    )\n    DO_block_ptr = (\n        DO\n        + o_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[:, None] * e\n        + tl.arange(0, e)[None, :]\n    )\n\n    ##### init dkv\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n\n    ##### compute dk, dv inter\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        # compute in subblock\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            off_block1 -= CBLOCK\n\n            if i < NUM_BLOCK - 1:  # if not add this, may have bug\n                k = tl.load(K_block_ptr, mask=off_block1[:, None] < n, other=0.0).to(\n                    tl.float32\n                )\n                v_trans = tl.load(\n                    V_trans_block_ptr, mask=off_block1[None, :] < n, other=0.0\n                ).to(tl.float32)\n\n                dk_inter_trans = tl.dot(dkv, v_trans)\n                dv_inter = tl.dot(k, dkv)\n\n                dk_trans = dk_inter_trans + tl.load(\n                    DK_trans_block_ptr, mask=off_block1[None, :] < n, other=0.0\n                )\n                dv = dv_inter + tl.load(\n                    DV_block_ptr, mask=off_block1[:, None] < n, other=0.0\n                )\n\n                tl.store(\n                    DK_trans_block_ptr,\n                    dk_trans.to(DK_trans_block_ptr.dtype.element_ty),\n                    mask=off_block1[None, :] < n,\n                )\n                tl.store(\n                    DV_block_ptr,\n                    dv.to(DV_block_ptr.dtype.element_ty),\n                    mask=off_block1[:, None] < n,\n                )\n\n        # update dkv in subblock\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            off_block2 -= CBLOCK\n\n            do = tl.load(DO_block_ptr, mask=off_block2[:, None] < n, other=0.0).to(\n                tl.float32\n            )\n            q_trans = tl.load(\n                Q_trans_block_ptr, mask=off_block2[None, :] < n, other=0.0\n            ).to(tl.float32)\n            dkv_current += tl.dot(q_trans, do)\n\n        dkv += dkv_current\n\n\nclass LightningAttention2NoDecay(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v):\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n\n        b, h, n, d = q.shape\n        e = v.shape[-1]\n        o = torch.empty((b, h, n, e), dtype=q.dtype, device=q.device)\n\n        BLOCK = 64\n        NUM_BLOCK = triton.cdiv(q.shape[2], BLOCK)\n        # parallel over channel\n        BLOCK_MODEL = min(triton.next_power_of_2(e), 32)\n        grid = (b * h, triton.cdiv(e, BLOCK_MODEL))\n\n        _fwd_kernel[grid](\n            q,\n            k,\n            v,\n            o,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            BLOCK_MODEL=BLOCK_MODEL,\n        )\n\n        ctx.save_for_backward(q, k, v)\n\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        q, k, v = ctx.saved_tensors\n\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        do = do.contiguous()\n\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n\n        b, h, n, d = q.shape\n        e = v.shape[-1]\n\n        # block size\n        BLOCK = 64\n        NUM_BLOCK = triton.cdiv(n, BLOCK)\n        # compute block size\n        CBLOCK = 32\n        NUM_CBLOCK = BLOCK // CBLOCK\n\n        # for intra part, compute in parallel\n        grid = (b * h, NUM_BLOCK)\n        _bwd_intra_kernel[grid](\n            q,\n            k,\n            v,\n            do,\n            dq,\n            dk,\n            dv,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            CBLOCK=CBLOCK,\n            NUM_CBLOCK=NUM_CBLOCK,\n        )\n\n        # for inter part, compute in sequencial\n        grid = (b * h,)\n        _bwd_inter_kernel[grid](\n            q,\n            k,\n            v,\n            do,\n            dq,\n            dk,\n            dv,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            CBLOCK=CBLOCK,\n            NUM_CBLOCK=NUM_CBLOCK,\n        )\n\n        return dq, dk, dv\n\n\nlightning_attn2_no_decay = LightningAttention2NoDecay.apply\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel '_dequantize_rowwise' performs row-wise dequantization on a given input tensor 'x'. Each row is processed individually by multiplying each element with a stored maximum value from 'state_x' and a constant inverse of 127. The result is stored in 'output_ptr'. The kernel operates over blocks of data defined by 'BLOCK_SIZE'. The 'dequantize_rowwise' function initializes the output tensor and launches the Triton kernel. It calculates the nearest power of two for the number of columns and sets the execution grid to iterate over the number of rows.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _dequantize_rowwise(x_ptr, state_x_ptr, output_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    row_start = pid * BLOCK_SIZE\n    row_end = min(row_start + BLOCK_SIZE, M)\n\n    for row in range(row_start, row_end):\n        max_val = tl.load(state_x_ptr + row)\n        inv_127 = 1.0 / 127.0\n        row_offset = row * N\n        for col in range(N):\n            x_val = tl.load(x_ptr + row_offset + col)\n            output_val = x_val * max_val * inv_127\n            tl.store(output_ptr + row_offset + col, output_val)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef dequantize_rowwise(x, state_x):\n    M, N = x.shape\n    output = torch.empty_like(x, dtype=torch.float32, device=x.device)\n    \n    # Calculate the nearest power of two for the number of columns\n    BLOCK_SIZE = 128\n    grid = (triton.cdiv(M, BLOCK_SIZE),)\n    \n    # Launch the Triton kernel\n    _dequantize_rowwise[grid](x, state_x, output, M, N, BLOCK_SIZE)\n    \n    return output\n\nimport torch\n\n# Example input tensor and state tensor\nx = torch.randint(0, 128, (1024, 512), device='cuda')\nstate_x = torch.rand(1024, device='cuda')\n\n# Perform row-wise dequantization\noutput = dequantize_rowwise(x, state_x)\n\nprint(output)\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for rowwise dequantization\n@triton.jit\ndef _dequantize_rowwise(\n    x_ptr,\n    state_x,\n    output_ptr,\n    inv_127,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n    max_val = tl.load(state_x + pid)\n    output = max_val * x * inv_127\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n\n# Function to call the Triton kernel\ndef dequantize_rowwise(x: torch.Tensor, state_x: torch.Tensor):\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.float16)\n\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n\n    assert x.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (x.shape[0],)\n    _dequantize_rowwise[grid](x, state_x, output, 1.0 / 127, n_elements, BLOCK_SIZE=x.shape[1], P2=P2)\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `kldivergence_kernel` function is a Triton kernel designed to calculate the Kullback-Leibler (KL) divergence between two input vectors `x` and `y`. It takes pointers to these vectors and an output vector where results will be stored. The kernel processes data in blocks, each of which contains `BLOCK_SIZE` elements. It uses Triton's `tl.program_id` to identify the program's ID and compute the block's starting index. The kernel then loads elements from `x` and `y`, computes the KL divergence for each element using `x * tl.log(x / y)`, and stores the results in the output vector. The `kldivergence` function serves as a Python wrapper around this kernel, taking PyTorch tensors `x` and `y` as input and returning the KL divergence as output.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef kldivergence_kernel(\n    x_ptr,  # Pointer to input vector x\n    y_ptr,  # Pointer to input vector y\n    output_ptr,  # Pointer to output vector\n    n_elements,  # Number of elements in the vectors\n    BLOCK_SIZE: tl.constexpr,  # Block size\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    block_start = pid * BLOCK_SIZE  # Compute the starting index for this block\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # Create offsets for this block\n    mask = offsets < n_elements  # Create a mask to handle the last block\n\n    x = tl.load(x_ptr + offsets, mask=mask)  # Load elements from x\n    y = tl.load(y_ptr + offsets, mask=mask)  # Load elements from y\n\n    # Compute the KL divergence for each element\n    kl_div = x * tl.log(x / y)\n\n    # Store the results in the output vector\n    tl.store(output_ptr + offsets, kl_div, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n# Python wrapper function\ndef kldivergence(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    # Ensure x and y are on the same device and have the same shape\n    assert x.device == y.device, \"x and y must be on the same device\"\n    assert x.shape == y.shape, \"x and y must have the same shape\"\n\n    # Create an output tensor\n    output = torch.empty_like(x)\n\n    # Get the number of elements\n    n_elements = x.numel()\n\n    # Launch the kernel\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)  # Compute the grid size\n    kldivergence_kernel[grid](\n        x, y, output, n_elements, BLOCK_SIZE\n    )\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef kldivergence_kernel(x_ptr,  # *Pointer* to first input vector.\n                        y_ptr,  # *Pointer* to second input vector.\n                        output_ptr,  # *Pointer* to output vector.\n                        n_elements,  # Size of the vector.\n                        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n                        # NOTE: `constexpr` so it can be used as a shape value.\n                        ):\n\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * tl.log(x / y)\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef kldivergence(x: torch.Tensor, y: torch.Tensor):\n    output = torch.empty_like(x)\n    assert x.is_cuda and y.is_cuda and output.is_cuda\n    n_elements = output.numel()\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n\n    kldivergence_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton kernel implements the forward pass of a context attention mechanism. It is a custom implementation of a scaled dot-product attention, optimized for the GPU using Triton. The key function here is '_fwd_kernel' which computes attention scores and applies them to values to produce the output.\n\n            The '_fwd_kernel' function takes several tensor inputs: Q (queries), K (keys), V (values), and outputs the result to the Out tensor. It also takes various indexing and stride parameters to handle batched and multi-headed inputs.\n\n            The function computes the dot product of Q and K, applies scaling, and uses the softmax function to normalize these scores. Then it applies these normalized scores to V to get the final output. This entire process is batched and operates over multiple attention heads in parallel.\n\n            The 'context_attention_fwd' function acts as a wrapper to prepare arguments and call the Triton kernel, handling grid and block size configurations, and ensuring optimal performance based on input dimensions and GPU architecture.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, Out,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_ob, stride_oh, stride_om,\n    n_head, n_ctx, scale,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    # Get the current block index\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(n_ctx, BLOCK_M)\n    num_pid_n = tl.cdiv(n_ctx, BLOCK_N)\n    num_pid_in_batch = num_pid_m * num_pid_n\n    batch_id = pid // num_pid_in_batch\n    pid %= num_pid_in_batch\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    # Compute the offsets for the current block\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n\n    # Compute the base offsets for the current head and batch\n    q_ptrs = Q + (batch_id * stride_qb + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qh)\n    k_ptrs = K + (batch_id * stride_kb + offs_n[:, None] * stride_km + offs_d[None, :] * stride_kh)\n    v_ptrs = V + (batch_id * stride_vb + offs_n[:, None] * stride_vm + offs_d[None, :] * stride_vh)\n    o_ptrs = Out + (batch_id * stride_ob + offs_m[:, None] * stride_om + offs_d[None, :] * stride_oh)\n\n    # Initialize the output and the attention scores\n    acc = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)\n\n    # Compute the dot product of Q and K\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    qk = tl.dot(q, k, trans_b=True) * scale\n\n    # Apply the softmax function to the attention scores\n    qk = tl.softmax(qk, axis=1)\n\n    # Compute the weighted sum of V\n    v = tl.load(v_ptrs)\n    acc += tl.dot(qk, v)\n\n    # Store the result to the output tensor\n    tl.store(o_ptrs, acc)\n\nimport triton\nimport triton.language as tl\nimport torch\n\ndef context_attention_fwd(Q, K, V, Out, n_head, n_ctx, scale):\n    # Get the dimensions of the input tensors\n    batch_size, _, _ = Q.shape\n\n    # Define the block and grid sizes\n    BLOCK_M = 16\n    BLOCK_N = 16\n    BLOCK_D = 64\n    grid = (triton.cdiv(n_ctx, BLOCK_M) * triton.cdiv(n_ctx, BLOCK_N) * batch_size,)\n\n    # Define the strides for the input and output tensors\n    stride_qb = Q.stride(0)\n    stride_qh = Q.stride(1)\n    stride_qm = Q.stride(2)\n\n    stride_kb = K.stride(0)\n    stride_kh = K.stride(1)\n    stride_km = K.stride(2)\n\n    stride_vb = V.stride(0)\n    stride_vh = V.stride(1)\n    stride_vm = V.stride(2)\n\n    stride_ob = Out.stride(0)\n    stride_oh = Out.stride(1)\n    stride_om = Out.stride(2)\n\n    # Launch the Triton kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out,\n        stride_qb, stride_qh, stride_qm,\n        stride_kb, stride_kh, stride_km,\n        stride_vb, stride_vh, stride_vm,\n        stride_ob, stride_oh, stride_om,\n        n_head, n_ctx, scale,\n        BLOCK_M, BLOCK_N, BLOCK_D\n    )\n\n# Example usage\nbatch_size = 2\nn_head = 4\nn_ctx = 128\nd_model = 64\nscale = 1.0 / (d_model ** 0.5)\n\nQ = torch.randn(batch_size, n_head, n_ctx, d_model, device='cuda')\nK = torch.randn(batch_size, n_head, n_ctx, d_model, device='cuda')\nV = torch.randn(batch_size, n_head, n_ctx, d_model, device='cuda')\nOut = torch.empty_like(Q)\n\ncontext_attention_fwd(Q, K, V, Out, n_head, n_ctx, scale)\n\nprint(Out)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\nimport math\nimport torch.nn.functional as F\n\nTESLA = \"Tesla\" in torch.cuda.get_device_name(0)\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    B_Start_Loc,\n    B_Seqlen,  # B_LOC \u5185\u90e8\u8bb0\u5f55\u6bcf\u4e2abatch \u8f93\u5165\u7684\u771f\u5b9e\u4f4d\u7f6e\uff0c B_SEQ_len \u8bb0\u5f55\u5f53\u524d\u8f93\u5165\u7684\u771f\u5b9e\u957f\u5ea6\n    Out,\n    Req_to_tokens,\n    B_req_idx,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    kv_group_num,\n    b_prompt_cache_len,\n    head_dim: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n\n    cur_kv_head = cur_head // kv_group_num\n\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (offs_d[None, :] < head_dim), other=0.0)\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + prompt_cache_len, cur_batch_seq_len + prompt_cache_len)\n\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        kv_loc = tl.load(\n            Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=(start_n + offs_n) < block_end_loc,\n            other=0,\n        )\n        off_k = kv_loc[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd\n        k = tl.load(\n            K + off_k, mask=((start_n + offs_n[None, :]) < block_end_loc) & (offs_d[:, None] < head_dim), other=0.0\n        )\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] + prompt_cache_len >= start_n + offs_n[None, :], qk, float(\"-100000000.0\"))\n\n        # -- compute m_ij, p, l_ij\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        # -- update m_i and l_i\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        # -- update output accumulator --\n        # scale p\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        # scale acc\n        acc_scale = l_i / l_i_new * alpha\n        acc_scale = tl.where(offs_m + prompt_cache_len >= start_n, acc_scale, 1.0)\n        acc = acc * acc_scale[:, None]\n        # update acc\n        off_v = kv_loc[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n        v = tl.load(\n            V + off_v, mask=((start_n + offs_n[:, None]) < block_end_loc) & (offs_d[None, :] < head_dim), other=0.0\n        )\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_i = l_i_new\n        m_i = m_i_new\n    # initialize pointers to output\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (offs_d[None, :] < head_dim))\n    return\n\n\n@torch.no_grad()\ndef context_attention_fwd(\n    q, k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len, req_to_token_indexs\n):\n    BLOCK = 128 if not TESLA else 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    head_dim = Lq\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n\n    sm_scale = 1.0 / (Lq ** 0.5)  # \u8ba1\u7b97scale\u7cfb\u6570\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n\n    num_warps = 4 if Lk <= 64 else 8\n    _fwd_kernel[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        b_start_loc,\n        b_seq_len,\n        o,\n        req_to_token_indexs,\n        b_req_idx,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        req_to_token_indexs.stride(0),\n        req_to_token_indexs.stride(1),\n        kv_group_num=kv_group_num,\n        b_prompt_cache_len=b_prompt_cache_len,\n        head_dim=head_dim,\n        BLOCK_M=BLOCK,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton kernel is named `_swiglu_fwd_kernel`. It performs the forward computation for the Swish-Gated Linear Units (Swiglu). The kernel operates on two input matrices `X` and `Y`, producing an output matrix `OUT`. The main computation involves element-wise multiplication of `x` with the sigmoid activation of `x`, further multiplied by `y`. The function `_swiglu_fwd` acts as a wrapper to set up and invoke the Triton kernel. The inputs are split into two parts: `x` and `y`. The function configures grid dimensions and ensures data is contiguous before execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _swiglu_fwd_kernel(X, Y, OUT, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    x = tl.load(X + offsets, mask=mask)\n    y = tl.load(Y + offsets, mask=mask)\n\n    # Compute the sigmoid of x\n    sigmoid_x = 1 / (1 + tl.exp(-x))\n\n    # Compute the element-wise multiplication\n    out = sigmoid_x * x * y\n\n    tl.store(OUT + offsets, out, mask=mask)\n\nimport torch\nfrom triton.runtime import launch_kernel\n\ndef _swiglu_fwd(x, y, out):\n    assert x.is_contiguous(), \"Input x must be contiguous\"\n    assert y.is_contiguous(), \"Input y must be contiguous\"\n    assert out.is_contiguous(), \"Output out must be contiguous\"\n    assert x.shape == y.shape, \"Input x and y must have the same shape\"\n    assert out.shape == x.shape, \"Output out must have the same shape as input x\"\n\n    N = x.numel()\n    BLOCK_SIZE = 1024\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n\n    # Convert tensors to Triton pointers\n    x_ptr = x.contiguous().data_ptr()\n    y_ptr = y.contiguous().data_ptr()\n    out_ptr = out.contiguous().data_ptr()\n\n    # Launch the kernel\n    launch_kernel(_swiglu_fwd_kernel, grid, (x_ptr, y_ptr, out_ptr, N, BLOCK_SIZE))\n\n# Example usage\nif __name__ == \"__main__\":\n    N = 1024 * 1024\n    x = torch.randn(N, device='cuda')\n    y = torch.randn(N, device='cuda')\n    out = torch.empty_like(x)\n\n    _swiglu_fwd(x, y, out)\n\n    print(out)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Forward Triton kernel for Swish-Gated Linear Units (Swiglu)\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 32}),\n        triton.Config({'BLOCK_N': 64}),\n        triton.Config({'BLOCK_N': 128}),\n        triton.Config({'BLOCK_N': 256}),\n        triton.Config({'BLOCK_N': 512}),\n        triton.Config({'BLOCK_N': 1024}),\n    ],\n    key=['ncols'],\n)\n@triton.jit\ndef _swiglu_fwd_kernel(\n    X, Y, OUT, stride_x_row, stride_y_row, stride_out_row, ncols, BLOCK_N: tl.constexpr\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)\n\n# Function to invoke the forward kernel\ndef _swiglu_fwd(xy, out=None):\n    if xy.stride(-1) != 1:\n        xy = xy.contiguous()\n    batch_shape = xy.shape[:-1]\n    xy = xy.reshape(-1, xy.shape[-1])\n    x, y = xy.chunk(2, dim=-1)\n    if out is None:\n        out = torch.empty_like(x)\n    else:\n        out = out.reshape(-1, out.shape[-1])\n        assert out.shape == x.shape\n    assert out.stride(-1) == 1\n    M, N = x.shape\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_N']))\n    with torch.cuda.device(x.device.index):\n        _swiglu_fwd_kernel[grid](x, y, out, x.stride(0), y.stride(0), out.stride(0), N)\n    return out.reshape(*batch_shape, out.shape[-1])\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe code defines a Triton kernel named `matmul_kernel` for performing matrix multiplication and a function `matmul` to call this kernel. The main functionality is to multiply two matrices `a` and `b` with specific blocking dimensions using Triton's parallel execution capabilities. The input matrices must satisfy the condition that the number of columns in `a` is four times the number of rows in `b`. The `matmul` function initializes an output matrix `c` and calculates the appropriate grid size for launching the kernel. `matmul_kernel` reads blocks of data from `a` and `b`, performs dot product operations, and writes the result into `c`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr\n):\n    # Matrix multiplication using Triton\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m = pid // grid_n\n    pid_n = pid % grid_n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n    B = tl.zeros((BLOCK_K, BLOCK_N), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        # Load the blocks of A and B\n        a_indices = rm[:, None] * stride_am + rk[None, :] * stride_ak\n        b_indices = rk[:, None] * stride_bk + rn[None, :] * stride_bn\n        A = tl.load(a_ptr + a_indices, mask=k + rk[None, :] < K, other=0.0)\n        B = tl.load(b_ptr + b_indices, mask=k + rk[:, None] < K, other=0.0)\n\n        # Perform the dot product\n        acc += tl.dot(A, B)\n\n    # Store the result in C\n    c_indices = rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    tl.store(c_ptr + c_indices, acc, mask=rm[:, None] < M & rn[None, :] < N)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef matmul(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    # Ensure the input matrices satisfy the condition\n    assert a.shape[1] == 4 * b.shape[0], \"The number of columns in a must be four times the number of rows in b\"\n\n    # Get the dimensions of the input matrices\n    M, K = a.shape\n    _, N = b.shape\n\n    # Initialize the output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Define the block sizes\n    BLOCK_M = 16\n    BLOCK_N = 16\n    BLOCK_K = 32\n\n    # Calculate the grid size\n    grid = lambda meta: (\n        (M + meta['BLOCK_M'] - 1) // meta['BLOCK_M'] * (N + meta['BLOCK_N'] - 1) // meta['BLOCK_N'],\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        BLOCK_K=BLOCK_K\n    )\n\n    return c\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef get_autotune_config():\n    return [\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 256,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 256,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 32,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 4,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n    ]\n\n@triton.autotune(\n    configs=get_autotune_config(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    tl.static_assert(\n        K % (4 * BLOCK_SIZE_K) == 0,\n        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",\n    )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0).to(tl.int8)  # \u8f6c\u6362\u4e3a int8 \u7c7b\u578b\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << (2 * i)\n            b = ((b_uint8 & mask) >> (2 * i)).to(tl.int8)  # \u8f6c\u6362\u4e3a int8 \u7c7b\u578b\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)  # \u4f7f\u7528 int8 \u7c7b\u578b\n            accumulator += tl.dot(a, (b - tensor_full), out_dtype=tl.int32)  # \u4fdd\u6301 a \u548c b \u4e3a int8\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef matmul(a, b):\n    assert (\n        a.shape[1] == b.shape[0] * 4\n    ), \"Incompatible dimensions, the weight matrix need to be packed\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    _, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.int32)\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n    )\n    matmul_kernel[grid](\n        a,\n        b,\n        c,\n        M,\n        N,\n        K,\n        a.stride(0),\n        a.stride(1),\n        b.stride(0),\n        b.stride(1),\n        c.stride(0),\n        c.stride(1),\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel `_fwd_kernel_aligned` that performs a forward pass for an attention mechanism with relative positional embeddings, and a wrapper function `_attention_rel_h_rel_w_kernel_aligned_device` to execute this kernel on a device. The kernel processes the input tensors Q, K, V (queries, keys, values) and an additional tensor B0 that contains relative positional information, performing scaled dot-product attention. The result is stored in the output tensor Out. The computation is parallelized across grid dimensions, where `start_m` and `off_hz` define the computation space for each block.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(\n    Q, K, V, B0, Out,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vm,\n    stride_ob, stride_oh, stride_om,\n    stride_b0b, stride_b0h, stride_b0m,\n    nheads, size_m, size_n, size_k,\n    sm_scale, block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(size_m, block_m)\n    num_pid_n = tl.cdiv(size_n, block_n)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % num_pid_in_group) // num_pid_m\n\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_h = tl.arange(0, nheads)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_h[None, :] * stride_qh)\n    k_ptrs = K + (offs_n[None, :] * stride_kn + offs_h[:, None] * stride_kh)\n    v_ptrs = V + (offs_n[None, :] * stride_vm + offs_h[:, None] * stride_vh)\n    b0_ptrs = B0 + (offs_m[:, None] * stride_b0m + offs_h[None, :] * stride_b0h)\n\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for start_n in range(0, size_n, block_n):\n        k_block_ptr = k_ptrs + start_n * stride_kn\n        v_block_ptr = v_ptrs + start_n * stride_vm\n        b0_block_ptr = b0_ptrs + start_n * stride_b0m\n\n        q = tl.load(q_ptrs)\n        k = tl.load(k_block_ptr)\n        v = tl.load(v_block_ptr)\n        b0 = tl.load(b0_block_ptr)\n\n        qk = tl.dot(q, k, trans_b=True)\n        qk += b0\n        qk *= sm_scale\n        qk = tl.softmax(qk, axis=1)\n\n        o = tl.dot(qk, v)\n        acc += o\n\n    out_ptrs = Out + (offs_m[:, None] * stride_om + offs_h[None, :] * stride_oh)\n    tl.store(out_ptrs, acc)\n\nimport torch\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(Q, K, V, B0, Out, sm_scale):\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_K = 32\n\n    # Get the dimensions of the input tensors\n    batch_size, nheads, size_m, size_k = Q.shape\n    size_n = K.shape[2]\n\n    # Define the grid and block dimensions\n    grid = (triton.cdiv(size_m, BLOCK_M) * triton.cdiv(size_n, BLOCK_N) * nheads,)\n\n    # Define the strides for the input and output tensors\n    stride_qb = Q.stride(0)\n    stride_qh = Q.stride(1)\n    stride_qm = Q.stride(2)\n    stride_kb = K.stride(0)\n    stride_kh = K.stride(1)\n    stride_kn = K.stride(2)\n    stride_vb = V.stride(0)\n    stride_vh = V.stride(1)\n    stride_vm = V.stride(2)\n    stride_ob = Out.stride(0)\n    stride_oh = Out.stride(1)\n    stride_om = Out.stride(2)\n    stride_b0b = B0.stride(0)\n    stride_b0h = B0.stride(1)\n    stride_b0m = B0.stride(2)\n\n    # Launch the kernel\n    _fwd_kernel_aligned[grid](\n        Q, K, V, B0, Out,\n        stride_qb, stride_qh, stride_qm,\n        stride_kb, stride_kh, stride_kn,\n        stride_vb, stride_vh, stride_vm,\n        stride_ob, stride_oh, stride_om,\n        stride_b0b, stride_b0h, stride_b0m,\n        nheads, size_m, size_n, size_k,\n        sm_scale, BLOCK_M, BLOCK_N, BLOCK_K\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(\n    Q, K, V, B0, sm_scale,\n    Out,\n    stride_qh, stride_qm, stride_qk,\n    stride_kh, stride_kn, stride_kk,\n    stride_vh, stride_vk, stride_vn,\n    stride_oh, stride_om, stride_on,\n    stride_b0h, stride_b0m,\n    Z,\n    H,\n    N_CTX,\n    P_SEQ,\n    OUT_DTYPE: tl.constexpr,\n    BIAS_LAST_SIZE: tl.constexpr,\n    B0_NUMEL: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + kv_offset,\n        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + kv_offset,\n        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(OUT_DTYPE)\n    lo = 0\n    hi = N_CTX + P_SEQ\n\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = (tl.arange(0, BLOCK_N) %\n                         BIAS_LAST_SIZE) + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                 * stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k, out_dtype=OUT_DTYPE)\n\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                     * stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += (b0 + b1)\n\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p.to(OUT_DTYPE), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    acc = acc / l_i[:, None]\n\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    tl.store(O_block_ptr, acc.to(OUT_DTYPE))\n\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(q, k, v, rel_h_w, sm_scale, o,\n                                                 BLOCK_M,\n                                                 BLOCK_N,\n                                                 num_warps,\n                                                 num_stages):\n    _, Lk, _ = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert q.size() == k.size()\n    assert q.size() == v.size()\n    assert q.size(-2) == rel_h_w.size(-2)\n    assert (q.dtype == torch.bfloat16 or q.dtype == torch.float16)\n    assert k.dtype == q.dtype\n    assert v.dtype == k.dtype\n    assert o.dtype == v.dtype\n    assert rel_h_w.dtype == q.dtype\n    assert rel_h_w.size(-1) == 128\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n    assert P_SEQ == 0\n    assert rel_h_w.is_contiguous(), str(rel_h_w.stride())\n    _fwd_kernel_aligned[grid](\n        q, k, v,\n        rel_h_w,\n        sm_scale,\n        o,\n        q.stride(1), q.stride(2), q.stride(3),\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        o.stride(1), o.stride(2), o.stride(3),\n        rel_h_w.stride(1), rel_h_w.stride(2),\n        q.shape[0],\n        q.shape[1],\n        q.shape[2],\n        P_SEQ,\n        OUT_DTYPE=tl.float16 if q.dtype == torch.float16 else tl.bfloat16,\n        BIAS_LAST_SIZE=(rel_h_w.size(-1) // 2),\n        B0_NUMEL=rel_h_w.size(-1),\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        BLOCK_DMODEL=Lk,\n        num_warps=num_warps,\n        num_stages=num_stages)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel 'ff_llama' is designed to perform a fused operation involving matrix multiplications and element-wise operations. It multiplies input 'x' with two weight matrices 'w1' and 'w3', applies an element-wise SILU activation on the result of 'w1', and scales the inputs using a root mean square (RMS) weight. The operation also normalizes the input using an L2-norm.\n\nThe kernel function is invoked by 'kernel_ff', which takes in four tensors: 'x', 'w1', 'w3', and 'rms_w'. It expects these tensors to be of type float16 or int8 for weights and performs the computations accordingly. The result is stored in an output tensor which is reshaped to match the batch and sequence dimensions of 'x'.\n\nKey parameters like 'BLOCK_SIZE_M', 'BLOCK_SIZE_N', and 'BLOCK_SIZE_K' determine the tiling size for the computations, optimizing memory access and parallel execution.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef ff_llama(\n    x_ptr, w1_ptr, w3_ptr, rms_w_ptr, output_ptr,\n    M, N, K,\n    stride_xm, stride_xn,\n    stride_w1k, stride_w1n,\n    stride_w3k, stride_w3n,\n    stride_rms_n,\n    stride_outm, stride_outn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) // num_pid_m\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xn)\n    w1_ptrs = w1_ptr + (offs_k[:, None] * stride_w1k + offs_n[None, :] * stride_w1n)\n    w3_ptrs = w3_ptr + (offs_k[:, None] * stride_w3k + offs_n[None, :] * stride_w3n)\n\n    accumulator1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    accumulator3 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        x = tl.load(x_ptrs)\n        w1 = tl.load(w1_ptrs)\n        w3 = tl.load(w3_ptrs)\n\n        accumulator1 += tl.dot(x, w1)\n        accumulator3 += tl.dot(x, w3)\n\n        x_ptrs += BLOCK_SIZE_K * stride_xn\n        w1_ptrs += BLOCK_SIZE_K * stride_w1k\n        w3_ptrs += BLOCK_SIZE_K * stride_w3k\n\n    # Apply SILU activation to accumulator1\n    accumulator1 = tl.sigmoid(accumulator1) * accumulator1\n\n    # Normalize using L2 norm\n    rms_w = tl.load(rms_w_ptr + offs_n)\n    l2_norm = tl.sqrt(tl.sum(accumulator1 * accumulator1, axis=1) + 1e-6)\n    l2_norm = l2_norm[:, None]\n    accumulator1 /= l2_norm\n\n    # Scale by RMS weight\n    accumulator1 *= rms_w\n\n    # Combine results\n    output = accumulator1 * accumulator3\n\n    # Store the result\n    out_ptrs = output_ptr + (offs_m[:, None] * stride_outm + offs_n[None, :] * stride_outn)\n    tl.store(out_ptrs, output)\n\nimport torch\n\ndef kernel_ff(x, w1, w3, rms_w):\n    assert x.dtype in [torch.float16, torch.int8]\n    assert w1.dtype in [torch.float16, torch.int8]\n    assert w3.dtype in [torch.float16, torch.int8]\n    assert rms_w.dtype == torch.float16\n\n    M, K = x.shape\n    N = w1.shape[1]\n\n    output = torch.empty((M, N), dtype=torch.float16, device=x.device)\n\n    BLOCK_SIZE_M = 16\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_K = 32\n\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\n    ff_llama[grid](\n        x, w1, w3, rms_w, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        w1.stride(0), w1.stride(1),\n        w3.stride(0), w3.stride(1),\n        rms_w.stride(0),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n    )\n\n    return output\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef ff_llama(\n    a_ptr, w1_ptr, w3_ptr, out_ptr, rms_w_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_w1k, stride_w1n,\n    stride_w3k, stride_w3n,\n    stride_outm, stride_outn,\n    stride_rms_w,\n    USE_FP8: tl.constexpr,\n    EPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    w1 and w3 are weights (linear layers)\n    F.silu(w1(x)) * w3(x)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    w1_ptrs = w1_ptr + (offs_k[:, None] * stride_w1k + offs_bn[None, :] * stride_w1n)\n    w3_ptrs = w3_ptr + (offs_k[:, None] * stride_w3k + offs_bn[None, :] * stride_w3n)\n    acc1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    acc2 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n    a_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs)\n        a_sum += tl.extra.cuda.libdevice.pow(a.to(tl.float32), 2)\n        rms_w = tl.load(rms_w_ptrs)\n        if USE_FP8:\n            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n            rms_w = rms_w.to(tl.float16)\n        a = a * rms_w\n        b = tl.load(w1_ptrs)\n        if USE_FP8:\n            b = b.to(tl.float8e5, bitcast=True)\n            b = b.to(tl.float32)\n            b = b.to(tl.float16)\n        acc1 += tl.dot(a, b)\n        c = tl.load(w3_ptrs)\n        if USE_FP8:\n            c = c.to(tl.float8e5, bitcast=True)\n            c = c.to(tl.float32)\n            c = c.to(tl.float16)\n        acc2 += tl.dot(a, c)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        w1_ptrs += BLOCK_SIZE_K * stride_w1k\n        w3_ptrs += BLOCK_SIZE_K * stride_w3k\n\n        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n\n    a_mean = tl.sum(a_sum, axis=1) / K + EPS\n    a_norm = tl.math.rsqrt(a_mean)\n    acc1 = acc1 * a_norm[:, None]\n    acc2 = acc2 * a_norm[:, None]\n    accumulator = (acc1 * tl.sigmoid(acc1)) * acc2\n\n    offs_outm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_outn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (stride_outm * offs_outm[:, None] + stride_outn * offs_outn[None, :])\n    out_mask = (offs_outm[:, None] < M) & (offs_outn[None, :] < N)\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\n\ndef kernel_ff(x: torch.Tensor, w1: torch.Tensor, w3: torch.Tensor, rms_w: torch.Tensor) -> torch.Tensor:\n    assert x.dtype == torch.float16\n    assert w1.dtype == w3.dtype == rms_w.dtype\n    assert w1.dtype in [torch.int8, torch.float16]\n    assert w1.shape == w3.shape\n\n    w1_t = w1.t()\n    w3_t = w3.t()\n\n    batch, seq_len, dim = x.shape\n    M, K = batch * seq_len, dim\n\n    N = w1_t.shape[1]\n    assert K == w1_t.shape[0]\n    assert w1_t.shape == w3_t.shape\n    x_reshape = x.reshape(M, K)\n    out = torch.empty((M, N), dtype=x.dtype, device=x.device)\n    grid = lambda META: (triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]),)\n    ff_llama[grid](\n        x_reshape, w1_t, w3_t, out, rms_w,\n        M, N, K,\n        *x_reshape.stride(),\n        *w1_t.stride(),\n        *w3_t.stride(),\n        *out.stride(),\n        *rms_w.stride(),\n        USE_FP8=w1_t.dtype != torch.float16,\n        EPS=1e-6,\n        BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=64,\n        num_stages=2, num_warps=4\n    )\n    out = out.view(batch, seq_len, -1)\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton kernel `_fwd_kernel_destindex_copy_quantize_kv` performs quantization and copies key-value pairs from a source to a destination tensor. It uses index remapping specified in `Dest_loc` to determine where the quantized data should be stored. The kernel operates over a grid defined by sequences and heads, divides data into groups, computes scaling factors for quantization, and stores both quantized data and scaling factors. The function `destindex_copy_quantize_kv` is a wrapper that sets up inputs and configuration for the kernel execution.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    src_ptr,  # Pointer to the source tensor\n    dest_ptr,  # Pointer to the destination tensor\n    dest_loc_ptr,  # Pointer to the destination location tensor\n    scale_ptr,  # Pointer to the scaling factors tensor\n    batch_size,  # Batch size\n    seq_len,  # Sequence length\n    head_dim,  # Head dimension\n    num_heads,  # Number of heads\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelization\n):\n    # Compute the grid and block indices\n    pid = tl.program_id(axis=0)\n    num_groups = (batch_size * seq_len * num_heads) // BLOCK_SIZE\n    group_id = pid % num_groups\n    batch_id = group_id // (seq_len * num_heads)\n    seq_id = (group_id % (seq_len * num_heads)) // num_heads\n    head_id = group_id % num_heads\n\n    # Compute the base indices for the source and destination tensors\n    src_base = (batch_id * seq_len * num_heads + seq_id * num_heads + head_id) * head_dim\n    dest_base = (batch_id * seq_len * num_heads + seq_id * num_heads + head_id) * head_dim\n\n    # Compute the destination location for this group\n    dest_loc = tl.load(dest_loc_ptr + group_id)\n\n    # Compute the scaling factor for this group\n    src_data = tl.load(src_ptr + src_base, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n    max_val = tl.max(src_data, axis=0)\n    min_val = tl.min(src_data, axis=0)\n    scale = (max_val - min_val) / 127.0\n\n    # Quantize the data\n    quantized_data = tl.round((src_data - min_val) / scale)\n\n    # Store the quantized data and scaling factor\n    tl.store(dest_ptr + dest_loc * head_dim, quantized_data, mask=tl.arange(0, BLOCK_SIZE) < head_dim)\n    tl.store(scale_ptr + group_id, scale)\n\nimport torch\n\ndef destindex_copy_quantize_kv(src, dest, dest_loc, scale, batch_size, seq_len, head_dim, num_heads):\n    # Ensure the tensors are on the same device\n    assert src.device == dest.device == dest_loc.device == scale.device, \"All tensors must be on the same device\"\n\n    # Get the device and data type\n    device = src.device\n    dtype = src.dtype\n\n    # Define the grid and block sizes\n    BLOCK_SIZE = 128\n    grid_size = (batch_size * seq_len * num_heads) // BLOCK_SIZE\n\n    # Launch the kernel\n    _fwd_kernel_destindex_copy_quantize_kv[grid_size, BLOCK_SIZE](\n        src_ptr=src,\n        dest_ptr=dest,\n        dest_loc_ptr=dest_loc,\n        scale_ptr=scale,\n        batch_size=batch_size,\n        seq_len=seq_len,\n        head_dim=head_dim,\n        num_heads=num_heads,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    # Synchronize the device\n    torch.cuda.synchronize(device)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K,\n    Dest_loc,\n    Out,\n    Out_scale,\n    stride_k_bs,\n    stride_k_h,\n    stride_k_g,\n    stride_k_d,\n    stride_o_bs,\n    stride_o_h,\n    stride_o_g,\n    stride_o_d,\n    stride_os_bs,\n    stride_os_h,\n    stride_os_g,\n    group_size,\n    BLOCK_GROUP_NUM: tl.constexpr,\n    BLOCK_GROUP_DIM: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    offs_g = tl.arange(0, BLOCK_GROUP_NUM)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    src_data = tl.load(\n        K + cur_index * stride_k_bs + cur_head * stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :],\n        mask=offs_g[:, None] < group_size,\n        other=0.0,\n    )\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.element_ty)\n    q_src_data = (src_data / data_scale[:, None]).to(tl.int8)\n\n    o_ptrs = Out + dest_index * stride_o_bs + cur_head * stride_o_h + offs_g[:, None] * stride_o_g + offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + cur_head * stride_os_h + offs_g\n    tl.store(o_ptrs, q_src_data, mask=offs_g[:, None] < group_size)\n    tl.store(os_ptrs, data_scale, mask=offs_g < group_size)\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    quant_group_dim = 8\n\n    assert head_dim % quant_group_dim == 0, \"error head dim, can not been supported to copy quant kv\"\n    grid = (seq_len, head_num)\n    num_warps = 1\n\n    group_size = head_dim // quant_group_dim\n    group_dim = quant_group_dim\n\n    K = K.view((K.shape[0], K.shape[1], group_size, group_dim))\n    Out = Out.view(Out.shape[0], Out.shape[1], group_size, group_dim)\n\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K,\n        DestLoc,\n        Out,\n        Out_scale,\n        K.stride(0),\n        K.stride(1),\n        K.stride(2),\n        K.stride(3),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        Out.stride(3),\n        Out_scale.stride(0),\n        Out_scale.stride(1),\n        Out_scale.stride(2),\n        group_size,\n        BLOCK_GROUP_NUM=triton.next_power_of_2(group_size),\n        BLOCK_GROUP_DIM=group_dim,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton code provided contains a kernel function for computing RoPE (Rotary Position Embeddings) and two main Python functions to facilitate this. The kernel `_rope_embedding` calculates the RoPE embedding by utilizing trigonometric properties to transform the input tensor. The functions `_rope_embedding_forward_impl` and `_rope_embedding_backward_impl` handle the forward and backward passes of the embedding respectively, using the kernel.\n\n- `_rope_embedding` is the Triton JIT-compiled kernel that performs the RoPE transformation on the input matrix `Q` using precomputed cosine and sine matrices. It applies the mathematical operation `Q * cos + rotate_half(Q) * sin` where `rotate_half` splits and rotates the matrix.\n\n- `_rope_embedding_forward_impl` is a Python function that prepares inputs for the forward operation by reshaping and transposing the input `Q`. It calculates optimal settings using `calculate_settings` and invokes the Triton kernel with these settings.\n\n- `_rope_embedding_backward_impl` prepares the input for the backward pass, similarly reshaping and transposing, and calls the Triton kernel to compute gradients based on the modified RoPE operation.\n\nCommon variables:\n- `Q` represents the input tensor.\n- `cos` and `sin` are tensors storing cosine and sine values.\n- `head_dim` and `n_heads` define the dimensions and number of heads for the input tensor.\n- `BLOCK_SIZE` and `num_warps` optimize computation on the GPU.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _rope_embedding(\n    Q,  # Pointer to the input tensor\n    cos,  # Pointer to the cosine tensor\n    sin,  # Pointer to the sine tensor\n    Q_out,  # Pointer to the output tensor\n    stride_qb, stride_qh, stride_qd,  # Strides for Q\n    stride_cosb, stride_cosd,  # Strides for cos\n    stride_sinb, stride_sind,  # Strides for sin\n    stride_q_outb, stride_q_outh, stride_q_outd,  # Strides for Q_out\n    n_elements,  # Number of elements in the batch\n    n_heads,  # Number of heads\n    head_dim,  # Dimension of each head\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelization\n):\n    pid = tl.program_id(axis=0)\n    batch_start = pid * BLOCK_SIZE\n    batch_end = min(batch_start + BLOCK_SIZE, n_elements)\n\n    for b in range(batch_start, batch_end):\n        for h in range(n_heads):\n            for d in range(head_dim):\n                q_val = tl.load(Q + b * stride_qb + h * stride_qh + d * stride_qd)\n                cos_val = tl.load(cos + b * stride_cosb + d * stride_cosd)\n                sin_val = tl.load(sin + b * stride_sinb + d * stride_sind)\n\n                # Split and rotate half\n                q1 = q_val[:head_dim // 2]\n                q2 = q_val[head_dim // 2:]\n                q2_rotated = tl.concat([q2[1:], q2[:1]])\n\n                # Apply RoPE transformation\n                q_rotated = q1 * cos_val + q2_rotated * sin_val\n\n                # Store the result\n                tl.store(Q_out + b * stride_q_outb + h * stride_q_outh + d * stride_q_outd, q_rotated)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef calculate_settings(Q, cos, sin, BLOCK_SIZE):\n    n_elements, n_heads, head_dim = Q.shape\n    grid = (n_elements // BLOCK_SIZE + (n_elements % BLOCK_SIZE > 0),)\n    return grid, n_heads, head_dim, BLOCK_SIZE\n\ndef _rope_embedding_forward_impl(Q, cos, sin, BLOCK_SIZE, num_warps=4):\n    n_elements, n_heads, head_dim = Q.shape\n    Q_out = torch.empty_like(Q)\n\n    grid, n_heads, head_dim, BLOCK_SIZE = calculate_settings(Q, cos, sin, BLOCK_SIZE)\n\n    _rope_embedding[grid](\n        Q, cos, sin, Q_out,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        cos.stride(0), cos.stride(1),\n        sin.stride(0), sin.stride(1),\n        Q_out.stride(0), Q_out.stride(1), Q_out.stride(2),\n        n_elements, n_heads, head_dim, BLOCK_SIZE,\n        num_warps=num_warps\n    )\n\n    return Q_out\n\ndef _rope_embedding_backward_impl(dQ, cos, sin, BLOCK_SIZE, num_warps=4):\n    n_elements, n_heads, head_dim = dQ.shape\n    dQ_out = torch.empty_like(dQ)\n\n    grid, n_heads, head_dim, BLOCK_SIZE = calculate_settings(dQ, cos, sin, BLOCK_SIZE)\n\n    _rope_embedding[grid](\n        dQ, cos, sin, dQ_out,\n        dQ.stride(0), dQ.stride(1), dQ.stride(2),\n        cos.stride(0), cos.stride(1),\n        sin.stride(0), sin.stride(1),\n        dQ_out.stride(0), dQ_out.stride(1), dQ_out.stride(2),\n        n_elements, n_heads, head_dim, BLOCK_SIZE,\n        num_warps=num_warps\n    )\n\n    return dQ_out\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\nMAX_FUSED_SIZE = 65536  # 2**16\nnext_power_of_2 = triton.next_power_of_2\n\ndef calculate_settings(n):\n    BLOCK_SIZE = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n\nROPE_GROUP_SIZE = 4\n\n@triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\n@triton.jit\ndef _rope_embedding(\n    Q,     Q_row_stride,\n    cos, cos_row_stride,\n    sin, sin_row_stride,\n    seqlen,\n    head_dim        : tl.constexpr,\n    n_heads         : tl.constexpr,\n    BACKWARD_PASS   : tl.constexpr,\n    BLOCK_SIZE      : tl.constexpr,\n    ROPE_GROUP_SIZE : tl.constexpr = 4,\n):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    row_position  = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets  = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n\n    sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n    cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n\n    if BACKWARD_PASS:\n        # See our blog post for more info.\n        sin1 = -sin1\n    \n    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\n\n    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n\n        # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\n        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\n\n        tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\n        tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\n    \n\n\ndef _rope_embedding_forward_impl(Q, cos, sin):\n    Q = Q.transpose(1, 2).clone()\n    cos, sin = cos.squeeze(), sin.squeeze()\n    batch, seq_len, n_heads, head_dim = Q.shape\n    Q = Q.reshape(batch*seq_len, n_heads*head_dim)\n    n_rows, n_cols = Q.shape\n    assert(seq_len <= cos.shape[0])\n\n    # [TODO] Changing blocksize to head_dim//2 seems to have\n    # some concurrency / un-deterministic issues.\n    BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\n\n    # group_size = 4 # 4 or 8, too large group_size can hurt performance.\n    div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\n    n_groups = div + (mod != 0)\n\n    _rope_embedding[(n_rows, n_groups, )](\n          Q,   Q.stride(0),\n        cos, cos.stride(0),\n        sin, sin.stride(0),\n        seq_len,\n        head_dim, n_heads,\n        BACKWARD_PASS = False,\n        BLOCK_SIZE = BLOCK_SIZE,\n        num_warps  = num_warps,\n    )\n    Q = Q.view(batch, seq_len, n_heads, head_dim)\n    Q = Q.transpose(1, 2)\n    return Q, cos, sin, n_groups, BLOCK_SIZE, num_warps\n\n\ndef _rope_embedding_backward_impl(dY, cos, sin, n_groups, BLOCK_SIZE, num_warps):\n    dY = dY.transpose(1, 2)\n    batch, seq_len, n_heads, head_dim = dY.shape\n    dY = dY.reshape(batch*seq_len, n_heads*head_dim)\n    # Must be reshape not view\n    n_rows, n_cols = dY.shape\n\n    _rope_embedding[(n_rows, n_groups, )](\n        dY,  dY .stride(0),\n        cos, cos.stride(0),\n        sin, sin.stride(0),\n        seq_len, head_dim, n_heads,\n        BACKWARD_PASS = True,\n        BLOCK_SIZE = BLOCK_SIZE,\n        num_warps  = num_warps,\n    )\n    dY = dY.view(batch, seq_len, n_heads, head_dim)\n    dY = dY.transpose(1, 2)\n    return dY\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton operator code is for a matrix-vector multiplication kernel, named `mv_kernel`, optimized for GPU execution. It takes a matrix `A`, a vector `B`, and computes their product storing the result in vector `C`. The key parameters are `N` and `M`, representing the dimensions of matrix `A` (N x M). The kernel utilizes configurable block sizes (`BLOCK_N`, `BLOCK_M`) to process sub-portions of the matrix and vector, enabling efficient parallel computation. It uses Triton's `@autotune` decorator to optimize configurations based on the input sizes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 128, 'BLOCK_M': 32}, num_warps=4),\n        triton.Config({'BLOCK_N': 64, 'BLOCK_M': 64}, num_warps=4),\n        triton.Config({'BLOCK_N': 32, 'BLOCK_M': 128}, num_warps=4),\n    ],\n    key=['N', 'M']\n)\n@triton.jit\ndef mv_kernel(\n    A_ptr,  # Pointer to the matrix A\n    B_ptr,  # Pointer to the vector B\n    C_ptr,  # Pointer to the output vector C\n    N,      # Number of rows in A\n    M,      # Number of columns in A (and length of B)\n    stride_A_N,  # Stride of A along the N dimension\n    stride_A_M,  # Stride of A along the M dimension\n    stride_B,    # Stride of B (which is 1)\n    stride_C,    # Stride of C (which is 1)\n    BLOCK_N: tl.constexpr,  # Block size for N\n    BLOCK_M: tl.constexpr   # Block size for M\n):\n    # Compute the row index for this block\n    pid = tl.program_id(axis=0)\n    row_start = pid * BLOCK_N\n\n    # Compute the column index for this block\n    col_start = tl.program_id(axis=1) * BLOCK_M\n\n    # Initialize the output block\n    C_block = tl.zeros((BLOCK_N,), dtype=tl.float32)\n\n    # Iterate over the columns of A and elements of B\n    for col in range(col_start, col_start + BLOCK_M):\n        if col < M:\n            # Load the column of A and the element of B\n            A_block = tl.load(A_ptr + row_start * stride_A_N + col * stride_A_M, mask=row_start + tl.arange(0, BLOCK_N) < N, other=0.0)\n            B_val = tl.load(B_ptr + col * stride_B)\n            # Perform the dot product\n            C_block += A_block * B_val\n\n    # Store the result in C\n    tl.store(C_ptr + row_start * stride_C, C_block, mask=row_start + tl.arange(0, BLOCK_N) < N)\n\nimport torch\nimport triton\n\n# Define the matrix-vector multiplication function\ndef matrix_vector_multiply(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor):\n    assert A.dim() == 2, \"A must be a 2D tensor\"\n    assert B.dim() == 1, \"B must be a 1D tensor\"\n    assert C.dim() == 1, \"C must be a 1D tensor\"\n    assert A.shape[1] == B.shape[0], \"A and B dimensions must be compatible\"\n    assert A.shape[0] == C.shape[0], \"A and C dimensions must be compatible\"\n\n    N, M = A.shape\n\n    # Launch the kernel\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_N']), triton.cdiv(M, meta['BLOCK_M']))\n    mv_kernel[grid](\n        A, B, C,\n        N, M,\n        A.stride(0), A.stride(1),\n        B.stride(0),\n        C.stride(0)\n    )\n\n# Example usage\nN, M = 1024, 512\nA = torch.randn((N, M), device='cuda')\nB = torch.randn((M,), device='cuda')\nC = torch.zeros((N,), device='cuda')\n\nmatrix_vector_multiply(A, B, C)\n\n# Verify the result\nC_ref = torch.matmul(A, B)\nprint(\"Max error:\", torch.max(torch.abs(C - C_ref)))\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": m, \"BLOCK_N\": n}, num_stages=s, num_warps=w)\n        for m in [32, 64, 128]\n        for n in [1, 2, 4, 8]\n        for s in [3, 4]\n        for w in [4, 8]\n    ],\n    key=[\"M\", \"N\"],\n)\n@triton.jit\ndef mv_kernel(\n    A,\n    B,\n    C,\n    N,\n    M,\n    stride_an,\n    stride_am,\n    stride_bm,\n    stride_cn,\n    BLOCK_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offset_n = pid * BLOCK_N + tl.arange(0, BLOCK_N)[:, None]\n    offset_m = tl.arange(0, BLOCK_M)[None, :]\n    n_mask = offset_n < N\n    A_ptrs = A + offset_n * stride_an + offset_m * stride_am\n    B_ptrs = B + offset_m * stride_bm\n    acc = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    for m in range(0, M, BLOCK_M):\n        m_mask = m + offset_m < M\n        a = tl.load(A_ptrs, mask=n_mask & m_mask, other=0.0).to(tl.float32)\n        b = tl.load(B_ptrs, mask=m_mask, other=0.0).to(tl.float32)\n        acc += a * b\n        A_ptrs += BLOCK_M * stride_am\n        B_ptrs += BLOCK_M * stride_bm\n\n    acc = tl.sum(acc, axis=1)\n    C_ptrs = C + offset_n * stride_cn\n    tl.store(C_ptrs, acc[:, None], mask=n_mask)\n\n\ndef mv(inp, vec):\n    logging.debug(\"GEMS MV\")\n    assert inp.shape[1] == vec.shape[0], \"incompatible dimensions\"\n    N, M = inp.shape\n    out = torch.empty((N,), device=inp.device, dtype=inp.dtype)\n    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_N\"]),)\n    with torch.cuda.device(inp.device):\n        mv_kernel[grid](\n            inp,\n            vec,\n            out,\n            N,\n            M,\n            inp.stride(0),\n            inp.stride(1),\n            vec.stride(0),\n            out.stride(0),\n        )\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The given Triton kernel, `rms_matmul_rbe`, performs a matrix multiplication between an input tensor `x` and a weight matrix `weight`, with an RMS normalization and optional Rotary Embeddings (RBE) applied. It is intended for transformer models, handling operations in an efficient, parallel manner. The wrapper function `rms_matmul_rbe_wrapper` orchestrates this kernel, ensuring type compatibility and shaping the output tensor appropriately. It facilitates matrix multiplication with support for FP16 and INT8 data types, adjusting computation based on parameters like number of heads and head dimension.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_matmul_rbe(\n    x_ptr,  # pointer to the input tensor\n    weight_ptr,  # pointer to the weight matrix\n    rbe_ptr,  # pointer to the rotary embeddings (optional)\n    output_ptr,  # pointer to the output tensor\n    M,  # number of rows in the input tensor\n    N,  # number of columns in the input tensor\n    K,  # number of columns in the weight matrix\n    H,  # number of heads\n    head_dim,  # dimension of each head\n    rbe_stride,  # stride for rotary embeddings\n    use_rbe: tl.constexpr,  # flag to indicate if RBE is used\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (offs_am[:, None] * N + offs_k[None, :]) * tl.float16\n    w_ptrs = weight_ptr + (offs_k[:, None] * K + offs_bn[None, :]) * tl.float16\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        x_block = tl.load(x_ptrs)\n        w_block = tl.load(w_ptrs)\n        if use_rbe:\n            rbe_ptrs = rbe_ptr + (offs_am[:, None] * rbe_stride + k + offs_k[None, :]) * tl.float16\n            rbe_block = tl.load(rbe_ptrs)\n            x_block = x_block * rbe_block\n\n        accumulator += tl.dot(x_block, w_block)\n        x_ptrs += BLOCK_SIZE_K * N\n        w_ptrs += BLOCK_SIZE_K * K\n\n    # RMS normalization\n    rms = tl.sqrt(tl.sum(accumulator * accumulator, axis=1) / N)\n    accumulator = accumulator / rms[:, None]\n\n    output_ptrs = output_ptr + (offs_am[:, None] * N + offs_bn[None, :]) * tl.float16\n    tl.store(output_ptrs, accumulator.to(tl.float16))\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef rms_matmul_rbe_wrapper(x, weight, rbe=None, use_rbe=False):\n    M, N = x.shape\n    K = weight.shape[1]\n    H = 1  # Assuming a single head for simplicity, can be adjusted\n    head_dim = N  # Assuming head_dim is the same as N for simplicity, can be adjusted\n\n    # Ensure the input tensors are in the correct format\n    x = x.to(torch.float16)\n    weight = weight.to(torch.float16)\n    if use_rbe:\n        rbe = rbe.to(torch.float16)\n\n    # Allocate output tensor\n    output = torch.empty((M, K), device=x.device, dtype=torch.float16)\n\n    # Define grid and block sizes\n    BLOCK_SIZE_M = 16\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_K = 16\n    GROUP_SIZE_M = 8\n\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(K, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    rms_matmul_rbe[grid](\n        x, weight, rbe, output,\n        M, N, K, H, head_dim,\n        rbe.stride(0) if use_rbe else 0,\n        use_rbe,\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M\n    )\n\n    return output\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rbe_triton(x_ptr, out_ptr,\n               M, K,\n               stride_x_batch, stride_x_m, stride_x_n,\n               stride_out_batch, stride_out_m, stride_out_n,\n               start_token_position,\n               THETA: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_n = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K // 2) * 2  # take only even numbers\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + stride_x_m * offs_m[:, None] + stride_x_n * offs_n[None, :])\n    x_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    real = tl.load(x_ptrs, mask=x_real_mask, other=0.0)\n    x_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    imag = tl.load(x_ptrs + 1, mask=x_imag_mask, other=0.0)\n    tl.debug_barrier()\n    start_block = start_token_position + pid_m * BLOCK_SIZE_M\n    cos, sin = get_freq_multi_tokens(offs_cn=offs_n, starting_idx=start_block, theta=THETA, NB_TOKENS=BLOCK_SIZE_M)\n\n    out_real = real * cos - imag * sin\n    out_imag = real * sin + imag * cos\n    tl.debug_barrier()\n    out_ptrs = out_ptr + (\n            pid_batch * stride_out_batch + stride_out_m * offs_m[:, None] + stride_out_n * offs_n[None, :])\n    out_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    tl.store(out_ptrs, out_real, mask=out_real_mask)\n    out_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    tl.store(out_ptrs + 1, out_imag, mask=out_imag_mask)\n\n\n@triton.jit\ndef rms_matmul_rbe(\n        x_ptr, w_ptr, rms_w_ptr, out_ptr,\n        M, N, K,\n        stride_x_batch, stride_x_m, stride_x_k,\n        stride_w_k, stride_w_n,\n        stride_rms_w,\n        stride_out_batch, stride_out_m, stride_out_n,\n        start_token_position,\n        USE_FP8: tl.constexpr,\n        RBE_EPILOGUE: tl.constexpr,\n        THETA: tl.constexpr,\n        EPS: tl.constexpr,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    Prologue: RMS\n    Epilogue: nothing or Rotary embeddings\n    c = ROBE((rms(a) * rms_w) @ b)\n    \"\"\"\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + offs_m[:, None] * stride_x_m + offs_k[None, :] * stride_x_k)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_w_k + offs_n[None, :] * stride_w_n)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n    x_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        x = tl.load(x_ptrs)\n        x_sum += tl.extra.cuda.libdevice.pow(x.to(tl.float32), 2)\n        rms_w = tl.load(rms_w_ptrs)  # TODO add an assert that rms_w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n            rms_w = rms_w.to(tl.float16)\n        x = x * rms_w\n        w = tl.load(w_ptrs)  # TODO add an assert that w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            w = w.to(tl.float8e5, bitcast=True)\n            w = w.to(tl.float32)\n            w = w.to(tl.float16)\n        accumulator += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x_k\n        w_ptrs += BLOCK_SIZE_K * stride_w_k\n        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n    x_mean = tl.sum(x_sum, axis=1) / K + EPS\n    x_norm = tl.math.rsqrt(x_mean)\n    accumulator = accumulator * x_norm[:, None]\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (\n                pid_batch * stride_out_batch + offs_m[:, None] * stride_out_m + offs_n[None, :] * stride_out_n)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\n\ndef rms_matmul_rbe_wrapper(x: torch.Tensor, weight: torch.Tensor, rms_w: torch.Tensor, use_rbe: bool, start_pos: int,\n                           n_heads: int, head_dim: int):\n    # \u786e\u4fdd weight \u548c rms_w \u7684\u6570\u636e\u7c7b\u578b\u4e00\u81f4\n    assert weight.dtype in [torch.float16, torch.int8], \"Only torch.float16 or torch.int8 are supported for weight\"\n    \n    # \u786e\u4fdd rms_w \u548c weight \u7684 dtype \u4e00\u81f4\n    if rms_w.dtype != weight.dtype:\n        # print(f\"rms_w dtype: {rms_w.dtype}, weight dtype: {weight.dtype}\")\n        rms_w = rms_w.to(weight.dtype)  # \u5982\u679c\u7c7b\u578b\u4e0d\u4e00\u81f4\uff0c\u5c06 rms_w \u8f6c\u6362\u4e3a\u4e0e weight \u4e00\u81f4\u7684\u7c7b\u578b\n\n    # \u521b\u5efa\u8f93\u51fa\u5f20\u91cf\n    batch, M, K = x.shape\n    weight_t = weight.t()\n    K_W, N = weight_t.shape\n    assert K == K_W\n\n    out = torch.empty((batch, M, N), dtype=weight_t.dtype, device=weight_t.device)\n    out_ptr = triton.reinterpret(out, tl.float8e5 if out.dtype == torch.int8 else tl.float16)\n\n    grid = lambda META: (\n        batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]))\n\n    rms_matmul_rbe[grid](\n        x_ptr=x,\n        w_ptr=weight_t, rms_w_ptr=rms_w, out_ptr=out_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=x.stride(0), stride_x_m=x.stride(1), stride_x_k=x.stride(2),\n        stride_w_k=weight_t.stride(0), stride_w_n=weight_t.stride(1),\n        stride_rms_w=rms_w.stride(0),\n        stride_out_batch=out.stride(0), stride_out_m=out.stride(1), stride_out_n=out.stride(2),\n        start_token_position=start_pos,\n        USE_FP8=weight_t.dtype == torch.int8,\n        RBE_EPILOGUE=use_rbe,\n        THETA=10000.,\n        EPS=1e-6,\n        BLOCK_SIZE_M=16, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n        num_stages=4, num_warps=4\n    )\n    out = out.view(batch, M, n_heads, head_dim)\n    return out\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton code implements a layer normalization operation in a custom manner using Triton kernels for both forward and backward passes. The primary functions are `layer_norm_forward` and `layer_norm_backward`, and they leverage two Triton kernels: `_layer_norm_forward_kernel` and `_layer_norm_backward_kernel`. \n\n    `layer_norm_forward` accepts input tensors `X`, `W`, `B` and a small constant `eps`. It reshapes `X`, calculates appropriate settings, and initializes output tensors `Y`, `Mean`, and `RSTD`. The `_layer_norm_forward_kernel` is then launched, which computes mean and variance for each row, applies normalization, and stores results in `Y`.\n\n    `layer_norm_backward` computes gradients for inputs, weights, and biases given a gradient `dY`. It utilizes `_layer_norm_backward_kernel` to iteratively compute these gradients across rows.\n\n    `LigerLayerNormFunction` is an autograd function wrapping these operations for PyTorch differentiation, ensuring input tensors are contiguous for efficient computation. The forward method calls `layer_norm_forward` and saves relevant tensors for backward computation, while the backward method computes gradients using `layer_norm_backward`.\n\n    Key variables:\n    - `n_cols`, `n_rows`: represent matrix dimensions.\n    - `BLOCK_SIZE`, `num_warps`: Triton-specific execution settings.\n    - `tl.constexpr`: Triton constant expressions for kernel parameters.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_forward_kernel(X, Y, Mean, RSTD, W, B, n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, BLOCK_SIZE)\n    mask = col_idx < n_cols\n\n    # Load data\n    x = tl.load(X + row_idx * n_cols + col_idx, mask=mask, other=0.0)\n    w = tl.load(W + col_idx, mask=mask, other=0.0)\n    b = tl.load(B + col_idx, mask=mask, other=0.0)\n\n    # Compute mean\n    mean = tl.sum(x, axis=0) / n_cols\n    tl.store(Mean + row_idx, mean)\n\n    # Compute variance\n    x_centered = x - mean\n    var = tl.sum(x_centered * x_centered, axis=0) / n_cols\n    rstd = 1.0 / tl.sqrt(var + eps)\n    tl.store(RSTD + row_idx, rstd)\n\n    # Normalize and scale\n    y = (x_centered * rstd) * w + b\n    tl.store(Y + row_idx * n_cols + col_idx, y, mask=mask)\n\n@triton.jit\ndef _layer_norm_backward_kernel(dY, X, W, B, Mean, RSTD, dX, dW, dB, n_cols, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, BLOCK_SIZE)\n    mask = col_idx < n_cols\n\n    # Load data\n    dy = tl.load(dY + row_idx * n_cols + col_idx, mask=mask, other=0.0)\n    x = tl.load(X + row_idx * n_cols + col_idx, mask=mask, other=0.0)\n    w = tl.load(W + col_idx, mask=mask, other=0.0)\n    b = tl.load(B + col_idx, mask=mask, other=0.0)\n    mean = tl.load(Mean + row_idx)\n    rstd = tl.load(RSTD + row_idx)\n\n    # Compute x_centered\n    x_centered = x - mean\n\n    # Compute gradients\n    d_var = tl.sum(dy * x_centered * w, axis=0) * (-0.5) * rstd * rstd * rstd\n    d_mean = tl.sum(dy * w, axis=0) * (-rstd) + d_var * (-2.0) * tl.sum(x_centered, axis=0) / n_cols\n    d_x = (dy * w * rstd) + (2.0 * d_var * x_centered / n_cols) + (d_mean / n_cols)\n    d_w = tl.sum(dy * x_centered * rstd, axis=0)\n    d_b = tl.sum(dy, axis=0)\n\n    # Store gradients\n    tl.store(dX + row_idx * n_cols + col_idx, d_x, mask=mask)\n    tl.atomic_add(dW + col_idx, d_w, mask=mask)\n    tl.atomic_add(dB + col_idx, d_b, mask=mask)\n\nimport torch\n\ndef layer_norm_forward(X, W, B, eps=1e-5):\n    n_rows, n_cols = X.shape\n    Y = torch.empty_like(X)\n    Mean = torch.empty((n_rows,), device=X.device, dtype=X.dtype)\n    RSTD = torch.empty((n_rows,), device=X.device, dtype=X.dtype)\n\n    grid = (n_rows,)\n    BLOCK_SIZE = 128\n    num_warps = 4\n\n    _layer_norm_forward_kernel[grid](X, Y, Mean, RSTD, W, B, n_cols, eps, BLOCK_SIZE, num_warps=num_warps)\n\n    return Y, Mean, RSTD\n\ndef layer_norm_backward(dY, X, W, B, Mean, RSTD, eps=1e-5):\n    n_rows, n_cols = X.shape\n    dX = torch.empty_like(X)\n    dW = torch.zeros_like(W)\n    dB = torch.zeros_like(B)\n\n    grid = (n_rows,)\n    BLOCK_SIZE = 128\n    num_warps = 4\n\n    _layer_norm_backward_kernel[grid](dY, X, W, B, Mean, RSTD, dX, dW, dB, n_cols, BLOCK_SIZE, num_warps=num_warps)\n\n    return dX, dW, dB\n\nclass LigerLayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, B, eps=1e-5):\n        X = X.contiguous()\n        W = W.contiguous()\n        B = B.contiguous()\n\n        Y, Mean, RSTD = layer_norm_forward(X, W, B, eps)\n        ctx.save_for_backward(X, W, B, Mean, RSTD, eps)\n        return Y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        X, W, B, Mean, RSTD, eps = ctx.saved_tensors\n        dX, dW, dB = layer_norm_backward(grad_output, X, W, B, Mean, RSTD, eps)\n        return dX, dW, dB, None\n\nimport torch.nn as nn\n\nclass LigerLayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5):\n        super(LigerLayerNorm, self).__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, x):\n        return LigerLayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n\n# Example usage\nx = torch.randn(32, 64, device='cuda')\nln = LigerLayerNorm(64).to('cuda')\ny = ln(x)\ny.backward(torch.randn_like(y))\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport functools\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\ndef ensure_contiguous(fn):\n    @functools.wraps(fn)\n    def wrapper(ctx, *args, **kwargs):\n        def maybe_to_contiguous(x):\n            return x.contiguous() if isinstance(x, torch.Tensor) else x\n\n        args = [maybe_to_contiguous(arg) for arg in args]\n        kwargs = {k: maybe_to_contiguous(v) for k, v in kwargs.items()}\n        return fn(ctx, *args, **kwargs)\n\n    return wrapper\n\n\ndef calculate_settings(n):\n    MAX_FUSED_SIZE = 65536\n    BLOCK_SIZE = triton.next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(\n            f\"Cannot launch Triton kernel since n = {n} exceeds \"\n            f\"the recommended Triton blocksize = {MAX_FUSED_SIZE}.\"\n        )\n\n    num_warps = 4\n    if BLOCK_SIZE >= 32768:\n        num_warps = 32 if not is_hip() else 16\n    elif BLOCK_SIZE >= 8192:\n        num_warps = 16\n    elif BLOCK_SIZE >= 2048:\n        num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _layer_norm_forward_kernel(\n    Y_ptr,  # pointer to output, shape (n_rows, n_cols)\n    Y_row_stride,  # stride of each row in output\n    X_ptr,  # pointer to input, shape (n_rows, n_cols)\n    X_row_stride,  # stride of each row in input\n    W_ptr,  # pointer to weights, shape (n_cols,)\n    W_row_stride,  # stride of each row in weights\n    B_ptr,  # pointer to bias, shape (n_cols,)\n    B_row_stride,  # stride of each row in bias\n    Mean_ptr,  # pointer to mean, shape (n_rows,)\n    Mean_row_stride,  # stride of each row in mean\n    RSTD_ptr,  # pointer to rstd, shape (n_rows,)\n    RSTD_row_stride,  # stride of each row in rstd\n    n_cols,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    Mean_ptr += row_idx * Mean_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    B_row = tl.load(B_ptr + col_offsets, mask=mask, other=0)\n\n    mean = tl.sum(X_row, axis=0) / n_cols\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / n_cols\n    rstd = tl.rsqrt(var + eps)\n\n    tl.store(Mean_ptr, mean)\n    tl.store(RSTD_ptr, rstd)\n\n    Y_row = (X_row - mean) * rstd * W_row + B_row\n\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n\n@triton.jit\ndef _layer_norm_backward_kernel(\n    X_ptr,  # pointer to input, shape (n_rows, n_cols)\n    W_ptr,  # pointer to weights, shape (n_cols,)\n    Mean_ptr,  # pointer to mean, shape (n_rows,)\n    RSTD_ptr,  # pointer to rstd, shape (n_rows,)\n    DX_ptr,  # pointer to input grad, shape (n_rows, n_cols)\n    DW_ptr,  # pointer to weights grad, shape (n_cols,)\n    DB_ptr,  # pointer to bias grad, shape (n_cols,)\n    DY_ptr,  # pointer to output grad, shape (n_rows, n_cols)\n    stride_x,  # stride of each row in input\n    stride_dx,  # stride of each row in input grad\n    stride_dw,  # stride of each row in weights grad\n    stride_db,  # stride of each row in bias grad\n    stride_dy,  # stride of each row in output grad\n    n_rows,\n    n_cols,\n    rows_per_program: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    dtype: tl.constexpr,\n):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < n_cols\n\n    dw_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    db_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    X_ptr += row_start * stride_x\n    Mean_ptr += row_start\n    RSTD_ptr += row_start\n    DX_ptr += row_start * stride_dx\n    DY_ptr += row_start * stride_dy\n\n    for _ in range(row_start, row_end):\n        x = tl.load(X_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(W_ptr + cols, mask=mask, other=0.0)\n        dy = tl.load(DY_ptr + cols, mask=mask, other=0.0)\n        mean = tl.load(Mean_ptr)\n        rstd = tl.load(RSTD_ptr)\n\n        x_hat = (x - mean) * rstd\n        wdy = w * dy\n        c1 = tl.sum(x_hat * wdy, axis=0) / n_cols\n        c2 = tl.sum(wdy, axis=0) / n_cols\n        dx = (wdy - (x_hat * c1 + c2)) * rstd\n        tl.store(DX_ptr + cols, dx.to(dtype), mask=mask)\n\n        dw_row += dy * x_hat\n        db_row += dy\n\n        X_ptr += stride_x\n        Mean_ptr += 1\n        RSTD_ptr += 1\n        DX_ptr += stride_dx\n        DY_ptr += stride_dy\n\n    tl.store(DW_ptr + row_block_id * stride_dw + cols, dw_row.to(dtype), mask=mask)\n    tl.store(DB_ptr + row_block_id * stride_db + cols, db_row.to(dtype), mask=mask)\n\ndef layer_norm_forward(X, W, B, eps):\n    shape = X.shape\n    dim = shape[-1]\n    X = X.view(-1, dim)\n    n_rows, n_cols = X.shape\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n    Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)\n    Mean = torch.empty(n_rows, dtype=X.dtype, device=X.device)\n    RSTD = torch.empty(n_rows, dtype=X.dtype, device=X.device)\n    assert (\n        X.shape[1] == W.shape[0]\n    ), f\"Incompatible hidden size dimension between input tensor with shape[1] = {X.shape[1]} and weight tensor with shape[0] = {W.shape[0]}\"\n\n    _layer_norm_forward_kernel[(n_rows,)](\n        Y,\n        Y.stride(0),\n        X,\n        X.stride(0),\n        W,\n        W.stride(0),\n        B,\n        B.stride(0),\n        Mean,\n        Mean.stride(0),\n        RSTD,\n        RSTD.stride(0),\n        n_cols,\n        eps,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return Y.view(*shape), X, Mean, RSTD, BLOCK_SIZE, num_warps\n\ndef layer_norm_backward(dY, X, W, B, Mean, RSTD):\n    shape = dY.shape\n    dim = shape[-1]\n    dY = dY.view(-1, dim)\n    n_rows, n_cols = dY.shape\n\n    DX = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)\n    sm_count = torch.cuda.get_device_properties(X.device).multi_processor_count\n    _DW = torch.empty((sm_count, n_cols), dtype=W.dtype, device=W.device)\n    _DB = torch.empty((sm_count, n_cols), dtype=W.dtype, device=W.device)\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n    if n_cols > BLOCK_SIZE:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n\n    rows_per_program = math.ceil(n_rows / sm_count)\n    grid = (sm_count,)\n    triton_dtype = tl.float32 if X.dtype == torch.float32 else tl.bfloat16\n    _layer_norm_backward_kernel[grid](\n        X,\n        W,\n        Mean,\n        RSTD,\n        DX,\n        _DW,\n        _DB,\n        dY,\n        X.stride(0),\n        DX.stride(0),\n        _DW.stride(0),\n        _DB.stride(0),\n        dY.stride(0),\n        n_rows,\n        n_cols,\n        rows_per_program,\n        BLOCK_SIZE=BLOCK_SIZE,\n        dtype=triton_dtype,\n    )\n\n    DW = _DW.sum(dim=0).to(W.dtype)\n    DB = _DB.sum(dim=0).to(W.dtype)\n\n    DX = DX.view(*shape)\n    return DX, DW, DB\n\nclass LigerLayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    @ensure_contiguous\n    def forward(ctx, X, W, B, eps):\n        Y, X, Mean, RSTD, BLOCK_SIZE, num_warps = layer_norm_forward(X, W, B, eps)\n        ctx.save_for_backward(X, W, B, Mean, RSTD)\n        return Y\n\n    @staticmethod\n    @ensure_contiguous\n    def backward(ctx, dY):\n        X, W, B, Mean, RSTD = ctx.saved_tensors\n        DX, DW, DB = layer_norm_backward(dY, X, W, B, Mean, RSTD)\n        return DX, DW, DB, None\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel `_fwd_kernel` performs a context attention mechanism on the given input tensors Q, K, and V. \n        It scales the dot product of Q and K by `sm_scale` to compute attention scores and applies a softmax function. \n        The result is used to weight the values in V, producing the output stored in `Out`. \n        The kernel uses parameters like `B_Start_Loc`, `B_Seqlen`, and `B_req_idx` to manage variable sequence lengths and batching. \n        It operates over blocks of data with dimensions defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_DMODEL`, adapting for different head groups using `kv_group_num`.\n\n        The Python function `context_attention_fwd` sets up the Triton kernel for execution. \n        It configures the grid and block dimensions based on input shapes and Tesla architecture considerations. \n        The function calculates a scaling factor for attention scores and passes necessary strides and shape parameters to the Triton kernel for execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, Out,\n    B_Start_Loc, B_Seqlen, B_req_idx,\n    stride_qb, stride_qh, stride_qd,\n    stride_kb, stride_kh, stride_kd,\n    stride_vb, stride_vh, stride_vd,\n    stride_ob, stride_oh, stride_od,\n    nheads, seq_len, kv_group_num, sm_scale,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = tl.program_id(axis=1)\n    head_id = tl.program_id(axis=2)\n\n    # Compute the start and end indices for the sequence\n    start_loc = tl.load(B_Start_Loc + bid)\n    seqlen = tl.load(B_Seqlen + bid)\n    req_idx = tl.load(B_req_idx + bid)\n\n    # Compute the block indices\n    block_m = pid * BLOCK_M\n    block_n = head_id * (seq_len // kv_group_num) + (bid % kv_group_num) * BLOCK_N\n\n    # Compute the offsets for Q, K, and V\n    q_offset = bid * stride_qb + head_id * stride_qh\n    k_offset = bid * stride_kb + (head_id // kv_group_num) * stride_kh\n    v_offset = bid * stride_vb + (head_id // kv_group_num) * stride_vh\n\n    # Initialize the output block\n    acc = tl.zeros((BLOCK_M, BLOCK_DMODEL), dtype=tl.float32)\n\n    # Compute the attention scores\n    for n in range(0, seqlen, BLOCK_N):\n        k_block_ptr = tl.make_block_ptr(\n            base=K + k_offset,\n            shape=(seq_len, BLOCK_DMODEL),\n            strides=(stride_kd, stride_kd),\n            offsets=(n, 0),\n            block_shape=(BLOCK_N, BLOCK_DMODEL),\n            order=(1, 0)\n        )\n        q_block_ptr = tl.make_block_ptr(\n            base=Q + q_offset,\n            shape=(seq_len, BLOCK_DMODEL),\n            strides=(stride_qd, stride_qd),\n            offsets=(block_m, 0),\n            block_shape=(BLOCK_M, BLOCK_DMODEL),\n            order=(1, 0)\n        )\n        q = tl.load(q_block_ptr)\n        k = tl.load(k_block_ptr)\n        qk = tl.dot(q, k, allow_tf32=True)\n        qk = qk * sm_scale\n\n        # Apply the softmax function\n        qk = tl.softmax(qk, axis=1)\n\n        # Compute the weighted values\n        v_block_ptr = tl.make_block_ptr(\n            base=V + v_offset,\n            shape=(seq_len, BLOCK_DMODEL),\n            strides=(stride_vd, stride_vd),\n            offsets=(n, 0),\n            block_shape=(BLOCK_N, BLOCK_DMODEL),\n            order=(1, 0)\n        )\n        v = tl.load(v_block_ptr)\n        acc += tl.dot(qk, v, allow_tf32=True)\n\n    # Store the output\n    out_block_ptr = tl.make_block_ptr(\n        base=Out + bid * stride_ob + head_id * stride_oh,\n        shape=(seq_len, BLOCK_DMODEL),\n        strides=(stride_od, stride_od),\n        offsets=(block_m, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    tl.store(out_block_ptr, acc)\n\nimport triton\nimport torch\n\ndef context_attention_fwd(Q, K, V, B_Start_Loc, B_Seqlen, B_req_idx, nheads, seq_len, kv_group_num, sm_scale):\n    # Get the shapes and strides\n    B, H, T, D = Q.shape\n    assert K.shape == (B, H // kv_group_num, T, D)\n    assert V.shape == (B, H // kv_group_num, T, D)\n    assert B_Start_Loc.shape == (B,)\n    assert B_Seqlen.shape == (B,)\n    assert B_req_idx.shape == (B,)\n\n    # Allocate the output tensor\n    Out = torch.empty_like(Q)\n\n    # Configure the grid and block dimensions\n    grid = (T // 128, B, H)\n    block = (128, 128, 1)\n\n    # Launch the kernel\n    _fwd_kernel[grid, block](\n        Q, K, V, Out,\n        B_Start_Loc, B_Seqlen, B_req_idx,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        nheads, seq_len, kv_group_num, sm_scale,\n        BLOCK_M=128, BLOCK_N=128, BLOCK_DMODEL=D\n    )\n\n    return Out\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\nTESLA = \"Tesla\" in torch.cuda.get_device_name(0)\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    Out,\n    B_Start_Loc,\n    B_Seqlen,\n    Req_to_tokens,\n    B_req_idx,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    kv_group_num,\n    b_prompt_cache_len,\n    H: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n\n    cur_kv_head = cur_head // kv_group_num\n\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len, cur_batch_seq_len + prompt_cache_len)\n\n    # causal mask\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        kv_loc = tl.load(\n            Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=(start_n + offs_n) < block_end_loc,\n            other=0,\n        )\n        off_k = kv_loc[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :]) < block_end_loc, other=0.0)\n        qk = tl.dot(q, k)\n\n        mask = offs_m[:, None] + prompt_cache_len >= (start_n + offs_n[None, :])\n        qk = tl.where(mask, qk * sm_scale, -1.0e8)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n\n        # -- update m_i and l_i\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        # -- update output accumulator --\n        acc = acc * alpha[:, None]\n        # update acc\n        off_v = kv_loc[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None]) < block_end_loc, other=0.0)\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        # update m_i and l_i\n        m_i = m_ij\n\n    acc = acc / l_i[:, None]\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n\n\n@torch.no_grad()\ndef context_attention_fwd(\n    q, k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len, req_to_token_indexs\n):\n    BLOCK_M = 128 if not TESLA else 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128, 256}\n\n    # \u8ba1\u7b97scale\u7cfb\u6570, \u5e76\u4e58\u4ee5 1/log(2) = 1.4426950408889634,\n    # \u7b97\u5b50\u5185\u90e8\u4f7f\u7528 tl.math.exp2 \u6765\u4f7f\u8ba1\u7b97\u4e0e\u6807\u51c6attention\u7b49\u4ef7\u3002\n    sm_scale = 1.0 / (Lq ** 0.5) * 1.4426950408889634\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = lambda meta: (triton.cdiv(max_input_len, meta[\"BLOCK_M\"]), batch * head, 1)\n\n    BLOCK_N = BLOCK_M\n    num_warps = 4 if Lk <= 64 else 8\n    num_stages = 1\n\n    _fwd_kernel[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        o,\n        b_start_loc,\n        b_seq_len,\n        req_to_token_indexs,\n        b_req_idx,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        req_to_token_indexs.stride(0),\n        req_to_token_indexs.stride(1),\n        kv_group_num=kv_group_num,\n        b_prompt_cache_len=b_prompt_cache_len,\n        H=head,\n        BLOCK_DMODEL=Lk,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        num_warps=num_warps,\n        num_stages=num_stages,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This script defines a Triton-based custom cross-entropy loss function for PyTorch, supporting features like label smoothing and large-scale parallelization across distributed systems. It leverages two main Triton kernels: `cross_entropy_fwd_kernel` for the forward pass and `cross_entropy_bwd_kernel` for the backward pass.\n\n    The forward kernel (`cross_entropy_fwd_kernel`) computes the log-sum-exp (LSE) and cross-entropy loss, considering label smoothing if applicable. The backward kernel (`cross_entropy_bwd_kernel`) computes the gradients with respect to the logits using the pre-computed LSE and the provided gradient of the losses. These kernels are invoked by the `CrossEntropyLoss` class, which defines the forward and backward static methods.\n\n    The `CrossEntropyLoss` class manages data pointers, block size definitions, and control flow for managing grid dimensions and kernel execution, including communication between distributed systems when necessary.\n\n    The `cross_entropy_loss` function provides a simple interface for users to apply this custom loss function by wrapping around the `CrossEntropyLoss.apply` call.\n\n    Input tensors include:\n    - `logits`: the predicted logits.\n    - `labels`: the true labels.\n    Optional parameters allow for configuration of label smoothing, ignored indices, and distributed settings.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    logits_ptr, labels_ptr, loss_ptr, lse_ptr, smoothing, ignored_index, n_classes, n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    logits = tl.load(logits_ptr + offsets, mask=mask, other=-float('inf'))\n    labels = tl.load(labels_ptr + offsets, mask=mask, other=0)\n\n    # Compute log-sum-exp (LSE)\n    max_logits = tl.max(logits, axis=0)\n    logits_sub_max = logits - max_logits\n    exp_logits = tl.exp(logits_sub_max)\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n    lse = max_logits + tl.log(sum_exp_logits)\n\n    # Compute cross-entropy loss\n    loss = tl.where(labels == ignored_index, 0.0, -logits[labels] + lse)\n    if smoothing > 0.0:\n        loss += smoothing * (lse - tl.log(n_classes))\n\n    tl.store(loss_ptr + offsets, loss, mask=mask)\n    tl.store(lse_ptr + offsets, lse, mask=mask)\n\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    logits_ptr, labels_ptr, lse_ptr, grad_loss_ptr, grad_logits_ptr, smoothing, ignored_index, n_classes, n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    logits = tl.load(logits_ptr + offsets, mask=mask, other=-float('inf'))\n    labels = tl.load(labels_ptr + offsets, mask=mask, other=0)\n    lse = tl.load(lse_ptr + offsets, mask=mask, other=0.0)\n    grad_loss = tl.load(grad_loss_ptr + offsets, mask=mask, other=0.0)\n\n    # Compute gradients\n    exp_logits = tl.exp(logits - lse)\n    grad = exp_logits * grad_loss\n    grad = tl.where(labels == ignored_index, 0.0, grad)\n    if smoothing > 0.0:\n        grad += smoothing * (grad_loss - grad_loss / n_classes)\n\n    tl.store(grad_logits_ptr + offsets, grad, mask=mask)\n\nimport torch\nimport triton\nimport triton.runtime\n\nclass CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, labels, smoothing=0.0, ignored_index=-100, n_classes=1000, distributed=False):\n        logits = logits.contiguous()\n        labels = labels.contiguous()\n        n_elements = logits.numel() // n_classes\n\n        # Allocate memory for loss and LSE\n        loss = torch.empty_like(logits)\n        lse = torch.empty_like(logits)\n\n        # Define block size\n        BLOCK_SIZE = 1024\n\n        # Launch forward kernel\n        grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n        cross_entropy_fwd_kernel[grid, BLOCK_SIZE](\n            logits, labels, loss, lse, smoothing, ignored_index, n_classes, n_elements\n        )\n\n        ctx.save_for_backward(logits, labels, lse)\n        ctx.smoothing = smoothing\n        ctx.ignored_index = ignored_index\n        ctx.n_classes = n_classes\n        ctx.distributed = distributed\n\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_loss):\n        logits, labels, lse = ctx.saved_tensors\n        n_elements = logits.numel() // ctx.n_classes\n\n        # Allocate memory for gradients\n        grad_logits = torch.empty_like(logits)\n\n        # Define block size\n        BLOCK_SIZE = 1024\n\n        # Launch backward kernel\n        grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n        cross_entropy_bwd_kernel[grid, BLOCK_SIZE](\n            logits, labels, lse, grad_loss, grad_logits, ctx.smoothing, ctx.ignored_index, ctx.n_classes, n_elements\n        )\n\n        return grad_logits, None, None, None, None, None\n\ndef cross_entropy_loss(logits, labels, smoothing=0.0, ignored_index=-100, n_classes=1000, distributed=False):\n    return CrossEntropyLoss.apply(logits, labels, smoothing, ignored_index, n_classes, distributed)\n\nimport torch\n\n# Example inputs\nlogits = torch.randn(128, 1000, device='cuda')\nlabels = torch.randint(0, 1000, (128,), device='cuda')\n\n# Compute loss\nloss = cross_entropy_loss(logits, labels, smoothing=0.1, ignored_index=-100, n_classes=1000)\n\n# Backward pass\nloss.mean().backward()\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    loss_ptr,  # data ptrs\n    lse_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    n_rows,\n    logits_row_stride,  # strides\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    # if SPLIT (e.g. tensor parallel), don't include the LSE in the loss since it's not the final LSE\n    SPLIT: tl.constexpr,\n):\n    # Triton kernel implementation for the forward pass of cross-entropy with label smoothing.\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(tl.float32)\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols, (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx)\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n            else:\n                loss = 0.0\n        if not SPLIT:\n            loss += lse_square_scale * lse * lse\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    dlogits_ptr,  # data ptrs\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    logits_row_stride,  # strides\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    # Triton kernel implementation for the backward pass of cross-entropy with label smoothing.\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(tl.float32)\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * probs, mask=col_offsets < n_cols)\n\nclass CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        logits,\n        labels,\n        smoothing,\n        lse_square_scale=0.0,\n        ignored_index=-100,\n        inplace_backward=False,\n        process_group=None,\n    ):\n        # CrossEntropyLoss forward function leveraging the Triton kernel.\n        n_rows, n_cols = logits.shape\n        assert labels.shape == (n_rows,)\n        world_size = 1 if process_group is None else torch.distributed.get_world_size(process_group)\n        total_classes = world_size * n_cols\n        rank = 0 if process_group is None else torch.distributed.get_rank(process_group)\n        class_start_idx = rank * n_cols\n\n        if logits.stride(-1) != 1:\n            logits = logits.contiguous()\n        MAX_BLOCK_SIZE = 64 * 1024\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), MAX_BLOCK_SIZE)\n        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else (16 if BLOCK_SIZE < 128 * 1024 else 32))\n        split = world_size > 1 or n_cols > MAX_BLOCK_SIZE\n        n_splits = (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE\n        loss_shape = (n_splits, n_rows) if n_splits > 1 else (n_rows,)\n        losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        lse = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_fwd_kernel[(n_rows, n_splits)](\n                losses,  # data ptrs\n                lse,\n                logits,\n                labels,\n                smoothing,\n                lse_square_scale,\n                ignored_index,\n                total_classes,\n                class_start_idx,\n                n_cols,  # shapes\n                n_rows,\n                logits.stride(0),  # strides\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n                SPLIT=split,\n            )\n\n        if split:\n            if world_size > 1:\n                lse_allgather = torch.empty(world_size, n_rows, dtype=lse.dtype, device=lse.device)\n                torch.distributed.all_gather_into_tensor(lse_allgather, lse, group=process_group)\n                handle_losses = torch.distributed.all_reduce(\n                    losses, op=torch.distributed.ReduceOp.SUM, group=process_group, async_op=True\n                )\n                lse = torch.logsumexp(lse_allgather, dim=0)\n                handle_losses.wait()\n            else:\n                lse = torch.logsumexp(lse, dim=0)\n                losses = losses.sum(dim=0)\n            losses += lse\n            if lse_square_scale != 0.0:\n                losses += lse_square_scale * lse.square()\n            losses.masked_fill_(labels == ignored_index, 0.0)\n\n        ctx.save_for_backward(logits, lse, labels)\n        ctx.smoothing = smoothing\n        ctx.lse_square_scale = lse_square_scale\n        ctx.ignored_index = ignored_index\n        ctx.total_classes = total_classes\n        ctx.class_start_idx = class_start_idx\n        ctx.inplace_backward = inplace_backward\n        return losses\n\n    @staticmethod\n    def backward(ctx, grad_losses):\n        logits, lse, labels = ctx.saved_tensors\n        dlogits = logits if ctx.inplace_backward else torch.empty_like(logits)\n        n_rows, n_cols = logits.shape\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 4 * 1024)\n        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else 16)\n        grid = lambda META: (n_rows, triton.cdiv(n_cols, META[\"BLOCK_SIZE\"]))  # noqa\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_bwd_kernel[grid](\n                dlogits,  # data ptrs\n                grad_losses,\n                logits,\n                lse,\n                labels,\n                ctx.smoothing,\n                ctx.lse_square_scale,\n                ctx.ignored_index,\n                ctx.total_classes,\n                ctx.class_start_idx,\n                n_cols,  # shapes\n                logits.stride(0),  # strides\n                dlogits.stride(0),\n                grad_losses.stride(0),\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n            )\n        return dlogits, None, None, None, None, None, None, None\n\ndef cross_entropy_loss(\n    logits: torch.Tensor,\n    labels: torch.Tensor,\n    label_smoothing: float = 0.0,\n    lse_square_scale: float = 0.0,\n    ignored_index=-100,\n    inplace_backward: bool = False,\n    process_group=None,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    CrossEntropyLoss wrapper function for calling the custom autograd Function.\n    \"\"\"\n    return CrossEntropyLoss.apply(\n        logits,\n        labels,\n        label_smoothing,\n        lse_square_scale,\n        ignored_index,\n        inplace_backward,\n        process_group,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a masked selection operation using Triton, a language designed for high-performance GPU computing. The main function, `masked_select`, takes two tensors, `inp` and `mask`, and selects elements from `inp` based on `mask`. If `mask[i]` is true, `inp[i]` is included in the output.\n\n        The kernel `masked_select_kernel` is written in Triton. It processes data in blocks defined by `BLOCK_SIZE`. For each block, it computes offsets and checks if they are within `n_elements`. It loads input data and selection masks, computes output offsets using a prefix sum, and stores the selected input elements to the output buffer if the corresponding `mask` element is true.\n\n        The `cfggen` function generates a list of Triton configuration objects, varying the block size and number of warps to optimize the kernel execution. The `broadcastable` function checks whether two shapes are compatible for broadcasting, a feature that aligns tensors of different shapes for operations like addition or multiplication.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef masked_select_kernel(\n    inp_ptr,  # Pointer to the input tensor\n    mask_ptr,  # Pointer to the mask tensor\n    out_ptr,  # Pointer to the output tensor\n    n_elements,  # Number of elements in the input and mask tensors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallel processing\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask_offsets = tl.max_contiguous(tl.multiple_of(offsets, BLOCK_SIZE), BLOCK_SIZE)\n\n    # Load input and mask\n    inp = tl.load(inp_ptr + offsets, mask=offsets < n_elements)\n    mask = tl.load(mask_ptr + mask_offsets, mask=mask_offsets < n_elements)\n\n    # Compute output offsets using prefix sum\n    selected_offsets = tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    selected_count = tl.zeros((1,), dtype=tl.int32)\n    for i in range(BLOCK_SIZE):\n        if mask[i]:\n            selected_offsets[i] = selected_count\n            selected_count += 1\n\n    # Store selected elements to the output buffer\n    for i in range(BLOCK_SIZE):\n        if mask[i]:\n            tl.store(out_ptr + selected_offsets[i], inp[i])\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef masked_select(inp: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    assert inp.is_cuda and mask.is_cuda, \"Input and mask tensors must be on the same CUDA device.\"\n    assert inp.numel() == mask.numel(), \"Input and mask tensors must have the same number of elements.\"\n\n    n_elements = inp.numel()\n    out = torch.empty(n_elements, dtype=inp.dtype, device=inp.device)\n\n    # Define the grid and block size\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    masked_select_kernel[grid](inp, mask, out, n_elements, BLOCK_SIZE=1024)\n\n    # Trim the output to the actual number of selected elements\n    selected_count = torch.sum(mask).item()\n    return out[:selected_count]\n\ndef cfggen():\n    configs = []\n    for block_size in [128, 256, 512, 1024]:\n        for num_warps in [1, 2, 4]:\n            configs.append(triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps))\n    return configs\n\ndef broadcastable(shape1, shape2):\n    if len(shape1) > len(shape2):\n        shape2 = (1,) * (len(shape1) - len(shape2)) + shape2\n    elif len(shape2) > len(shape1):\n        shape1 = (1,) * (len(shape2) - len(shape1)) + shape1\n\n    for s1, s2 in zip(shape1, shape2):\n        if s1 != s2 and s1 != 1 and s2 != 1:\n            return False\n    return True\n\n# Example usage\ninp = torch.tensor([1, 2, 3, 4, 5], device='cuda')\nmask = torch.tensor([True, False, True, False, True], device='cuda')\n\nselected = masked_select(inp, mask)\nprint(selected)  # Output: tensor([1, 3, 5], device='cuda:0')\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n\nShape = Tuple[int]\n\ndef broadcastable(s1: Shape, s2: Shape) -> bool:\n    r1 = len(s1)\n    if r1 == 0:\n        return True\n    r2 = len(s2)\n    if r2 == 0:\n        return True\n\n    s1, s2 = (s1, s2) if r1 >= r2 else (s2, s1)\n    r1, r2 = (r1, r2) if r1 >= r2 else (r2, r1)\n\n    d = r1 - r2\n    for i in range(r2):\n        if s1[d + i] == 1 or s2[i] == 1 or s1[d + i] == s2[i]:\n            continue\n        return False\n    return True\n\n\ndef cfggen():\n    configs = [\n        triton.Config({\"BLOCK_SIZE\": bs}, num_warps=w)\n        for w in [4, 8, 16, 32]\n        for bs in [256, 512, 1024, 2048, 4096]\n    ]\n    return configs\n\n\n\n@triton.autotune(configs=cfggen(), key=[\"n_elements\"])\n@triton.jit\ndef masked_select_kernel(\n    inp_ptr,\n    select_mask_ptr,\n    prefix_sum_ptr,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    inp = tl.load(inp_ptr + offsets, mask=mask, other=0.0)\n    select_mask = tl.load(select_mask_ptr + offsets, mask=mask, other=0.0).to(tl.int1)\n    out_offset = tl.load(prefix_sum_ptr + offsets, mask=mask, other=0.0) - 1\n\n    tl.store(out_ptr + out_offset, inp, mask=(select_mask and mask))\n\n\ndef masked_select(inp, mask):\n    logging.debug(\"GEMS MASKED SELECT\")\n\n    inp_shape = tuple(inp.shape)\n    mask_shape = tuple(mask.shape)\n\n    assert broadcastable(\n        inp_shape, mask_shape\n    ), \"The shapes of the `mask` and the `input` tensor must be broadcastable\"\n    inp, mask = torch.broadcast_tensors(inp, mask)\n\n    inp = inp.contiguous()\n    mask = mask.contiguous()\n\n    mask_flattened = mask.ravel()\n\n    prefix_sum = mask_flattened.cumsum(axis=0)\n    out = torch.empty(prefix_sum[-1].item(), dtype=inp.dtype, device=inp.device)\n\n    n_elements = inp.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    with torch.cuda.device(inp.device):\n        masked_select_kernel[grid](inp, mask_flattened, prefix_sum, out, n_elements)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton operator `_fwd_kernel` is designed to perform a custom scaled dot-product attention computation, which is a key component in Transformer models. The operator works with batched inputs of query (`Q`), key (`K`), and value (`V`) tensors, scaling factor (`sm_scale`), and batch metadata for sequence positions (`B_Start_Loc`) and sequence lengths (`B_Seqlen`). The function outputs results to `Out`.\n\nThe `context_attention_fwd` function is a wrapper that prepares the inputs and launches the `_fwd_kernel` with appropriate configurations.\n\nThe main logic of `_fwd_kernel` includes:\n- Iterating over blocks of keys and values, based on a block size `BLOCK`.\n- Computing the dot-product `qk` between `Q` and `K` blocks, and scaling by `sm_scale`.\n- Applying a sliding window attention mask to limit the attention score computation to a specific range.\n- Updating accumulators for the output using numerically stable techniques for exponentiation and scaling.\n- Writing the computed attention outputs back to `Out`.\n\nThe grid and number of warps are chosen based on input sizes to optimize parallel execution.\n\n`context_attention_fwd` configures the execution of `_fwd_kernel` and ensures the correct computation across the batch and attention heads.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, Out, B_Start_Loc, B_Seqlen, sm_scale,\n    stride_qb, stride_qh, stride_qd,\n    stride_kb, stride_kh, stride_kd,\n    stride_vb, stride_vh, stride_vd,\n    stride_ob, stride_oh, stride_od,\n    nheads, seqlen, BLOCK: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // nheads\n    hid = pid % nheads\n\n    # Pointers for Q, K, V, and Out\n    Q += bid * stride_qb + hid * stride_qh\n    K += bid * stride_kb + hid * stride_kh\n    V += bid * stride_vb + hid * stride_vh\n    Out += bid * stride_ob + hid * stride_oh\n\n    # Offsets for the current block\n    q_offsets = tl.arange(0, BLOCK)\n    k_offsets = tl.arange(0, BLOCK)\n    v_offsets = tl.arange(0, BLOCK)\n\n    # Load Q, K, V blocks\n    q = tl.load(Q + q_offsets[:, None] * stride_qd)\n    k = tl.load(K + k_offsets[None, :] * stride_kd)\n    v = tl.load(V + v_offsets[None, :] * stride_vd)\n\n    # Compute the dot-product qk\n    qk = tl.dot(q, k, allow_tf32=True) * sm_scale\n\n    # Apply sliding window attention mask\n    start_loc = tl.load(B_Start_Loc + bid)\n    seqlen = tl.load(B_Seqlen + bid)\n    qk = tl.where((q_offsets[:, None] + start_loc) >= (k_offsets[None, :] + start_loc - BLOCK), qk, float('-inf'))\n\n    # Compute softmax and scale\n    m = tl.max(qk, 1)\n    qk = qk - m[:, None]\n    qk = tl.exp(qk)\n    qk = qk / tl.sum(qk, 1)[:, None]\n\n    # Compute the output\n    out = tl.dot(qk, v, allow_tf32=True)\n\n    # Write the output back to Out\n    tl.store(Out + q_offsets[:, None] * stride_od, out)\n\nimport triton\nimport triton.language as tl\nimport torch\n\ndef context_attention_fwd(Q, K, V, B_Start_Loc, B_Seqlen, sm_scale, BLOCK=128):\n    # Get the shapes\n    batch_size, nheads, seqlen, d_head = Q.shape\n\n    # Allocate output tensor\n    Out = torch.empty_like(Q)\n\n    # Define grid and block sizes\n    grid = (batch_size * nheads,)\n\n    # Launch the kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out, B_Start_Loc, B_Seqlen, sm_scale,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        nheads, seqlen, BLOCK\n    )\n\n    return Out\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\nimport math\nimport torch.nn.functional as F\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    B_Start_Loc,\n    B_Seqlen,  # B_LOC \u5185\u90e8\u8bb0\u5f55\u6bcf\u4e2abatch \u8f93\u5165\u7684\u771f\u5b9e\u4f4d\u7f6e\uff0c B_SEQ_len \u8bb0\u5f55\u5f53\u524d\u8f93\u5165\u7684\u771f\u5b9e\u957f\u5ea6\n    Out,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    sliding_window,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n\n    cur_kv_head = cur_head // kv_group_num\n\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n            start_n = tl.multiple_of(start_n, BLOCK_N)\n            # -- compute qk ----\n            k = tl.load(\n                k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,\n                mask=(start_n + offs_n[None, :]) < cur_batch_seq_len,\n                other=0.0,\n            )\n            \n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, k)\n            qk *= sm_scale\n            # [SYM] mask outside of windows\uff0c\u4f7f\u7528\u5927\u8d1f\u6570\u4ee3\u66ff -inf\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, -1e9)\n            qk = tl.where((start_n + offs_n[None, :]) > (offs_m[:, None] - sliding_window), qk, -1e9)\n\n            # -- compute m_ij, p, l_ij\n            m_ij = tl.max(qk, 1)\n            # \u9632\u6b62 m_ij \u4e3a -1e9 \u5bfc\u81f4\u7684\u6570\u503c\u95ee\u9898\n            m_ij = tl.where(m_ij == -1e9, 0.0, m_ij)\n            p = tl.exp(qk - m_ij[:, None])\n            l_ij = tl.sum(p, 1)\n            \n            # -- update m_i and l_i\n            m_i_new = tl.maximum(m_i, m_ij)\n            alpha = tl.exp(m_i - m_i_new)\n            beta = tl.exp(m_ij - m_i_new)\n            l_i_new = alpha * l_i + beta * l_ij\n            l_i_new = tl.where(l_i_new == 0.0, 1e-9, l_i_new)  # \u9632\u6b62\u9664\u96f6\n            \n            # -- update output accumulator --\n            # scale p\n            p_scale = beta / l_i_new\n            p = p * p_scale[:, None]\n            # scale acc\n            acc_scale = l_i / l_i_new * alpha\n            acc = acc * acc_scale[:, None]\n            # update acc\n            v = tl.load(\n                v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,\n                mask=(start_n + offs_n[:, None]) < cur_batch_seq_len,\n                other=0.0,\n            )\n\n            p = p.to(v.dtype)\n            acc += tl.dot(p, v)\n            # update m_i and l_i\n            l_i = l_i_new\n            m_i = m_i_new\n    # initialize pointers to output\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n\n\n@torch.no_grad()\ndef context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, sliding_window):\n    BLOCK = 128\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n\n    sm_scale = 1.0 / (Lq ** 0.5)  # \u8ba1\u7b97scale\u7cfb\u6570\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n\n    num_warps = 4 if Lk <= 64 else 8\n    _fwd_kernel[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        b_start_loc,\n        b_seq_len,\n        o,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        kv_group_num=kv_group_num,\n        sliding_window=sliding_window,\n        BLOCK_M=BLOCK,\n        BLOCK_DMODEL=Lk,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton operator implements a forward pass of a multi-head attention mechanism optimized for block processing on GPUs. The main kernel `_attn_fwd` takes as input query (Q), key (K), and value (V) matrices along with their scaling factors (Q_scale, K_scale), and computes the attention output (Out).\n\n            - `_attn_fwd_inner`: This function performs the core computation of attention mechanism within a certain range defined by BLOCK_M and BLOCK_N. It calculates attention scores, applies masks, performs softmax normalization, and accumulates the results. It handles different stages of computation as defined by STAGE parameter, which affects the processing range and logic.\n\n            - `_attn_fwd`: This function orchestrates the overall attention computation by preparing input offsets and pointers, invoking `_attn_fwd_inner` for block processing, and storing the results back into the output tensor. It initializes the necessary variables and handles tensor strides for multi-dimensional input data. It computes results in stages to handle large sequence lengths efficiently.\n\n            - `forward`: This is a wrapper function to set up and launch the Triton kernel `_attn_fwd`. It initializes the output tensor, computes the grid for kernel launch, and manages input dimensions and strides.\n\n            The code is optimized for performance by processing blocks of data at a time and leveraging GPU parallelism through Triton. Key constants include BLOCK_M, BLOCK_N, HEAD_DIM, and STAGE which dictate the computational granularity and flow.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nBLOCK_M = 128\nBLOCK_N = 64\nHEAD_DIM = 64\nSTAGE = 2\n\n# Inner computation function\n@triton.jit\ndef _attn_fwd_inner(\n    Q, K, V, Out,\n    Q_scale, K_scale,\n    stride_qm, stride_qh, stride_kn, stride_kh, stride_vm, stride_vh, stride_om, stride_oh,\n    M, N, H, head_dim, stage,\n    sm_scale: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr\n):\n    # Compute block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    pid_h = tl.program_id(2)\n\n    # Compute offsets\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_h = pid_h * HEAD_DIM + tl.arange(0, HEAD_DIM)\n\n    # Initialize pointers\n    Q_ptr = Q + (offs_m[:, None] * stride_qm + offs_h[None, :] * stride_qh)\n    K_ptr = K + (offs_n[:, None] * stride_kn + offs_h[None, :] * stride_kh)\n    V_ptr = V + (offs_n[:, None] * stride_vm + offs_h[None, :] * stride_vh)\n    Out_ptr = Out + (offs_m[:, None] * stride_om + offs_h[None, :] * stride_oh)\n\n    # Load Q, K, V\n    q = tl.load(Q_ptr, mask=offs_m[:, None] < M, other=0.0)\n    k = tl.load(K_ptr, mask=offs_n[:, None] < N, other=0.0)\n    v = tl.load(V_ptr, mask=offs_n[:, None] < N, other=0.0)\n\n    # Scale Q and K\n    q = q * Q_scale\n    k = k * K_scale\n\n    # Compute attention scores\n    logits = tl.dot(q, k, trans_b=True) * sm_scale\n\n    # Apply softmax\n    logits = tl.softmax(logits, axis=1)\n\n    # Compute output\n    out = tl.dot(logits, v)\n\n    # Store output\n    tl.store(Out_ptr, out, mask=offs_m[:, None] < M)\n\n# Main kernel\n@triton.jit\ndef _attn_fwd(\n    Q, K, V, Out,\n    Q_scale, K_scale,\n    stride_qm, stride_qh, stride_kn, stride_kh, stride_vm, stride_vh, stride_om, stride_oh,\n    M, N, H, head_dim,\n    sm_scale: tl.constexpr,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr, STAGE: tl.constexpr\n):\n    # Compute grid size\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    grid_h = H\n\n    # Launch inner kernel\n    _attn_fwd_inner[grid_m, grid_n, grid_h](\n        Q, K, V, Out,\n        Q_scale, K_scale,\n        stride_qm, stride_qh, stride_kn, stride_kh, stride_vm, stride_vh, stride_om, stride_oh,\n        M, N, H, head_dim, STAGE,\n        sm_scale, BLOCK_M, BLOCK_N, HEAD_DIM\n    )\n\n# Wrapper function\ndef forward(Q, K, V, Q_scale, K_scale, sm_scale, M, N, H, head_dim):\n    # Initialize output tensor\n    Out = tl.zeros((M, H, head_dim), dtype=Q.dtype)\n\n    # Compute strides\n    stride_qm = Q.stride(0)\n    stride_qh = Q.stride(1)\n    stride_kn = K.stride(0)\n    stride_kh = K.stride(1)\n    stride_vm = V.stride(0)\n    stride_vh = V.stride(1)\n    stride_om = Out.stride(0)\n    stride_oh = Out.stride(1)\n\n    # Compute grid size\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    grid_h = H\n\n    # Launch kernel\n    _attn_fwd[grid_m, grid_n, grid_h](\n        Q, K, V, Out,\n        Q_scale, K_scale,\n        stride_qm, stride_qh, stride_kn, stride_kh, stride_vm, stride_vh, stride_om, stride_oh,\n        M, N, H, head_dim,\n        sm_scale, BLOCK_M, BLOCK_N, HEAD_DIM, STAGE\n    )\n\n    return Out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n        K_scale_ptr += lo // BLOCK_N\n        K_ptrs += HEAD_DIM * lo\n        V_ptrs += HEAD_DIM * lo\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask = k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        if STAGE == 2:\n            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n            qk = qk + tl.where(mask, 0, -1.0e6)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask = (offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        acc += tl.dot(p, v, out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i, m_i\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr  \n              ):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N) \n    \n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX \n                                    )\n    acc, l_i, _ = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    2, offs_m, offs_n, N_CTX \n                                    )\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.float16)\n    stage = 3\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=4)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `_bgmv_shrink_kernel` performs a batched generalized matrix-vector multiplication (BGMV) with a \"shrink\" operation using a lora (low-rank adaptation) mechanism. The kernel takes inputs and scales the product of the inputs with a set of lora weights, storing the results in an output tensor.\n\n            The inputs to the kernel are:\n            - `input_ptr`: pointer to the input data.\n            - `lora_ptr`: pointer to the lora weight matrix.\n            - `out_ptr`: pointer to the output tensor.\n            - `N`: the number of columns in the lora matrix.\n            - `K`: the number of rows in the lora matrix.\n            - `lora_indices`: indices indicating which lora matrix to use for each batch.\n            - `scaling`: a scaling factor applied to the result.\n            - Strides (`xm_stride`, `xk_stride`, etc.) define how to traverse the input, lora, and output tensors.\n            - `BLOCK_N`, `BLOCK_K`, `SPLIT_K`: these constants define the tile size for loading data and parallelism.\n\n            The kernel iterates over the input tensor in blocks of `BLOCK_K` rows. For each block, it multiplies the input block by the corresponding lora weight block, accumulates the results, applies the scaling, and stores the result in the output tensor. If `SPLIT_K` is greater than 1, it performs an atomic add to handle reduction across multiple kernel instances.\n\n            The wrapper function `_bgmv_shrink` sets up the kernel execution environment. It ensures the inputs are contiguous, determines batch size, and prepares kernel grid dimensions based on `SPLIT_K` and batch size. Then, it launches the kernel with the given parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_shrink_kernel(\n    input_ptr,  # Pointer to the input data\n    lora_ptr,   # Pointer to the lora weight matrix\n    out_ptr,    # Pointer to the output tensor\n    N,          # Number of columns in the lora matrix\n    K,          # Number of rows in the lora matrix\n    lora_indices,  # Indices indicating which lora matrix to use for each batch\n    scaling,    # Scaling factor applied to the result\n    xm_stride,  # Stride for input tensor\n    xk_stride,  # Stride for input tensor\n    lora_n_stride,  # Stride for lora matrix\n    lora_k_stride,  # Stride for lora matrix\n    out_m_stride,   # Stride for output tensor\n    out_n_stride,   # Stride for output tensor\n    BLOCK_N: tl.constexpr,  # Tile size for columns\n    BLOCK_K: tl.constexpr,  # Tile size for rows\n    SPLIT_K: tl.constexpr,  # Number of splits for reduction\n):\n    pid = tl.program_id(axis=0)\n    batch_id = pid // (N // BLOCK_N)\n    block_n_id = pid % (N // BLOCK_N)\n\n    lora_index = lora_indices[batch_id]\n    lora_ptr = lora_ptr + lora_index * N * K\n\n    for split_k_id in range(SPLIT_K):\n        block_k_id = split_k_id * (K // BLOCK_K)\n        input_block_ptr = input_ptr + batch_id * K + block_k_id * BLOCK_K\n        lora_block_ptr = lora_ptr + block_k_id * BLOCK_K * lora_n_stride\n\n        acc = tl.zeros((BLOCK_K, BLOCK_N), dtype=tl.float32)\n\n        for k in range(0, K, BLOCK_K):\n            input_block = tl.load(input_block_ptr + k * xk_stride, mask=k + tl.arange(0, BLOCK_K) < K)\n            lora_block = tl.load(lora_block_ptr + k * lora_k_stride, mask=k + tl.arange(0, BLOCK_K) < K)\n            acc += tl.dot(input_block, lora_block)\n\n        out_block_ptr = out_ptr + batch_id * N + block_n_id * BLOCK_N * out_n_stride\n        if SPLIT_K > 1:\n            tl.atomic_add(out_block_ptr, acc * scaling)\n        else:\n            tl.store(out_block_ptr, acc * scaling)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _bgmv_shrink(input, lora, out, lora_indices, scaling, BLOCK_N=128, BLOCK_K=64, SPLIT_K=1):\n    assert input.is_contiguous(), \"Input tensor must be contiguous\"\n    assert lora.is_contiguous(), \"Lora tensor must be contiguous\"\n    assert out.is_contiguous(), \"Output tensor must be contiguous\"\n\n    B, K = input.shape\n    N = lora.shape[1]\n\n    # Determine grid dimensions\n    grid = (B * (N // BLOCK_N),)\n\n    # Launch the kernel\n    _bgmv_shrink_kernel[grid](\n        input_ptr=input,\n        lora_ptr=lora,\n        out_ptr=out,\n        N=N,\n        K=K,\n        lora_indices=lora_indices,\n        scaling=scaling,\n        xm_stride=input.stride(0),\n        xk_stride=input.stride(1),\n        lora_n_stride=lora.stride(1),\n        lora_k_stride=lora.stride(2),\n        out_m_stride=out.stride(0),\n        out_n_stride=out.stride(1),\n        BLOCK_N=BLOCK_N,\n        BLOCK_K=BLOCK_K,\n        SPLIT_K=SPLIT_K\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n# from .utils import get_lora_op_configs\n\n@triton.jit\ndef _bgmv_shrink_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    lora_indices,\n    scaling,\n    xm_stride,\n    xk_stride,\n    l0_stride,\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n):\n    pid_sk = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n\n    offset_n = tl.arange(0, BLOCK_N)\n    offset_k = tl.arange(0, BLOCK_K) + pid_sk * BLOCK_K\n    a_ptr = input_ptr + cur_batch * xm_stride\n    b_ptr = lora_ptr + l0_stride * lora_index\n    accumulator = tl.zeros((BLOCK_N, ), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K * SPLIT_K):\n        current_k = k + offset_k\n        current_k_c = tl.max_contiguous(current_k, BLOCK_K)\n        tiled_a = tl.load(\n            a_ptr + current_k_c,\n            mask=current_k < K,\n            other=0.0,\n        )\n        b_ptr_mask = (offset_n[:, None] < N) & (current_k[None, :] < K)\n\n        tiled_b = tl.load(\n            b_ptr + offset_n[:, None] * lora_k_stride +\n            current_k[None, :] * lora_n_stride,\n            mask=b_ptr_mask,\n            other=0.0,\n        )\n\n        accumulator += tl.sum(tiled_a * tiled_b, 1)\n    accumulator *= scaling\n    offset_cn = tl.arange(0, BLOCK_N)\n    c_ptr = out_ptr + cur_batch * cm_stride + offset_cn * cn_stride\n    c_mask = offset_cn < N\n    if SPLIT_K == 1:\n        tl.store(c_ptr, accumulator, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptr, accumulator, mask=c_mask)\n\n\n@torch.inference_mode()\ndef _bgmv_shrink(\n    inputs: torch.Tensor,\n    lora_a_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    scaling: float = 1.0,\n) -> None:\n    assert inputs.dtype == lora_a_weights.dtype\n    assert inputs.dtype in [torch.float16, torch.bfloat16]\n    assert lora_a_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(1) == lora_a_weights.size(-1)\n    assert inputs.is_contiguous()\n\n    if lora_a_weights.ndim == 4:\n        assert lora_a_weights.size(1) == 1\n        lora_a_weights = lora_a_weights.squeeze(dim=1)\n    else:\n        assert lora_a_weights.ndim == 3\n    assert lora_a_weights.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    batches = lora_indices_tensor.size(0)\n    N, K = lora_a_weights.shape[-2:]\n    BLOCK_N = triton.next_power_of_2(N)\n    # config = get_lora_op_configs(\"bgmv_shrink\", batches, K)\n\n    grid = lambda META: (\n        META[\"SPLIT_K\"],\n        batches,\n    )\n    _bgmv_shrink_kernel[grid](\n        inputs,\n        lora_a_weights,\n        output_tensor,\n        N,\n        K,\n        lora_indices_tensor,\n        scaling,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_a_weights.stride(0),\n        lora_a_weights.stride(1),\n        lora_a_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        BLOCK_N=BLOCK_N,\n        BLOCK_K=256,\n        SPLIT_K=64,\n\n        # **config,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator performs a forward softmax operation on token logits. The kernel function '_fwd_kernel_token_softmax' computes the softmax on a per-head, per-batch basis for each token sequence. It reads the logits from 'Logics', uses a block of size 'BLOCK_SIZE' to load and process data, applies the softmax normalization, and writes the output probabilities to 'Prob_Out'. The stride parameters manage how data is laid out in memory. The utility function 'token_softmax_fwd' prepares the data and configures the kernel launch, handling batched and multi-headed data.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logits,  # Input logits\n    Prob_Out,  # Output probabilities\n    stride_logit_b,  # Stride for batch dimension in logits\n    stride_logit_h,  # Stride for head dimension in logits\n    stride_logit_s,  # Stride for sequence dimension in logits\n    stride_prob_b,  # Stride for batch dimension in probabilities\n    stride_prob_h,  # Stride for head dimension in probabilities\n    stride_prob_s,  # Stride for sequence dimension in probabilities\n    n_tokens,  # Number of tokens in each sequence\n    BLOCK_SIZE: tl.constexpr  # Block size for processing\n):\n    # Get the batch and head indices\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n\n    # Compute the offset for the current batch and head\n    logits_offset = batch_idx * stride_logit_b + head_idx * stride_logit_h\n    prob_offset = batch_idx * stride_prob_b + head_idx * stride_prob_h\n\n    # Load the logits for the current batch and head\n    logits = tl.load(Logits + logits_offset + tl.arange(0, BLOCK_SIZE) * stride_logit_s, mask=tl.arange(0, BLOCK_SIZE) < n_tokens, other=-float('inf'))\n\n    # Compute the max value for numerical stability\n    max_val = tl.max(logits, axis=0)\n\n    # Subtract the max value for numerical stability\n    logits = logits - max_val\n\n    # Compute the exponentials\n    exp_logits = tl.exp(logits)\n\n    # Compute the sum of exponentials\n    sum_exp = tl.sum(exp_logits, axis=0)\n\n    # Compute the softmax probabilities\n    probs = exp_logits / sum_exp\n\n    # Store the probabilities\n    tl.store(Prob_Out + prob_offset + tl.arange(0, BLOCK_SIZE) * stride_prob_s, probs, mask=tl.arange(0, BLOCK_SIZE) < n_tokens)\n\nimport torch\n\ndef token_softmax_fwd(logits: torch.Tensor, prob_out: torch.Tensor, block_size: int):\n    # Get the dimensions\n    batch_size, n_heads, n_tokens = logits.shape\n\n    # Define the grid and block dimensions\n    grid = (batch_size, n_heads)\n    block = (block_size,)\n\n    # Define the strides\n    stride_logit_b = logits.stride(0)\n    stride_logit_h = logits.stride(1)\n    stride_logit_s = logits.stride(2)\n    stride_prob_b = prob_out.stride(0)\n    stride_prob_h = prob_out.stride(1)\n    stride_prob_s = prob_out.stride(2)\n\n    # Launch the kernel\n    _fwd_kernel_token_softmax[grid, block](\n        logits, prob_out,\n        stride_logit_b, stride_logit_h, stride_logit_s,\n        stride_prob_b, stride_prob_h, stride_prob_s,\n        n_tokens, BLOCK_SIZE=block_size\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random logits tensor\n    batch_size = 2\n    n_heads = 3\n    n_tokens = 4\n    logits = torch.randn((batch_size, n_heads, n_tokens), device='cuda')\n\n    # Create an output tensor for probabilities\n    prob_out = torch.empty_like(logits)\n\n    # Define the block size\n    block_size = 128\n\n    # Perform the forward softmax\n    token_softmax_fwd(logits, prob_out, block_size)\n\n    # Print the results\n    print(\"Logits:\")\n    print(logits)\n    print(\"Probabilities:\")\n    print(prob_out)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for forward token softmax\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logics, B_Start_Loc, B_Seqlen,\n    Prob_Out,\n    stride_logic_h, stride_logic_bs,\n    stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    row = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_in_all_start_index + col_offsets) * stride_logic_bs,\n                  mask=col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    tl.store(Prob_Out + cur_head * stride_prob_h + (cur_batch_in_all_start_index + col_offsets)\n             * stride_prob_bs, softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n\n# Function to launch the Triton kernel\n@torch.no_grad()\ndef token_softmax_fwd(Logics, B_Start_Loc, B_Seqlen, Prob_Out, max_input_len):\n    BLOCK_SIZE = triton.next_power_of_2(max_input_len)\n    batch, head_num = B_Start_Loc.shape[0], Logics.shape[0]\n\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n\n    _fwd_kernel_token_softmax[(batch, head_num)](\n        Logics, B_Start_Loc, B_Seqlen,\n        Prob_Out,\n        Logics.stride(0), Logics.stride(1),\n        Prob_Out.stride(0), Prob_Out.stride(1),\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton-based kernel is a matrix multiplication operator implemented as a function `matmul_kernel`. \n        The kernel performs matrix multiplication of two input matrices `a` and `b`, and writes the result into matrix `c`. \n        The function `matmul` serves as a wrapper around the kernel to manage configuration and dispatch execution.\n\n        `matmul_kernel` operates with block-wise parallelism, leveraging Triton's capabilities to divide work across multiple GPU threads. \n        The key parameters include BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K, which define the dimensions of the computational blocks \n        used for tiling the input matrices and optimizing data throughput. \n\n        Inputs to the kernel include pointers to the matrices (`a_ptr`, `b_ptr`, `c_ptr`), dimensions (`M`, `N`, `K`), and strides for each matrix. \n        The `stride_am`, `stride_ak`, etc., define how to traverse the matrix elements linearly in memory. \n\n        The outer function `matmul` defines configuration settings based on the data type, ensuring compatibility \n        and optimizing execution by choosing suitable block sizes, group sizes, and other meta parameters \n        that influence performance like number of stages and warps.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    # Compute the block ID in a 1D program grid\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # Compute the block offsets\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) * stride_am\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) * stride_bn\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Load the blocks of A and B\n    for k in range(0, K, BLOCK_SIZE_K):\n        offs_ak = (k + offs_k) * stride_ak\n        a = tl.load(a_ptr + offs_am[:, None] + offs_ak[None, :])\n        b = tl.load(b_ptr + offs_ak[:, None] + offs_bn[None, :])\n        acc += tl.dot(a, b)\n\n    # Compute the output block offsets\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c = tl.load(c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N), other=0.0)\n    c += acc\n    tl.store(c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn, c, mask=(offs_cm[:, None] < M) & (offs_cn[None, :] < N))\n\nimport triton\nimport triton.language as tl\n\ndef matmul(a, b, c, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, num_stages, num_warps):\n    # Compute the grid size\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K,\n        GROUP_SIZE_M,\n        num_stages=num_stages,\n        num_warps=num_warps\n    )\n\nimport torch\n\n# Example matrices\nM, N, K = 1024, 1024, 1024\na = torch.randn((M, K), device='cuda')\nb = torch.randn((K, N), device='cuda')\nc = torch.zeros((M, N), device='cuda')\n\n# Configuration parameters\nBLOCK_SIZE_M = 16\nBLOCK_SIZE_N = 16\nBLOCK_SIZE_K = 32\nGROUP_SIZE_M = 8\nnum_stages = 3\nnum_warps = 4\n\n# Perform matrix multiplication\nmatmul(a, b, c, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, num_stages, num_warps)\n\n# Verify the result\ntorch.allclose(c, torch.matmul(a, b))\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    ret[\"name\"] = f\"{kernel.name} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                  M, N, K,  #\n                  stride_am, stride_ak,  #\n                  stride_bk, stride_bn,  #\n                  stride_cm, stride_cn,  #\n                  BLOCK_SIZE_M: tl.constexpr,  #\n                  BLOCK_SIZE_N: tl.constexpr,  #\n                  BLOCK_SIZE_K: tl.constexpr,  #\n                  GROUP_SIZE_M: tl.constexpr,  #\n                  ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if (c_ptr.dtype.element_ty == tl.float8e4nv):\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef matmul(a, b):\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )\n    matmul_kernel[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel named `bmm_kernel` performs batched matrix multiplication (BMM) for 3D tensors A and B, producing an output tensor O. It operates in blocks defined by tile sizes (TILE_M, TILE_N, TILE_K), allowing efficient parallel computation on the GPU. The inputs are:\n        - `A`, `B`: Input matrices for multiplication with shapes (batch, M, K) and (batch, K, N) respectively.\n        - `O`: Output matrix with shape (batch, M, N).\n        - `M`, `N`, `K`: Dimensions of the matrices.\n        The kernel computes partial results in tiles and accumulates them. The `bmm` function wraps the kernel call, handling tensor contiguity and preparing output storage.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef bmm_kernel(\n    A_ptr, B_ptr, O_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn,\n    stride_om, stride_on,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    # Compute the program ID in a 3D grid\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_k = tl.cdiv(K, BLOCK_K)\n    num_pid_in_batch = num_pid_m * num_pid_n\n    batch_id = pid // num_pid_in_batch\n    pid = pid % num_pid_in_batch\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    # Compute the offsets for the current tile\n    offs_am = (batch_id * stride_am + pid_m * BLOCK_M + tl.arange(0, BLOCK_M))[:, None]\n    offs_bn = (batch_id * stride_bn + pid_n * BLOCK_N + tl.arange(0, BLOCK_N))[None, :]\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # Pointers to the current tile\n    A_tile_ptr = A_ptr + (offs_am * stride_ak + offs_k[None, :])\n    B_tile_ptr = B_ptr + (offs_k[:, None] * stride_bn + offs_bn)\n\n    # Accumulate the result in a shared memory tile\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        # Load the current tile of A and B\n        a = tl.load(A_tile_ptr)\n        b = tl.load(B_tile_ptr)\n\n        # Perform the matrix multiplication\n        acc += tl.dot(a, b)\n\n        # Move to the next tile\n        A_tile_ptr += BLOCK_K * stride_ak\n        B_tile_ptr += BLOCK_K * stride_bk\n\n    # Store the result in the output tensor\n    offs_om = (batch_id * stride_om + pid_m * BLOCK_M + tl.arange(0, BLOCK_M))[:, None]\n    offs_on = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N))[None, :]\n    O_tile_ptr = O_ptr + (offs_om * stride_on + offs_on)\n    tl.store(O_tile_ptr, acc)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef bmm(A: torch.Tensor, B: torch.Tensor, O: torch.Tensor):\n    # Ensure the input tensors are contiguous\n    A = A.contiguous()\n    B = B.contiguous()\n    O = O.contiguous()\n\n    # Extract the dimensions\n    batch, M, K = A.shape\n    batch, K, N = B.shape\n\n    # Define the grid and block sizes\n    BLOCK_M, BLOCK_N, BLOCK_K = 16, 16, 16\n    grid = (batch * tl.cdiv(M, BLOCK_M) * tl.cdiv(N, BLOCK_N),)\n\n    # Launch the kernel\n    bmm_kernel[grid](\n        A, B, O,\n        M, N, K,\n        A.stride(1), A.stride(2), B.stride(1), B.stride(2),\n        O.stride(1), O.stride(2),\n        BLOCK_M, BLOCK_N, BLOCK_K\n    )\n\n# Example usage\nbatch, M, K, N = 2, 128, 128, 128\nA = torch.randn((batch, M, K), device='cuda')\nB = torch.randn((batch, K, N), device='cuda')\nO = torch.empty((batch, M, N), device='cuda')\n\nbmm(A, B, O)\n\n# Verify the result\nexpected = torch.bmm(A, B)\ntorch.testing.assert_allclose(O, expected)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\ndef heur_divisible_m(args):\n    return args[\"M\"] % args[\"TILE_M\"] == 0\n\n\ndef heur_divisible_n(args):\n    return args[\"N\"] % args[\"TILE_N\"] == 0\n\n\ndef heur_divisible_k(args):\n    return args[\"K\"] % args[\"TILE_K\"] == 0\n\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\"TILE_M\": 32, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 1},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 128, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 32, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 1},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 128, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n    ],\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.heuristics(\n    {\n        \"DIVISIBLE_M\": heur_divisible_m,\n        \"DIVISIBLE_N\": heur_divisible_n,\n        \"DIVISIBLE_K\": heur_divisible_k,\n    }\n)\n@triton.jit\ndef bmm_kernel(\n    A,\n    B,\n    O,\n    M,\n    N,\n    K,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    DIVISIBLE_M: tl.constexpr,\n    DIVISIBLE_N: tl.constexpr,\n    DIVISIBLE_K: tl.constexpr,\n):\n    # batch offsets\n    pid_b = tl.program_id(2)\n    A += pid_b * M * K\n    B += pid_b * K * N\n    O += pid_b * M * N\n\n    pidx = tl.program_id(0)\n    pidy = tl.program_id(1)\n\n    if GROUP_M == 1:\n        pid_m, pid_n = pidx, pidy\n    else:\n        # reorder CTAs\n        gridx = tl.num_programs(0)\n        gridy = tl.num_programs(1)\n        pid = pidx + pidy * gridx\n\n        num_CTA_per_group = gridy * GROUP_M\n\n        group_id = pid // num_CTA_per_group\n        inner_group_id = pid % num_CTA_per_group\n        if (group_id * GROUP_M + GROUP_M) > gridx:\n            GROUP_SIZE = gridx % GROUP_M\n        else:\n            GROUP_SIZE = GROUP_M\n        pid_m = group_id * GROUP_M + inner_group_id % GROUP_SIZE\n        pid_n = inner_group_id // GROUP_SIZE\n\n    offs_m = pid_m * TILE_M + tl.arange(0, TILE_M)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n\n    if not DIVISIBLE_M:\n        mask_m = offs_m < M\n    if not DIVISIBLE_N:\n        mask_n = offs_n < N\n\n    a_ptrs = A + offs_m[:, None] * K + offs_k[None, :]\n    b_ptrs = B + offs_k[:, None] * N + offs_n[None, :]\n    o_ptrs = O + offs_m[:, None] * N + offs_n[None, :]\n\n    num_iters = tl.cdiv(K, TILE_K)\n    o = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\n    for _ in range(num_iters):\n        if DIVISIBLE_K:\n            if DIVISIBLE_M:\n                mask_a = None\n            else:\n                mask_a = mask_m[:, None]\n            if DIVISIBLE_N:\n                mask_b = None\n            else:\n                mask_b = mask_n[None, :]\n        else:\n            mask_k = offs_k < K\n            if DIVISIBLE_M:\n                mask_a = mask_k[None, :]\n            else:\n                mask_a = mask_m[:, None] & mask_k[None, :]\n            if DIVISIBLE_N:\n                mask_b = mask_k[:, None]\n            else:\n                mask_b = mask_k[:, None] & mask_n[None, :]\n\n        a = tl.load(a_ptrs, mask_a)\n        b = tl.load(b_ptrs, mask_b)\n\n        offs_k += TILE_K\n        a_ptrs += TILE_K\n        b_ptrs += TILE_K * N\n\n        o += tl.dot(a, b, allow_tf32=False)\n\n    if DIVISIBLE_M and DIVISIBLE_N:\n        mask_c = None\n    elif DIVISIBLE_M and not DIVISIBLE_N:\n        mask_c = mask_n[None, :]\n    elif not DIVISIBLE_M and DIVISIBLE_N:\n        mask_c = mask_m[:, None]\n    else:\n        mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(o_ptrs, o, mask_c)\n\n\ndef bmm(A, B):\n    logging.debug(\"GEMS BMM\")\n    batch, M, K = A.shape\n    _, _, N = B.shape\n    A = A.contiguous()\n    B = B.contiguous()\n    out = torch.empty((batch, M, N), dtype=A.dtype, device=A.device)\n\n    grid_fn = lambda meta: (\n        triton.cdiv(meta[\"M\"], meta[\"TILE_M\"]),\n        triton.cdiv(meta[\"N\"], meta[\"TILE_N\"]),\n        batch,\n    )\n    with torch.cuda.device(A.device):\n        bmm_kernel[grid_fn](A, B, out, M, N, K)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implementation aims to accelerate the attention mechanism commonly used in transformer architectures. The primary functions are `_fwd_kernel` and `_bwd_kernel`, handling the forward and backward passes respectively.\n\n            The `_fwd_kernel` function computes the scaled dot-product attention. It takes as input query `Q`, key `K`, and value `V` tensors, along with a scale factor `sm_scale`. It outputs the attention result `Out`. The function divides the computation into blocks defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_DMODEL`. Within each block, it computes the dot product between `Q` and `K`, applies the softmax to get attention scores, and uses these scores to weight the `V` tensor. The results are stored in `Out`. The function also computes intermediate normalization constants `L` and `M` for stability.\n\n            The `_bwd_kernel` handles the backward pass for the attention mechanism. It calculates gradients for `Q`, `K`, and `V` using the chain rule and updates these gradients incrementally by processing the input in blocks similar to the forward pass.\n\n            The `attention` class serves as a wrapper for these kernel operations, supporting both forward and backward computations using PyTorch's autograd functionality. It handles grid setup and memory allocations for intermediate results.\n\n            The `BLOCK` constant typically set to 128 defines the tile size for the computation, which directly influences performance and memory usage. The context `ctx` stores necessary information across forward and backward calls to allow efficient gradient computation.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nBLOCK = 128\n\n@triton.jit\ndef _fwd_kernel(Q, K, V, Out, L, M, sm_scale, stride_qm, stride_kn, stride_vm, stride_vn, stride_om, stride_on, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_heads = Q.shape[0]\n    head_id = pid % num_heads\n    block_id = pid // num_heads\n\n    # Pointers to Q, K, V, and Out\n    Q += head_id * stride_qm + block_id * BLOCK_M\n    K += head_id * stride_kn\n    V += head_id * stride_vm\n    Out += head_id * stride_om + block_id * BLOCK_M\n\n    # Pointers to L and M\n    L += head_id * (Out.shape[1] // BLOCK_M) + block_id\n    M += head_id * (Out.shape[1] // BLOCK_M) + block_id\n\n    # Offsets for blocks\n    offsets_m = tl.arange(0, BLOCK_M)\n    offsets_n = tl.arange(0, BLOCK_N)\n    offsets_k = tl.arange(0, BLOCK_DMODEL)\n\n    # Load Q and V\n    q = tl.load(Q + offsets_m[:, None] * stride_qm + offsets_k[None, :] * BLOCK_DMODEL)\n    v = tl.load(V + offsets_n[None, :] * stride_vn + offsets_k[:, None] * BLOCK_DMODEL)\n\n    # Initialize L and M\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n\n    for start_n in range(0, K.shape[1], BLOCK_N):\n        # Load K\n        k = tl.load(K + start_n * stride_kn + offsets_k[:, None] * BLOCK_DMODEL + offsets_n[None, :] * BLOCK_N)\n\n        # Compute QK^T\n        qk = tl.dot(q, k, allow_tf32=True)\n        qk = qk * sm_scale\n\n        # Compute attention scores\n        qk = qk + tl.where(m_i[:, None] >= qk, 0, m_i[:, None] - qk)\n        m_i = tl.max(m_i, tl.max(qk, 1))\n\n        # Compute L\n        l_i_new = tl.logsumexp(qk, 1)\n        l_i = l_i + tl.exp(l_i_new - l_i)\n\n        # Compute softmax\n        p = tl.exp(qk - m_i[:, None])\n\n        # Compute Out\n        out = tl.dot(p, v, allow_tf32=True)\n        tl.store(Out + start_n * stride_on + offsets_m[:, None] * stride_om + offsets_n[None, :], out)\n\n    # Store L and M\n    tl.store(L, l_i)\n    tl.store(M, m_i)\n\n@triton.jit\ndef _bwd_kernel(Q, K, V, Out, GradOut, GradQ, GradK, GradV, L, M, sm_scale, stride_qm, stride_kn, stride_vm, stride_vn, stride_om, stride_on, stride_gqm, stride_gkn, stride_gvm, stride_gvn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_heads = Q.shape[0]\n    head_id = pid % num_heads\n    block_id = pid // num_heads\n\n    # Pointers to Q, K, V, Out, GradOut, GradQ, GradK, GradV\n    Q += head_id * stride_qm + block_id * BLOCK_M\n    K += head_id * stride_kn\n    V += head_id * stride_vm\n    Out += head_id * stride_om + block_id * BLOCK_M\n    GradOut += head_id * stride_on + block_id * BLOCK_M\n    GradQ += head_id * stride_gqm + block_id * BLOCK_M\n    GradK += head_id * stride_gkn\n    GradV += head_id * stride_gvm\n\n    # Pointers to L and M\n    L += head_id * (Out.shape[1] // BLOCK_M) + block_id\n    M += head_id * (Out.shape[1] // BLOCK_M) + block_id\n\n    # Offsets for blocks\n    offsets_m = tl.arange(0, BLOCK_M)\n    offsets_n = tl.arange(0, BLOCK_N)\n    offsets_k = tl.arange(0, BLOCK_DMODEL)\n\n    # Load Q and V\n    q = tl.load(Q + offsets_m[:, None] * stride_qm + offsets_k[None, :] * BLOCK_DMODEL)\n    v = tl.load(V + offsets_n[None, :] * stride_vn + offsets_k[:, None] * BLOCK_DMODEL)\n\n    # Load L and M\n    l_i = tl.load(L)\n    m_i = tl.load(M)\n\n    for start_n in range(0, K.shape[1], BLOCK_N):\n        # Load K\n        k = tl.load(K + start_n * stride_kn + offsets_k[:, None] * BLOCK_DMODEL + offsets_n[None, :] * BLOCK_N)\n\n        # Compute QK^T\n        qk = tl.dot(q, k, allow_tf32=True)\n        qk = qk * sm_scale\n\n        # Compute attention scores\n        qk = qk + tl.where(m_i[:, None] >= qk, 0, m_i[:, None] - qk)\n        p = tl.exp(qk - m_i[:, None])\n\n        # Load GradOut\n        grad_out = tl.load(GradOut + start_n * stride_on + offsets_m[:, None] * stride_om + offsets_n[None, :])\n\n        # Compute GradV\n        grad_v = tl.dot(p, grad_out, allow_tf32=True)\n        tl.atomic_add(GradV + start_n * stride_gvn + offsets_n[None, :] * stride_gvm + offsets_k[:, None] * BLOCK_DMODEL, grad_v)\n\n        # Compute GradQ\n        grad_q = tl.dot(grad_out, v, allow_tf32=True)\n        tl.atomic_add(GradQ + offsets_m[:, None] * stride_gqm + offsets_k[None, :] * BLOCK_DMODEL, grad_q)\n\n        # Compute GradK\n        grad_k = tl.dot(q, grad_out, allow_tf32=True)\n        tl.atomic_add(GradK + start_n * stride_gkn + offsets_k[:, None] * BLOCK_DMODEL + offsets_n[None, :] * BLOCK_N, grad_k)\n\n    # Compute GradL\n    grad_l = tl.load(GradOut + offsets_m[:, None] * stride_om + offsets_n[None, :])\n    grad_l = tl.sum(grad_l, 1)\n    tl.atomic_add(L, grad_l)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale,\n    L, M,\n    Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on,\n    Z, H, N_CTX, D0,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    # initialize offsets for store\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    # initialize pointer to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    stride_qh_2d = stride_qh // stride_qm // stride_qk\n\n    q_tile_ptr = tl.make_block_ptr(base=Q,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_qm, stride_qk),\n                                   offsets=(\n                                       off_hz * stride_qh_2d + start_m * BLOCK_M, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    k_tile_ptr = tl.make_block_ptr(base=K,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_kn, stride_kk),\n                                   offsets=(off_hz * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_N, BLOCK_DMODEL),\n                                   order=(1, 0))\n    v_tile_ptr = tl.make_block_ptr(base=V,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_vk, stride_vn),\n                                   offsets=(off_hz * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_N, BLOCK_DMODEL),\n                                   order=(1, 0))\n    out_tile_ptr = tl.make_block_ptr(base=Out,\n                                     shape=(D0, BLOCK_DMODEL),\n                                     strides=(stride_om, stride_on),\n                                     offsets=(off_hz * stride_qh_2d + start_m * BLOCK_M, 0),\n                                     block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                     order=(1, 0))\n    # load q: it will stay in SRAM throughout\n    q = tl.load(q_tile_ptr)\n\n    # loop over k, v and update accumulators\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        # -- compute qk ----\n        k = tl.load(k_tile_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= (\n            start_n + offs_n[None, :]), qk, float(\"-inf\"))\n        # compute new m\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        # correct old l\n        l_prev *= tl.exp(m_prev - m_curr)\n        # attention weights\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        # rescale operands of matmuls\n        l_rcp = 1. / l_curr\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None]\n        # update acc\n        p = p.to(tl.float16)\n        v = tl.load(v_tile_ptr, boundary_check=(0, 1))\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n        # update pointers\n        k_tile_ptr = tl.advance(k_tile_ptr, [BLOCK_N, 0])\n        v_tile_ptr = tl.advance(v_tile_ptr, [BLOCK_N, 0])\n    # rematerialize offsets to save registers\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    # write back l and m\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_prev)\n    tl.store(m_ptrs, m_prev)\n\n    acc = acc.to(tl.float16)\n    tl.store(out_tile_ptr, acc, boundary_check=(0, 1))\n\n\n@triton.jit\ndef _bwd_preprocess(\n    Out, DO, L,\n    NewDO, Delta,\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    # load\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n    denom = tl.load(L + off_m).to(tl.float32)\n    # compute\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    # write-back\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n    tl.store(Delta + off_m, delta)\n\n\n@triton.jit\ndef _bwd_kernel(\n    Q, K, V, sm_scale, Out, DO,\n    DQ, DK, DV,\n    L, M,\n    D,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    Z, H, N_CTX, D0,\n    num_block,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    # init tile_ptr\n    stride_qz_2d = stride_qz // stride_qm // stride_qk\n    stride_qh_2d = stride_qh // stride_qm // stride_qk\n\n    q_tile_ptr = tl.make_block_ptr(base=Q,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_qm, stride_qk),\n                                   offsets=(\n                                       off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    k_tile_ptr = tl.make_block_ptr(base=K,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_kn, stride_kk),\n                                   offsets=(\n                                       off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    v_tile_ptr = tl.make_block_ptr(base=V,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_vk, stride_vn),\n                                   offsets=(\n                                       off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    do_tile_ptr = tl.make_block_ptr(base=DO,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    dq_tile_ptr = tl.make_block_ptr(base=DQ,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    dk_tile_ptr = tl.make_block_ptr(base=DK,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    dv_tile_ptr = tl.make_block_ptr(base=DV,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    # offset pointers for batch/head\n    DQ += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = start_n * BLOCK_M\n        # initialize row/col offsets\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        # initialize pointers to value-like data\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n        # pointer to row-wise quantities in value-like data\n        D_ptrs = D + off_hz * N_CTX\n        m_ptrs = M + off_hz * N_CTX\n        # initialize dv amd dk\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        # k and v stay in SRAM throughout\n        k = tl.load(k_tile_ptr, boundary_check=(0, 1))\n        v = tl.load(v_tile_ptr, boundary_check=(0, 1))\n        # loop over rows\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            # load q, k, v, do on-chip\n            q = tl.load(q_tile_ptr, boundary_check=(0, 1))\n            # recompute p = softmax(qk, dim=-1).T\n            # NOTE: `do` is pre-divided by `l`; no normalization here\n            qk = tl.dot(q, tl.trans(k))\n            qk = tl.where(offs_m_curr[:, None] >= (\n                offs_n[None, :]), qk, float(\"-inf\"))\n            m = tl.load(m_ptrs + offs_m_curr)\n            p = tl.exp(qk * sm_scale - m[:, None])\n            # compute dv\n            do = tl.load(do_tile_ptr, boundary_check=(0, 1))\n            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n            # compute dp = dot(v, do)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            # compute ds = p * (dp - delta[:, None])\n            ds = p * dp * sm_scale\n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n            # compute dq\n            dq = tl.load(dq_tile_ptr)\n            dq += tl.dot(ds.to(tl.float16), k)\n            tl.store(dq_tile_ptr, dq)\n            # increment pointers\n            dq_ptrs += BLOCK_M * stride_qm\n            q_tile_ptr = tl.advance(q_tile_ptr, [BLOCK_M, 0])\n            do_tile_ptr = tl.advance(do_tile_ptr, [BLOCK_M, 0])\n            dq_tile_ptr = tl.advance(dq_tile_ptr, [BLOCK_M, 0])\n        q_tile_ptr = tl.advance(q_tile_ptr, [lo + (1 - num_block) * BLOCK_M, 0])\n        do_tile_ptr = tl.advance(do_tile_ptr, [lo + (1 - num_block) * BLOCK_M, 0])\n        dq_tile_ptr = tl.advance(dq_tile_ptr, [lo + (1 - num_block) * BLOCK_M, 0])\n        # increment tile pointers\n        k_tile_ptr = tl.advance(k_tile_ptr, [BLOCK_M, 0])\n        v_tile_ptr = tl.advance(v_tile_ptr, [BLOCK_M, 0])\n        # write-back\n        tl.store(dv_tile_ptr, dv.to(tl.float16), boundary_check=(0, 1))\n        tl.store(dk_tile_ptr, dk.to(tl.float16), boundary_check=(0, 1))\n        dv_tile_ptr = tl.advance(dv_tile_ptr, [BLOCK_M, 0])\n        dk_tile_ptr = tl.advance(dk_tile_ptr, [BLOCK_M, 0])\n\n\nclass _attention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, sm_scale):\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        assert Lk in {16, 32, 64, 128}\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty(\n            (q.shape[0] * q.shape[1], q.shape[2]),\n            device=q.device,\n            dtype=torch.float32)\n        m = torch.empty(\n            (q.shape[0] * q.shape[1], q.shape[2]),\n            device=q.device,\n            dtype=torch.float32)\n        num_warps = 4 if Lk <= 64 else 8\n        D0 = q.shape[0] * q.shape[1] * q.shape[2]\n        _fwd_kernel[grid](\n            q, k, v, sm_scale,\n            L, m,\n            o,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], D0,\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n            BLOCK_DMODEL=Lk, num_warps=num_warps,\n            num_stages=2,\n        )\n\n        ctx.save_for_backward(q, k, v, o, L, m)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = Lk\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        BLOCK = 128\n        q, k, v, o, l, m = ctx.saved_tensors\n        do = do.contiguous()\n        dq = torch.zeros_like(q, dtype=torch.float32)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        do_scaled = torch.empty_like(do)\n        delta = torch.empty_like(l)\n        D0 = q.shape[0] * q.shape[1] * q.shape[2]\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n            o, do, l,\n            do_scaled, delta,\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n        )\n        _bwd_kernel[(ctx.grid[1],)](\n            q, k, v, ctx.sm_scale,\n            o, do_scaled,\n            dq, dk, dv,\n            l, m,\n            delta,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], D0,\n            ctx.grid[0],\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n            num_stages=1,\n        )\n        return dq, dk, dv, None\n\n\nattention = _attention.apply\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel code provides a function `rotary_embedding` that applies rotary positional embeddings to query and key tensors used in transformers. The function handles two cases based on whether a key cache is provided. In the first case, without a cache, the `rotary_embedding_kernel` is used, while in the second case, with a cache, `fused_rotary_embedding_kernel_v2` is employed. The kernels perform similar operations: loading cosine and sine embeddings, applying them to query and key tensors to modify their values using the rotary embedding technique, and then storing the modified values. Inputs include the tensors for query (q) and key (k), cosine and sine tables for the embeddings, and optionally, a key cache (`k_cache`), block tables, and context lengths. Outputs are the rotated query and possibly updated key tensor in the cache. Key constants like `Q_HEAD_NUM` and `HEAD_DIM` define the number of heads and dimension of each head.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nQ_HEAD_NUM = 16  # Number of heads\nHEAD_DIM = 64    # Dimension of each head\n\n@triton.jit\ndef rotary_embedding_kernel(\n    q_ptr, k_ptr, cos_ptr, sin_ptr, q_head_num, head_dim,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    head_id = pid // (head_dim // BLOCK_SIZE)\n    block_id = pid % (head_dim // BLOCK_SIZE)\n\n    q_offset = (head_id * head_dim + block_id * BLOCK_SIZE) * tl.int8\n    k_offset = (head_id * head_dim + block_id * BLOCK_SIZE) * tl.int8\n    cos_offset = (block_id * BLOCK_SIZE) * tl.int8\n    sin_offset = (block_id * BLOCK_SIZE) * tl.int8\n\n    q = tl.load(q_ptr + q_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n    k = tl.load(k_ptr + k_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n    cos = tl.load(cos_ptr + cos_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=1.0)\n    sin = tl.load(sin_ptr + sin_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n\n    q_rotated = q * cos - k * sin\n    k_rotated = q * sin + k * cos\n\n    tl.store(q_ptr + q_offset, q_rotated, mask=tl.arange(0, BLOCK_SIZE) < head_dim)\n    tl.store(k_ptr + k_offset, k_rotated, mask=tl.arange(0, BLOCK_SIZE) < head_dim)\n\n@triton.jit\ndef fused_rotary_embedding_kernel_v2(\n    q_ptr, k_ptr, k_cache_ptr, cos_ptr, sin_ptr, q_head_num, head_dim,\n    block_table_ptr, context_length_ptr, block_size: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    head_id = pid // (head_dim // BLOCK_SIZE)\n    block_id = pid % (head_dim // BLOCK_SIZE)\n\n    q_offset = (head_id * head_dim + block_id * BLOCK_SIZE) * tl.int8\n    k_offset = (head_id * head_dim + block_id * BLOCK_SIZE) * tl.int8\n    cos_offset = (block_id * BLOCK_SIZE) * tl.int8\n    sin_offset = (block_id * BLOCK_SIZE) * tl.int8\n\n    q = tl.load(q_ptr + q_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n    k = tl.load(k_ptr + k_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n    cos = tl.load(cos_ptr + cos_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=1.0)\n    sin = tl.load(sin_ptr + sin_offset, mask=tl.arange(0, BLOCK_SIZE) < head_dim, other=0.0)\n\n    q_rotated = q * cos - k * sin\n    k_rotated = q * sin + k * cos\n\n    tl.store(q_ptr + q_offset, q_rotated, mask=tl.arange(0, BLOCK_SIZE) < head_dim)\n\n    block_idx = tl.load(block_table_ptr + head_id * block_size + block_id)\n    context_length = tl.load(context_length_ptr + head_id)\n    k_cache_offset = (head_id * head_dim + block_idx * BLOCK_SIZE) * tl.int8\n\n    tl.store(k_cache_ptr + k_cache_offset, k_rotated, mask=tl.arange(0, BLOCK_SIZE) < context_length)\n\n### Python Wrapper\n\nimport torch\n\ndef rotary_embedding(q, k, cos, sin, k_cache=None, block_table=None, context_length=None, block_size=None):\n    q_head_num, head_dim = q.shape\n    assert q_head_num == Q_HEAD_NUM\n    assert head_dim == HEAD_DIM\n\n    if k_cache is None:\n        grid = (Q_HEAD_NUM * (HEAD_DIM // 32),)\n        rotary_embedding_kernel[grid](\n            q, k, cos, sin, Q_HEAD_NUM, HEAD_DIM, BLOCK_SIZE=32\n        )\n    else:\n        assert block_table is not None\n        assert context_length is not None\n        assert block_size is not None\n        grid = (Q_HEAD_NUM * (HEAD_DIM // 32),)\n        fused_rotary_embedding_kernel_v2[grid](\n            q, k, k_cache, cos, sin, Q_HEAD_NUM, HEAD_DIM, block_table, context_length, block_size, BLOCK_SIZE=32\n        )\n\n    return q, k\n", "label": "from typing import Optional\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef rotary_embedding_kernel(\n    q,\n    k,\n    cos,\n    sin,\n    q_token_stride,\n    q_head_stride,\n    k_token_stride,\n    k_head_stride,\n    head_dim_stride,\n    cos_token_stride,\n    cos_stride,\n    q_total_tokens,\n    Q_HEAD_NUM: tl.constexpr,\n    KV_GROUP_NUM: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    BLOCK_TOKENS: tl.constexpr,  # token range length\n):\n    cur_head_idx = tl.program_id(0)\n    cur_token_block_idx = tl.program_id(1)\n\n    tokens_range = cur_token_block_idx * BLOCK_TOKENS + tl.arange(0, BLOCK_TOKENS)\n    dim_range0 = tl.arange(0, HEAD_DIM // 2)\n    dim_range1 = tl.arange(HEAD_DIM // 2, HEAD_DIM)\n\n    off_cos_sin = tokens_range[:, None] * cos_token_stride + dim_range0[None, :] * cos_stride\n    loaded_cos = tl.load(cos + off_cos_sin, mask=(tokens_range[:, None] < q_total_tokens), other=0.0)\n    loaded_sin = tl.load(sin + off_cos_sin, mask=(tokens_range[:, None] < q_total_tokens), other=0.0)\n\n    off_q0 = (\n        tokens_range[:, None, None] * q_token_stride\n        + cur_head_idx * q_head_stride\n        + dim_range0[None, None, :] * head_dim_stride\n    )\n    off_q1 = (\n        tokens_range[:, None, None] * q_token_stride\n        + cur_head_idx * q_head_stride\n        + dim_range1[None, None, :] * head_dim_stride\n    )\n    loaded_q0 = tl.load(\n        q + off_q0,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n        other=0.0,\n    )\n    loaded_q1 = tl.load(\n        q + off_q1,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n        other=0.0,\n    )\n    out_q0 = loaded_q0 * loaded_cos[:, None, :] - loaded_q1 * loaded_sin[:, None, :]\n    out_q1 = loaded_q0 * loaded_sin[:, None, :] + loaded_q1 * loaded_cos[:, None, :]\n\n    tl.store(\n        q + off_q0,\n        out_q0,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n    )\n    tl.store(\n        q + off_q1,\n        out_q1,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n    )\n\n    handle_kv = cur_head_idx % KV_GROUP_NUM == 0\n    if handle_kv:\n        k_head_idx = cur_head_idx // KV_GROUP_NUM\n        off_k0 = (\n            tokens_range[:, None, None] * k_token_stride\n            + k_head_idx * k_head_stride\n            + dim_range0[None, None, :] * head_dim_stride\n        )\n        off_k1 = (\n            tokens_range[:, None, None] * k_token_stride\n            + k_head_idx * k_head_stride\n            + dim_range1[None, None, :] * head_dim_stride\n        )\n        loaded_k0 = tl.load(\n            k + off_k0,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n            other=0.0,\n        )\n        loaded_k1 = tl.load(\n            k + off_k1,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n            other=0.0,\n        )\n        out_k0 = loaded_k0 * loaded_cos[:, None, :] - loaded_k1 * loaded_sin[:, None, :]\n        out_k1 = loaded_k0 * loaded_sin[:, None, :] + loaded_k1 * loaded_cos[:, None, :]\n        tl.store(\n            k + off_k0,\n            out_k0,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n        )\n        tl.store(\n            k + off_k1,\n            out_k1,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n        )\n\n\n@triton.jit\ndef fused_rotary_embedding_kernel_v2(\n    q,\n    k,\n    cos,\n    sin,\n    kv_cache,\n    BLOCK_TABLES,\n    context_lengths,\n    q_token_stride,\n    q_head_stride,\n    k_token_stride,\n    k_head_stride,\n    head_dim_stride,\n    cos_token_stride,\n    cos_stride,\n    cacheb_stride,\n    cacheh_stride,\n    cachebs_stride,\n    cached_stride,\n    bts_stride,\n    btb_stride,\n    block_size,\n    q_total_tokens,\n    Q_HEAD_NUM: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n):\n    block_head_index = tl.program_id(0)\n    if block_head_index >= Q_HEAD_NUM:\n        return\n    block_token_index = tl.program_id(1)\n\n    dim_range0 = tl.arange(0, HEAD_DIM // 2)\n    dim_range1 = tl.arange(HEAD_DIM // 2, HEAD_DIM)\n\n    off_q0 = block_token_index * q_token_stride + block_head_index * q_head_stride + dim_range0 * head_dim_stride\n    off_q1 = block_token_index * q_token_stride + block_head_index * q_head_stride + dim_range1 * head_dim_stride\n    off_k0 = block_token_index * k_token_stride + block_head_index * k_head_stride + dim_range0 * head_dim_stride\n    off_k1 = block_token_index * k_token_stride + block_head_index * k_head_stride + dim_range1 * head_dim_stride\n\n    loaded_q0 = tl.load(\n        q + off_q0,\n    )\n    loaded_q1 = tl.load(\n        q + off_q1,\n    )\n\n    loaded_k0 = tl.load(\n        k + off_k0,\n    )\n\n    loaded_k1 = tl.load(\n        k + off_k1,\n    )\n\n    off_cos_sin = block_token_index * cos_token_stride + dim_range0 * cos_stride\n\n    loaded_cos = tl.load(cos + off_cos_sin, mask=(block_token_index < q_total_tokens), other=0.0)\n    loaded_sin = tl.load(sin + off_cos_sin, mask=(block_token_index < q_total_tokens), other=0.0)\n\n    out_q0 = loaded_q0 * loaded_cos - loaded_q1 * loaded_sin\n    out_q1 = loaded_q0 * loaded_sin + loaded_q1 * loaded_cos\n\n    out_k0 = loaded_k0 * loaded_cos - loaded_k1 * loaded_sin\n    out_k1 = loaded_k0 * loaded_sin + loaded_k1 * loaded_cos  # total_tokens, head_num, head_dim\n\n    past_kv_seq_len = tl.load(context_lengths + block_token_index) - 1\n\n    last_block_idx = past_kv_seq_len // block_size\n    block_table_ptr = BLOCK_TABLES + block_token_index * bts_stride\n    block_ids = tl.load(block_table_ptr + last_block_idx * btb_stride, mask=(block_token_index < q_total_tokens))\n    offsets_in_last_block = (past_kv_seq_len % block_size) * cachebs_stride\n\n    kv_range0 = (\n        block_ids * cacheb_stride\n        + block_head_index * cacheh_stride\n        + offsets_in_last_block\n        + dim_range0 * cached_stride\n    )\n    kv_range1 = (\n        block_ids * cacheb_stride\n        + block_head_index * cacheh_stride\n        + offsets_in_last_block\n        + dim_range1 * cached_stride\n    )\n\n    tl.store(\n        kv_cache + kv_range0,\n        out_k0,\n    )\n    tl.store(\n        kv_cache + kv_range1,\n        out_k1,\n    )\n\n    # concat\n    tl.store(\n        q + off_q0,\n        out_q0,\n    )\n    tl.store(\n        q + off_q1,\n        out_q1,\n    )\n\n\ndef rotary_embedding(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    k_cache: Optional[torch.Tensor] = None,\n    block_tables: Optional[torch.Tensor] = None,\n    kv_lengths: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Args:\n        q: query tensor, [total_tokens, head_num, head_dim]\n        k: key tensor, [total_tokens, kv_head_num, head_dim]\n        cos: cosine for rotary embedding, [max_position_len, head_dim]\n        sin: sine for rotary embedding, [max_position_len, head_dim]\n        k_cache (torch.Tensor):  Blocked key cache. [num_blocks, num_kv_heads, block_size, head_dim]\n        kv_lengths, Past key/value sequence lengths plus current sequence length for each sequence. [bsz]\n        block_tables: Block tables for each sequence. [bsz, max_blocks_per_sequence]\n    \"\"\"\n    q_total_tokens, q_head_num, head_dim = q.shape\n    assert q.size(0) == k.size(0)\n    BLOCK_TOKENS = 4\n\n    if head_dim >= 512:\n        num_warps = 16\n    elif head_dim >= 256:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    k_head_num = k.size(1)\n    q_token_stride, q_head_stride, head_dim_stride = q.stride()\n    k_token_stride, k_head_stride, _ = k.stride()\n    cos_token_stride, cos_stride = cos.stride()\n\n    assert q_head_num % k_head_num == 0\n    kv_group_num = q_head_num // k_head_num\n\n    if k_cache == None:\n        grid = lambda META: (\n            q_head_num,\n            triton.cdiv(q_total_tokens, META[\"BLOCK_TOKENS\"]),\n        )\n        rotary_embedding_kernel[grid](\n            q,\n            k,\n            cos,\n            sin,\n            q_token_stride,\n            q_head_stride,\n            k_token_stride,\n            k_head_stride,\n            head_dim_stride,\n            cos_token_stride,\n            cos_stride,\n            q_total_tokens,\n            Q_HEAD_NUM=q_head_num,\n            KV_GROUP_NUM=kv_group_num,\n            HEAD_DIM=head_dim,\n            BLOCK_TOKENS=BLOCK_TOKENS,\n            num_warps=num_warps,\n        )\n    else:\n        grid = (triton.next_power_of_2(q_head_num), q_total_tokens)\n        fused_rotary_embedding_kernel_v2[grid](\n            q,\n            k,\n            cos,\n            sin,\n            k_cache,\n            block_tables,\n            kv_lengths,\n            q_token_stride,\n            q_head_stride,\n            k_token_stride,\n            k_head_stride,\n            head_dim_stride,\n            cos_token_stride,\n            cos_stride,\n            k_cache.stride(0),\n            k_cache.stride(1),\n            k_cache.stride(2),\n            k_cache.stride(3),\n            block_tables.stride(0),\n            block_tables.stride(1),\n            k_cache.size(-2),\n            q_total_tokens,\n            Q_HEAD_NUM=q_head_num,\n            HEAD_DIM=head_dim,\n            num_warps=num_warps,\n        )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel `chunk_global_reversed_cumsum_scalar_kernel` performs a reversed cumulative sum operation on a 3D input tensor `s` with dimensions (B, H, T). The kernel processes each (B, H) independently, iterating backwards over the last dimension T in chunks of size `BT`. During each iteration, it computes the cumulative sum from the current chunk to the beginning of the dimension and stores the result in the output tensor `o`. The input tensor `s` is assumed to be of shape (B, H, T), and the output tensor `z` is of the same shape as `s`. The `chunk_global_reversed_cumsum_scalar` function wraps this kernel, handling the initialization of the output tensor and launching the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(\n    s_ptr,  # Pointer to the input tensor\n    o_ptr,  # Pointer to the output tensor\n    B,      # Batch size\n    H,      # Number of heads\n    T,      # Sequence length\n    BT,     # Chunk size\n    BLOCK_SIZE: tl.constexpr\n):\n    # Get the batch and head indices\n    pid = tl.program_id(0)\n    bid = pid // H\n    hid = pid % H\n\n    # Compute the starting and ending indices for the chunk\n    chunk_start = T - (pid % (T // BT)) * BT\n    chunk_end = max(chunk_start - BT, 0)\n\n    # Compute the offset for the current (B, H) block\n    offset = bid * H * T + hid * T\n\n    # Initialize the cumulative sum\n    cumsum = 0.0\n\n    # Iterate over the chunk in reverse order\n    for t in range(chunk_start - 1, chunk_end - 1, -1):\n        # Compute the global index\n        idx = offset + t\n\n        # Load the value from the input tensor\n        s_val = tl.load(s_ptr + idx)\n\n        # Update the cumulative sum\n        cumsum += s_val\n\n        # Store the result in the output tensor\n        tl.store(o_ptr + idx, cumsum)\n\nimport torch\n\ndef chunk_global_reversed_cumsum_scalar(s, BT):\n    # Get the dimensions of the input tensor\n    B, H, T = s.shape\n\n    # Initialize the output tensor\n    o = torch.zeros_like(s)\n\n    # Define the grid and block sizes\n    grid = (B * H, 1, 1)\n    block = (1, 1, 1)\n\n    # Launch the kernel\n    chunk_global_reversed_cumsum_scalar_kernel[grid, block](\n        s, o, B, H, T, BT, BLOCK_SIZE=1\n    )\n\n    return o\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=4),\n    ],\n    key=[]\n)\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(\n    s,\n    o,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n\n\ndef chunk_global_reversed_cumsum_scalar(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T = s.shape\n    dtype = dtype or s.dtype\n    grid = (B * H,)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_reversed_cumsum_scalar_kernel[grid](\n        s, z,\n        T=T\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The function `flash_attn_triton` implements a flash attention mechanism using Triton. The key inputs are matrices `q`, `k`, and `v`, representing queries, keys, and values. It performs scaled dot-product attention optionally with causal masking. The function uses a Triton kernel `_fwd_kernel` to efficiently compute the attention output. The result is stored in `o`, which has the same shape as `q`. Major parameters include `BLOCK_M` and `BLOCK_N`, determining the block sizes for computations.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for forward pass of flash attention\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, O,  # Pointers to matrices\n    stride_qb, stride_qh, stride_qm,  # Strides for Q\n    stride_kb, stride_kh, stride_kn,  # Strides for K\n    stride_vb, stride_vh, stride_vn,  # Strides for V\n    stride_ob, stride_oh, stride_om,  # Strides for O\n    N_CTX,  # Matrix width\n    BLOCK_M: tl.constexpr,  # Block size for M\n    BLOCK_N: tl.constexpr,  # Block size for N\n    HEAD_DIM: tl.constexpr,  # Head dimension\n    CAUSAL: tl.constexpr  # Causal masking\n):\n    # Matrix multiplication\n    pid = tl.program_id(axis=0)\n    bid = tl.program_id(axis=1)\n    hid = tl.program_id(axis=2)\n\n    # Compute the block indices\n    start_m = pid * BLOCK_M\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_b = bid\n    offs_h = hid\n\n    # Initialize pointers\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_qm, stride_qh),\n        offsets=(start_m, 0),\n        block_shape=(BLOCK_M, HEAD_DIM),\n        order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K,\n        shape=(HEAD_DIM, N_CTX),\n        strides=(stride_kh, stride_kn),\n        offsets=(0, 0),\n        block_shape=(HEAD_DIM, BLOCK_N),\n        order=(0, 1)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_vn, stride_vh),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, HEAD_DIM),\n        order=(1, 0)\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=O,\n        shape=(N_CTX, HEAD_DIM),\n        strides=(stride_om, stride_oh),\n        offsets=(start_m, 0),\n        block_shape=(BLOCK_M, HEAD_DIM),\n        order=(1, 0)\n    )\n\n    # Load Q, K, V blocks\n    q = tl.load(Q_block_ptr)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n\n    # Compute attention scores\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(0, HEAD_DIM, 16):\n        qk = tl.dot(q[:, i:i+16], k[i:i+16, :])\n        acc += qk\n\n    # Apply scaling\n    acc *= 1.0 / tl.sqrt(HEAD_DIM)\n\n    # Apply causal masking if needed\n    if CAUSAL:\n        mask = (offs_m[:, None] >= offs_n[None, :]).to(tl.float32)\n        acc = acc * mask\n\n    # Softmax\n    acc = tl.softmax(acc, axis=1)\n\n    # Compute output\n    o = tl.dot(acc, v)\n\n    # Store the result\n    tl.store(O_block_ptr, o)\n\n# Wrapper function for flash attention\ndef flash_attn_triton(q, k, v, o, BLOCK_M, BLOCK_N, causal=False):\n    # Get dimensions\n    B, H, N_CTX, HEAD_DIM = q.shape\n\n    # Launch the kernel\n    grid = (N_CTX // BLOCK_M, B, H)\n    _fwd_kernel[grid](\n        q, k, v, o,\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        v.stride(0), v.stride(1), v.stride(2),\n        o.stride(0), o.stride(1), o.stride(2),\n        N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM, causal\n    )\n\n# Example usage\nimport torch\n\n# Example tensors\nB, H, N_CTX, HEAD_DIM = 2, 4, 128, 64\nq = torch.randn((B, H, N_CTX, HEAD_DIM), device='cuda')\nk = torch.randn((B, H, N_CTX, HEAD_DIM), device='cuda')\nv = torch.randn((B, H, N_CTX, HEAD_DIM), device='cuda')\no = torch.empty((B, H, N_CTX, HEAD_DIM), device='cuda')\n\n# Call the flash attention function\nflash_attn_triton(q, k, v, o, BLOCK_M=16, BLOCK_N=16, causal=True)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef flash_attn_triton(q, k, v, causal=True, sm_scale=1):\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n\n    o = torch.empty_like(q)\n\n    BLOCK_M = 128\n    BLOCK_N = 64\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    num_warps = 4 if Lk <= 64 else 8\n    _fwd_kernel[grid](\n        q, k, v, sm_scale,\n        L,\n        o,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n        q.shape[0], q.shape[1], q.shape[2],\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, DIM=Lk,\n        IS_CAUSAL=causal,\n        num_warps=num_warps,\n        num_stages=4)\n\n    return o\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale,\n    L,\n    O,\n    stride_q_bs, stride_q_head, stride_q_seqlen, stride_q_dim,\n    stride_k_bs, stride_k_head, stride_k_seqlen, stride_k_dim,\n    stride_v_bs, stride_v_head, stride_v_seqlen, stride_v_dim,\n    stride_o_bs, stride_o_head, stride_o_seqlen, stride_o_dim,\n    BS, HEAD, SEQLEN,\n    BLOCK_M: tl.constexpr,\n    DIM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_bs_head = tl.program_id(1)\n\n    qkv_base_offset = off_bs_head * stride_q_head\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qkv_base_offset,\n        shape=(SEQLEN, DIM),\n        strides=(stride_q_seqlen, stride_q_dim),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, DIM),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qkv_base_offset,\n        shape=(DIM, SEQLEN),\n        strides=(stride_k_dim, stride_k_seqlen),\n        offsets=(0, 0),\n        block_shape=(DIM, BLOCK_N),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qkv_base_offset,\n        shape=(SEQLEN, DIM),\n        strides=(stride_k_seqlen, stride_v_dim),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, DIM),\n        order=(1, 0),\n    )\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, BLOCK_N)\n    max = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    denom = tl.zeros([BLOCK_M], dtype=tl.float32)\n    out_buffer = tl.zeros([BLOCK_M, DIM], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(tl.float16)\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else SEQLEN\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(off_m[:, None] >= (start_n + off_n[None, :]), qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n\n        max_new = tl.maximum(max, tl.max(qk, 1))\n        alpha = tl.math.exp2(max - max_new)\n        nume = tl.math.exp2(qk - max_new[:, None])\n        out_scale = denom * 0 + alpha\n        out_buffer *= out_scale[:, None]\n        out_buffer += tl.dot(nume.to(tl.float16), v)\n        denom = denom * alpha + tl.sum(nume, 1)\n        max = max_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    out_buffer = out_buffer / denom[:, None]\n    l_ptr = L + off_bs_head * SEQLEN + off_m\n    tl.store(l_ptr, max + tl.math.log2(denom))\n    O_block_ptr = tl.make_block_ptr(\n        base=O + qkv_base_offset,\n        shape=(SEQLEN, DIM),\n        strides=(stride_o_seqlen, stride_o_dim),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, DIM),\n        order=(1, 0),\n    )\n    tl.store(O_block_ptr, out_buffer.to(tl.float16))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton-based code efficiently processes query (q) and key (k) matrices by converting them to int8 format for reduced precision and space saving. The process involves two primary Triton kernels: q_kernel_per_block_int8 and k_kernel_per_block_int8. Each kernel takes input tensors, scales them, quantizes them into int8, and stores both the quantized data and scaling factors.\n\n        The per_block_int8 function manages this conversion process. It takes q and k tensors, prepares int8 tensor outputs and corresponding scaling factors, reshapes the inputs for uniform processing, and then launches the kernels.\n\n        Main variables/functions:\n        - q_kernel_per_block_int8: Converts the query matrix block by block into int8.\n        - k_kernel_per_block_int8: Converts the key matrix block by block into int8.\n        - per_block_int8: Wrapper function that handles input reshaping, kernel execution, and output storage.\n\n        Inputs:\n        - q: Query matrix (Tensor).\n        - k: Key matrix (Tensor).\n        - BLKQ: Block size for query processing.\n        - BLKK: Block size for key processing.\n\n        Outputs:\n        - q_int8: Int8 representation of query matrix.\n        - q_scale: Scaling factors for query blocks.\n        - k_int8: Int8 representation of key matrix.\n        - k_scale: Scaling factors for key blocks.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef q_kernel_per_block_int8(q, q_int8, q_scale, BLKQ: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLKQ\n    q_block = tl.load(q + block_start : block_start + BLKQ)\n    max_val = tl.max(q_block, axis=0)\n    min_val = tl.min(q_block, axis=0)\n    scale = (max_val - min_val) / 255.0\n    q_scaled = (q_block - min_val) / scale\n    q_int8_block = tl.cast(tl.floor(q_scaled + 0.5), tl.int8)\n    tl.store(q_int8 + block_start, q_int8_block)\n    tl.store(q_scale + pid, scale)\n\n@triton.jit\ndef k_kernel_per_block_int8(k, k_int8, k_scale, BLKK: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLKK\n    k_block = tl.load(k + block_start : block_start + BLKK)\n    max_val = tl.max(k_block, axis=0)\n    min_val = tl.min(k_block, axis=0)\n    scale = (max_val - min_val) / 255.0\n    k_scaled = (k_block - min_val) / scale\n    k_int8_block = tl.cast(tl.floor(k_scaled + 0.5), tl.int8)\n    tl.store(k_int8 + block_start, k_int8_block)\n    tl.store(k_scale + pid, scale)\n\nimport torch\n\ndef per_block_int8(q, k, BLKQ, BLKK):\n    # Ensure the inputs are on the same device\n    device = q.device\n\n    # Get the shapes of the input tensors\n    q_shape = q.shape\n    k_shape = k.shape\n\n    # Calculate the number of blocks for q and k\n    num_q_blocks = (q_shape[0] + BLKQ - 1) // BLKQ\n    num_k_blocks = (k_shape[0] + BLKK - 1) // BLKK\n\n    # Prepare the output tensors\n    q_int8 = torch.empty((q_shape[0], q_shape[1]), dtype=torch.int8, device=device)\n    q_scale = torch.empty((num_q_blocks, q_shape[1]), dtype=torch.float32, device=device)\n    k_int8 = torch.empty((k_shape[0], k_shape[1]), dtype=torch.int8, device=device)\n    k_scale = torch.empty((num_k_blocks, k_shape[1]), dtype=torch.float32, device=device)\n\n    # Launch the kernels\n    grid_q = (num_q_blocks, 1, 1)\n    grid_k = (num_k_blocks, 1, 1)\n\n    q_kernel_per_block_int8[grid_q](q, q_int8, q_scale, BLKQ)\n    k_kernel_per_block_int8[grid_k](k, k_int8, k_scale, BLKK)\n\n    return q_int8, q_scale, k_int8, k_scale\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef q_kernel_per_block_int8(X, X_int8, BLK: tl.constexpr, Scale, L, C: tl.constexpr, scale_stride):\n    # Triton kernel for processing block of query matrix in int8\n    off_b = tl.program_id(1) \n    off_blk = tl.program_id(0)\n    x_offset = off_b * L * C \n    offs_m = off_blk*BLK + tl.arange(0, BLK)\n    offs_k = tl.arange(0, C)\n\n    x_ptrs = X + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    x_int8_ptrs = X_int8 + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    scale_ptrs = Scale + off_b * scale_stride + off_blk  \n\n    x = tl.load(x_ptrs, mask=offs_m[:, None] < L)\n    x *= (C**-0.5 * 1.44269504)\n    scale = tl.max(tl.abs(x)) / 127.\n    x_int8 = x / scale\n    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)\n    x_int8 = x_int8.to(tl.int8)\n    tl.store(x_int8_ptrs, x_int8, mask=offs_m[:, None] < L)\n    tl.store(scale_ptrs, scale)\n\n@triton.jit\ndef k_kernel_per_block_int8(X, X_int8, BLK: tl.constexpr, Scale, L, C: tl.constexpr, scale_stride):\n    # Triton kernel for processing block of key matrix in int8\n    off_b = tl.program_id(1) \n    off_blk = tl.program_id(0)\n    x_offset = off_b * L * C \n    offs_m = off_blk*BLK + tl.arange(0, BLK)\n    offs_k = tl.arange(0, C)\n\n    x_ptrs = X + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    x_int8_ptrs = X_int8 + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    scale_ptrs = Scale + off_b * scale_stride + off_blk  \n\n    x = tl.load(x_ptrs, mask=offs_m[:, None] < L)\n    scale = tl.max(tl.abs(x)) / 127.\n    x_int8 = x / scale\n    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)\n    x_int8 = x_int8.to(tl.int8)\n    tl.store(x_int8_ptrs, x_int8, mask=offs_m[:, None] < L)\n    tl.store(scale_ptrs, scale)\n\n\ndef per_block_int8(q, k, BLKQ=128, BLKK=64):\n    # Function to initialize and launch Triton kernels for processing q and k\n    q_int8 = torch.empty_like(q, dtype=torch.int8)\n    k_int8 = q_int8.clone()\n\n    if q.dim() == 3:\n        q_scale = torch.empty((q.shape[-3], (q.shape[-2] + BLKQ - 1) // BLKQ, 1), device=q.device, dtype=torch.float32)\n        k_scale = torch.empty((k.shape[-3], (k.shape[-2] + BLKK - 1) // BLKK, 1), device=q.device, dtype=torch.float32)\n    elif q.dim() == 4:\n        q_scale = torch.empty((q.shape[-4], q.shape[-3], (q.shape[-2] + BLKQ - 1) // BLKQ, 1), device=q.device, dtype=torch.float32)\n        k_scale = torch.empty((k.shape[-4], k.shape[-3], (k.shape[-2] + BLKK - 1) // BLKK, 1), device=q.device, dtype=torch.float32)\n\n    q = q.view(-1, q.shape[-2], q.shape[-1])\n    k = k.view(-1, k.shape[-2], k.shape[-1])\n\n    B, L, C = q.shape\n    grid = ((L+BLKQ-1)//BLKQ, B, )\n    q_kernel_per_block_int8[grid](\n        q, \n        q_int8,\n        BLKQ,\n        q_scale,\n        L, C, q_scale.stride(0) if q_scale.dim() == 3 else q_scale.stride(1),\n    )\n\n    grid = ((L+BLKK-1)//BLKK, B, )\n    k_kernel_per_block_int8[grid](\n        k, \n        k_int8,\n        BLKK,\n        k_scale,\n        L, C, k_scale.stride(0) if k_scale.dim() == 3 else k_scale.stride(1),\n    )\n\n    return q_int8, q_scale, k_int8, k_scale\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel for performing an attention forward pass, specifically implementing the scaled dot-product attention mechanism. The main function, `forward(q, k, v, q_scale, k_scale)`, takes query (`q`), key (`k`), and value (`v`) tensors, along with scaling factors for query and key. It uses Triton to parallelize the computation over a grid, where each block computes partial results for the attention mechanism, accumulating weighted values into an output tensor `o`. The `_attn_fwd` kernel operates on blocks of size `BLOCK_M` by `BLOCK_N` and adjusts the attention scores using scaling factors and masks to ensure numerical stability and attention weight correctness. It relies on helper function `_attn_fwd_inner` to process stages of attention computation, updating accumulated values (`acc`), normalization factors (`l_i`), and maximum scores (`m_i`) iteratively.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nBLOCK_M = 128\nBLOCK_N = 128\nBLOCK_K = 32\nGROUP_SIZE_M = 8\n\n@triton.jit\ndef _attn_fwd_inner(\n    q_ptr, k_ptr, v_ptr, o_ptr,\n    q_scale, k_scale,\n    stride_qm, stride_kn, stride_vm, stride_vn, stride_om, stride_on,\n    M, N, K,\n    m_i, l_i,\n    acc,\n    offset_m, offset_n,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    # Pointers to the start of the block\n    q_block_ptr = q_ptr + offset_m * stride_qm\n    k_block_ptr = k_ptr + offset_n * stride_kn\n    v_block_ptr = v_ptr + offset_n * stride_vm\n\n    # Load the query and key blocks\n    q = tl.load(q_block_ptr, mask=offset_m + tl.arange(0, BLOCK_M) < M, other=0.0)\n    k = tl.load(k_block_ptr, mask=offset_n + tl.arange(0, BLOCK_N) < N, other=0.0)\n\n    # Compute the attention scores\n    q = q * q_scale\n    k = k * k_scale\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        qk = tl.dot(q, k)\n        qk = qk * (1.0 / tl.sqrt(K))\n        qk = qk + m_i\n        qk = tl.exp(qk - l_i)\n        acc = acc + qk\n\n    # Load the value block\n    v = tl.load(v_block_ptr, mask=offset_n + tl.arange(0, BLOCK_N) < N, other=0.0)\n\n    # Compute the weighted sum\n    o = tl.dot(acc, v)\n\n    # Store the result\n    o_ptr = o_ptr + offset_m * stride_om + offset_n * stride_on\n    tl.store(o_ptr, o, mask=offset_m + tl.arange(0, BLOCK_M) < M)\n\n@triton.jit\ndef _attn_fwd(\n    q_ptr, k_ptr, v_ptr, o_ptr,\n    q_scale, k_scale,\n    stride_qm, stride_kn, stride_vm, stride_vn, stride_om, stride_on,\n    M, N, K,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offset_m = pid_m * BLOCK_M\n    offset_n = pid_n * BLOCK_N\n\n    m_i = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    l_i = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        _attn_fwd_inner(\n            q_ptr, k_ptr, v_ptr, o_ptr,\n            q_scale, k_scale,\n            stride_qm, stride_kn, stride_vm, stride_vn, stride_om, stride_on,\n            M, N, K,\n            m_i, l_i,\n            acc,\n            offset_m, offset_n,\n            BLOCK_M, BLOCK_N, BLOCK_K, GROUP_SIZE_M\n        )\n\ndef forward(q, k, v, q_scale, k_scale):\n    M, K = q.shape\n    N, _ = v.shape\n    o = triton.empty((M, N), dtype=q.dtype, device=q.device)\n\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n    _attn_fwd[grid](\n        q, k, v, o,\n        q_scale, k_scale,\n        q.stride(0), k.stride(0), v.stride(0), v.stride(1), o.stride(0), o.stride(1),\n        M, N, K,\n        BLOCK_M, BLOCK_N, BLOCK_K, GROUP_SIZE_M\n    )\n\n    return o\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n        K_scale_ptr += lo // BLOCK_N\n        K_ptrs += HEAD_DIM * lo\n        V_ptrs += HEAD_DIM * lo\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask=k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        if STAGE == 2:\n            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n            qk = qk + tl.where(mask, 0, -1.0e6)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask=(offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        \n        acc += tl.dot(p, v.to(tl.float16), out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i, m_i\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N)\n    \n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask=(offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX)\n    acc, l_i, _ = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                  start_m,  \n                                  BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                  2, offs_m, offs_n, N_CTX)\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask=(offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.bfloat16)\n    stage = 3\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=3)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel 'rotary_kernel' performs rotary positional encoding on input tensor X using cosine and sine values. It operates with varying sequence lengths (varlen) or fixed lengths. The kernel processes data in blocks, handling interleaved or non-interleaved data layouts, and considers optional conjugate transformation. The 'apply_rotary' function is a wrapper that sets up the necessary parameters and configurations for the kernel to execute on the input tensors, handling various input shapes and conditions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rotary_kernel(\n    X_ptr,  # Pointer to the input tensor\n    cos_ptr,  # Pointer to the cosine values\n    sin_ptr,  # Pointer to the sine values\n    Y_ptr,  # Pointer to the output tensor\n    N,  # Number of elements in the sequence\n    D,  # Dimension of the input tensor\n    varlen,  # Flag for variable length sequences\n    interleaved,  # Flag for interleaved data layout\n    conjugate,  # Flag for conjugate transformation\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallel processing\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the input data\n    X = tl.load(X_ptr + offsets, mask=offsets < N * D, other=0.0)\n\n    # Compute the indices for cos and sin\n    seq_idx = offsets // D\n    dim_idx = offsets % D\n\n    # Load the cosine and sine values\n    cos = tl.load(cos_ptr + seq_idx, mask=seq_idx < N, other=1.0)\n    sin = tl.load(sin_ptr + seq_idx, mask=seq_idx < N, other=0.0)\n\n    # Apply the rotary positional encoding\n    if interleaved:\n        X_even = X[::2]\n        X_odd = X[1::2]\n        Y_even = X_even * cos - X_odd * sin\n        Y_odd = X_even * sin + X_odd * cos\n        Y = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        Y[::2] = Y_even\n        Y[1::2] = Y_odd\n    else:\n        X_even = X[::2]\n        X_odd = X[1::2]\n        Y_even = X_even * cos - X_odd * sin\n        Y_odd = X_even * sin + X_odd * cos\n        Y = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        Y[::2] = Y_even\n        Y[1::2] = Y_odd\n\n    # Apply conjugate transformation if needed\n    if conjugate:\n        Y = tl.conj(Y)\n\n    # Store the result\n    tl.store(Y_ptr + offsets, Y, mask=offsets < N * D)\n\nimport torch\n\ndef apply_rotary(X, cos, sin, varlen=False, interleaved=True, conjugate=False):\n    N, D = X.shape\n    assert cos.shape == (N,)\n    assert sin.shape == (N,)\n\n    # Convert tensors to Triton-compatible format\n    X_triton = X.contiguous().view(-1)\n    cos_triton = cos.contiguous()\n    sin_triton = sin.contiguous()\n    Y_triton = torch.empty_like(X_triton)\n\n    # Define the grid and block sizes\n    BLOCK_SIZE = 1024\n    grid_size = (N * D + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    rotary_kernel[grid_size, BLOCK_SIZE](\n        X_triton, cos_triton, sin_triton, Y_triton, N, D, varlen, interleaved, conjugate, BLOCK_SIZE\n    )\n\n    # Reshape the output tensor\n    Y = Y_triton.view(N, D)\n    return Y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Union, Optional\n\n\n@triton.jit\ndef rotary_kernel(\n    OUT,  # Pointers to matrices\n    X,\n    COS,\n    SIN,\n    CU_SEQLENS,\n    SEQLEN_OFFSETS,  # this could be int or a pointer\n    # Matrix dimensions\n    seqlen,\n    rotary_dim,\n    seqlen_ro,\n    # strides\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_nheads,\n    stride_out_headdim,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_nheads,\n    stride_x_headdim,\n    # Meta-parameters\n    BLOCK_K: tl.constexpr,\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    INTERLEAVED: tl.constexpr,\n    CONJUGATE: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads\n\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(\n            COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0\n        ).to(tl.float32)\n        sin = tl.load(\n            SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0\n        ).to(tl.float32)\n        x0 = tl.load(\n            X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\n        ).to(tl.float32)\n        x1 = tl.load(\n            X + rotary_dim_half * stride_x_headdim,\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\n            other=0.0,\n        ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n        tl.store(\n            OUT + rotary_dim_half * stride_out_headdim,\n            o1,\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\n        )\n    else:\n        rk_swap = rk + ((rk + 1) % 2) * 2 - 1  # 1, 0, 3, 2, 5, 4, ...\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(\n            COS,\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\n            other=1.0,\n        ).to(tl.float32)\n        sin = tl.load(\n            SIN,\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\n            other=0.0,\n        ).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(\n            tl.float32\n        )\n        x1 = tl.load(\n            X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0\n        ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))\n\ndef apply_rotary(\n    x: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    seqlen_offsets: Union[int, torch.Tensor] = 0,\n    cu_seqlens: Optional[torch.Tensor] = None,\n    max_seqlen: Optional[int] = None,\n    interleaved=False,\n    inplace=False,\n    conjugate=False,\n) -> torch.Tensor:\n    is_varlen = cu_seqlens is not None\n    if not is_varlen:\n        batch, seqlen, nheads, headdim = x.shape\n    else:\n        assert max_seqlen is not None, \"If cu_seqlens is passed in, then max_seqlen must be passed\"\n        total_seqlen, nheads, headdim = x.shape\n        batch_p_1 = cu_seqlens.shape[0]\n        batch = batch_p_1 - 1\n        seqlen = max_seqlen\n    seqlen_ro, rotary_dim = cos.shape\n    assert sin.shape == cos.shape\n    rotary_dim *= 2\n    assert rotary_dim <= headdim, \"rotary_dim must be <= headdim\"\n    assert headdim <= 256, \"Only support headdim <= 256\"\n    assert seqlen_ro >= seqlen, \"seqlen_ro must be >= seqlen\"\n\n    assert (\n        cos.dtype == sin.dtype\n    ), f\"cos and sin must have the same dtype, got {cos.dtype} and {sin.dtype}\"\n    assert (\n        x.dtype == cos.dtype\n    ), f\"Input and cos/sin must have the same dtype, got {x.dtype} and {cos.dtype}\"\n\n    cos, sin = cos.contiguous(), sin.contiguous()\n    if isinstance(seqlen_offsets, torch.Tensor):\n        assert seqlen_offsets.shape == (batch,)\n        assert seqlen_offsets.dtype in [torch.int32, torch.int64]\n        seqlen_offsets = seqlen_offsets.contiguous()\n    else:\n        assert seqlen_offsets + seqlen <= seqlen_ro\n\n    output = torch.empty_like(x) if not inplace else x\n    if rotary_dim < headdim and not inplace:\n        output[..., rotary_dim:].copy_(x[..., rotary_dim:])\n\n    BLOCK_K = (\n        32\n        if rotary_dim <= 32\n        else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\n    )\n    grid = lambda META: (triton.cdiv(seqlen, META[\"BLOCK_M\"]), batch, nheads)\n    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 128 else 4)\n\n    with torch.cuda.device(x.device.index):\n        rotary_kernel[grid](\n            output,  # data ptrs\n            x,\n            cos,\n            sin,\n            cu_seqlens,\n            seqlen_offsets,\n            seqlen,  # shapes\n            rotary_dim,\n            seqlen_ro,\n            output.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\n            output.stride(-3),  # seqlen_stride or total_seqlen_stride\n            output.stride(-2),  # nheads_stride\n            output.stride(-1),  # headdim_stride\n            x.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\n            x.stride(-3),  # seqlen stride or total_seqlen_stride\n            x.stride(-2),  # nheads stride\n            x.stride(-1),  # headdim stride\n            BLOCK_K,\n            isinstance(seqlen_offsets, torch.Tensor),\n            is_varlen,\n            interleaved,\n            conjugate,\n            BLOCK_M,\n        )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is designed to compute the softmax function for each row of a given input matrix. The kernel is named `softmax_kernel` and is annotated with `@triton.jit` for just-in-time compilation, enabling it to run on GPUs efficiently. \n\n            The `softmax_kernel` function takes several inputs: `output_ptr`, `input_ptr`, `input_row_stride`, `output_row_stride`, `n_cols`, and a constant `BLOCK_SIZE`. It computes the softmax for each row independently, exploiting parallel execution by launching one kernel instance per row of the matrix. \n\n            The kernel loads a row of data into a local variable, applies numerical stabilization by subtracting the maximum value of the row, computes the exponential of each element, normalizes the results, and stores them back into the output matrix.\n\n            The `softmax` function is a wrapper that sets up the parameters for the kernel execution, such as determining the `BLOCK_SIZE` and `num_warps` based on the matrix dimensions. It allocates space for the output and calls the `softmax_kernel` with a 1D launch grid corresponding to the number of rows in the input.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr,  # Pointer to the output matrix\n    input_ptr,   # Pointer to the input matrix\n    input_row_stride,  # Stride between elements in the same row of the input matrix\n    output_row_stride,  # Stride between elements in the same row of the output matrix\n    n_cols,      # Number of columns in the matrix\n    BLOCK_SIZE: tl.constexpr  # Block size for parallel execution\n):\n    # Get the row index for this thread block\n    row_idx = tl.program_id(0)\n    \n    # Initialize a block of memory to store the row data\n    row_data = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Load the row data into the block\n    for i in range(n_cols):\n        row_data[i] = tl.load(input_ptr + row_idx * input_row_stride + i)\n    \n    # Compute the maximum value in the row for numerical stability\n    max_val = tl.max(row_data, axis=0)\n    \n    # Subtract the maximum value from each element in the row\n    row_data = tl.exp(row_data - max_val)\n    \n    # Compute the sum of the exponentials\n    sum_exp = tl.sum(row_data, axis=0)\n    \n    # Normalize the row data\n    row_data = row_data / sum_exp\n    \n    # Store the normalized row data back into the output matrix\n    for i in range(n_cols):\n        tl.store(output_ptr + row_idx * output_row_stride + i, row_data[i])\n\nimport torch\nimport triton\n\ndef softmax(input_tensor: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is on the GPU\n    assert input_tensor.is_cuda, \"Input tensor must be on the GPU\"\n    \n    # Get the dimensions of the input tensor\n    n_rows, n_cols = input_tensor.shape\n    \n    # Determine the block size and number of warps\n    BLOCK_SIZE = 128  # Adjust this based on your GPU architecture\n    num_warps = 4  # Adjust this based on your GPU architecture\n    \n    # Allocate space for the output tensor\n    output_tensor = torch.empty_like(input_tensor)\n    \n    # Launch the kernel\n    softmax_kernel[torch.arange(n_rows), (BLOCK_SIZE,)](\n        output_tensor,  # Pointer to the output matrix\n        input_tensor,   # Pointer to the input matrix\n        n_cols,         # Number of columns in the matrix\n        n_cols,         # Stride between elements in the same row (same as n_cols for a contiguous matrix)\n        n_cols,         # Number of columns in the matrix\n        BLOCK_SIZE      # Block size for parallel execution\n    )\n    \n    return output_tensor\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # The rows of the softmax are independent, so we parallelize across those\n    row_idx = tl.program_id(0)\n    # The stride represents how much we need to increase the pointer to advance 1 row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    # The block size is the next power of two greater than n_cols, so we can fit each\n    # row in a single block\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    # Subtract maximum for numerical stability\n    row_minus_max = row - tl.max(row, axis=0)\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    # Write back output to DRAM\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n    # The block size is the smallest power of two greater than the number of columns in `x`\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Another trick we can use is to ask the compiler to use more threads per row by\n    # increasing the number of warps (`num_warps`) over which each row is distributed.\n    # You will see in the next tutorial how to auto-tune this value in a more natural\n    # way so you don't have to come up with manual heuristics yourself.\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    y = torch.empty_like(x)\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n    # f the input matrix\n    softmax_kernel[(n_rows, )](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_cols,\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return y\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `fused_native_layer_norm_no_welford` function performs layer normalization on input tensors using Triton. The Triton kernel, `triton_red_fused_native_layer_norm_no_welford`, computes the mean and standard deviation across a specified dimension and normalizes the input tensor accordingly. The kernel uses block-based parallelism controlled by `XBLOCK` and `RBLOCK` constants for efficient computation on GPUs. Inputs are `primals_1`, `primals_2`, and `primals_3`. The function outputs the normalized tensor, a tensor storing mean values, and a tensor storing inverse standard deviation values.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef triton_red_fused_native_layer_norm_no_welford(\n    X,  # input tensor\n    Y,  # output tensor\n    Mean,  # mean tensor\n    Rstd,  # inverse standard deviation tensor\n    weight,  # weight tensor\n    bias,  # bias tensor\n    stride,  # stride of the input tensor\n    N,  # number of elements in the reduction dimension\n    eps,  # small value to avoid division by zero\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr\n):\n    # Get the program ID\n    pid = tl.program_id(0)\n    # Compute the start and end indices for the block\n    block_start = pid * XBLOCK\n    block_end = min(block_start + XBLOCK, stride)\n    \n    # Initialize the mean and variance accumulators\n    sum_x = 0.0\n    sum_x2 = 0.0\n    \n    # Iterate over the block\n    for i in range(block_start, block_end):\n        # Load the input value\n        x = tl.load(X + i * N)\n        # Accumulate the mean and variance\n        sum_x += x\n        sum_x2 += x * x\n    \n    # Compute the mean and variance for the block\n    mean = sum_x / N\n    var = (sum_x2 / N) - (mean * mean)\n    \n    # Compute the inverse standard deviation\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Store the mean and inverse standard deviation\n    tl.store(Mean + pid, mean)\n    tl.store(Rstd + pid, rstd)\n    \n    # Normalize the input tensor\n    for i in range(block_start, block_end):\n        # Load the input value\n        x = tl.load(X + i * N)\n        # Normalize the input value\n        y = (x - mean) * rstd\n        # Apply weight and bias\n        y = y * tl.load(weight + i) + tl.load(bias + i)\n        # Store the normalized value\n        tl.store(Y + i * N, y)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef fused_native_layer_norm_no_welford(primals_1, primals_2, primals_3, eps=1e-5):\n    # Ensure inputs are on the same device\n    device = primals_1.device\n    assert primals_2.device == device and primals_3.device == device\n    \n    # Get the shape of the input tensor\n    N = primals_1.shape[-1]\n    stride = primals_1.numel() // N\n    \n    # Allocate output tensors\n    Y = torch.empty_like(primals_1)\n    Mean = torch.empty((stride,), device=device, dtype=torch.float32)\n    Rstd = torch.empty((stride,), device=device, dtype=torch.float32)\n    \n    # Define block sizes\n    XBLOCK = 128\n    RBLOCK = 128\n    \n    # Launch the kernel\n    grid = (stride // XBLOCK + (stride % XBLOCK > 0),)\n    triton_red_fused_native_layer_norm_no_welford[grid](\n        primals_1, Y, Mean, Rstd, primals_2, primals_3, stride, N, eps, XBLOCK, RBLOCK\n    )\n    \n    return Y, Mean, Rstd\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\n\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch.ops.inductor._reinterpret_tensor\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 1024,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 2048,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n    ],\n    key=[\"xnumel\", \"rnumel\"],\n)\n@triton.jit\ndef triton_red_fused_native_layer_norm_no_welford(\n    in_out_ptr0,\n    in_out_ptr1,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = _tmp3 + tmp2\n        _tmp3 = tmp4\n    tmp3 = tl.sum(_tmp3, 1)[:, None]\n    tmp5 = rnumel  # 4096.0\n    tmp6 = tmp3 / tmp5\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp6, None)\n    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp8 = tmp7.to(tl.float32)\n        tmp9 = tmp8 - tmp6\n        tmp10 = tmp9 * tmp9\n        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])\n        tmp13 = _tmp12 + tmp11\n        _tmp12 = tmp13\n    tmp12 = tl.sum(_tmp12, 1)[:, None]\n    tmp14 = rnumel  # 4096.0\n    tmp15 = tmp12 / tmp14\n    tmp16 = 1e-05\n    tmp17 = tmp15 + tmp16\n    tmp18 = libdevice.rsqrt(tmp17)\n    tl.debug_barrier()\n    tl.store(in_out_ptr1 + (x0), tmp18, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp19 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp20 = tmp19.to(tl.float32)\n        tmp21 = tmp20 - tmp6\n        tmp22 = tmp21 * tmp18\n        tmp24 = tmp23.to(tl.float32)\n        tmp25 = tmp22 * tmp24\n        tmp27 = tmp26.to(tl.float32)\n        tmp28 = tmp25 + tmp27\n        tmp29 = tmp28.to(tl.float32)\n        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)\n\ndef fused_native_layer_norm_no_welford(primals_1, primals_2, primals_3):\n    S, D = primals_3.shape\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((S, 1), (1, S), torch.float32)\n        buf1 = buf0\n        del buf0  # reuse\n        buf2 = empty_strided_cuda((S, 1), (1, S), torch.float32)\n        buf3 = reinterpret_tensor(buf2, (S, 1), (1, 1), 0)\n        del buf2  # reuse\n        buf4 = empty_strided_cuda((S, D), (D, 1), torch.bfloat16)\n        stream0 = get_raw_stream(0)\n        grid = lambda META: (triton.cdiv(S, META[\"XBLOCK\"]),)\n        triton_red_fused_native_layer_norm_no_welford[grid](\n            buf1, buf3, primals_3, primals_1, primals_2, buf4, S, D\n        )\n    return (\n        buf4,\n        primals_3,\n        buf1,\n        buf3,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel named `mul_kernel` takes a source tensor `src` and multiplies each of its elements by a constant exponent compensator. The result is stored in a destination tensor `dst`. The operation is parallelized over blocks of data of size `BLOCK_SIZE`. The constant exponent compensator is set to `2.0 ** (127 - 15)`. The main functionality is to perform element-wise multiplication with the compensator for a segment of the tensor defined by `BLOCK_SIZE` and the current program ID.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef mul_kernel(src_ptr, dst_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the starting index for this program\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Create a mask to avoid out-of-bounds access\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load the data from the source tensor\n    src = tl.load(src_ptr + offsets, mask=mask)\n\n    # Define the constant exponent compensator\n    compensator = 2.0 ** (127 - 15)\n\n    # Perform the element-wise multiplication\n    dst = src * compensator\n\n    # Store the result in the destination tensor\n    tl.store(dst_ptr + offsets, dst, mask=mask)\n\nimport triton\nimport triton.runtime\nimport torch\n\ndef triton_mul(src, dst, block_size=1024):\n    # Ensure the source and destination tensors are on the same device\n    assert src.device == dst.device, \"Source and destination tensors must be on the same device\"\n    assert src.dtype == torch.float32, \"Source tensor must be of type float32\"\n    assert dst.dtype == torch.float32, \"Destination tensor must be of type float32\"\n\n    # Get the number of elements\n    n_elements = src.numel()\n\n    # Define the grid and block dimensions\n    grid = (triton.cdiv(n_elements, block_size),)\n\n    # Launch the kernel\n    mul_kernel[grid](src, dst, n_elements, BLOCK_SIZE=block_size)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a source tensor\n    src = torch.randn(1024 * 1024, device=\"cuda\", dtype=torch.float32)\n\n    # Create a destination tensor with the same shape and type\n    dst = torch.empty_like(src)\n\n    # Call the Triton kernel\n    triton_mul(src, dst)\n\n    # Print the first few elements of the source and destination tensors\n    print(\"Source tensor (first 10 elements):\")\n    print(src[:10])\n    print(\"Destination tensor (first 10 elements):\")\n    print(dst[:10])\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel to multiply each element in the source tensor by a constant exponent compensator\n@triton.jit\ndef mul_kernel(src, dst, BLOCK_SIZE: tl.constexpr):\n    # Define a constant exponent compensator\n    exponent_compensator: tl.constexpr = 2.0 ** (127 - 15)\n    # Calculate the indices for the current program ID\n    idxs = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    # Load elements from the source tensor\n    x = tl.load(src + idxs)\n    # Multiply each element by the exponent compensator\n    y = x * exponent_compensator\n    # Store the result in the destination tensor\n    tl.store(dst + idxs, y)\n\n# Function to launch the Triton kernel\ndef launch_mul_kernel(src, BLOCK_SIZE=1):\n    # Create an empty tensor for the result\n    dst = torch.empty(src.shape, dtype=torch.float32, device='cuda')\n    # Launch the Triton kernel\n    mul_kernel[(src.shape[0] // BLOCK_SIZE,)](src, dst, BLOCK_SIZE)\n    return dst\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The function `dropout` applies the dropout operation using a custom Triton kernel `_dropout`. The main logic of `_dropout` involves reading input data from `x_ptr` and a mask from `x_keep_ptr`, scaling non-zero entries by `1/(1-p)`, and storing the result in `output_ptr`. It operates on data in blocks of size `BLOCK_SIZE`. The `dropout` function prepares inputs, defines grid size, and calls `_dropout` with the specified probability `p`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _dropout(x_ptr, x_keep_ptr, output_ptr, n_elements, p, BLOCK_SIZE: tl.constexpr):\n    # Compute the block ID and the starting index for this block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Compute the offsets for the elements in this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the input and mask data\n    x = tl.load(x_ptr + offsets, mask=offsets < n_elements)\n    x_keep = tl.load(x_keep_ptr + offsets, mask=offsets < n_elements)\n\n    # Apply the dropout operation\n    scale = 1.0 / (1.0 - p)\n    output = tl.where(x_keep != 0, x * scale, 0.0)\n\n    # Store the result\n    tl.store(output_ptr + offsets, output, mask=offsets < n_elements)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef dropout(x, p):\n    # Ensure the input tensor is on the GPU\n    assert x.is_cuda, \"Input tensor must be on the GPU\"\n\n    # Generate the dropout mask\n    x_keep = torch.rand_like(x) > p\n\n    # Convert the mask to a float tensor\n    x_keep = x_keep.to(dtype=x.dtype)\n\n    # Define the grid size\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n\n    # Allocate output tensor\n    output = torch.empty_like(x)\n\n    # Launch the Triton kernel\n    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE)\n\n    return output\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _dropout(\n    x_ptr,  # pointer to the input\n    x_keep_ptr,  # pointer to a mask of 0s and 1s\n    output_ptr,  # pointer to the output\n    n_elements,  # number of elements in the `x` tensor\n    p,  # probability that an element of `x` is changed to zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    # Load data\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n    # The line below is the crucial part, described in the paragraph above!\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    # Write-back output\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef dropout(x, x_keep, p):\n    output = torch.empty_like(x)\n    assert x.is_contiguous()\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe function `destindex_copy_quantize_kv` leverages a Triton kernel `_fwd_kernel_destindex_copy_quantize_kv` to process an input tensor `K`, performing an index-based copy and quantization operation. The inputs to the kernel include `K`, `DestLoc`, `Out`, and `Out_scale`. The kernel copies data from `K` to `Out` at specified indices from `DestLoc`, scaling and quantizing it to int8 format. The scale is calculated to allow the quantized data to utilize the range of int8 efficiently. The grid is defined by the sequence length `seq_len`, and uses one warp.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K_ptr,  # Pointer to the input tensor K\n    DestLoc_ptr,  # Pointer to the destination indices\n    Out_ptr,  # Pointer to the output tensor\n    Out_scale_ptr,  # Pointer to the output scale\n    seq_len,  # Sequence length\n    block_size: tl.constexpr  # Block size\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * block_size\n\n    # Compute the scale\n    max_val = -float('inf')\n    min_val = float('inf')\n    for i in range(block_start, block_start + block_size):\n        if i < seq_len:\n            k_val = tl.load(K_ptr + i)\n            max_val = tl.max(max_val, k_val)\n            min_val = tl.min(min_val, k_val)\n\n    scale = (max_val - min_val) / 255.0\n\n    # Write the scale to the output scale tensor\n    tl.store(Out_scale_ptr + pid, scale)\n\n    # Perform the copy and quantization\n    for i in range(block_start, block_start + block_size):\n        if i < seq_len:\n            k_val = tl.load(K_ptr + i)\n            dest_index = tl.load(DestLoc_ptr + i)\n            quantized_val = tl.round((k_val - min_val) / scale)\n            quantized_val = tl.to_int8(quantized_val)\n            tl.store(Out_ptr + dest_index, quantized_val)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = K.shape[0]\n    block_size = 128  # Adjust block size as needed\n\n    # Launch the kernel\n    grid = (seq_len + block_size - 1) // block_size\n    _fwd_kernel_destindex_copy_quantize_kv[grid, block_size](\n        K,  # Input tensor K\n        DestLoc,  # Destination indices\n        Out,  # Output tensor\n        Out_scale,  # Output scale tensor\n        seq_len,  # Sequence length\n        block_size  # Block size\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    seq_len = 1024\n    K = torch.randn(seq_len, dtype=torch.float32, device='cuda')\n    DestLoc = torch.randint(0, seq_len, (seq_len,), dtype=torch.int32, device='cuda')\n    Out = torch.zeros(seq_len, dtype=torch.int8, device='cuda')\n    Out_scale = torch.zeros((seq_len + 127) // 128, dtype=torch.float32, device='cuda')\n\n    destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale)\n\n    print(\"Quantized Output:\", Out)\n    print(\"Scales:\", Out_scale)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d,\n    stride_o_bs, stride_o_h, stride_o_d,\n    stride_os_bs, stride_os_h, stride_os_d,\n    head_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] * stride_k_h + stride_k_d * offs_d[None, :], \n                       mask=offs_h[:, None] < head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.).to(tl.float16)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[:, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n\n\n@torch.no_grad()\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K, DestLoc, Out, Out_scale,\n        K.stride(0), K.stride(1), K.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        Out_scale.stride(0), Out_scale.stride(1), Out_scale.stride(2),\n        head_num,\n        BLOCK_DMODEL=head_dim,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `_fwd_kernel_token_att2` and a PyTorch wrapper function `token_att_fwd2`. The kernel computes attention over tokens using probability and value tensors to produce an output tensor. It takes into account batch and head dimensions, and uses sliding window attention. The wrapper function prepares the kernel execution by setting up grid dimensions and other parameters. \n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    prob_ptr,  # Pointer to the probability tensor (B, H, S, W)\n    value_ptr,  # Pointer to the value tensor (B, H, S, D)\n    output_ptr,  # Pointer to the output tensor (B, H, S, D)\n    B,  # Batch size\n    H,  # Number of heads\n    S,  # Sequence length\n    D,  # Dimension of the value tensor\n    W,  # Window size\n    stride_prob_b,  # Stride for batch in prob tensor\n    stride_prob_h,  # Stride for head in prob tensor\n    stride_prob_s,  # Stride for sequence in prob tensor\n    stride_prob_w,  # Stride for window in prob tensor\n    stride_value_b,  # Stride for batch in value tensor\n    stride_value_h,  # Stride for head in value tensor\n    stride_value_s,  # Stride for sequence in value tensor\n    stride_value_d,  # Stride for dimension in value tensor\n    stride_output_b,  # Stride for batch in output tensor\n    stride_output_h,  # Stride for head in output tensor\n    stride_output_s,  # Stride for sequence in output tensor\n    stride_output_d,  # Stride for dimension in output tensor\n    BLOCK_SIZE_S: tl.constexpr,  # Block size for sequence\n    BLOCK_SIZE_D: tl.constexpr,  # Block size for dimension\n    BLOCK_SIZE_W: tl.constexpr  # Block size for window\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (H * S)\n    hid = (pid % (H * S)) // S\n    sid = (pid % (H * S)) % S\n\n    # Offsets for batch, head, and sequence\n    prob_offset = bid * stride_prob_b + hid * stride_prob_h + sid * stride_prob_s\n    value_offset = bid * stride_value_b + hid * stride_value_h + sid * stride_value_s\n    output_offset = bid * stride_output_b + hid * stride_output_h + sid * stride_output_s\n\n    # Load the probability and value tensors\n    prob = tl.load(prob_ptr + prob_offset + tl.arange(0, BLOCK_SIZE_W), mask=tl.arange(0, BLOCK_SIZE_W) < W, other=0.0)\n    value = tl.load(value_ptr + value_offset + tl.arange(0, BLOCK_SIZE_D), mask=tl.arange(0, BLOCK_SIZE_D) < D, other=0.0)\n\n    # Compute the output\n    output = tl.zeros((BLOCK_SIZE_D,), dtype=tl.float32)\n    for w in range(W):\n        output += prob[w] * value\n\n    # Store the output\n    tl.store(output_ptr + output_offset + tl.arange(0, BLOCK_SIZE_D), output, mask=tl.arange(0, BLOCK_SIZE_D) < D)\n\nimport torch\n\ndef token_att_fwd2(prob, value, window_size):\n    B, H, S, W = prob.shape\n    B, H, S, D = value.shape\n\n    # Allocate output tensor\n    output = torch.empty((B, H, S, D), device=prob.device, dtype=prob.dtype)\n\n    # Define grid and block dimensions\n    grid = (B * H * S,)\n    block = (128,)  # Adjust block size as needed\n\n    # Launch the kernel\n    _fwd_kernel_token_att2[grid, block](\n        prob, value, output,\n        B, H, S, D, W,\n        prob.stride(0), prob.stride(1), prob.stride(2), prob.stride(3),\n        value.stride(0), value.stride(1), value.stride(2), value.stride(3),\n        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n        BLOCK_SIZE_S=1,  # Adjust block size for sequence\n        BLOCK_SIZE_D=128,  # Adjust block size for dimension\n        BLOCK_SIZE_W=window_size  # Adjust block size for window\n    )\n\n    return output\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    Prob,\n    V,\n    Out,\n    Req_to_tokens,\n    B_req_idx,\n    B_Start_Loc,\n    B_Seqlen,\n    B_Att_Start_Loc,\n    B_Att_Seqlen,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    stride_ph,\n    stride_pbs,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    sliding_window,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    # Triton kernel for computing token attention\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n\n    v_loc_off = (\n        cur_batch_req_idx * stride_req_to_tokens_b + (cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    )\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_att_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=(start_n + offs_n) < cur_att_seq_len, other=0.0)\n        v_loc = tl.load(\n            Req_to_tokens + v_loc_off + start_n * stride_req_to_tokens_s,\n            mask=(start_n + offs_n + cur_batch_start_index) < cur_batch_seq_len,\n            other=0.0,\n        )\n        v_value = tl.load(\n            V + v_offs + v_loc[:, None] * stride_vbs,\n            mask=(start_n + offs_n[:, None] + cur_batch_start_index) < cur_batch_seq_len,\n            other=0.0,\n        )\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_att_fwd2(\n    prob, v, out, Req_to_tokens, B_req_idx, B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, sliding_window\n):\n    # Launch the Triton kernel for token attention\n    BLOCK = 128\n    batch, head = B_req_idx.shape[0], prob.shape[0]\n    grid = (batch, head)\n    num_warps = 4\n    dim = v.shape[-1]\n\n    kv_group_num = prob.shape[0] // v.shape[1]\n\n    _fwd_kernel_token_att2[grid](\n        prob,\n        v,\n        out,\n        Req_to_tokens,\n        B_req_idx,\n        B_Start_Loc,\n        B_Seqlen,\n        B_Att_Start_Loc,\n        B_Att_Seqlen,\n        Req_to_tokens.stride(0),\n        Req_to_tokens.stride(1),\n        prob.stride(0),\n        prob.stride(1),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        out.stride(0),\n        out.stride(1),\n        out.stride(2),\n        kv_group_num=kv_group_num,\n        sliding_window=sliding_window,\n        BLOCK_DMODEL=dim,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel named `var_len_copy_kernel_triton` copies variable-length segments from one array to another. It takes the starting indices and lengths of these segments for both the source and destination arrays. It uses `BLOCK_SIZE` to divide the work into manageable chunks. The kernel is launched by the `launch_var_len_copy_triton` function, which sets the block size and grid size based on the number of segments. It passes memory pointers and block size to the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef var_len_copy_kernel_triton(\n    src_ptr,  # Pointer to the source array\n    dst_ptr,  # Pointer to the destination array\n    src_indices_ptr,  # Pointer to the starting indices of segments in the source array\n    dst_indices_ptr,  # Pointer to the starting indices of segments in the destination array\n    lengths_ptr,  # Pointer to the lengths of the segments\n    num_segments,  # Number of segments to copy\n    BLOCK_SIZE: tl.constexpr  # Block size\n):\n    pid = tl.program_id(axis=0)  # Get the program ID (block ID)\n    num_blocks = (num_segments + BLOCK_SIZE - 1) // BLOCK_SIZE  # Calculate the number of blocks\n\n    # Compute the range of segments for this block\n    block_start = pid * BLOCK_SIZE\n    block_end = min(block_start + BLOCK_SIZE, num_segments)\n\n    for segment_id in range(block_start, block_end):\n        src_start = tl.load(src_indices_ptr + segment_id)  # Load the starting index of the segment in the source array\n        dst_start = tl.load(dst_indices_ptr + segment_id)  # Load the starting index of the segment in the destination array\n        length = tl.load(lengths_ptr + segment_id)  # Load the length of the segment\n\n        # Copy the segment\n        for i in range(length):\n            src_idx = src_start + i\n            dst_idx = dst_start + i\n            dst_ptr[dst_idx] = tl.load(src_ptr + src_idx)\n\nimport torch\n\ndef launch_var_len_copy_triton(src, dst, src_indices, dst_indices, lengths, device='cuda'):\n    # Convert inputs to torch tensors\n    src = torch.tensor(src, device=device)\n    dst = torch.tensor(dst, device=device)\n    src_indices = torch.tensor(src_indices, device=device)\n    dst_indices = torch.tensor(dst_indices, device=device)\n    lengths = torch.tensor(lengths, device=device)\n\n    # Number of segments\n    num_segments = len(lengths)\n\n    # Block size\n    BLOCK_SIZE = 128\n\n    # Grid size\n    grid_size = (num_segments + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    var_len_copy_kernel_triton[grid_size, BLOCK_SIZE](\n        src, dst, src_indices, dst_indices, lengths, num_segments, BLOCK_SIZE\n    )\n\n    return dst\n\nimport torch\n\n# Example data\nsrc = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ndst = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nsrc_indices = [0, 3, 6]\ndst_indices = [1, 4, 7]\nlengths = [3, 3, 3]\n\n# Convert to tensors\nsrc = torch.tensor(src, device='cuda')\ndst = torch.tensor(dst, device='cuda')\nsrc_indices = torch.tensor(src_indices, device='cuda')\ndst_indices = torch.tensor(dst_indices, device='cuda')\nlengths = torch.tensor(lengths, device='cuda')\n\n# Launch the kernel\ndst = launch_var_len_copy_triton(src, dst, src_indices, dst_indices, lengths)\n\n# Print the result\nprint(dst)  # Output: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n", "label": "\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location, new_a_start, new_a_location,\n                               BLOCK_SIZE: tl.constexpr):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=new_offset < length)\n\ndef launch_var_len_copy_triton(old_a_start, old_a_len, old_location, new_a_start, new_a_location):\n    BLOCK_SIZE = 256\n    grid_size = (len(old_a_start),)\n\n    var_len_copy_kernel_triton[grid_size](\n        old_a_start, old_a_len, old_location, new_a_start, new_a_location, BLOCK_SIZE)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel named `_score_kernel` is used to compute the attention score matrix for a transformer model. It uses block-level matrix multiplication and an optimized approach for performance. The function `get_score` is a wrapper around the kernel, managing input parameters and kernel execution.\n            \n            The kernel takes matrices `Q` (queries), `K` (keys), `M` (masking values), and produces the output `Out`. It operates on blocks of size `BLOCK_M x BLOCK_N`, with parameters such as strides for Q, K, and Out, along with constants like `BLOCK_M`, `BLOCK_N`, and `BLOCK_DMODEL`. The main computation involves loading the blocks of Q and K, computing their dot product, applying scale and mask, and storing results in `Out`.\n\n            The `get_score` function manages grid configuration, sets the scaling factor, and handles exceptions like resource constraints by reducing block sizes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _score_kernel(\n    Q, K, M, Out,\n    stride_qm, stride_qk,\n    stride_km, stride_kk,\n    stride_om, stride_on,\n    stride_m, \n    BLOCK_M: tl.constexpr, \n    BLOCK_N: tl.constexpr, \n    BLOCK_DMODEL: tl.constexpr,\n    scale: tl.constexpr\n):\n    # Compute program ID\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(Q.shape[0], BLOCK_M)\n    num_pid_n = tl.cdiv(K.shape[1], BLOCK_N)\n    num_pid_in_block_m = pid % num_pid_m\n    num_pid_in_block_n = pid // num_pid_m\n\n    # Compute the block offsets\n    offs_m = num_pid_in_block_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = num_pid_in_block_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n\n    # Compute the block pointers\n    Q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    K_ptrs = K + (offs_k[:, None] * stride_km + offs_n[None, :] * stride_kk)\n    M_ptrs = M + (offs_m[:, None] * stride_m + offs_n[None, :])\n    Out_ptrs = Out + (offs_m[:, None] * stride_om + offs_n[None, :] * stride_on)\n\n    # Load the blocks\n    q = tl.load(Q_ptrs)\n    k = tl.load(K_ptrs)\n    m = tl.load(M_ptrs, mask=offs_m[:, None] < Q.shape[0], other=-float('inf'))\n\n    # Compute the dot product\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, BLOCK_DMODEL, 16):\n        qk = q[:, k:k+16]\n        kk = k[:, k:k+16]\n        acc += tl.dot(qk, kk, trans_b=True)\n\n    # Apply scale and mask\n    acc *= scale\n    acc += m\n\n    # Store the results\n    tl.store(Out_ptrs, acc, mask=offs_m[:, None] < Q.shape[0])\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef get_score(Q, K, M, Out, scale):\n    # Get the shapes of the input tensors\n    M, K = Q.shape\n    N = K.shape[1]\n\n    # Set the block sizes\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_DMODEL = 64\n\n    # Compute the grid size\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n    # Set the strides\n    stride_qm = Q.stride(0)\n    stride_qk = Q.stride(1)\n    stride_km = K.stride(0)\n    stride_kk = K.stride(1)\n    stride_om = Out.stride(0)\n    stride_on = Out.stride(1)\n    stride_m = M.stride(0) if M is not None else 1\n\n    # Run the kernel\n    try:\n        _score_kernel[grid](\n            Q, K, M, Out,\n            stride_qm, stride_qk,\n            stride_km, stride_kk,\n            stride_om, stride_on,\n            stride_m,\n            BLOCK_M, BLOCK_N, BLOCK_DMODEL,\n            scale\n        )\n    except triton.OutOfResources as e:\n        # If out of resources, reduce block sizes and retry\n        BLOCK_M = 64\n        BLOCK_N = 64\n        BLOCK_DMODEL = 32\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n        _score_kernel[grid](\n            Q, K, M, Out,\n            stride_qm, stride_qk,\n            stride_km, stride_kk,\n            stride_om, stride_on,\n            stride_m,\n            BLOCK_M, BLOCK_N, BLOCK_DMODEL,\n            scale\n        )\n\n    return Out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n_BLOCK_N = 64\n_BLOCK_M = 64\n\n@triton.heuristics(\n    {\n        \"IS_EVEN_M\": lambda args: args[\"N_CTX\"] % args[\"BLOCK_M\"] == 0,\n        \"IS_EVEN_N\": lambda args: args[\"NKV_CTX\"] % args[\"BLOCK_N\"] == 0,\n    }\n)\n@triton.jit\ndef _score_kernel(\n    Q, K, M, sm_scale, Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,  #\n    stride_kz, stride_kh, stride_kn, stride_kk,  #\n    stride_oz, stride_oh, stride_on,\n    Z, H, H_KV, #\n    N_CTX,  #\n    ROUND_CTX,\n    NKV_CTX,\n    sliding_window_offset,\n    sliding_window_size,\n    SLIDING_WINDOW: tl.constexpr,\n    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,\n    IS_EVEN_M: tl.constexpr,\n    IS_EVEN_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,  #\n    BLOCK_DMODEL: tl.constexpr,  #\n    BLOCK_N: tl.constexpr,  #\n):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H//H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, NKV_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, start_n * BLOCK_N),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634   # 1/log(2)\n\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0,1), padding_option=\"zero\")\n\n        m = tl.load(m_ptrs)\n\n        # calc qk\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[None, :] \\\n                 + start_m - start_n * BLOCK_N + sliding_window_offset\n\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = (dist >= sliding_window_size)\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk) # (BLOCK_M, BLOCK_N)\n\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n\n        if not IS_EVEN_N:\n            p = tl.where(\n                ((tl.arange(0, BLOCK_M) + start_m) < N_CTX)[:, None],\n                p, 0\n            )\n\n        o += tl.sum(p, axis=0)\n\n\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N # orange\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o.to(Out.type.element_ty), mask = o_range < NKV_CTX)\n\ndef get_score(q, k, m, sliding_window, complement_sliding_window):\n    N_CTX = q.size(-2)\n    NKV_CTX = k.size(-2)\n    ROUND_CTX = m.size(-1)\n    ret = torch.zeros(\n        (q.size(0), q.size(1), k.size(2)),\n        dtype=k.dtype, device=k.device\n    )\n    if sliding_window is not None:\n        sliding_window_offset, sliding_window_size = sliding_window\n    else:\n        sliding_window_offset, sliding_window_size = None, None\n\n    grid = lambda META: (\n        triton.cdiv(k.shape[2], META[\"BLOCK_N\"]),\n        q.shape[0] * q.shape[1]\n    )\n    sm_scale = 1 / math.sqrt(q.size(-1))\n\n    global _BLOCK_N\n    global _BLOCK_M\n\n    try:\n        _score_kernel[grid](\n            q, k, m, sm_scale, ret,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            ret.stride(0), ret.stride(1), ret.stride(2),\n            q.size(0), q.size(1), k.size(1),\n            N_CTX, ROUND_CTX, NKV_CTX,\n            sliding_window_offset,\n            sliding_window_size,\n            SLIDING_WINDOW=(sliding_window is not None),\n            COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n            BLOCK_M=_BLOCK_M,\n            BLOCK_N=_BLOCK_N,\n            BLOCK_DMODEL=q.size(-1)\n        )\n    except triton.OutOfResources as E:\n        from warnings import warn\n        _BLOCK_N = _BLOCK_N // 2\n        _BLOCK_M = _BLOCK_M // 2\n        warn(f\"Triton Attention Output Resources. {E}\\nUse smaller block size {_BLOCK_N}.\")\n        _score_kernel[grid](\n            q, k, m, sm_scale, ret,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            ret.stride(0), ret.stride(1), ret.stride(2),\n            q.size(0), q.size(1), k.size(1),\n            N_CTX, ROUND_CTX, NKV_CTX,\n            sliding_window_offset,\n            sliding_window_size,\n            SLIDING_WINDOW=(sliding_window is not None),\n            COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n            BLOCK_M=_BLOCK_M,\n            BLOCK_N=_BLOCK_N,\n            BLOCK_DMODEL=q.size(-1)\n        )\n\n    return ret\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This code defines a Triton kernel `batched_vecmat_kernel` that performs a batched vector-matrix multiplication on GPUs using Triton. The kernel takes matrices A and B as input, computes the vector-matrix product, and stores the result in the output matrix. The function `batched_vecmat` wraps around this kernel to set up the inputs and the computation grid and manages the invocation of the kernel. The main parameters are the matrix dimensions M, N, K, and block sizes for each dimension. The kernel iterates over the K dimension in blocks to compute partial products and accumulates the results in a tile of the output matrix.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef batched_vecmat_kernel(\n    A_ptr,  # Pointer to the input matrix A (batch, M, K)\n    B_ptr,  # Pointer to the input matrix B (batch, K, N)\n    C_ptr,  # Pointer to the output matrix C (batch, M, N)\n    M,      # Number of rows in A and C\n    N,      # Number of columns in B and C\n    K,      # Number of columns in A and rows in B\n    stride_A_batch,  # Stride of A in the batch dimension\n    stride_A_m,      # Stride of A in the M dimension\n    stride_A_k,      # Stride of A in the K dimension\n    stride_B_batch,  # Stride of B in the batch dimension\n    stride_B_k,      # Stride of B in the K dimension\n    stride_B_n,      # Stride of B in the N dimension\n    stride_C_batch,  # Stride of C in the batch dimension\n    stride_C_m,      # Stride of C in the M dimension\n    stride_C_n,      # Stride of C in the N dimension\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for the M dimension\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for the N dimension\n    BLOCK_SIZE_K: tl.constexpr,  # Block size for the K dimension\n    GROUP_SIZE_M: tl.constexpr    # Group size for the M dimension\n):\n    # Compute the batch index\n    batch_idx = tl.program_id(2)\n    # Compute the block indices in the M and N dimensions\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = GROUP_SIZE_M * group_id\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # Compute the block offsets in the M and N dimensions\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Compute the batch offsets\n    A_batch_ptr = A_ptr + batch_idx * stride_A_batch\n    B_batch_ptr = B_ptr + batch_idx * stride_B_batch\n    C_batch_ptr = C_ptr + batch_idx * stride_C_batch\n\n    # Initialize the output tile\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension in blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the input tiles\n        a = tl.load(A_batch_ptr + offs_m[:, None] * stride_A_m + (k + offs_k[None, :]) * stride_A_k, mask=(offs_m[:, None] < M) & (k + offs_k[None, :] < K), other=0.0)\n        b = tl.load(B_batch_ptr + (k + offs_k[:, None]) * stride_B_k + offs_n[None, :] * stride_B_n, mask=(k + offs_k[:, None] < K) & (offs_n[None, :] < N), other=0.0)\n\n        # Perform the matrix multiplication\n        acc += tl.dot(a, b)\n\n    # Store the result in the output matrix\n    C_batch_ptr += offs_m[:, None] * stride_C_m + offs_n[None, :] * stride_C_n\n    tl.store(C_batch_ptr, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\nimport triton\nimport triton.language as tl\nimport torch\n\ndef batched_vecmat(A, B, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M):\n    # Ensure A and B are on the same device\n    assert A.device == B.device, \"A and B must be on the same device\"\n    device = A.device\n\n    # Ensure A and B have the correct dimensions\n    assert A.shape[1] == M and A.shape[2] == K, f\"A must have shape (batch, {M}, {K})\"\n    assert B.shape[1] == K and B.shape[2] == N, f\"B must have shape (batch, {K}, {N})\"\n\n    # Allocate the output matrix C\n    C = torch.empty((A.shape[0], M, N), device=device, dtype=A.dtype)\n\n    # Define the grid and block dimensions\n    grid = (\n        triton.cdiv(M, BLOCK_SIZE_M),\n        triton.cdiv(N, BLOCK_SIZE_N),\n        A.shape[0]\n    )\n\n    # Launch the kernel\n    batched_vecmat_kernel[grid](\n        A, B, C,\n        M, N, K,\n        A.stride(0), A.stride(1), A.stride(2),\n        B.stride(0), B.stride(1), B.stride(2),\n        C.stride(0), C.stride(1), C.stride(2),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M\n    )\n\n    return C\n\nimport torch\n\n# Define the matrix dimensions\nM, N, K = 1024, 1024, 1024\nbatch_size = 32\n\n# Create random input matrices A and B\nA = torch.randn((batch_size, M, K), device='cuda', dtype=torch.float32)\nB = torch.randn((batch_size, K, N), device='cuda', dtype=torch.float32)\n\n# Define the block sizes\nBLOCK_SIZE_M = 16\nBLOCK_SIZE_N = 16\nBLOCK_SIZE_K = 16\nGROUP_SIZE_M = 8\n\n# Perform the batched vector-matrix multiplication\nC = batched_vecmat(A, B, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M)\n\n# Print the result\nprint(C)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef batched_vecmat_kernel(\n        A,  # shape: [dim_m, dim_k]\n        B,  # shape: [dim_m, dim_n, dim_k]\n        dim_m, dim_n, dim_k,\n        output,\n        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):\n    m_index = tl.program_id(0)\n    n_index = tl.program_id(1)\n    output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n        + (n_index * block_n + tl.arange(0, block_n))[None, :]\n\n    vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n    k_blocks = dim_k // block_k\n    for k_index in range(k_blocks):\n        a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, :]\n        a = tl.load(A + a_tile)\n\n        b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n            + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n        b = tl.load(B + b_tile)\n\n        expanded_a, _ = tl.broadcast(a, b)\n        vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n\n    tl.store(output + output_tile, vecmat)\n\n\ndef batched_vecmat(\n    M, N, K, block_m, block_n, block_k, num_warps=4, num_stages=1\n):\n\n    A = torch.randn(M, K, device='cuda', dtype=torch.float32)  # shape: [M, K]\n    B = torch.randn(M, N, K, device='cuda', dtype=torch.float32)  # shape: [M, N, K]\n    output = torch.zeros(M, N, device='cuda', dtype=torch.float32)  # \u8f93\u51fa\u5f20\u91cf\uff0cshape: [M, N]\n\n    assert K % block_k == 0, \"\"\n    assert M % block_m == 0, \"\"\n    assert N % block_n == 0, \"\"\n\n    grid = (M // block_m, N // block_n)\n\n    # \u8c03\u7528 Triton Kernel\n    batched_vecmat_kernel[grid](\n        A,\n        B,\n        M, N, K,\n        output,\n        block_m=block_m,\n        block_n=block_n,\n        block_k=block_k,\n        num_warps=num_warps,\n        num_stages=num_stages\n    )\n\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton operator computes the cross-entropy loss and its gradient for a given set of logits and labels. The implementation includes two kernels: one for the forward pass (`cross_entropy_fwd_kernel`) and one for the backward pass (`cross_entropy_bwd_kernel`). \n\n    The `cross_entropy_fwd_kernel` function calculates the loss, the log-sum-exp (lse), and the z_loss from the logits and labels. It iterates over each row and processes a block of columns at a time. It handles optional label smoothing and scaling of logits. \n\n    The `cross_entropy_bwd_kernel` function computes gradients of the logits (dlogits) based on the loss gradient (dloss) from the forward pass. It adjusts probabilities according to the logits and applies optional label smoothing.\n\n    The `cross_entropy_fwd` function sets up and launches the forward kernel, and it returns the computed loss, lse, and z_loss. The `cross_entropy_bwd` function launches the backward kernel to compute and return the dlogits.\n\n    Inputs include: logits, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, and flags like HAS_SMOOTHING and SPLIT. Outputs include loss, lse, z_loss for forward, and dlogits for backward.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Forward Pass Kernel\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    logits_ptr,  # *float32\n    labels_ptr,  # *int32\n    loss_ptr,    # *float32\n    lse_ptr,     # *float32\n    z_loss_ptr,  # *float32\n    smoothing: tl.float32,\n    logit_scale: tl.float32,\n    lse_square_scale: tl.float32,\n    ignored_index: tl.int32,\n    total_classes: tl.int32,\n    class_start_idx: tl.int32,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    row_start = pid * BLOCK_SIZE\n    col_start = class_start_idx\n\n    # Initialize accumulators\n    lse = -float('inf')\n    z_loss = 0.0\n    loss = 0.0\n\n    # Iterate over the block of columns\n    for col in range(col_start, col_start + BLOCK_SIZE):\n        if col >= total_classes:\n            break\n        logit = logits_ptr[row_start + col]\n        logit_scaled = logit * logit_scale\n        lse = tl.max(lse, logit_scaled)\n        z_loss += tl.exp(logit_scaled)\n\n    # Compute log-sum-exp (lse)\n    lse = lse + tl.log(z_loss)\n\n    # Compute loss\n    label = labels_ptr[pid]\n    if label != ignored_index:\n        if HAS_SMOOTHING:\n            smoothed_label = (1.0 - smoothing) * (label == col) + smoothing / total_classes\n            loss += -tl.sum(smoothed_label * (logit_scaled - lse))\n        else:\n            loss += -logit_scaled + lse\n\n    # Write results to output pointers\n    tl.store(loss_ptr + pid, loss)\n    tl.store(lse_ptr + pid, lse)\n    tl.store(z_loss_ptr + pid, z_loss)\n\n# Backward Pass Kernel\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    logits_ptr,  # *float32\n    labels_ptr,  # *int32\n    dloss_ptr,   # *float32\n    dlogits_ptr, # *float32\n    lse_ptr,     # *float32\n    z_loss_ptr,  # *float32\n    smoothing: tl.float32,\n    logit_scale: tl.float32,\n    ignored_index: tl.int32,\n    total_classes: tl.int32,\n    class_start_idx: tl.int32,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    row_start = pid * BLOCK_SIZE\n    col_start = class_start_idx\n\n    # Load lse and z_loss\n    lse = tl.load(lse_ptr + pid)\n    z_loss = tl.load(z_loss_ptr + pid)\n    dloss = tl.load(dloss_ptr + pid)\n\n    # Initialize dlogits\n    dlogits = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Iterate over the block of columns\n    for col in range(col_start, col_start + BLOCK_SIZE):\n        if col >= total_classes:\n            break\n        logit = logits_ptr[row_start + col]\n        logit_scaled = logit * logit_scale\n        prob = tl.exp(logit_scaled - lse)\n        if HAS_SMOOTHING:\n            smoothed_label = (1.0 - smoothing) * (labels_ptr[pid] == col) + smoothing / total_classes\n            dlogits[col - col_start] = dloss * (prob - smoothed_label)\n        else:\n            dlogits[col - col_start] = dloss * (prob - (labels_ptr[pid] == col))\n\n    # Write results to output pointer\n    for col in range(col_start, col_start + BLOCK_SIZE):\n        if col >= total_classes:\n            break\n        tl.store(dlogits_ptr + row_start + col, dlogits[col - col_start])\n\n# Wrapper Functions\ndef cross_entropy_fwd(\n    logits,  # *float32\n    labels,  # *int32\n    smoothing: float = 0.0,\n    logit_scale: float = 1.0,\n    lse_square_scale: float = 1.0,\n    ignored_index: int = -100,\n    total_classes: int = 1000,\n    class_start_idx: int = 0,\n    BLOCK_SIZE: int = 128,\n    HAS_SMOOTHING: bool = False,\n    SPLIT: bool = False\n):\n    assert logits.shape[0] == labels.shape[0]\n    assert logits.shape[1] == total_classes\n\n    loss = triton.empty_like(logits, shape=(logits.shape[0],))\n    lse = triton.empty_like(logits, shape=(logits.shape[0],))\n    z_loss = triton.empty_like(logits, shape=(logits.shape[0],))\n\n    grid = (logits.shape[0],)\n    cross_entropy_fwd_kernel[grid](\n        logits, labels, loss, lse, z_loss, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, HAS_SMOOTHING, SPLIT\n    )\n\n    return loss, lse, z_loss\n\ndef cross_entropy_bwd(\n    logits,  # *float32\n    labels,  # *int32\n    dloss,   # *float32\n    lse,     # *float32\n    z_loss,  # *float32\n    smoothing: float = 0.0,\n    logit_scale: float = 1.0,\n    ignored_index: int = -100,\n    total_classes: int = 1000,\n    class_start_idx: int = 0,\n    BLOCK_SIZE: int = 128,\n    HAS_SMOOTHING: bool = False,\n    SPLIT: bool = False\n):\n    assert logits.shape[0] == labels.shape[0]\n    assert logits.shape[1] == total_classes\n\n    dlogits = triton.empty_like(logits)\n\n    grid = (logits.shape[0],)\n    cross_entropy_bwd_kernel[grid](\n        logits, labels, dloss, dlogits, lse, z_loss, smoothing, logit_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, HAS_SMOOTHING, SPLIT\n    )\n\n    return dlogits\n\nimport triton\nimport triton.language as tl\n\n# Example usage\nlogits = triton.random((128, 1000), dtype=tl.float32)\nlabels = triton.random((128,), dtype=tl.int32, low=0, high=1000)\n\nloss, lse, z_loss = cross_entropy_fwd(logits, labels, smoothing=0.1, logit_scale=1.0, ignored_index=-100, total_classes=1000, class_start_idx=0, BLOCK_SIZE=128, HAS_SMOOTHING=True, SPLIT=False)\n\ndloss = triton.ones((128,), dtype=tl.float32)\ndlogits = cross_entropy_bwd(logits, labels, dloss, lse, z_loss, smoothing=0.1, logit_scale=1.0, ignored_index=-100, total_classes=1000, class_start_idx=0, BLOCK_SIZE=128, HAS_SMOOTHING=True, SPLIT=False)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    loss_ptr,  # data ptrs\n    lse_ptr,\n    z_loss_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    n_rows,\n    logits_row_stride,  # strides\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n            else:\n                loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    dlogits_ptr,  # data ptrs\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    logits_row_stride,  # strides\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, (dloss * logit_scale) * probs, mask=col_offsets < n_cols)\n\ndef cross_entropy_fwd(\n    logits, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, HAS_SMOOTHING, SPLIT\n):\n    n_rows, n_cols = logits.shape\n    loss = torch.empty((n_rows, n_cols), dtype=torch.float32, device=logits.device)\n    lse = torch.empty((n_rows, n_cols), dtype=torch.float32, device=logits.device)\n    z_loss = torch.empty((n_rows, n_cols), dtype=torch.float32, device=logits.device)\n    \n    grid = (n_rows, (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE)\n    \n    # \u8c03\u7528\u524d\u5411\u5185\u6838\uff0c\u4f20\u9012\u76f8\u5173\u53c2\u6570\n    cross_entropy_fwd_kernel[grid](\n        loss, lse, z_loss, logits, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, n_cols, n_rows, logits.stride(0), BLOCK_SIZE, HAS_SMOOTHING, SPLIT\n    )\n    \n    # \u6253\u5370\u635f\u5931\u3001LSE\u548cz_loss\uff0c\u5e2e\u52a9\u8c03\u8bd5\n    print(f\"Forward loss: {loss}\")\n    print(f\"Forward LSE: {lse}\")\n    print(f\"Forward z_loss: {z_loss}\")\n    \n    return loss, lse, z_loss\n\ndef cross_entropy_bwd(\n    dloss, logits, lse, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, HAS_SMOOTHING\n):\n    n_rows, n_cols = logits.shape\n    dlogits = torch.empty_like(logits)\n    \n    grid = (n_rows, (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE)\n    \n    # \u8c03\u7528\u53cd\u5411\u5185\u6838\uff0c\u4f20\u9012\u76f8\u5173\u53c2\u6570\n    cross_entropy_bwd_kernel[grid](\n        dlogits, dloss, logits, lse, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, n_cols, logits.stride(0), dlogits.stride(0), dloss.stride(0), BLOCK_SIZE, HAS_SMOOTHING\n    )\n    \n    # \u6253\u5370\u53cd\u5411\u68af\u5ea6\uff0c\u5e2e\u52a9\u8c03\u8bd5\n    print(f\"Backward dlogits: {dlogits}\")\n    \n    return dlogits\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel function, `_fwd_kernel_int8kv`, implements the forward pass for an attention mechanism optimized for int8 key and value matrices. It computes the attention scores between a query (Q) and key (K) matrix, applies a softmax scaling, and multiplies the result by a value (V) matrix to produce the output (Out). The kernel uses several input parameters, such as strides and batch information, to optimize memory access patterns. The function `context_attention_fwd_ppl_int8kv` acts as a wrapper, setting up the grid and launching the kernel with appropriate parameters.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_int8kv(\n    Q, K, V, Out,\n    stride_qb, stride_qh, stride_qd,\n    stride_kb, stride_kh, stride_kd,\n    stride_vb, stride_vh, stride_vd,\n    stride_ob, stride_oh, stride_od,\n    n_heads, head_dim, n_ctx,\n    BLOCK_SIZE: tl.constexpr,\n    BLOCK_SIZE_HEAD: tl.constexpr\n):\n    # Get program ID\n    pid = tl.program_id(axis=0)\n    # Compute the batch and head indices\n    bid = pid // n_heads\n    hid = pid % n_heads\n\n    # Pointers to the query, key, and value matrices\n    Q_ptr = Q + bid * stride_qb + hid * stride_qh\n    K_ptr = K + bid * stride_kb + hid * stride_kh\n    V_ptr = V + bid * stride_vb + hid * stride_vh\n    Out_ptr = Out + bid * stride_ob + hid * stride_oh\n\n    # Allocate shared memory for the attention scores\n    scores = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n\n    # Iterate over the context length in blocks\n    for i in range(0, n_ctx, BLOCK_SIZE):\n        # Load the query block\n        q = tl.load(Q_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_qd + tl.arange(0, head_dim))\n        # Load the key block\n        k = tl.load(K_ptr + i + tl.arange(0, BLOCK_SIZE) * stride_kd + tl.arange(0, head_dim)[:, None])\n\n        # Compute the attention scores\n        scores = tl.dot(q, k, allow_tf32=False)\n\n        # Apply the softmax scaling\n        max_score = tl.max(scores, 1)\n        scores = scores - max_score[:, None]\n        scores = tl.exp(scores)\n\n        # Normalize the scores\n        sum_scores = tl.sum(scores, 1)\n        scores = scores / sum_scores[:, None]\n\n        # Load the value block\n        v = tl.load(V_ptr + i + tl.arange(0, BLOCK_SIZE) * stride_vd + tl.arange(0, head_dim)[:, None])\n\n        # Compute the output\n        out = tl.dot(scores, v, allow_tf32=False)\n\n        # Store the output\n        tl.store(Out_ptr + tl.arange(0, BLOCK_SIZE) * stride_od + tl.arange(0, head_dim)[:, None], out)\n\n        # Move to the next block\n        Q_ptr += BLOCK_SIZE * stride_qd\n        K_ptr += BLOCK_SIZE * stride_kd\n        V_ptr += BLOCK_SIZE * stride_vd\n        Out_ptr += BLOCK_SIZE * stride_od\n\nimport torch\n\ndef context_attention_fwd_ppl_int8kv(Q, K, V, Out, n_heads, head_dim, n_ctx, BLOCK_SIZE=128, BLOCK_SIZE_HEAD=64):\n    # Get the grid size\n    grid = (Q.shape[0] * n_heads,)\n\n    # Launch the kernel\n    _fwd_kernel_int8kv[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        n_heads, head_dim, n_ctx,\n        BLOCK_SIZE, BLOCK_SIZE_HEAD\n    )\n\n# Example usage\nQ = torch.randint(-128, 128, (1, 8, 64), dtype=torch.int8, device='cuda')\nK = torch.randint(-128, 128, (1, 8, 64), dtype=torch.int8, device='cuda')\nV = torch.randint(-128, 128, (1, 8, 64), dtype=torch.int8, device='cuda')\nOut = torch.zeros((1, 8, 64), dtype=torch.int8, device='cuda')\n\ncontext_attention_fwd_ppl_int8kv(Q, K, V, Out, n_heads=8, head_dim=64, n_ctx=64)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\nTESLA = \"Tesla\" in torch.cuda.get_device_name(0)\n\n@triton.jit\ndef _fwd_kernel_int8kv(\n    Q,\n    K,\n    V,\n    sm_scale,\n    Out,\n    B_Start_Loc,\n    B_Seqlen,\n    b_prompt_cache_len,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kb,\n    stride_kh,\n    stride_ks,\n    stride_kd,\n    stride_vb,\n    stride_vh,\n    stride_vs,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    H: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n\n    cur_kv_head = cur_head // kv_group_num\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len, cur_batch_seq_len + prompt_cache_len)\n    # causal mask\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        # k = tl.load(\n        #     k_ptrs + (start_n + offs_n[None, :]) * stride_ks,\n        #     mask=(start_n + offs_n[None, :]) < block_end_loc,\n        #     other=0,\n        # )\n        off_k = (\n            cur_batch * stride_kb\n            + (start_n + offs_n[None, :]) * stride_ks\n            + cur_kv_head * stride_kh\n            + offs_d[:, None] * stride_kd\n        )\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :]) < block_end_loc, other=0.0)\n\n        qk = tl.dot(q, k)\n        mask = (offs_m[:, None] + prompt_cache_len) >= (start_n + offs_n[None, :])\n        qk = tl.where(mask, qk * sm_scale, -1.0e8)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n\n        # -- update m_i and l_i\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        # -- update output accumulator --\n        acc = acc * alpha[:, None]\n        # update acc\n        # v = tl.load(\n        #     v_ptrs + (start_n + offs_n[:, None]) * stride_vs,\n        #     mask=(start_n + offs_n[:, None]) < block_end_loc,\n        #     other=0.0,\n        # )\n        off_v = (\n            cur_batch * stride_vb\n            + (start_n + offs_n[:, None]) * stride_vs\n            + cur_kv_head * stride_vh\n            + offs_d[None, :] * stride_vd\n        )\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None]) < block_end_loc, other=0.0)\n\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        # update m_i\n        m_i = m_ij\n\n    acc = acc / l_i[:, None]\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n\n\n@torch.no_grad()\ndef context_attention_fwd_ppl_int8kv(q, k, v, o, b_start_loc, b_seq_len, max_input_len, b_prompt_cache_len):\n    BLOCK_M = 128 if not TESLA else 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128, 256}\n\n    # \u8ba1\u7b97scale\u7cfb\u6570, \u5e76\u4e58\u4ee5 1/log(2) = 1.4426950408889634,\n    # \u7b97\u5b50\u5185\u90e8\u4f7f\u7528 tl.math.exp2 \u6765\u4f7f\u8ba1\u7b97\u4e0e\u6807\u51c6attention\u7b49\u4ef7\u3002\n    sm_scale = 1.0 / (Lq ** 0.5) * 1.4426950408889634\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = lambda meta: (triton.cdiv(max_input_len, meta[\"BLOCK_M\"]), batch * head, 1)\n    BLOCK_N = BLOCK_M\n    num_warps = 4 if Lk <= 64 else 8\n    num_stages = 1\n\n    _fwd_kernel_int8kv[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        o,\n        b_start_loc,\n        b_seq_len,\n        b_prompt_cache_len,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        k.stride(3),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        v.stride(3),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        kv_group_num=kv_group_num,\n        H=head,\n        BLOCK_DMODEL=Lk,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        num_warps=num_warps,\n        num_stages=num_stages,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `masked_add_kernel` and a wrapper function `masked_add`. The kernel performs an element-wise operation on the input tensors, applying a mask to skip certain elements. The `masked_add` function sets up the necessary parameters and calls the kernel. It modifies `grad` by adding `p_data` scaled by `alpha` only where the mask is 0.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef masked_add_kernel(\n    grad_ptr,  # Pointer to the grad tensor\n    p_data_ptr,  # Pointer to the p_data tensor\n    mask_ptr,  # Pointer to the mask tensor\n    grad_stride,  # Stride of the grad tensor\n    p_data_stride,  # Stride of the p_data tensor\n    mask_stride,  # Stride of the mask tensor\n    alpha,  # Scaling factor\n    n_elements,  # Number of elements\n    BLOCK_SIZE: tl.constexpr  # Block size\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the data\n    grad = tl.load(grad_ptr + offsets, mask=offsets < n_elements)\n    p_data = tl.load(p_data_ptr + offsets, mask=offsets < n_elements)\n    mask = tl.load(mask_ptr + offsets, mask=offsets < n_elements)\n\n    # Apply the mask and add\n    result = tl.where(mask == 0, grad + p_data * alpha, grad)\n\n    # Store the result back to grad\n    tl.store(grad_ptr + offsets, result, mask=offsets < n_elements)\n\nimport torch\n\ndef masked_add(grad, p_data, mask, alpha):\n    assert grad.shape == p_data.shape == mask.shape, \"Input tensors must have the same shape\"\n    assert grad.is_cuda and p_data.is_cuda and mask.is_cuda, \"Input tensors must be on the same GPU\"\n\n    n_elements = grad.numel()\n    grid = (triton.cdiv(n_elements, 1024),)\n\n    grad_ptr = grad.data_ptr()\n    p_data_ptr = p_data.data_ptr()\n    mask_ptr = mask.data_ptr()\n\n    grad_stride = grad.stride(0)\n    p_data_stride = p_data.stride(0)\n    mask_stride = mask.stride(0)\n\n    masked_add_kernel[grid](\n        grad_ptr,\n        p_data_ptr,\n        mask_ptr,\n        grad_stride,\n        p_data_stride,\n        mask_stride,\n        alpha,\n        n_elements,\n        BLOCK_SIZE=1024\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    grad = torch.randn(1024, device='cuda')\n    p_data = torch.randn(1024, device='cuda')\n    mask = torch.randint(0, 2, (1024,), device='cuda', dtype=torch.int32)\n    alpha = 0.5\n\n    masked_add(grad, p_data, mask, alpha)\n\n    print(\"grad after masked_add:\", grad)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef masked_add_kernel(grad_ptr,\n                      p_ptr,\n                      p_mask_ptr,\n                      n_elements,\n                      alpha,\n                      BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    p_mask = tl.load(p_mask_ptr + offsets, mask=mask).to(tl.int1)\n    mask = mask & ~p_mask\n    p = tl.load(p_ptr + offsets, mask=mask)\n    grad = tl.load(grad_ptr + offsets, mask=mask)\n    grad += p * alpha\n    tl.store(grad_ptr + offsets, grad, mask=mask)\n\ndef masked_add(grad: torch.Tensor, p_data: torch.Tensor, p_mask: torch.Tensor, alpha: float = 0):\n    '''\n    equivalent to\n    grad.add_(p.data * (1 - p.mask), alpha=decay)\n    '''\n    assert grad.is_cuda and p_data.is_cuda and p_mask.is_cuda\n    assert (grad.layout, p_data.layout, p_mask.layout) == (torch.strided, torch.strided, torch.strided)\n    assert grad.stride() == p_data.stride() == p_mask.stride()\n    n_elements = grad.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    masked_add_kernel[grid](grad, p_data, p_mask, n_elements, alpha, BLOCK_SIZE=1024)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel function '_triton_rope' performs a rotational position encoding operation (rope) on query (q) and key (k) matrices. It applies sine and cosine transformations, controlled by the 'cos' and 'sin' inputs, on these matrices. The function 'rope_forward' orchestrates this by preparing the data and invoking the kernel. It rearranges the dimensions of input tensors and calculates padding sizes to fit hardware constraints. The function returns transformed q, k, cos, and sin tensors.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(q, k, cos, sin, q_batch_stride, q_head_stride, q_seq_stride, q_embed_stride, \n                 k_batch_stride, k_head_stride, k_seq_stride, k_embed_stride, \n                 cos_seq_stride, cos_embed_stride, sin_seq_stride, sin_embed_stride, \n                 seq_len, embed_dim, BLOCK_SIZE: \n    pid = tl.program_id(axis=0)\n    batch_id = pid // (seq_len * embed_dim)\n    seq_id = (pid % (seq_len * embed_dim)) // embed_dim\n    embed_id = (pid % (seq_len * embed_dim)) % embed_dim\n\n    q_offset = batch_id * q_batch_stride + seq_id * q_seq_stride + embed_id * q_embed_stride\n    k_offset = batch_id * k_batch_stride + seq_id * k_seq_stride + embed_id * k_embed_stride\n    cos_offset = seq_id * cos_seq_stride + embed_id * cos_embed_stride\n    sin_offset = seq_id * sin_seq_stride + embed_id * sin_embed_stride\n\n    q_val = tl.load(q + q_offset)\n    k_val = tl.load(k + k_offset)\n    cos_val = tl.load(cos + cos_offset)\n    sin_val = tl.load(sin + sin_offset)\n\n    q_new = q_val * cos_val - k_val * sin_val\n    k_new = k_val * cos_val + q_val * sin_val\n\n    tl.store(q + q_offset, q_new)\n    tl.store(k + k_offset, k_new)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef rope_forward(q, k, cos, sin):\n    assert q.shape == k.shape, \"Query and Key tensors must have the same shape\"\n    assert q.shape[-1] == cos.shape[-1] == sin.shape[-1], \"Embedding dimension mismatch\"\n    assert q.shape[-2] == cos.shape[-2] == sin.shape[-2], \"Sequence length mismatch\"\n\n    batch_size, num_heads, seq_len, embed_dim = q.shape\n\n    # Rearrange dimensions to fit hardware constraints\n    q = q.contiguous()\n    k = k.contiguous()\n    cos = cos.contiguous()\n    sin = sin.contiguous()\n\n    # Calculate padding sizes\n    block_size = 128  # Example block size, can be adjusted\n    padded_seq_len = (seq_len + block_size - 1) // block_size * block_size\n    padded_embed_dim = (embed_dim + block_size - 1) // block_size * block_size\n\n    # Pad tensors if necessary\n    if padded_seq_len > seq_len or padded_embed_dim > embed_dim:\n        q = torch.nn.functional.pad(q, (0, padded_embed_dim - embed_dim, 0, padded_seq_len - seq_len))\n        k = torch.nn.functional.pad(k, (0, padded_embed_dim - embed_dim, 0, padded_seq_len - seq_len))\n        cos = torch.nn.functional.pad(cos, (0, padded_embed_dim - embed_dim, 0, padded_seq_len - seq_len))\n        sin = torch.nn.functional.pad(sin, (0, padded_embed_dim - embed_dim, 0, padded_seq_len - seq_len))\n\n    # Launch the kernel\n    grid = (batch_size * padded_seq_len * padded_embed_dim, )\n    _triton_rope[grid](\n        q, k, cos, sin,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        cos.stride(0), cos.stride(1), cos.stride(2), cos.stride(3),\n        sin.stride(0), sin.stride(1), sin.stride(2), sin.stride(3),\n        padded_seq_len, padded_embed_dim, block_size\n    )\n\n    # Remove padding if necessary\n    q = q[:, :, :seq_len, :embed_dim]\n    k = k[:, :, :seq_len, :embed_dim]\n    cos = cos[:, :, :seq_len, :embed_dim]\n    sin = sin[:, :, :seq_len, :embed_dim]\n\n    return q, k, cos, sin\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr,\n    q_row_stride,\n    k_ptr,\n    k_row_stride,\n    cos,\n    cos_row_stride,\n    sin,\n    sin_row_stride,\n    sl,\n    bs: tl.constexpr,\n    n_qh: tl.constexpr,\n    n_kh: tl.constexpr,\n    hd: tl.constexpr,\n    pad_n_qh: tl.constexpr,\n    pad_n_kh: tl.constexpr,\n    pad_hd: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr = False,\n):\n    pid = tl.program_id(0)\n\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n\n    cos_row_idx = pid % (sl)\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0).to(sin_row.dtype)\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0).to(sin_row.dtype)\n\n    second_half_q_offsets = first_half_q_offsets + (hd // 2)\n    second_half_k_offsets = first_half_k_offsets + (hd // 2)\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask, other=0).to(sin_row.dtype)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask, other=0).to(sin_row.dtype)\n\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n\n\ndef rope_forward(q, k, cos, sin):\n    q = q.transpose(1, 2)\n    k = k.transpose(1, 2)\n\n    batch_size, seq_len, n_q_head, head_dim = q.shape\n    n_kv_head = k.shape[2]\n    pad_hd = triton.next_power_of_2(head_dim)\n    pad_n_q_head = triton.next_power_of_2(n_q_head)\n    pad_n_kv_head = triton.next_power_of_2(n_kv_head)\n    BLOCK_SIZE = max(pad_n_q_head, pad_n_kv_head)\n\n    n_row = batch_size * seq_len\n\n    q = q.contiguous()\n    k = k.contiguous()\n    cos = cos.contiguous()\n    sin = sin.contiguous()\n\n    _triton_rope[(n_row,)](\n        q,\n        q.stride(1),\n        k,\n        k.stride(1),\n        cos,\n        cos.stride(-2),\n        sin,\n        sin.stride(-2),\n        seq_len,\n        batch_size,\n        n_q_head,\n        n_kv_head,\n        head_dim,\n        pad_n_q_head,\n        pad_n_kv_head,\n        pad_hd,\n        BLOCK_SIZE=BLOCK_SIZE,\n        BACKWARD_PASS=False,\n    )\n    return q.transpose(1, 2), k.transpose(1, 2), cos, sin\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs a backward operation for a batched matrix multiplication. The main function is _bmm_chunk_bwd_kernel, which computes the gradient with respect to the matrix 'a' and optionally adds a residual term. The function uses loop tiling for efficient computation and leverages block sizes for both M, N, and CS dimensions.\n\n            The kernel takes several pointers for data input/output (a_ptr, dout_ptr, db_ptr, res_ptr), along with various dimensions and strides for indexing. It has a configurable BLOCK_SIZE for efficient data loading and processing. The kernel uses tl.dot for matrix multiplication and optionally adds a residual stored at res_ptr if HAS_RESIDUAL is True. \n\n            The function _bmm_chunk_bwd is a Python wrapper that handles the preparation of input tensors and their metadata, like strides and shapes, then launches the Triton kernel with an appropriate grid of block dimensions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bmm_chunk_bwd_kernel(\n    a_ptr, dout_ptr, db_ptr, res_ptr,  # Pointers to input/output data\n    M, N, K, CS,  # Dimensions\n    stride_am, stride_ak,  # Strides for matrix a\n    stride_dout_bk, stride_dout_bn,  # Strides for matrix dout\n    stride_db_km, stride_db_kn,  # Strides for matrix db\n    stride_res_m,  # Stride for residual matrix\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for M dimension\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for N dimension\n    BLOCK_SIZE_CS: tl.constexpr,  # Block size for CS dimension\n    HAS_RESIDUAL: tl.constexpr  # Flag to indicate if there is a residual term\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_batch = num_pid_m * num_pid_n\n    batch_id = pid // num_pid_in_batch\n    pid_mn = pid % num_pid_in_batch\n    pid_m = pid_mn // num_pid_n\n    pid_n = pid_mn % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_CS)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    dout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_bk + offs_n[None, :] * stride_dout_bn)\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_km + offs_n[None, :] * stride_db_kn)\n\n    a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K)\n    dout_mask = (offs_k[:, None] < K) & (offs_n[None, :] < N)\n\n    a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n    dout = tl.load(dout_ptrs, mask=dout_mask, other=0.0)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_CS):\n        db = tl.dot(a, dout)\n        acc += db\n\n    db_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.atomic_add(db_ptrs, acc, mask=db_mask)\n\n    if HAS_RESIDUAL:\n        res_ptrs = res_ptr + (offs_m * stride_res_m)\n        res = tl.load(res_ptrs, mask=offs_m < M, other=0.0)\n        db += res\n        tl.store(db_ptrs, db, mask=db_mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _bmm_chunk_bwd(a, dout, db, res=None, BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_CS=32):\n    # Ensure the inputs are contiguous\n    a = a.contiguous()\n    dout = dout.contiguous()\n    db = db.contiguous()\n    if res is not None:\n        res = res.contiguous()\n\n    # Get dimensions\n    B, M, K = a.shape\n    B, K, N = dout.shape\n    B, M, N = db.shape\n\n    # Get strides\n    stride_am = a.stride(1)\n    stride_ak = a.stride(2)\n    stride_dout_bk = dout.stride(1)\n    stride_dout_bn = dout.stride(2)\n    stride_db_km = db.stride(1)\n    stride_db_kn = db.stride(2)\n    stride_res_m = res.stride(1) if res is not None else 0\n\n    # Launch the kernel\n    grid = (B * triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n    _bmm_chunk_bwd_kernel[grid](\n        a, dout, db, res,\n        M, N, K, CS,\n        stride_am, stride_ak,\n        stride_dout_bk, stride_dout_bn,\n        stride_db_km, stride_db_kn,\n        stride_res_m,\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_CS,\n        res is not None\n    )\n\n# Example usage\nB, M, K, N = 2, 1024, 1024, 1024\na = torch.randn((B, M, K), device='cuda')\ndout = torch.randn((B, K, N), device='cuda')\ndb = torch.zeros((B, M, N), device='cuda')\nres = torch.randn((B, M), device='cuda')\n\n_bmm_chunk_bwd(a, dout, db, res)\n", "label": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2),\n    ],\n    key=['chunk_size', 'K'],\n)\n@triton.jit\ndef _bmm_chunk_bwd_kernel(\n    a_ptr, dout_ptr, db_ptr, res_ptr,\n    seqlen, chunk_size, K, ngroups,\n    stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak,\n    stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_csize_m, stride_dout_csize_n,\n    stride_db_batch, stride_db_seqlen, stride_db_head, stride_db_k,\n    stride_res_batch, stride_res_seqlen, stride_res_head, stride_res_k,\n    dot_dtype: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_CS: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    a_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\n    dout_ptr += pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m)\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS), other=0.0).to(dot_dtype)\n        a = tl.load(a_ptrs, mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS) & (offs_n[None, :] < K), other=0.0).to(dot_dtype)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += pid_b * stride_res_batch + pid_c * chunk_size * stride_res_seqlen + pid_h * stride_res_head\n        res_ptrs = res_ptr + (offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k)\n        res = tl.load(res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)).to(tl.float32)\n        acc += res\n    db = acc.to(db_ptr.dtype.element_ty)\n\n    db_ptr += pid_b * stride_db_batch + pid_c * chunk_size * stride_db_seqlen + pid_h * stride_db_head\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k)\n    tl.store(db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K))\n\ndef _bmm_chunk_bwd(a, dout, residual=None, out=None):\n    has_groups = a.dim() == 4\n    if not has_groups:\n        batch, seqlen, k = a.shape\n    else:\n        batch, seqlen, ngroups, k = a.shape\n    nchunks, chunk_size = dout.shape[1], dout.shape[-1]\n    if a.stride(-1) != 1 and a.stride(-2) != 1:\n        a = a.contiguous()\n    if dout.stride(-1) != 1 and dout.stride(-2) != 1:\n        dout = dout.contiguous()\n    if residual is not None:\n        assert residual.shape == (batch, seqlen, k) if not has_groups else (batch, seqlen, ngroups, k)\n        if residual.stride(-1) != 1 and residual.stride(1) != 1:\n            residual = residual.contiguous()\n    if out is not None:\n        assert out.shape == a.shape\n        assert out.stride(-1) == 1 or out.stride(1) == 1\n    else:\n        out = torch.empty_like(a)\n    dot_dtype = (tl.bfloat16 if a.dtype == torch.bfloat16 or dout.dtype == torch.bfloat16 else\n                 (tl.float16 if a.dtype == torch.float16 or dout.dtype == torch.float16 else tl.float32))\n    grid = lambda META: (triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(k, META['BLOCK_SIZE_N']), batch,\n                    nchunks if not has_groups else nchunks * ngroups)\n    residual_strides = ((residual.stride(0), residual.stride(1), 0 if not has_groups else residual.stride(2),\n                         residual.stride(-1))\n                        if residual is not None else (0, 0, 0, 0))\n    with torch.cuda.device(a.device.index):\n        _bmm_chunk_bwd_kernel[grid](\n            a, dout, out, residual,\n            int(seqlen), int(chunk_size), int(k), int(ngroups if has_groups else 1),\n            a.stride(0), a.stride(1), 0 if not has_groups else a.stride(2), a.stride(-1),\n            dout.stride(0), dout.stride(1), 0 if not has_groups else dout.stride(2), dout.stride(-2), dout.stride(-1),\n            out.stride(0), out.stride(1), 0 if not has_groups else out.stride(2), out.stride(-1),\n            residual_strides[0], residual_strides[1], residual_strides[2], residual_strides[3],\n            dot_dtype,\n            HAS_RESIDUAL=residual is not None,\n        )\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton operator computes the Kullback-Leibler divergence (KL divergence) between predicted log-probabilities and ground truth distributions. It includes both forward and backward kernels. The `kldiv_forward_triton` function handles the forward pass, while `kldiv_backward_triton` takes care of the backward pass.\n\n    The main function `kldiv_forward_triton` accepts the following inputs:\n    - `y_pred`: Predicted log-probabilities tensor with shape [BT, V].\n    - `y_true`: Ground truth probabilities tensor with the same shape [BT, V].\n    - `log_target`: A boolean indicating if the ground truth is in log-space.\n    - `reduction`: A string specifying the reduction method (\"none\", \"sum\", \"mean\", \"batchmean\").\n    - `eps`: A small value to ensure numerical stability in logarithms.\n\n    The kernel `_kldiv_kernel_forward` calculates KL divergence either directly or in log-space depending on `log_target`. If `reduction` is set to \"none\", results are stored directly; otherwise, a sum is computed.\n\n    The function `kldiv_backward_triton` computes the gradient of the loss with respect to the input. It requires:\n    - `target`: The ground truth tensor [BT, V].\n    - `grad_output`: Gradient tensor from subsequent layers.\n    - `new_grads`: Tensor to store the computed gradients.\n    - `log_target`: Boolean to indicate log-space ground truth.\n\n    The backward kernel `_kldiv_kernel_backward` performs differentiation considering whether the target is in log-space.\n\n    The `BLOCK_SIZE` and `num_warps` control the parallel computation's granularity and efficiency.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nBLOCK_SIZE = 256\nNUM_WARPS = 4\n\n# Forward Kernel\n@triton.jit\ndef _kldiv_kernel_forward(\n    y_pred_ptr, y_true_ptr, output_ptr, \n    BT, V, log_target, reduction, eps,\n    stride_y_pred_BT, stride_y_pred_V, \n    stride_y_true_BT, stride_y_true_V, \n    stride_output_BT, stride_output_V,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < BT * V\n\n    y_pred_offsets = tl.zeros((BLOCK_SIZE,), tl.int32) + offsets\n    y_true_offsets = tl.zeros((BLOCK_SIZE,), tl.int32) + offsets\n    output_offsets = tl.zeros((BLOCK_SIZE,), tl.int32) + offsets\n\n    y_pred = tl.load(y_pred_ptr + y_pred_offsets, mask=mask, other=0.0)\n    y_true = tl.load(y_true_ptr + y_true_offsets, mask=mask, other=0.0)\n\n    if log_target:\n        y_true = tl.exp(y_true)\n\n    kl_div = y_true * (tl.log(y_true + eps) - y_pred)\n    tl.store(output_ptr + output_offsets, kl_div, mask=mask)\n\n    if reduction == \"sum\":\n        block_sum = tl.sum(kl_div, axis=0)\n        tl.atomic_add(output_ptr, block_sum)\n    elif reduction == \"mean\":\n        block_sum = tl.sum(kl_div, axis=0)\n        block_mean = block_sum / (BT * V)\n        tl.atomic_add(output_ptr, block_mean)\n    elif reduction == \"batchmean\":\n        block_sum = tl.sum(kl_div, axis=0)\n        block_mean = block_sum / BT\n        tl.atomic_add(output_ptr, block_mean)\n\n# Forward Wrapper\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction, eps):\n    BT, V = y_pred.shape\n    output = tl.zeros((BT, V), dtype=tl.float32)\n\n    grid = (BT * V + BLOCK_SIZE - 1) // BLOCK_SIZE\n    _kldiv_kernel_forward[grid](\n        y_pred, y_true, output,\n        BT, V, log_target, reduction, eps,\n        y_pred.stride(0), y_pred.stride(1),\n        y_true.stride(0), y_true.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    if reduction in [\"sum\", \"mean\", \"batchmean\"]:\n        output = output.sum()\n\n    return output\n\n# Backward Kernel\n@triton.jit\ndef _kldiv_kernel_backward(\n    target_ptr, grad_output_ptr, new_grads_ptr,\n    BT, V, log_target,\n    stride_target_BT, stride_target_V,\n    stride_grad_output_BT, stride_grad_output_V,\n    stride_new_grads_BT, stride_new_grads_V,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < BT * V\n\n    target_offsets = tl.zeros((BLOCK_SIZE,), tl.int32) + offsets\n    grad_output_offsets = tl.zeros((BLOCK_SIZE,), tl.int32) + offsets\n    new_grads_offsets = tl.zeros((BLOCK_SIZE,), tl.int32) + offsets\n\n    target = tl.load(target_ptr + target_offsets, mask=mask, other=0.0)\n    grad_output = tl.load(grad_output_ptr + grad_output_offsets, mask=mask, other=0.0)\n\n    if log_target:\n        target = tl.exp(target)\n\n    new_grads = -target * grad_output\n    tl.store(new_grads_ptr + new_grads_offsets, new_grads, mask=mask)\n\n# Backward Wrapper\ndef kldiv_backward_triton(target, grad_output, log_target):\n    BT, V = target.shape\n    new_grads = tl.zeros((BT, V), dtype=tl.float32)\n\n    grid = (BT * V + BLOCK_SIZE - 1) // BLOCK_SIZE\n    _kldiv_kernel_backward[grid](\n        target, grad_output, new_grads,\n        BT, V, log_target,\n        target.stride(0), target.stride(1),\n        grad_output.stride(0), grad_output.stride(1),\n        new_grads.stride(0), new_grads.stride(1),\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return new_grads\n\nimport torch\n\n# Example usage\nBT, V = 1024, 128\ny_pred = torch.randn(BT, V, device='cuda')\ny_true = torch.randn(BT, V, device='cuda').softmax(dim=1)  # Ensure y_true is a valid probability distribution\nlog_target = False\nreduction = \"mean\"\neps = 1e-8\n\n# Forward pass\nloss = kldiv_forward_triton(y_pred, y_true, log_target, reduction, eps)\nprint(\"Loss:\", loss)\n\n# Backward pass\ngrad_output = torch.ones_like(loss, device='cuda')\ngrads = kldiv_backward_triton(y_true, grad_output, log_target)\nprint(\"Gradients:\", grads)\n", "label": "from typing import Literal\nimport torch\nimport triton\nimport triton.language as tl\n\n\nMAX_FUSED_SIZE = 65536 // 4  # 65536 // 4 or 8 works the best\nREDUCTION_LITERAL = Literal[\"none\", \"sum\", \"mean\", \"batchmean\"]\n_REDUCTION_MODE_BATCHMEAN = tl.constexpr(3)\n\n@triton.jit\ndef _kldiv_kernel_forward(\n    y_ptr,  # [B, S], prediction ptr, the kernel expects the prediction in log-space\n    y_stride,  # int, prediction stride\n    gt_ptr,  # [B, S], ground truth ptr\n    gt_stride,  # int, ground truth stride\n    loss_ptr,  # [B] or [B, S] if reduction == _REDUCTION_MODE_NONE, output ptr\n    loss_stride,  # int, output stride\n    n_cols,  # int, number of columns in the input tensor\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n    reduction: tl.constexpr = _REDUCTION_MODE_BATCHMEAN,\n):\n    pid = tl.program_id(0).to(tl.int64)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n\n    loss_sum = 0.0\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n\n        # KL(y_true || y) = y_true * (log(y_true) - log(y))\n        # We compute KL(y_true || y) with y in the log-space\n        if not log_target:\n            loss = y_true * (tl.log(tl.maximum(y_true, eps)) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n\n        if reduction == 0:  # _REDUCTION_MODE_NONE\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss_sum += tl.sum(loss, axis=0)\n\n    if reduction != 0:\n        tl.store(loss_ptr, loss_sum)\n\n\n@triton.jit\ndef _kldiv_kernel_backward(\n    target_ptr,\n    target_stride,\n    new_grads_ptr,\n    new_grads_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n):\n    pid = tl.program_id(0).to(tl.int64)\n\n    target_ptr += pid * target_stride\n    new_grads_ptr += pid * new_grads_stride\n\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n\n        tl.store(new_grads_ptr + offsets, res, mask=mask)\n\n\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction, eps):  # [BT, V]\n    BT, V = y_pred.shape\n\n    BLOCK_SIZE = min(16384, triton.next_power_of_2(V))\n    num_warps = 4 if BLOCK_SIZE < 2048 else 8 if BLOCK_SIZE < 8192 else 16 if BLOCK_SIZE < 32768 else 32\n\n    grid = (BT,)\n    reduction = {\"none\": 0, \"sum\": 1, \"mean\": 2, \"batchmean\": 3}[reduction]\n\n    out_size = (BT, V) if reduction == 0 else (BT,)\n    output_tensor = torch.zeros(out_size, device=y_pred.device, dtype=torch.float32)\n\n    _kldiv_kernel_forward[grid](\n        y_pred,\n        y_pred.stride(0),\n        y_true,\n        y_true.stride(0),\n        output_tensor,\n        output_tensor.stride(0),\n        V,\n        eps=eps,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n        reduction=reduction,\n    )\n\n    if reduction == 3:  # _REDUCTION_MODE_BATCHMEAN\n        return output_tensor.sum() / BT\n    elif reduction == 1:  # _REDUCTION_MODE_SUM\n        return output_tensor.sum(dim=0)\n    elif reduction == 2:  # _REDUCTION_MODE_MEAN\n        return output_tensor.sum() / (BT * V)\n    else:\n        return output_tensor\n\n\ndef kldiv_backward_triton(target, grad_output, new_grads, log_target):\n    BT, V = target.shape\n\n    BLOCK_SIZE = min(16384, triton.next_power_of_2(V))\n    num_warps = 4 if BLOCK_SIZE < 2048 else 8 if BLOCK_SIZE < 8192 else 16 if BLOCK_SIZE < 32768 else 32\n\n    grid = (BT,)\n\n    _kldiv_kernel_backward[grid](\n        target,\n        target.stride(0),\n        new_grads,\n        new_grads.stride(0),\n        V,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n    )\n\n    if torch.equal(grad_output, torch.tensor(1.0, device=grad_output.device)):\n        return new_grads\n\n    return new_grads * grad_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton-based softmax operator for GPU execution, including a kernel function `softmax_kernel` and a wrapper function `softmax`.\n        The `softmax_kernel` computes the softmax function for each row of a 2D input tensor. It parallelizes across rows with one Triton program instance per row.\n        It loads row data, applies the softmax transformation by subtracting the maximum for numerical stability, and writes the result back to output memory.\n        The wrapper function `softmax` determines grid and block configuration based on input dimensions and manages memory, invoking the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    input_ptr,  # Pointer to the input tensor\n    output_ptr,  # Pointer to the output tensor\n    stride_row,  # Stride between rows in the input tensor\n    stride_col,  # Stride between columns in the input tensor\n    n_cols,  # Number of columns in the input tensor\n    BLOCK_SIZE: tl.constexpr  # Block size (number of elements processed per program instance)\n):\n    # Get the row index for this program instance\n    row_idx = tl.program_id(0)\n    \n    # Initialize the output row with zeros\n    row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Load the row data from the input tensor\n    for i in range(0, n_cols, BLOCK_SIZE):\n        cols = tl.arange(0, BLOCK_SIZE)\n        mask = cols < n_cols - i\n        row[cols] = tl.load(input_ptr + row_idx * stride_row + (i + cols) * stride_col, mask=mask)\n    \n    # Compute the maximum value in the row for numerical stability\n    max_val = tl.max(row, axis=0)\n    \n    # Subtract the maximum value from each element in the row\n    row -= max_val\n    \n    # Compute the exponential of each element\n    row = tl.exp(row)\n    \n    # Compute the sum of the exponentials\n    sum_exp = tl.sum(row, axis=0)\n    \n    # Normalize the row by dividing by the sum of exponentials\n    row /= sum_exp\n    \n    # Write the result back to the output tensor\n    for i in range(0, n_cols, BLOCK_SIZE):\n        cols = tl.arange(0, BLOCK_SIZE)\n        mask = cols < n_cols - i\n        tl.store(output_ptr + row_idx * stride_row + (i + cols) * stride_col, row[cols], mask=mask)\n\nimport torch\n\ndef softmax(input_tensor: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is on the GPU\n    input_tensor = input_tensor.cuda()\n    \n    # Get the dimensions of the input tensor\n    n_rows, n_cols = input_tensor.shape\n    \n    # Allocate memory for the output tensor\n    output_tensor = torch.empty_like(input_tensor, device=input_tensor.device)\n    \n    # Define the grid and block dimensions\n    grid = (n_rows, 1, 1)\n    block = (1024, 1, 1)  # Adjust block size as needed\n    \n    # Launch the kernel\n    softmax_kernel[grid, block](\n        input_tensor,  # Pointer to the input tensor\n        output_tensor,  # Pointer to the output tensor\n        input_tensor.stride(0),  # Stride between rows in the input tensor\n        input_tensor.stride(1),  # Stride between columns in the input tensor\n        n_cols,  # Number of columns in the input tensor\n        BLOCK_SIZE=block[0]  # Block size\n    )\n    \n    return output_tensor\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # The rows of the softmax are independent, so we parallelize across those\n    row_idx = tl.program_id(0)\n    # The stride represents how much we need to increase the pointer to advance 1 row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    # The block size is the next power of two greater than n_cols, so we can fit each\n    # row in a single block\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    # Subtract maximum for numerical stability\n    row_minus_max = row - tl.max(row, axis=0)\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    # Write back output to DRAM\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n    # The block size is the smallest power of two greater than the number of columns in `x`\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    y = torch.empty_like(x)\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row of the input matrix\n    softmax_kernel[(n_rows, )](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_cols,\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator implements a scalar-tensor power operation. The main function is `pow_func_scalar_tensor_kernel_rank_1`, which performs element-wise exponentiation of a tensor by a scalar. The kernel function is launched by the wrapper `pow_func_scalar_tensor_wrapper_rank_1`. It takes a scalar `val0`, a tensor `in0` (or a `StridedBuffer`), and outputs to `out0`. The operation is partitioned into tiles for efficient parallel processing. The wrapper also uses heuristics to determine optimal tile sizes and the number of warps.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef pow_func_scalar_tensor_kernel_rank_1(\n    val0: tl.float32,\n    in0_ptr: tl.tensor,\n    out0_ptr: tl.tensor,\n    n_elements: tl.int32,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in0_ptr + offsets, mask=mask)\n    y = tl.math.pow(x, val0)\n    tl.store(out0_ptr + offsets, y, mask=mask)\n\nimport triton\nimport triton.runtime as tr\nimport triton.language as tl\n\ndef pow_func_scalar_tensor_wrapper_rank_1(val0, in0, out0, BLOCK_SIZE=128, num_warps=4):\n    # Get the number of elements in the tensor\n    n_elements = in0.size\n\n    # Determine the grid size\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    pow_func_scalar_tensor_kernel_rank_1[grid_size, BLOCK_SIZE, num_warps](\n        val0,\n        in0.ptr,\n        out0.ptr,\n        n_elements,\n        BLOCK_SIZE\n    )\n\nimport torch\nimport triton\n\n# Example input tensor\nin0 = torch.tensor([1.0, 2.0, 3.0, 4.0], device='cuda')\n# Example scalar value\nval0 = 2.0\n# Output tensor\nout0 = torch.empty_like(in0)\n\n# Launch the wrapper\npow_func_scalar_tensor_wrapper_rank_1(val0, in0, out0)\n\n# Print the result\nprint(out0)  # Output: tensor([1., 4., 9., 16.])\n", "label": "import math\nfrom typing import Union\nimport torch\nimport triton\nfrom triton import language as tl\n\ntry:\n    from triton.language.extra.cuda.libdevice import pow as _pow\nexcept ImportError:\n    try:\n        from triton.language.math import pow as _pow\n    except ImportError:\n        from triton.language.libdevice import pow as _pow\n\ndef heuristics_for_tile_size(max_tile_size, *sizes):\n    ndim = len(sizes)\n    tile_sizes = [0 for _ in range(ndim)]\n    for i in range(ndim):\n        size = sizes[ndim - 1 - i]\n        tile_size = min(max_tile_size, triton.next_power_of_2(size))\n        tile_sizes[ndim - 1 - i] = tile_size\n        max_tile_size = max(1, max_tile_size // tile_size)\n    return tuple(tile_sizes)\n\ndef heuristics_for_num_warps(tile_size):\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n    \n\nclass StridedBuffer:\n    \"\"\"A drop-in replacement of torch.Tensor that can be used in wrapper generated by\n    PointwiseDynamicFunction. It allows us to use a different shape, stride, data\n    pointer that that of the base tensor.\n\n    It is a kind of reinterpretation of the base tensor. We make this class since we\n    cannot get a Tensor view with negative strides via torch APIs, while we need this\n    to implement flip op.\n\n    Although generated code can accept torch.Tensor & StridedBuffer, but StridedBuffer\n    may not have all the methods as torch.Tensors do. We add some attributes & methods\n    with the same name as torch.Tensor, which are used in the generated code. But we\n    may not cover all the methods, add one if what you need is missing here.\n\n    And can also be used in triton kernels since it also has dtype & data_ptr().\n    \"\"\"\n\n    def __init__(\n        self, base: torch.Tensor, shape=None, strides=None, dtype=None, offset=0\n    ):\n        self._base = base\n        self.dtype = dtype or base.dtype\n        if offset == 0:\n            self._data_ptr = self._base.data_ptr()\n        else:\n            offset = self.dtype.itemsize * offset\n            self._data_ptr = self._base.data_ptr() + offset\n        self.shape = tuple(shape if shape is not None else self._base.shape)\n        self._strides = tuple(strides if strides is not None else self._base.stride())\n        self.device = self._base.device\n        self.ndim = len(self.shape)\n\n    def stride(self):\n        return self._strides\n\n    def size(self):\n        return self.shape\n\n    def element_size(self):\n        return self.dtype.itemsize\n\n    def numel(self):\n        return math.prod(self.shape)\n\n    def dim(self):\n        return self.ndim\n\n    def unwrap(self):\n        return self._base\n\n    def data_ptr(self):\n        return self._data_ptr\n\n\ndef pow_func_scalar_tensor_wrapper_rank_1(val0, in0: Union[torch.Tensor, StridedBuffer], /, *, out0: Union[torch.Tensor, StridedBuffer]): \n    \"\"\"Generated wrapper function with Pointwise: scalar, StridedBuffer, StridedBuffer(a1!) -> StridedBuffer(a1!)\"\"\"\n    assert in0.shape == out0.shape, 'operand shapes mismatch'\n    # task partitioning\n    shape = out0.shape\n    num_tasks = out0.numel()\n    tile_sizes = heuristics_for_tile_size(512, *shape)\n    tile_size = math.prod(tile_sizes)\n    num_tiles = math.prod(triton.cdiv(size, tile_size) for size, tile_size in zip(shape, tile_sizes))\n    num_ctas = min(65536, num_tiles)\n    tiles_per_cta = triton.cdiv(num_tiles, num_ctas)\n    num_warps = heuristics_for_num_warps(tile_size)\n    one_tile_per_cta = tiles_per_cta==1\n    grid = (num_ctas, 1, 1)\n    # kernel launch\n    in0_strides = in0.stride()\n    in0_stride_order = (0,)\n    out0_strides = out0.stride()\n    out0_stride_order = (0,)\n    with torch.cuda._DeviceGuard(in0.device.index):\n        pow_func_scalar_tensor_kernel_rank_1[grid](\n            val0, in0, out0,\n            in0_strides[0], # stride for in0\n            in0_stride_order[0], # stride order for in0\n            out0_strides[0], # stride for out0\n            out0_stride_order[0], # stride orderfor out0\n            shape[0], # task indexing space\n            num_tasks, # num tasks\n            tiles_per_cta=tiles_per_cta, # tiles_per_cta\n            tile_size0=tile_sizes[0],\n            one_tile_per_cta=one_tile_per_cta,\n            num_warps=num_warps,\n        )\n    return out0\n\n@triton.jit\ndef pow_func_scalar_tensor(x, exponent):\n    return _pow(x.to(tl.float32), exponent)\n\n@triton.jit(do_not_specialize=['val0'])\ndef pow_func_scalar_tensor_kernel_rank_1(\n    val0,\n    in0_ptr: tl.tensor, # of tl.pointer_type\n    out0_ptr: tl.tensor, # of tl.pointer_type\n    in0_stride0: int, # strides for in0\n    in0_stride_order0: tl.constexpr, # stride order for in0\n    out0_stride0: int, # strides for out0\n    out0_stride_order0: tl.constexpr, # stride order for out0\n    s0: int, # task_space\n    num_tasks: int,\n    tiles_per_cta: int,\n    tile_size0: tl.constexpr,\n    one_tile_per_cta: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_tiles0 = tl.cdiv(s0, tile_size0)\n    if one_tile_per_cta: # monolitic kernel style\n        tile_id = pid\n        # pid multi index recontruction: we use c ordering, right axes changes fastest\n        tile_id0 = tile_id\n\n        # tile offsets\n        offset0 = tile_id0 * tile_size0\n        # loads\n        in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n        in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n        # compute\n        out0 = pow_func_scalar_tensor(val0, in0)\n\n        # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n        out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n        tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n    else: # grid-stride-loop style kernel\n        num_ctas = tl.num_programs(0)\n        for j in range(0, tiles_per_cta):\n            tile_id = pid + j * num_ctas\n            # pid multi index recontruction: we use c ordering, right axes changes fastest\n            tile_id0 = tile_id\n\n            # tile offsets\n            offset0 = tile_id0 * tile_size0\n            # loads\n            in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n            in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n            # compute\n            out0 = pow_func_scalar_tensor(val0, in0)\n\n            # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n            out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n            tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines two functions, `_l2_norm_fwd` and `_l2_norm_bwd`, utilizing Triton for parallel computation of L2 normalization and its backward pass on 2D input tensors.\n\n            1. `_l2_norm_fwd_1pass_kernel`: This Triton kernel is called in `_l2_norm_fwd`. It computes the L2 normalization for each row of the input tensor `X`. It loads data, computes variance, uses it to normalize input, and stores the result in output tensor `Y`.\n\n            2. `_l2_norm_bwd_kernel`: This Triton kernel is used in `_l2_norm_bwd`. It computes the backward pass for L2 normalization. It loads inputs `X` and gradient `DY`, calculates gradients w.r.t input, and stores results in `DX`.\n\n            Both kernels leverage parallel processing to efficiently compute row-wise L2 normalization. The functions `_l2_norm_fwd` and `_l2_norm_bwd` reshape the input tensors for compatibility and invoke these kernels with appropriate arguments.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for L2 normalization forward pass\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X, Y, M, N, stride_xm, stride_xn, stride_ym, stride_yn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE_M\n    offsets_m = block_start + tl.arange(0, BLOCK_SIZE_M)\n    offsets_n = tl.arange(0, BLOCK_SIZE_N)\n    x_ptrs = X + (offsets_m[:, None] * stride_xm + offsets_n[None, :] * stride_xn)\n    y_ptrs = Y + (offsets_m[:, None] * stride_ym + offsets_n[None, :] * stride_yn)\n\n    # Load data\n    x = tl.load(x_ptrs, mask=offsets_m[:, None] < M, other=0.0)\n\n    # Compute L2 norm\n    l2_norm = tl.sqrt(tl.sum(x * x, axis=1))\n\n    # Normalize\n    y = x / l2_norm[:, None]\n\n    # Store result\n    tl.store(y_ptrs, y, mask=offsets_m[:, None] < M)\n\n# Triton kernel for L2 normalization backward pass\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X, DY, DX, M, N, stride_xm, stride_xn, stride_dym, stride_dyn, stride_dxm, stride_dxn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE_M\n    offsets_m = block_start + tl.arange(0, BLOCK_SIZE_M)\n    offsets_n = tl.arange(0, BLOCK_SIZE_N)\n    x_ptrs = X + (offsets_m[:, None] * stride_xm + offsets_n[None, :] * stride_xn)\n    dy_ptrs = DY + (offsets_m[:, None] * stride_dym + offsets_n[None, :] * stride_dyn)\n    dx_ptrs = DX + (offsets_m[:, None] * stride_dxm + offsets_n[None, :] * stride_dxn)\n\n    # Load data\n    x = tl.load(x_ptrs, mask=offsets_m[:, None] < M, other=0.0)\n    dy = tl.load(dy_ptrs, mask=offsets_m[:, None] < M, other=0.0)\n\n    # Compute L2 norm\n    l2_norm = tl.sqrt(tl.sum(x * x, axis=1))\n\n    # Compute gradient w.r.t. input\n    dx = (dy - (tl.sum(dy * x, axis=1) / (l2_norm * l2_norm))[:, None] * x) / l2_norm[:, None]\n\n    # Store result\n    tl.store(dx_ptrs, dx, mask=offsets_m[:, None] < M)\n\nimport torch\n\ndef _l2_norm_fwd(X):\n    M, N = X.shape\n    Y = torch.empty_like(X)\n    grid = (triton.cdiv(M, 128),)\n    _l2_norm_fwd_1pass_kernel[grid](\n        X, Y, M, N, X.stride(0), X.stride(1), Y.stride(0), Y.stride(1),\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=N\n    )\n    return Y\n\ndef _l2_norm_bwd(X, DY):\n    M, N = X.shape\n    DX = torch.empty_like(X)\n    grid = (triton.cdiv(M, 128),)\n    _l2_norm_bwd_kernel[grid](\n        X, DY, DX, M, N, X.stride(0), X.stride(1), DY.stride(0), DY.stride(1), DX.stride(0), DX.stride(1),\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=N\n    )\n    return DX\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    # Compute mean and variance\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) \n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    mask = cols < N\n    y = x * rstd\n    # Write output\n    tl.store(Y + cols, y, mask=mask)\n\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X,  # pointer to the input\n    DY,  # pointer to the output gradient\n    DX,  # pointer to the input gradient\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x) \n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0).to(tl.float32)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var+eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n\ndef _l2_norm_fwd(\n    x, eps=1e-6\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, x.shape[-1])\n    if x.stride(-1) != 1:\n        x = x.contiguous()\n    assert x.stride(-1) == 1 \n    # allocate output\n    y = torch.empty_like(x)\n    assert y.stride(-1) == 1\n    N = x.shape[-1]\n    M = x.shape[0]\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    # heuristics for number of warps\n    with torch.cuda.device(x.device.index):\n        _l2_norm_fwd_1pass_kernel[(M,)](\n            x,\n            y,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return y.reshape(x_shape_og)\n\ndef _l2_norm_bwd(\n    x, dy, eps=1e-5,\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, dy.shape[-1])\n    dy = dy.reshape(-1, dy.shape[-1])\n    if dy.stride(-1) != 1:\n        dy = dy.contiguous()\n    assert dy.shape == x.shape\n    # allocate output\n    dx = torch.empty_like(x)\n    N = x.shape[-1]\n    M = x.shape[0]\n    assert x.stride(-1) == 1\n    assert dy.stride(-1) == 1\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    # heuristics for number of warps\n    with torch.cuda.device(x.device.index):\n        _l2_norm_bwd_kernel[(M,)](\n            x,\n            dy,\n            dx,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return dx.reshape(x_shape_og)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel performs layer normalization and its backward pass efficiently on CUDA. The `Fast_Layernorm` class is a PyTorch custom autograd function that uses two Triton kernels: `layernorm_forward` and `layernorm_backward`. These functions are responsible for the forward and backward computations of layer normalization. The `calculate_settings` function determines the block size and number of warps based on the number of columns to optimize for parallel execution. The `fast_layernorm` function is a utility to apply this custom layer normalization on an input tensor `X` using the provided layernorm module weights and biases.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef layernorm_forward(\n    X,  # input tensor\n    Y,  # output tensor\n    W,  # weight tensor\n    B,  # bias tensor\n    M,  # mean tensor\n    V,  # variance tensor\n    eps,  # epsilon\n    stride,  # stride of the input tensor\n    BLOCK_SIZE: tl.constexpr\n):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    x_ptrs = X + row * stride + cols\n    w_ptrs = W + cols\n    b_ptrs = B + cols\n\n    # Load data\n    x = tl.load(x_ptrs, mask=cols < stride, other=0.0)\n    w = tl.load(w_ptrs, mask=cols < stride, other=0.0)\n    b = tl.load(b_ptrs, mask=cols < stride, other=0.0)\n\n    # Compute mean\n    mean = tl.sum(x, axis=0) / stride\n    tl.store(M + row, mean)\n\n    # Compute variance\n    var = tl.sum(tl.sqr(x - mean), axis=0) / stride\n    tl.store(V + row, var)\n\n    # Normalize and scale\n    rstd = 1.0 / tl.sqrt(var + eps)\n    y = (x - mean) * rstd * w + b\n\n    # Store output\n    tl.store(Y + row * stride + cols, y, mask=cols < stride)\n\n@triton.jit\ndef layernorm_backward(\n    dY,  # gradient of output tensor\n    X,  # input tensor\n    W,  # weight tensor\n    B,  # bias tensor\n    M,  # mean tensor\n    V,  # variance tensor\n    dX,  # gradient of input tensor\n    dW,  # gradient of weight tensor\n    dB,  # gradient of bias tensor\n    eps,  # epsilon\n    stride,  # stride of the input tensor\n    BLOCK_SIZE: tl.constexpr\n):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    x_ptrs = X + row * stride + cols\n    w_ptrs = W + cols\n    b_ptrs = B + cols\n    dy_ptrs = dY + row * stride + cols\n    dx_ptrs = dX + row * stride + cols\n\n    # Load data\n    x = tl.load(x_ptrs, mask=cols < stride, other=0.0)\n    w = tl.load(w_ptrs, mask=cols < stride, other=0.0)\n    dy = tl.load(dy_ptrs, mask=cols < stride, other=0.0)\n\n    # Load mean and variance\n    mean = tl.load(M + row)\n    var = tl.load(V + row)\n    rstd = 1.0 / tl.sqrt(var + eps)\n\n    # Compute gradients\n    dx = dy * w * rstd\n    sum_dy = tl.sum(dy, axis=0)\n    sum_dy_x = tl.sum(dy * (x - mean), axis=0)\n    dx -= (sum_dy * (x - mean) * rstd + sum_dy_x * rstd * rstd * (x - mean)) / stride\n    dx *= rstd\n\n    # Compute dW and dB\n    dW += tl.sum(dy * (x - mean) * rstd, axis=0)\n    dB += tl.sum(dy, axis=0)\n\n    # Store gradients\n    tl.store(dx_ptrs, dx, mask=cols < stride)\n    tl.atomic_add(dW + cols, dW, mask=cols < stride)\n    tl.atomic_add(dB + cols, dB, mask=cols < stride)\n\nimport torch\nfrom torch.autograd import Function\n\nclass Fast_Layernorm(Function):\n    @staticmethod\n    def forward(ctx, X, W, B, eps=1e-5):\n        M = torch.empty_like(X[:, :1])\n        V = torch.empty_like(X[:, :1])\n        Y = torch.empty_like(X)\n\n        # Determine block size and number of warps\n        stride = X.shape[1]\n        BLOCK_SIZE = 128\n        num_warps = 4\n\n        # Launch Triton kernel\n        layernorm_forward[(X.shape[0],)](X, Y, W, B, M, V, eps, stride, BLOCK_SIZE, num_warps=num_warps)\n\n        ctx.save_for_backward(X, W, B, M, V, Y)\n        ctx.eps = eps\n\n        return Y\n\n    @staticmethod\n    def backward(ctx, dY):\n        X, W, B, M, V, Y = ctx.saved_tensors\n        eps = ctx.eps\n\n        dX = torch.empty_like(X)\n        dW = torch.zeros_like(W)\n        dB = torch.zeros_like(B)\n\n        # Determine block size and number of warps\n        stride = X.shape[1]\n        BLOCK_SIZE = 128\n        num_warps = 4\n\n        # Launch Triton kernel\n        layernorm_backward[(X.shape[0],)](dY, X, W, B, M, V, dX, dW, dB, eps, stride, BLOCK_SIZE, num_warps=num_warps)\n\n        return dX, dW, dB, None\n\n# Utility function to apply layer normalization\ndef fast_layernorm(X, W, B, eps=1e-5):\n    return Fast_Layernorm.apply(X, W, B, eps)\n\ndef calculate_settings(num_cols):\n    if num_cols < 128:\n        block_size = 32\n        num_warps = 2\n    elif num_cols < 256:\n        block_size = 64\n        num_warps = 4\n    else:\n        block_size = 128\n        num_warps = 8\n    return block_size, num_warps\n\nimport torch\n\n# Create input tensor, weights, and biases\nX = torch.randn(1024, 512, device='cuda')\nW = torch.randn(512, device='cuda', requires_grad=True)\nB = torch.randn(512, device='cuda', requires_grad=True)\n\n# Apply layer normalization\nY = fast_layernorm(X, W, B)\n\n# Compute loss and backpropagate\nloss = Y.sum()\nloss.backward()\n\nprint(W.grad)\nprint(B.grad)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\nnext_power_of_2 = triton.next_power_of_2\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int,):\n    BLOCK_SIZE : int = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef layernorm_forward(\n    Y, Y_row_stride,\n    X, X_row_stride,\n    W,\n    b,\n    r,\n    mu,\n    n_cols, eps,\n    BLOCK_SIZE : tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y  += row_idx * Y_row_stride\n    X  += row_idx * X_row_stride\n    r  += row_idx\n    mu += row_idx\n\n    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask = mask, other = 0).to(tl.float32)\n\n    mean_X  = tl.sum(X_row,   axis = 0) / n_cols\n    XX      = X_row - mean_X\n    row_var = tl.sum(XX * XX, axis = 0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store (r, inv_var)\n    tl.store (mu, mean_X)\n    output = (XX * inv_var) * W_row + b_row\n    tl.store(Y + col_offsets, output, mask = mask)\n\n@triton.jit\ndef layernorm_backward(\n    dY, dY_row_stride,\n    X,   X_row_stride,\n    W,\n    b,\n    r,\n    mu,\n    n_cols, eps,\n    BLOCK_SIZE : tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dY += row_idx * dY_row_stride\n    X  += row_idx *  X_row_stride\n    r  += row_idx\n    mu += row_idx\n\n    dY_row = tl.load(dY + col_offsets, mask = mask, other = 0).to(tl.float32)\n    X_row  = tl.load(X  + col_offsets, mask = mask, other = 0).to(tl.float32)\n    W_row  = tl.load(W  + col_offsets, mask = mask, other = 0).to(tl.float32)\n    b_row  = tl.load(b  + col_offsets, mask = mask, other = 0).to(tl.float32)\n\n    inv_var = tl.load(r) .to(tl.float32)\n    mean    = tl.load(mu).to(tl.float32)\n    normed  = (X_row - mean) * inv_var\n    dY_W = dY_row * W_row\n    dX_row = dY_W - tl.sum(dY_W, axis = 0) / n_cols - normed * tl.sum(dY_W * normed, axis = 0) / n_cols\n    dX_row = dX_row * inv_var\n    tl.store(dY + col_offsets, dX_row, mask = mask)\n\nclass Fast_Layernorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, b, eps):\n        shape = X.shape\n        dim = shape[-1]\n        X = X.view(-1, dim)\n        n_rows, n_cols = X.shape\n        BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n        Y  = torch.empty((n_rows, n_cols), dtype = X.dtype, device = \"cuda:0\")\n        r  = torch.empty(n_rows, dtype = torch.float32, device = \"cuda:0\")\n        mu = torch.empty(n_rows, dtype = torch.float32, device = \"cuda:0\")\n\n        layernorm_forward[(n_rows,)](\n            Y, Y.stride(0),\n            X, X.stride(0),\n            W,\n            b,\n            r,\n            mu,\n            n_cols, eps,\n            BLOCK_SIZE = BLOCK_SIZE,\n            num_warps  = num_warps,\n        )\n        ctx.eps = eps\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps  = num_warps\n        ctx.save_for_backward(X, W, b, r, mu)\n        return Y.view(*shape)\n    \n    @staticmethod\n    def backward(ctx, dY):\n        shape = dY.shape\n        dim = shape[-1]\n        dY = dY.view(-1, dim)\n        X, W, b, r, mu = ctx.saved_tensors\n        n_rows, n_cols = dY.shape\n\n        layernorm_backward[(n_rows,)](\n            dY, dY.stride(0),\n            X,  X .stride(0),\n            W,\n            b,\n            r,\n            mu,\n            n_cols, ctx.eps,\n            BLOCK_SIZE = ctx.BLOCK_SIZE,\n            num_warps  = ctx.num_warps,\n        )\n        dX = dY.view(*shape)\n        return dX, None, None, None, None\n    \ndef fast_layernorm(layernorm, X):\n    assert(layernorm.elementwise_affine is True)\n    W    = layernorm.weight\n    bias = layernorm.bias\n    eps = layernorm.variance_epsilon if \\\n        hasattr(layernorm, \"variance_epsilon\") \\\n        else layernorm.eps\n    out = Fast_Layernorm.apply(X, W, bias, eps)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel calculates the mean of a tensor along specified dimensions. The `mean_dim_kernel` is the main function, which operates on a 2D tensor `X` and computes the mean across a dimension specified by `M` and `N`, using block sizes `BLOCK_M` and `BLOCK_N`. `pid` calculates the program ID for distributed computation. `dim_compress` reorders tensor dimensions for efficient processing. The `mean_dim` function orchestrates the process, handling input transformation and kernel invocation.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel to compute the mean along specified dimensions\n@triton.jit\ndef mean_dim_kernel(X, M, N, out, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Compute program ID\n    pid = tl.program_id(0)\n    num_programs = tl.num_programs(0)\n    \n    # Compute the range of rows and columns this program will process\n    row_start = pid * BLOCK_M\n    row_end = min(row_start + BLOCK_M, M)\n    \n    # Initialize the sum and count for the mean calculation\n    sum_val = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    count = 0\n    \n    # Iterate over the rows assigned to this program\n    for row in range(row_start, row_end):\n        # Load the row from the tensor\n        row_data = tl.load(X + row * N + tl.arange(0, BLOCK_N))\n        # Accumulate the sum\n        sum_val += row_data\n        count += 1\n    \n    # Compute the mean\n    mean_val = sum_val / count\n    \n    # Store the result in the output tensor\n    tl.store(out + tl.arange(0, BLOCK_N), mean_val)\n\n# Wrapper function to handle input transformation and kernel invocation\ndef mean_dim(X, dim, BLOCK_M, BLOCK_N):\n    # Get the shape of the input tensor\n    M, N = X.shape\n    \n    # Check if the specified dimension is valid\n    if dim < 0 or dim >= len(X.shape):\n        raise ValueError(\"Invalid dimension\")\n    \n    # Allocate the output tensor\n    if dim == 0:\n        out_shape = (1, N)\n    else:\n        out_shape = (M, 1)\n    out = triton.empty(out_shape, dtype=triton.float32, device=X.device)\n    \n    # Compute the number of programs needed\n    num_programs = (M + BLOCK_M - 1) // BLOCK_M\n    \n    # Launch the kernel\n    mean_dim_kernel[(num_programs,)](X, M, N, out, BLOCK_M, BLOCK_N)\n    \n    return out\n\n# Example usage\nimport torch\n\n# Create a tensor\nX = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], device='cuda')\n\n# Compute the mean along dimension 0\nmean_dim_0 = mean_dim(X, dim=0, BLOCK_M=1, BLOCK_N=3)\nprint(\"Mean along dimension 0:\", mean_dim_0)\n\n# Compute the mean along dimension 1\nmean_dim_1 = mean_dim(X, dim=1, BLOCK_M=2, BLOCK_N=1)\nprint(\"Mean along dimension 1:\", mean_dim_1)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_dim_kernel(X, Mean, M, N, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Map the program id to the row of X it should compute.\n    pid = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)[:, None] \n    X = X + pid * N\n    Mean = Mean + pid\n    row_mask = pid < M\n\n    # Compute mean\n    _mean = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    for off in range(0, N, BLOCK_N):\n        cols = off + tl.arange(0, BLOCK_N)[None, :]\n        col_mask = cols < N\n        mask = row_mask and col_mask\n\n        a = tl.load(X + cols, mask, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=1) / N\n    mean = mean[:, None]\n    tl.store(Mean, mean, row_mask)\n\ndef dim_compress(inp: torch.Tensor, dims):\n    if isinstance(dims, int):\n        dims = [dims]\n    dim = inp.ndim\n    stride = inp.stride()\n    batch_dim = [i for i in range(dim) if i not in dims]\n    sorted_reduction_dim = sorted(dims, key=lambda x: stride[x], reverse=True)\n    order = batch_dim + sorted_reduction_dim\n    return inp.permute(order).contiguous()\n\ndef mean_dim(x, dim, keepdim=False, *, dtype=None):\n  if dtype is None:\n    dtype = x.dtype\n  \n  shape = list(x.shape)\n  if isinstance(dim, int):\n     dim = [dim]\n  dim = [d % x.ndim for d in dim]\n  x = dim_compress(x, dim)\n  N = 1\n  for i in dim:\n    N *= shape[i]\n    shape[i] = 1\n  M = x.numel() // N\n  out = torch.empty(shape, dtype=dtype, device=x.device)\n  grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]),)\n\n  with torch.cuda.device(x.device):\n    mean_dim_kernel[grid](x, out, M, N, BLOCK_M=8, BLOCK_N=8)\n  if not keepdim:\n    out = out.squeeze(dim)\n  return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton operator code provides a function called `fill_kv_cache`. This function is responsible for filling key/value states into a cache, intended for use in paged attention mechanisms. The function takes several tensor arguments including `k_states`, `v_states`, `k_caches`, `v_caches`, `q_start_loc`, `q_seq_length`, `kv_seq_length`, and `block_offsets`, as well as optional `k_scales_zeros` and `v_scales_zeros`. The `quant_policy` determines the quantization method: 0 for no quantization, 4 for int4 quantization, and 8 for int8 quantization. The operator utilizes two Triton JIT-decorated functions: `_fill_kv_cache_kernel` and `_fill_kv_cache_quant_kernel`. The former is used when no quantization is applied, while the latter applies either int4 or int8 quantization depending on `quant_policy`. Both kernels are executed over a grid, organized by `batch_size` and `max_num_blocks`, and employ pre-computed strides for efficient memory access.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fill_kv_cache_kernel(\n    k_states_ptr, v_states_ptr, k_caches_ptr, v_caches_ptr,\n    q_start_loc_ptr, q_seq_length_ptr, kv_seq_length_ptr, block_offsets_ptr,\n    k_scales_zeros_ptr, v_scales_zeros_ptr,\n    batch_size, max_num_blocks, num_heads, head_dim, block_size,\n    q_seq_length, kv_seq_length, quant_policy,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // max_num_blocks\n    block_id = pid % max_num_blocks\n\n    if bid >= batch_size:\n        return\n\n    q_start_loc = tl.load(q_start_loc_ptr + bid)\n    q_seq_length = tl.load(q_seq_length_ptr + bid)\n    kv_seq_length = tl.load(kv_seq_length_ptr + bid)\n    block_offset = tl.load(block_offsets_ptr + block_id)\n\n    k_cache_ptr = k_caches_ptr + bid * num_heads * head_dim * kv_seq_length + block_offset * head_dim\n    v_cache_ptr = v_caches_ptr + bid * num_heads * head_dim * kv_seq_length + block_offset * head_dim\n\n    for head in range(num_heads):\n        k_state_ptr = k_states_ptr + bid * num_heads * head_dim * q_seq_length + head * head_dim * q_seq_length\n        v_state_ptr = v_states_ptr + bid * num_heads * head_dim * q_seq_length + head * head_dim * q_seq_length\n\n        for i in range(block_size):\n            if q_start_loc + block_offset * block_size + i < q_seq_length:\n                k_cache_ptr[head_dim * i + tl.arange(0, head_dim)] = k_state_ptr[(q_start_loc + block_offset * block_size + i) * head_dim + tl.arange(0, head_dim)]\n                v_cache_ptr[head_dim * i + tl.arange(0, head_dim)] = v_state_ptr[(q_start_loc + block_offset * block_size + i) * head_dim + tl.arange(0, head_dim)]\n\n@triton.jit\ndef _fill_kv_cache_quant_kernel(\n    k_states_ptr, v_states_ptr, k_caches_ptr, v_caches_ptr,\n    q_start_loc_ptr, q_seq_length_ptr, kv_seq_length_ptr, block_offsets_ptr,\n    k_scales_zeros_ptr, v_scales_zeros_ptr,\n    batch_size, max_num_blocks, num_heads, head_dim, block_size,\n    q_seq_length, kv_seq_length, quant_policy,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // max_num_blocks\n    block_id = pid % max_num_blocks\n\n    if bid >= batch_size:\n        return\n\n    q_start_loc = tl.load(q_start_loc_ptr + bid)\n    q_seq_length = tl.load(q_seq_length_ptr + bid)\n    kv_seq_length = tl.load(kv_seq_length_ptr + bid)\n    block_offset = tl.load(block_offsets_ptr + block_id)\n\n    k_cache_ptr = k_caches_ptr + bid * num_heads * head_dim * kv_seq_length + block_offset * head_dim\n    v_cache_ptr = v_caches_ptr + bid * num_heads * head_dim * kv_seq_length + block_offset * head_dim\n\n    k_scale_ptr = k_scales_zeros_ptr + bid * num_heads * 2\n    v_scale_ptr = v_scales_zeros_ptr + bid * num_heads * 2\n\n    for head in range(num_heads):\n        k_state_ptr = k_states_ptr + bid * num_heads * head_dim * q_seq_length + head * head_dim * q_seq_length\n        v_state_ptr = v_states_ptr + bid * num_heads * head_dim * q_seq_length + head * head_dim * q_seq_length\n\n        k_scale = tl.load(k_scale_ptr + head * 2)\n        k_zero = tl.load(k_scale_ptr + head * 2 + 1)\n        v_scale = tl.load(v_scale_ptr + head * 2)\n        v_zero = tl.load(v_scale_ptr + head * 2 + 1)\n\n        for i in range(block_size):\n            if q_start_loc + block_offset * block_size + i < q_seq_length:\n                k_state = k_state_ptr[(q_start_loc + block_offset * block_size + i) * head_dim + tl.arange(0, head_dim)]\n                v_state = v_state_ptr[(q_start_loc + block_offset * block_size + i) * head_dim + tl.arange(0, head_dim)]\n\n                if quant_policy == 4:\n                    k_cache_ptr[head_dim * i + tl.arange(0, head_dim)] = (k_state * k_scale + k_zero).to(tl.int4)\n                    v_cache_ptr[head_dim * i + tl.arange(0, head_dim)] = (v_state * v_scale + v_zero).to(tl.int4)\n                elif quant_policy == 8:\n                    k_cache_ptr[head_dim * i + tl.arange(0, head_dim)] = (k_state * k_scale + k_zero).to(tl.int8)\n                    v_cache_ptr[head_dim * i + tl.arange(0, head_dim)] = (v_state * v_scale + v_zero).to(tl.int8)\n\n### Python Wrapper\n\n\nimport torch\n\n# Example tensors\nbatch_size = 2\nnum_heads = 4\nhead_dim = 64\nq_seq_length = 128\nkv_seq_length = 256\nmax_num_blocks = 4\nblock_size = 32\n\nk_states = torch.randn(batch_size, num_heads, head_dim, q_seq_length, device='cuda')\nv_states = torch.randn(batch_size, num_heads, head_dim, q_seq_length, device='cuda')\nk_caches = torch.zeros(batch_size, num_heads, head_dim, kv_seq_length, device='cuda')\nv_caches = torch.zeros(batch_size, num_heads, head_dim, kv_seq_length, device='cuda')\nq_start_loc = torch.tensor([0, 0], device='cuda')\nq_seq_length = torch.tensor([q_seq_length, q_seq_length], device='cuda')\nkv_seq_length = torch.tensor([kv_seq_length, kv_seq_length], device='cuda')\nblock_offsets = torch.tensor([0, 32, 64, 96], device='cuda')\n\n# Optional quantization parameters\nk_scales_zeros = torch.randn(batch_size, num_heads, 2, device='cuda')\nv_scales_zeros = torch.randn(batch_size, num_heads, 2, device='cuda')\n\n# Fill the cache\nk_caches, v_caches = fill_kv_cache(\n    k_states, v_states, k_caches, v_caches,\n    q_start_loc, q_seq_length, kv_seq_length, block_offsets,\n    k_scales_zeros, v_scales_zeros, quant_policy=4\n)\n\nprint(k_caches)\nprint(v_caches)\n", "label": "from typing import Literal\nimport torch\nimport triton\nimport triton.language as tl\n\n\nKERNEL_META = dict()\n\ndef get_kernel_meta(tensor: torch.Tensor):\n    \"\"\"kernel meta.\"\"\"\n    return KERNEL_META\n    \n@triton.jit\ndef _div_up(val, other):\n    return (val + other - 1) // other\n\n\n@triton.jit\ndef _quant_int8(val):\n    val_min = tl.min(val, 1)\n    val_max = tl.max(val, 1)\n    scales = (val_max - val_min) / 255\n    zeros = -val_min / scales\n    q_val = (val / scales[:, None] + zeros[:, None] + 0.5).to(tl.uint8)\n    return q_val, scales, zeros\n\n\n@triton.jit\ndef _quant_int4(val1, val2):\n    val1 = val1.to(tl.float32)\n    val2 = val2.to(tl.float32)\n    val_min = tl.min(tl.minimum(val1, val2), 1)\n    val_max = tl.max(tl.maximum(val1, val2), 1)\n    scales = (val_max - val_min) / 15\n    zeros = -val_min / scales\n    q_val1 = (val1 / scales[:, None] + zeros[:, None] + 0.5).to(tl.uint8)\n    q_val2 = (val2 / scales[:, None] + zeros[:, None] + 0.5).to(tl.uint8)\n    q_val = q_val1 + q_val2 * 16\n    return q_val, scales, zeros\n\n\n@triton.jit\ndef _fill_kv_cache_kernel(\n    KStates,\n    VStates,\n    KCaches,\n    VCaches,\n    QStartLoc,\n    QSeqLens,\n    KVSeqLens,\n    BlockOffsets,\n    num_heads: tl.constexpr,\n    head_dim: tl.constexpr,\n    head_dim_v: tl.constexpr,\n    stride_kss,\n    stride_ksh,\n    stride_ksd,\n    stride_vss,\n    stride_vsh,\n    stride_vsd,\n    stride_kcn: tl.constexpr,\n    stride_kcb: tl.constexpr,\n    stride_kch: tl.constexpr,\n    stride_kcd: tl.constexpr,\n    stride_vcn: tl.constexpr,\n    stride_vcb: tl.constexpr,\n    stride_vch: tl.constexpr,\n    stride_vcd: tl.constexpr,\n    stride_boff,\n    BLOCK: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_DV: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    \"\"\"fill kv cache kernel.\"\"\"\n    batch_id = tl.program_id(0)\n    block_id = tl.program_id(1)\n\n    # initialize\n    h_off = tl.arange(0, BLOCK_H)\n    d_off = tl.arange(0, BLOCK_D)\n\n    q_startloc = tl.load(QStartLoc + batch_id)\n    q_seqlen = tl.load(QSeqLens + batch_id)\n    kv_seqlen = tl.load(KVSeqLens + batch_id)\n    history_seqlen = kv_seqlen - q_seqlen\n\n    block0_first_tokenloc = history_seqlen % BLOCK\n\n    state_token_offset = tl.maximum(block_id * BLOCK - block0_first_tokenloc,\n                                    0)\n    kv_block_id = _div_up(history_seqlen + 1, BLOCK) - 1 + block_id\n    kv_block_id = min(kv_block_id, stride_boff - 1)\n    block_off = tl.load(BlockOffsets + batch_id * stride_boff + kv_block_id)\n\n    cur_startloc = q_startloc + state_token_offset\n    ks_ptr = KStates + cur_startloc * stride_kss\n    vs_ptr = VStates + cur_startloc * stride_vss\n\n    kc_ptr = KCaches + block_off * stride_kcn\n    vc_ptr = VCaches + block_off * stride_vcn\n\n    c_first_tokenloc = block0_first_tokenloc\n    if block_id != 0:\n        c_first_tokenloc *= 0\n    c_last_tokenloc = tl.minimum(\n        BLOCK, q_seqlen + block0_first_tokenloc - block_id * BLOCK)\n\n    for bidx in range(c_first_tokenloc, c_last_tokenloc):\n        sidx = bidx - c_first_tokenloc\n        mask = (h_off[:, None] < num_heads) & (d_off[None, :] < head_dim)\n        k = tl.load(ks_ptr + sidx * stride_kss + h_off[:, None] * stride_ksh +\n                    d_off[None, :] * stride_ksd,\n                    mask=mask)\n        tl.store(kc_ptr + bidx * stride_kcb + h_off[:, None] * stride_kch +\n                 d_off[None, :] * stride_kcd,\n                 k,\n                 mask=mask)\n\n        if BLOCK_DV > 0:\n            dv_off = tl.arange(0, BLOCK_DV)\n            maskv = (h_off[:, None] < num_heads) & (dv_off[None, :] <\n                                                    head_dim_v)\n            v = tl.load(vs_ptr + sidx * stride_vss +\n                        h_off[:, None] * stride_vsh +\n                        dv_off[None, :] * stride_vsd,\n                        mask=maskv)\n            tl.store(vc_ptr + bidx * stride_vcb + h_off[:, None] * stride_vch +\n                     dv_off[None, :] * stride_vcd,\n                     v,\n                     mask=maskv)\n\n\n@triton.jit\ndef _fill_kv_cache_quant_kernel(\n    KStates,\n    VStates,\n    KCaches,\n    VCaches,\n    KScalesZeros,\n    VScalesZeros,\n    QStartLoc,\n    QSeqLens,\n    KVSeqLens,\n    BlockOffsets,\n    num_heads: tl.constexpr,\n    head_dim: tl.constexpr,\n    head_dim_v: tl.constexpr,\n    stride_kss,\n    stride_ksh,\n    stride_ksd,\n    stride_vss,\n    stride_vsh,\n    stride_vsd,\n    stride_kcn: tl.constexpr,\n    stride_kcb: tl.constexpr,\n    stride_kch: tl.constexpr,\n    stride_kcd: tl.constexpr,\n    stride_vcn: tl.constexpr,\n    stride_vcb: tl.constexpr,\n    stride_vch: tl.constexpr,\n    stride_vcd: tl.constexpr,\n    stride_kszn: tl.constexpr,\n    stride_kszb: tl.constexpr,\n    stride_kszh: tl.constexpr,\n    stride_kszd: tl.constexpr,\n    stride_vszn: tl.constexpr,\n    stride_vszb: tl.constexpr,\n    stride_vszh: tl.constexpr,\n    stride_vszd: tl.constexpr,\n    quant_policy: tl.constexpr,\n    stride_boff,\n    BLOCK: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_DV: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    \"\"\"fill kv cache kernel with int4 and int8 quant fused.\n\n    Args:\n        stride_xss: stride of sequence length dim of key or value states\n        stride_xsh: stride of head_num dim of key or value states\n        stride_xsh: stride of head_size dim of key or value states\n        stride_xn: stride of page num dim\n        stride_xb: stride of block size dim\n        stride_xh: stride of head_num dim\n        stride_xd: stride of head_size dim\n    \"\"\"\n    batch_id = tl.program_id(0)\n    block_id = tl.program_id(1)\n    d_off = tl.arange(0, BLOCK_D)\n\n    # initialize\n    h_off = tl.arange(0, BLOCK_H)\n    szd_off = tl.arange(0, 2)\n\n    q_startloc = tl.load(QStartLoc + batch_id)\n    q_seqlen = tl.load(QSeqLens + batch_id)\n    kv_seqlen = tl.load(KVSeqLens + batch_id)\n    history_seqlen = kv_seqlen - q_seqlen\n\n    block0_first_tokenloc = history_seqlen % BLOCK\n\n    state_token_offset = tl.maximum(block_id * BLOCK - block0_first_tokenloc,\n                                    0)\n    kv_block_id = _div_up(history_seqlen + 1, BLOCK) - 1 + block_id\n    kv_block_id = min(kv_block_id, stride_boff - 1)\n    block_off = tl.load(BlockOffsets + batch_id * stride_boff + kv_block_id)\n\n    cur_startloc = q_startloc + state_token_offset\n    ks_ptr = KStates + cur_startloc * stride_kss\n    vs_ptr = VStates + cur_startloc * stride_vss\n\n    kc_ptr = KCaches + block_off * stride_kcn\n    vc_ptr = VCaches + block_off * stride_vcn\n\n    ksz_ptr = KScalesZeros + block_off * stride_kszn\n    vsz_ptr = VScalesZeros + block_off * stride_vszn\n\n    c_first_tokenloc = block0_first_tokenloc\n    if block_id != 0:\n        c_first_tokenloc *= 0\n    c_last_tokenloc = tl.minimum(\n        BLOCK, q_seqlen + block0_first_tokenloc - block_id * BLOCK)\n\n    for bidx in range(c_first_tokenloc, c_last_tokenloc):\n        sidx = bidx - c_first_tokenloc\n        mask = (h_off[:, None] < num_heads) & (d_off[None, :] < head_dim)\n        if quant_policy == 4:\n            k1 = tl.load(ks_ptr + sidx * stride_kss +\n                         h_off[:, None] * stride_ksh +\n                         d_off[None, :] * stride_ksd,\n                         mask=mask)\n            k2 = tl.load(ks_ptr + sidx * stride_kss +\n                         h_off[:, None] * stride_ksh +\n                         d_off[None, :] * stride_ksd + head_dim * stride_ksd,\n                         mask=mask)\n            q_k, k_scales, k_zeros = _quant_int4(k1, k2)\n        else:\n            k = tl.load(ks_ptr + sidx * stride_kss +\n                        h_off[:, None] * stride_ksh +\n                        d_off[None, :] * stride_ksd,\n                        mask=mask)\n            q_k, k_scales, k_zeros = _quant_int8(k)\n        tl.store(kc_ptr + bidx * stride_kcb + h_off[:, None] * stride_kch +\n                 d_off[None, :] * stride_kcd,\n                 q_k,\n                 mask=mask)\n        tl.store(ksz_ptr + bidx * stride_kszb + h_off[:, None] * stride_kszh +\n                 szd_off[None, :] * stride_kszd,\n                 k_scales[:, None],\n                 mask=(h_off[:, None] < num_heads) & (szd_off[None, :] < 1))\n        tl.store(ksz_ptr + bidx * stride_kszb + h_off[:, None] * stride_kszh +\n                 szd_off[None, :] * stride_kszd,\n                 k_zeros[:, None],\n                 mask=(h_off[:, None] < num_heads) & (szd_off[None, :] == 1))\n\n        if BLOCK_DV > 0:\n            if quant_policy == 4:\n                dv_off = tl.arange(0, BLOCK_DV //\n                                   2)  # int4 pack, half the head_dim\n                maskv = (h_off[:, None] < num_heads) & (dv_off[None, :] <\n                                                        head_dim_v // 2)\n                v1 = tl.load(vs_ptr + sidx * stride_vss +\n                             h_off[:, None] * stride_vsh +\n                             dv_off[None, :] * stride_vsd,\n                             mask=maskv)\n                v2 = tl.load(vs_ptr + sidx * stride_vss +\n                             h_off[:, None] * stride_vsh +\n                             dv_off[None, :] * stride_vsd +\n                             head_dim_v // 2 * stride_vsd,\n                             mask=maskv)\n                q_v, v_scales, v_zeros = _quant_int4(v1, v2)\n            else:\n                dv_off = tl.arange(0, BLOCK_DV)\n                maskv = (h_off[:, None] < num_heads) & (dv_off[None, :] <\n                                                        head_dim_v)\n                v = tl.load(vs_ptr + sidx * stride_vss +\n                            h_off[:, None] * stride_vsh +\n                            dv_off[None, :] * stride_vsd,\n                            mask=maskv)\n                q_v, v_scales, v_zeros = _quant_int8(v)\n            tl.store(vc_ptr + bidx * stride_vcb + h_off[:, None] * stride_vch +\n                     dv_off[None, :] * stride_vcd,\n                     q_v,\n                     mask=maskv)\n            tl.store(\n                vsz_ptr + bidx * stride_vszb + h_off[:, None] * stride_vszh +\n                szd_off[None, :] * stride_vszd,\n                v_scales[:, None],\n                mask=(h_off[:, None] < num_heads) & (szd_off[None, :] < 1))\n            tl.store(\n                vsz_ptr + bidx * stride_vszb + h_off[:, None] * stride_vszh +\n                szd_off[None, :] * stride_vszd,\n                v_zeros[:, None],\n                mask=(h_off[:, None] < num_heads) & (szd_off[None, :] == 1))\n\n\ndef fill_kv_cache(k_states: torch.Tensor,\n                  v_states: torch.Tensor,\n                  k_caches: torch.Tensor,\n                  v_caches: torch.Tensor,\n                  q_start_loc: torch.Tensor,\n                  q_seq_length: torch.Tensor,\n                  kv_seq_length: torch.Tensor,\n                  max_q_seq_length: int,\n                  block_offsets: torch.Tensor,\n                  k_scales_zeros: torch.Tensor = None,\n                  v_scales_zeros: torch.Tensor = None,\n                  quant_policy: Literal[0, 4, 8] = 0):\n    \"\"\"fill key/value state to cache for paged attention.\"\"\"\n\n    block_offsets = block_offsets.contiguous()\n    batch_size = block_offsets.size(0)\n    block_size, num_heads, head_dim = k_caches.size()[1:]\n    head_dim_v = v_states.size(-1)\n    max_num_blocks = triton.cdiv(max_q_seq_length, block_size) + 1\n\n    BLOCK = block_size\n    BLOCK_H = triton.next_power_of_2(num_heads)\n    BLOCK_D = triton.next_power_of_2(head_dim)\n    BLOCK_DV = triton.next_power_of_2(head_dim_v)\n    grid = [batch_size, max_num_blocks]\n    kernel_meta = get_kernel_meta(k_states)\n    if quant_policy == 0:\n        _fill_kv_cache_kernel[grid](\n            k_states,\n            v_states,\n            k_caches,\n            v_caches,\n            q_start_loc,\n            q_seq_length,\n            kv_seq_length,\n            block_offsets,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            head_dim_v=head_dim_v,\n            stride_kss=k_states.stride(-3),\n            stride_ksh=k_states.stride(-2),\n            stride_ksd=k_states.stride(-1),\n            stride_vss=v_states.stride(-3),\n            stride_vsh=v_states.stride(-2),\n            stride_vsd=v_states.stride(-1),\n            stride_kcn=k_caches.stride(0),\n            stride_kcb=k_caches.stride(1),\n            stride_kch=k_caches.stride(2),\n            stride_kcd=k_caches.stride(3),\n            stride_vcn=v_caches.stride(0),\n            stride_vcb=v_caches.stride(1),\n            stride_vch=v_caches.stride(2),\n            stride_vcd=v_caches.stride(3),\n            stride_boff=block_offsets.stride(0),\n            BLOCK=BLOCK,\n            BLOCK_D=BLOCK_D,\n            BLOCK_DV=BLOCK_DV,\n            BLOCK_H=BLOCK_H,\n            num_warps=4,\n            num_stages=3,\n            **kernel_meta,\n        )\n    else:\n        _fill_kv_cache_quant_kernel[grid](\n            k_states,\n            v_states,\n            k_caches,\n            v_caches,\n            k_scales_zeros,\n            v_scales_zeros,\n            q_start_loc,\n            q_seq_length,\n            kv_seq_length,\n            block_offsets,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            head_dim_v=head_dim_v,\n            stride_kss=k_states.stride(-3),\n            stride_ksh=k_states.stride(-2),\n            stride_ksd=k_states.stride(-1),\n            stride_vss=v_states.stride(-3),\n            stride_vsh=v_states.stride(-2),\n            stride_vsd=v_states.stride(-1),\n            stride_kcn=k_caches.stride(0),\n            stride_kcb=k_caches.stride(1),\n            stride_kch=k_caches.stride(2),\n            stride_kcd=k_caches.stride(3),\n            stride_vcn=v_caches.stride(0),\n            stride_vcb=v_caches.stride(1),\n            stride_vch=v_caches.stride(2),\n            stride_vcd=v_caches.stride(3),\n            stride_kszn=k_scales_zeros.stride(0),\n            stride_kszb=k_scales_zeros.stride(1),\n            stride_kszh=k_scales_zeros.stride(2),\n            stride_kszd=k_scales_zeros.stride(3),\n            stride_vszn=v_scales_zeros.stride(0),\n            stride_vszb=v_scales_zeros.stride(1),\n            stride_vszh=v_scales_zeros.stride(2),\n            stride_vszd=v_scales_zeros.stride(3),\n            quant_policy=quant_policy,\n            stride_boff=block_offsets.stride(0),\n            BLOCK=BLOCK,\n            BLOCK_D=BLOCK_D,\n            BLOCK_DV=BLOCK_DV,\n            BLOCK_H=BLOCK_H,\n            num_warps=4,\n            num_stages=3,\n            **kernel_meta,\n        )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs the ReLU (Rectified Linear Unit) operation on an input tensor. The `relu_kernel` function is the main computation unit. It reads elements from the input tensor, applies the ReLU operation, and writes the result back to the output tensor. The input tensor is pointed to by `x_ptr`, and the output tensor is pointed to by `out_ptr`. The kernel operates in parallel across multiple blocks, with each block processing a segment of the tensor. The block size is set to 1024 elements. The ReLU operation sets all negative elements to zero, keeping positive elements unchanged. \n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(\n    x_ptr,  # Pointer to the input tensor\n    out_ptr,  # Pointer to the output tensor\n    n_elements,  # Number of elements in the tensor\n    BLOCK_SIZE: tl.constexpr  # Block size (number of elements processed per block)\n):\n    # Compute the block ID and the starting index for this block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Create a vectorized index for the elements this block will process\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load the input elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Apply the ReLU operation\n    out = tl.where(x > 0, x, 0)\n\n    # Store the result back to the output tensor\n    tl.store(out_ptr + offsets, out, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n# Python wrapper function\ndef relu(x: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is on the GPU\n    assert x.is_cuda, \"Input tensor must be on the GPU\"\n\n    # Allocate the output tensor\n    out = torch.empty_like(x)\n\n    # Compute the number of elements\n    n_elements = x.numel()\n\n    # Compute the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    relu_kernel[grid, BLOCK_SIZE](\n        x_ptr=x,\n        out_ptr=out,\n        n_elements=n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return out\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    x = torch.randn(1024 * 1024, device=\"cuda\")\n\n    # Apply the ReLU operation\n    out = relu(x)\n\n    # Print the first few elements of the output\n    print(out[:10])\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef relu_kernel(x_ptr, out_ptr, N: tl.constexpr, block_size: tl.constexpr):\n    # Get the index of the current thread\n    pid = tl.program_id(0)\n    block_start = pid * block_size\n    offsets = block_start + tl.arange(0, block_size)\n    mask = offsets < N\n\n    # Load elements from global memory\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute linear layer\n    result = tl.where(x >= 0, x, 0.0)\n\n    # Write result to global memory\n    if pid == 0:\n        tl.store(out_ptr + offsets, result, mask=mask)\n\ndef relu(x):\n    # Prepare output tensor\n    out = torch.empty_like(x, dtype=torch.float32, device=x.device)\n    N = out.numel()\n\n    BLOCK_SIZE = 1024\n    num_blocks = (N + BLOCK_SIZE - 1) // BLOCK_SIZE  # Calculate the number of blocks needed\n\n    # Launch Triton kernel\n    grid = lambda meta: (triton.cdiv(N, BLOCK_SIZE), )\n\n    relu_kernel[grid](x, out, N, BLOCK_SIZE)\n\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code defines a series of GPU kernels designed to perform maximum value calculations on input tensors. It includes functions `max`, `max_dim`, and three Triton kernels: `max_kernel_1`, `max_kernel_2`, and `max_kernel`. The main purpose of these kernels is to compute the maximum values and their indices across different dimensions of the input tensor using parallel processing capabilities of GPUs.\n            - `max_kernel_1` processes a large input tensor in blocks to compute the maximum values for each block. It loads segments of the input, applies a mask for boundary handling, computes the max, and stores intermediate results in a mid buffer.\n            - `max_kernel_2` takes the results from `max_kernel_1` to compute the final maximum value for the entire tensor.\n            - `max_kernel` is used in `max_dim` to find maximum values along a specified dimension. It uses 2D grid dimensions to handle multi-dimensional data, computing the max value and its index along the specified axis.\n            - The `max` function acts as a wrapper to execute `max_kernel_1` and `max_kernel_2` sequentially.\n            - The `max_dim` function is an extension that allows finding max values along a specified dimension and returning both the values and their indices.\n            The kernels leverage Triton's just-in-time compilation to optimize for different hardware configurations.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define constants\nBLOCK_SIZE = 1024\nGRID_SIZE = 128\n\n# Kernel 1: Compute max values for each block and store in a mid buffer\n@triton.jit\ndef max_kernel_1(input_ptr, mid_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    max_val = tl.max(x, axis=0)\n    tl.store(mid_ptr + pid, max_val)\n\n# Kernel 2: Compute the final maximum value from the mid buffer\n@triton.jit\ndef max_kernel_2(mid_ptr, output_ptr, n_blocks, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_blocks\n    x = tl.load(mid_ptr + offsets, mask=mask)\n    max_val = tl.max(x, axis=0)\n    tl.store(output_ptr, max_val)\n\n# Kernel 3: Compute max values along a specified dimension\n@triton.jit\ndef max_kernel(input_ptr, output_ptr, index_ptr, n_elements, dim_size, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(input_ptr + offsets, mask=mask)\n    max_val, max_idx = tl.max_with_indices(x, axis=0)\n    tl.store(output_ptr + pid, max_val)\n    tl.store(index_ptr + pid, max_idx)\n\n# Wrapper function to execute max_kernel_1 and max_kernel_2\ndef max(input, output, stream=None):\n    n_elements = input.size\n    n_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    mid_buffer = triton.empty((n_blocks,), dtype=input.dtype, device=input.device)\n    \n    # Launch max_kernel_1\n    max_kernel_1[(n_blocks,)](input, mid_buffer, n_elements, BLOCK_SIZE, stream=stream)\n    \n    # Launch max_kernel_2\n    max_kernel_2[(1,)](mid_buffer, output, n_blocks, BLOCK_SIZE, stream=stream)\n\n# Wrapper function to compute max values along a specified dimension\ndef max_dim(input, output, index, dim, stream=None):\n    shape = input.shape\n    dim_size = shape[dim]\n    n_elements = input.size // dim_size\n    n_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch max_kernel\n    max_kernel[(n_blocks,)](input, output, index, n_elements, dim_size, BLOCK_SIZE, stream=stream)\n\nimport torch\n\n# Example input tensor\ninput_tensor = torch.tensor([1, 3, 2, 5, 4, 6, 8, 7], device='cuda')\n\n# Compute the maximum value\noutput_tensor = triton.empty((1,), dtype=input_tensor.dtype, device=input_tensor.device)\nmax(input_tensor, output_tensor)\nprint(\"Maximum value:\", output_tensor.item())\n\n# Compute the maximum value along a specified dimension\ninput_tensor_2d = torch.tensor([[1, 3, 2], [5, 4, 6], [8, 7, 9]], device='cuda')\noutput_tensor_2d = triton.empty((3,), dtype=input_tensor_2d.dtype, device=input_tensor_2d.device)\nindex_tensor_2d = triton.empty((3,), dtype=torch.int32, device=input_tensor_2d.device)\nmax_dim(input_tensor_2d, output_tensor_2d, index_tensor_2d, dim=1)\nprint(\"Maximum values along dim 1:\", output_tensor_2d)\nprint(\"Indices of maximum values along dim 1:\", index_tensor_2d)\n", "label": "import logging\nimport math\nfrom collections import namedtuple\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef max_kernel_1(\n    inp,\n    mid,\n    M,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    inp_ptrs = inp + offset\n    mask = offset < M\n    inp_val = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    max_val = tl.max(inp_val)\n    mid_ptr = mid + pid\n    tl.store(mid_ptr, max_val)\n\n\n@triton.jit\ndef max_kernel_2(mid, out, mid_size, BLOCK_MID: tl.constexpr):\n    offset = tl.arange(0, BLOCK_MID)\n    mid_ptrs = mid + offset\n    mask = offset < mid_size\n    mid_val = tl.load(mid_ptrs, mask=mask, other=-float(\"inf\"))\n    max_val = tl.max(mid_val)\n    tl.store(out, max_val)\n\n\ndef heur_block_n(args):\n    return triton.next_power_of_2(args[\"N\"])\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 8}, num_warps=8),\n        triton.Config({\"BLOCK_M\": 16}, num_warps=8),\n        triton.Config({\"BLOCK_M\": 32}, num_warps=8),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"BLOCK_N\": heur_block_n,\n    }\n)\n@triton.jit\ndef max_kernel(\n    inp,\n    out_value,\n    out_index,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    # set offset\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    offset_index = m_offset * K + pid_k\n    # set mask\n    mask1 = m_offset < M\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    inp_ptrs = inp + offset\n    inp_vals = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    result_value, result_index = tl.max(inp_vals, axis=1, return_indices=True)\n\n    out_value_ptrs = out_value + offset_index\n    out_index_ptrs = out_index + offset_index\n\n    tl.store(out_value_ptrs, result_value, mask=mask1)\n    tl.store(out_index_ptrs, result_index, mask=mask1)\n\n\ndef max(inp):\n    logging.debug(\"GEMS MAX\")\n    M = inp.numel()\n    block_size = triton.next_power_of_2(math.ceil(math.sqrt(M)))\n    mid_size = triton.cdiv(M, block_size)\n    block_mid = triton.next_power_of_2(mid_size)\n\n    dtype = inp.dtype\n    mid = torch.empty((mid_size,), dtype=dtype, device=inp.device)\n    out = torch.empty([], dtype=dtype, device=inp.device)\n\n    with torch.cuda.device(inp.device):\n        max_kernel_1[(mid_size, 1, 1)](inp, mid, M, block_size)\n        max_kernel_2[(1, 1, 1)](mid, out, mid_size, block_mid)\n    return out\n\n\ndef max_dim(inp, dim=None, keepdim=False):\n    logging.debug(\"GEMS MAX DIM\")\n    assert dim >= -inp.ndim and dim < inp.ndim, \"Invalid dim\"\n    shape = inp.shape\n    dim = dim % inp.ndim\n    N = shape[dim]\n    M = math.prod(shape[:dim])\n    K = inp.numel() // M // N\n\n    inp = inp.contiguous()\n\n    shape_list = list(shape)\n    shape_list[dim] = 1\n    out_value = torch.empty(shape_list, dtype=inp.dtype, device=inp.device)\n    out_index = torch.empty(shape_list, dtype=torch.int64, device=inp.device)\n\n    if not keepdim:\n        out_value = torch.squeeze(out_value, dim)\n        out_index = torch.squeeze(out_index, dim)\n\n    grid = lambda meta: (\n        triton.cdiv(M, meta[\"BLOCK_M\"]),\n        K,\n    )\n    with torch.cuda.device(inp.device):\n        max_kernel[grid](inp, out_value, out_index, M, N, K)\n    Max_out = namedtuple(\"max\", [\"values\", \"indices\"])\n    out = Max_out(values=out_value, indices=out_index)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implements a matrix multiplication operator with optimizations for memory hierarchy and parallel execution. The main functions are `swizzle_tile`, `linear_tile`, `mac_loop`, `first_wave`, and `full_tiles`.\n\n            - `swizzle_tile` and `linear_tile` calculate tile coordinates (pid_m, pid_n) based on the tile ID. Swizzling helps improve cache performance.\n            - `mac_loop` is the core computation loop, performing matrix multiplication for a range of iterations and writing results to the output matrix C.\n            - `first_wave` and `full_tiles` manage the scheduling and execution of the computation across available tiles, using a two-wave strategy for better resource utilization.\n            - The `matmul` class provides a high-level interface to execute the kernel and manage parameters like block sizes and the number of stages/warps.\n\n            Inputs and outputs:\n            - Inputs include matrices A, B, output matrix C, dimensions M, N, K, stride information for accessing matrices, and kernel parameters like block sizes and warp/stage numbers.\n            - Output is the product matrix C.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef swizzle_tile(pid, num_tiles_m, num_tiles_n):\n    # Swizzling logic to improve cache performance\n    pid_m = (pid % num_tiles_m) + (pid // num_tiles_n) * num_tiles_m\n    pid_n = (pid // num_tiles_m) + (pid % num_tiles_n)\n    return pid_m, pid_n\n\n@triton.jit\ndef linear_tile(pid, num_tiles_m, num_tiles_n):\n    # Linear tile calculation\n    pid_m = pid // num_tiles_n\n    pid_n = pid % num_tiles_n\n    return pid_m, pid_n\n\n@triton.jit\ndef mac_loop(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, pid_m, pid_n, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages):\n    # Load tiles from A and B into shared memory\n    a_tile = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n    b_tile = tl.zeros((BLOCK_K, BLOCK_N), dtype=tl.float32)\n    c_tile = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        # Load A and B tiles\n        a_tile = tl.load(A + (pid_m * BLOCK_M * stride_am) + (k * BLOCK_K * stride_ak) + tl.arange(0, BLOCK_M)[:, None] * stride_am + tl.arange(0, BLOCK_K) * stride_ak)\n        b_tile = tl.load(B + (pid_n * BLOCK_N * stride_bn) + (k * BLOCK_K * stride_bk) + tl.arange(0, BLOCK_K)[:, None] * stride_bk + tl.arange(0, BLOCK_N) * stride_bn)\n\n        # Perform matrix multiplication\n        c_tile += tl.dot(a_tile, b_tile)\n\n    # Write the result to the output matrix C\n    c_offsets = (pid_m * BLOCK_M * stride_cm) + (pid_n * BLOCK_N * stride_cn) + tl.arange(0, BLOCK_M)[:, None] * stride_cm + tl.arange(0, BLOCK_N) * stride_cn\n    tl.store(C + c_offsets, c_tile)\n\n@triton.jit\ndef first_wave(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages):\n    # First wave of tiles\n    pid = tl.program_id(axis=0)\n    num_tiles_m = (M + BLOCK_M - 1) // BLOCK_M\n    num_tiles_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m, pid_n = swizzle_tile(pid, num_tiles_m, num_tiles_n)\n    mac_loop(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, pid_m, pid_n, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages)\n\n@triton.jit\ndef full_tiles(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages):\n    # Full tiles\n    pid = tl.program_id(axis=0)\n    num_tiles_m = (M + BLOCK_M - 1) // BLOCK_M\n    num_tiles_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m, pid_n = linear_tile(pid, num_tiles_m, num_tiles_n)\n    mac_loop(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, pid_m, pid_n, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages)\n\n@triton.jit\ndef matmul(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages):\n    # Main matrix multiplication kernel\n    first_wave(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages)\n    full_tiles(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages)\n\nimport triton\nimport triton.language as tl\nimport torch\n\nclass MatMul:\n    def __init__(self, A, B, C, M, N, K, BLOCK_M=16, BLOCK_N=16, BLOCK_K=16, num_warps=4, num_stages=3):\n        self.A = A\n        self.B = B\n        self.C = C\n        self.M = M\n        self.N = N\n        self.K = K\n        self.BLOCK_M = BLOCK_M\n        self.BLOCK_N = BLOCK_N\n        self.BLOCK_K = BLOCK_K\n        self.num_warps = num_warps\n        self.num_stages = num_stages\n\n    def execute(self):\n        # Compute grid and block dimensions\n        num_tiles_m = (self.M + self.BLOCK_M - 1) // self.BLOCK_M\n        num_tiles_n = (self.N + self.BLOCK_N - 1) // self.BLOCK_N\n        grid = (num_tiles_m * num_tiles_n, 1, 1)\n\n        # Launch the kernel\n        matmul[grid](\n            self.A, self.B, self.C, self.M, self.N, self.K,\n            self.A.stride(0), self.A.stride(1), self.B.stride(0), self.B.stride(1), self.C.stride(0), self.C.stride(1),\n            self.BLOCK_M, self.BLOCK_N, self.BLOCK_K, self.num_warps, self.num_stages\n        )\n\n# Example usage\nM, N, K = 1024, 1024, 1024\nA = torch.randn((M, K), device='cuda')\nB = torch.randn((K, N), device='cuda')\nC = torch.zeros((M, N), device='cuda')\n\nmatmul_op = MatMul(A, B, C, M, N, K)\nmatmul_op.execute()\n\nprint(C)\n", "label": "import torch\nimport triton\nfrom triton import language as tl\n\n@triton.jit()\ndef swizzle_tile(tile_id,\n                 M, N, K,\n                 BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                 GROUP_M: tl.constexpr\n                 ):\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_id = tile_id // width\n    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (tile_id % group_size)\n    pid_n = (tile_id % width) // group_size\n    return pid_m, pid_n\n\n\n@triton.jit()\ndef linear_tile(tile_id,\n                M, N, K,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                GROUP_M: tl.constexpr\n                ):\n    pid_m = tile_id // tl.cdiv(N, BLOCK_N)\n    pid_n = tile_id % tl.cdiv(N, BLOCK_N)\n    return pid_m, pid_n\n\n\n@triton.jit()\ndef mac_loop(A, B, C,\n             M, N, K,\n             locks,\n             stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n             iters_per_tile,\n             start_iter, end_iter,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr):\n\n    # where are we in the grid\n    tile_id = start_iter // iters_per_tile\n    if GROUP_M  > 0:\n        pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n    else:\n        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak) + BLOCK_K * stride_ak * (start_iter % iters_per_tile)\n    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn) + BLOCK_K * stride_bk * (start_iter % iters_per_tile)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n\n    for current_iter in range(start_iter, end_iter):\n        a = tl.load(A)\n        b = tl.load(B)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n\n    if end_iter % iters_per_tile == 0:  # last iteration of the tile always happens before its start on another SM\n        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)  # compute inside the if/else to avoid spilling!\n        tl.store(C_, acc)\n        if start_iter % iters_per_tile != 0:  # only if tile has been partially processed\n            tl.atomic_xchg(locks + tile_id, 1)\n    else:\n        while tl.atomic_cas(locks + tile_id, 1, 1) != 1:\n            pass\n        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)  # compute inside the if/else to avoid spilling!\n        tl.atomic_add(C_, acc)\n\n\n@triton.jit()\ndef first_wave(\n        A, B, C,\n        M, N, K,\n        locks,\n        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n        total_full_tiles_streamk, total_partial_tiles_streamk, iters_per_tile,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr,\n        GROUP_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_iter = pid * total_full_tiles_streamk + tl.minimum(pid, total_partial_tiles_streamk)\n    last_iter = (pid + 1) * total_full_tiles_streamk + tl.minimum(pid + 1, total_partial_tiles_streamk)\n\n    while start_iter < last_iter:\n        end_iter = tl.minimum(start_iter + (iters_per_tile - start_iter % iters_per_tile), last_iter)\n        mac_loop(A, B, C,\n                 M, N, K,\n                 locks,\n                 stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n                 iters_per_tile,\n                 start_iter, end_iter,\n                 BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE,\n                 GROUP_M,\n                 )\n\n        start_iter = end_iter\n\n\n@triton.jit()\ndef full_tiles(\n        A, B, C,\n        M, N, K,\n        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n        total_tiles_streamk,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr,\n        GROUP_M: tl.constexpr,\n):\n    # first wave has done more tiles than there are SMs, we adjust pid\n    tile_id = tl.program_id(0) + total_tiles_streamk\n    if GROUP_M > 0:\n        pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n    else:\n        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n\n    # do matrix multiplication\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    # pointers\n    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        a = tl.load(A)\n        b = tl.load(B)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    acc = acc.to(tl.float16)  # restore C.dtype.element_ty\n    # rematerialize rm and rn to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    tl.store(C, acc)\n\n\nclass matmul(torch.autograd.Function):\n\n    _debug = False\n\n    @staticmethod\n    def set_debug(debug: bool):\n        matmul._debug = debug\n\n    @staticmethod\n    def _call(a: torch.Tensor, b: torch.Tensor, total_programs_streamk: int, BLK_M: int, BLK_N: int, BLK_K: int, two_tiles: bool, num_stages: int, num_warps: int):\n        device = a.device\n\n        assert a.is_contiguous() and b.is_contiguous(), \"non-contiguous inputs are not supported\"\n        # checks constraints\n        assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n        M, K = a.shape\n        _, N = b.shape\n        # accumulator types\n        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n        # compute grid (work to do per SM on the first wave)\n        total_blocks_M = triton.cdiv(M, BLK_M)\n        total_blocks_N = triton.cdiv(N, BLK_N)\n        iters_per_tile = triton.cdiv(K, BLK_K)\n        GROUP_M = 8  # 0 to disable swizzling\n        total_tiles = total_blocks_M * total_blocks_N\n\n        if total_programs_streamk > 0:  # Stream-K\n            # last wave may occupy less than total_programs_streamk SMs\n            total_tiles_streamk = total_tiles % total_programs_streamk\n            # for two-tile Stream-K + data-parallel from original paper\n            if two_tiles and total_tiles - total_tiles_streamk > total_programs_streamk:\n                total_tiles_streamk += total_programs_streamk\n            # remaining tiles are computed using classical blocking\n            total_blocking_tiles = total_tiles - total_tiles_streamk\n            total_iters_streamk = total_tiles_streamk * iters_per_tile\n            # iterations related to full waves\n            total_full_tiles_streamk = total_iters_streamk // total_programs_streamk\n            # iterations related to last (partial) wave\n            total_partial_tiles_streamk = total_iters_streamk % total_programs_streamk\n\n        else:  # all tiles are computed using classical blocking\n            total_blocking_tiles = total_tiles\n            total_tiles_streamk = 0\n            total_full_tiles_streamk = 0\n            total_partial_tiles_streamk = 0\n            total_iters_streamk = 0\n\n        if matmul._debug:\n            print(f\"M,N,K={M},{N},{K} ; BLK_M,N,K={BLK_M},{BLK_N},{BLK_K}\")\n            print(f\"{total_blocks_M=} x {total_blocks_N=} = {total_tiles=}\")\n            print(f\"{total_tiles_streamk=} + {total_blocking_tiles=} = {total_tiles=}\")\n            print(f\"{total_programs_streamk=}\")\n            print(f\"{total_blocking_tiles=}\")\n            print(f\"{iters_per_tile=}\")\n            print(f\"{total_iters_streamk=}\")\n\n        # allocates output\n        c = torch.empty((M, N), device=device, dtype=a.dtype)\n        # allocates locks to sync work accross SMs\n        locks = torch.zeros((total_tiles_streamk,), device=device, dtype=torch.int32)\n        k1 = first_wave[(total_programs_streamk,)](\n            a,\n            b,\n            c,\n            M,\n            N,\n            K,\n            locks,\n            a.stride(0),\n            a.stride(1),\n            b.stride(0),\n            b.stride(1),\n            c.stride(0),\n            c.stride(1),\n            total_full_tiles_streamk=total_full_tiles_streamk,\n            total_partial_tiles_streamk=total_partial_tiles_streamk,\n            iters_per_tile=iters_per_tile,\n            BLOCK_M=BLK_M,\n            BLOCK_N=BLK_N,\n            BLOCK_K=BLK_K,\n            ACC_TYPE=ACC_TYPE,\n            GROUP_M=GROUP_M,\n            num_stages=num_stages,\n            num_warps=num_warps,\n        )\n        if matmul._debug:\n            print(f\"{k1.n_regs} registers used, {k1.n_spills} spills\")\n        k2 = full_tiles[(total_blocking_tiles,)](\n            a,\n            b,\n            c,\n            M,\n            N,\n            K,\n            a.stride(0),\n            a.stride(1),\n            b.stride(0),\n            b.stride(1),\n            c.stride(0),\n            c.stride(1),\n            total_tiles_streamk=total_tiles_streamk,\n            BLOCK_M=BLK_M,\n            BLOCK_N=BLK_N,\n            BLOCK_K=BLK_K,\n            ACC_TYPE=ACC_TYPE,\n            GROUP_M=GROUP_M,\n            num_stages=num_stages,\n            num_warps=num_warps,\n        )\n        if matmul._debug:\n            print(f\"{k2.n_regs} registers used, {k2.n_spills} spills\")\n        return c\n\n    @staticmethod\n    def forward(ctx, a: torch.Tensor, b: torch.Tensor, grid: int, BLK_M=128, BLK_N=128, BLK_K=32, two_tiles=True, num_stages=3, num_warps=4):\n        return matmul._call(a=a, b=b, total_programs_streamk=grid, BLK_M=BLK_M, BLK_N=BLK_N, BLK_K=BLK_K, two_tiles=two_tiles, num_warps=num_warps, num_stages=num_stages)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel is designed for recurrent computations, specifically with chunked gating mechanisms. It contains both a forward pass function `_fwd_recurrence` and a backward pass function `_bwd_recurrence`, as well as a wrapper class `ChunkGateRecurrent`.\n\n        `_fwd_recurrence`: \n        - Inputs: `S`, `d`, `O`, `NUM_HEAD`, `NUM_BLOCK`, `D_MODEL_K`, `D_MODEL_V`, `BLOCK_MODEL_K`, `BLOCK_MODEL_V`, `last_kv`\n        - Outputs: Updated `O`\n        - Logic: Computes recurrent transformations for chunks of key/value pairs, optionally using the last key/value (`last_kv`). It iteratively applies transformations across `NUM_BLOCK` blocks using decay coefficients from `d`.\n\n        `_bwd_recurrence`: \n        - Inputs: `S`, `d`, `DI`, `DG`, `DL`, `DS`, `NUM_HEAD`, `NUM_BLOCK`, `D_MODEL_K`, `D_MODEL_V`, `BLOCK_MODEL_K`, `BLOCK_MODEL_V`\n        - Outputs: Updated `DI`, `DG`, and `DL`\n        - Logic: Performs the backward pass of the recurrent transformation. Computes gradients with respect to inputs by reversing the recurrent operation and accumulating gradients.\n\n        `ChunkGateRecurrent`: \n        - Wraps the forward and backward functions, managing input/output tensors and executing them on GPU.\n        - Methods: `forward` sets up input tensors and launches `_fwd_recurrence`. `backward` sets up gradient tensors and launches `_bwd_recurrence`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_recurrence(S, d, O, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V, last_kv, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    head_id = pid // NUM_BLOCK\n    block_id = pid % NUM_BLOCK\n\n    offset = head_id * D_MODEL_K * D_MODEL_V + block_id * BLOCK_MODEL_K * BLOCK_MODEL_V\n\n    S_block = S + offset\n    d_block = d + offset\n    O_block = O + offset\n    last_kv_block = last_kv + offset\n\n    for i in range(BLOCK_SIZE):\n        s = tl.load(S_block + i * BLOCK_MODEL_K)\n        o = tl.load(O_block + i * BLOCK_MODEL_V)\n        decay = tl.load(d_block + i * BLOCK_MODEL_K)\n\n        if last_kv:\n            last_k = tl.load(last_kv_block + i * BLOCK_MODEL_K)\n            last_v = tl.load(last_kv_block + i * BLOCK_MODEL_V)\n            o = decay * o + (1 - decay) * (s @ last_k)\n        else:\n            o = decay * o + (1 - decay) * s\n\n        tl.store(O_block + i * BLOCK_MODEL_V, o)\n\n@triton.jit\ndef _bwd_recurrence(S, d, DI, DG, DL, DS, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    head_id = pid // NUM_BLOCK\n    block_id = pid % NUM_BLOCK\n\n    offset = head_id * D_MODEL_K * D_MODEL_V + block_id * BLOCK_MODEL_K * BLOCK_MODEL_V\n\n    S_block = S + offset\n    d_block = d + offset\n    DI_block = DI + offset\n    DG_block = DG + offset\n    DL_block = DL + offset\n    DS_block = DS + offset\n\n    for i in range(BLOCK_SIZE - 1, -1, -1):\n        s = tl.load(S_block + i * BLOCK_MODEL_K)\n        d = tl.load(d_block + i * BLOCK_MODEL_K)\n        di = tl.load(DI_block + i * BLOCK_MODEL_V)\n        dg = tl.load(DG_block + i * BLOCK_MODEL_K)\n        dl = tl.load(DL_block + i * BLOCK_MODEL_V)\n\n        ds = (1 - d) * (di @ s.T)\n        dd = (1 - d) * (s @ di)\n        dl = d * dl + (1 - d) * di\n\n        tl.store(DI_block + i * BLOCK_MODEL_V, di)\n        tl.store(DG_block + i * BLOCK_MODEL_K, dg + dd)\n        tl.store(DL_block + i * BLOCK_MODEL_V, dl)\n        tl.store(DS_block + i * BLOCK_MODEL_K, ds)\n\nimport torch\n\nclass ChunkGateRecurrent:\n    def __init__(self, S, d, O, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V, last_kv=None):\n        self.S = S\n        self.d = d\n        self.O = O\n        self.NUM_HEAD = NUM_HEAD\n        self.NUM_BLOCK = NUM_BLOCK\n        self.D_MODEL_K = D_MODEL_K\n        self.D_MODEL_V = D_MODEL_V\n        self.BLOCK_MODEL_K = BLOCK_MODEL_K\n        self.BLOCK_MODEL_V = BLOCK_MODEL_V\n        self.last_kv = last_kv\n\n    def forward(self):\n        grid = (self.NUM_HEAD * self.NUM_BLOCK, )\n        BLOCK_SIZE = self.BLOCK_MODEL_K // self.BLOCK_MODEL_V\n        _fwd_recurrence[grid](self.S, self.d, self.O, self.NUM_HEAD, self.NUM_BLOCK, self.D_MODEL_K, self.D_MODEL_V, self.BLOCK_MODEL_K, self.BLOCK_MODEL_V, self.last_kv, BLOCK_SIZE)\n\n    def backward(self, DI, DG, DL, DS):\n        grid = (self.NUM_HEAD * self.NUM_BLOCK, )\n        BLOCK_SIZE = self.BLOCK_MODEL_K // self.BLOCK_MODEL_V\n        _bwd_recurrence[grid](self.S, self.d, DI, DG, DL, DS, self.NUM_HEAD, self.NUM_BLOCK, self.D_MODEL_K, self.D_MODEL_V, self.BLOCK_MODEL_K, self.BLOCK_MODEL_V, BLOCK_SIZE)\n\nimport torch\n\n# Example dimensions\nNUM_HEAD = 8\nNUM_BLOCK = 16\nD_MODEL_K = 64\nD_MODEL_V = 64\nBLOCK_MODEL_K = 16\nBLOCK_MODEL_V = 16\n\n# Initialize tensors\nS = torch.randn(NUM_HEAD, NUM_BLOCK, BLOCK_MODEL_K, BLOCK_MODEL_V, device='cuda')\nd = torch.randn(NUM_HEAD, NUM_BLOCK, BLOCK_MODEL_K, BLOCK_MODEL_V, device='cuda')\nO = torch.zeros(NUM_HEAD, NUM_BLOCK, BLOCK_MODEL_K, BLOCK_MODEL_V, device='cuda')\nlast_kv = torch.randn(NUM_HEAD, NUM_BLOCK, BLOCK_MODEL_K, BLOCK_MODEL_V, device='cuda')\n\n# Initialize wrapper\nchunk_gate_recurrent = ChunkGateRecurrent(S, d, O, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V, last_kv)\n\n# Forward pass\nchunk_gate_recurrent.forward()\n\n# Backward pass\nDI = torch.randn_like(O)\nDG = torch.randn_like(d)\nDL = torch.randn_like(O)\nDS = torch.zeros_like(S)\nchunk_gate_recurrent.backward(DI, DG, DL, DS)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\ntorch.backends.cudnn.allow_tf32 = True\n\n@triton.jit\ndef _fwd_recurrence(\n    S, d, \n    O,\n    NUM_HEAD, NUM_BLOCK, \n    D_MODEL_K: tl.constexpr, D_MODEL_V: tl.constexpr,\n    BLOCK_MODEL_K: tl.constexpr, BLOCK_MODEL_V: tl.constexpr,\n    last_kv: tl.tensor  # \u4e0d\u518d\u4f7f\u7528 Optional\n):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)    \n\n    S = S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K + tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n    O = O + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n\n    if last_kv is not None:\n        last_kv = last_kv + offset_bh * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n        acc = tl.load(last_kv).to(tl.float32)\n    else:\n        acc = tl.zeros([BLOCK_MODEL_K, BLOCK_MODEL_V], dtype=tl.float32)\n\n    tl.store(O, acc.to(O.dtype.element_ty))\n    O += D_MODEL_K * D_MODEL_V\n    d = d + offset_bh * NUM_BLOCK\n    for i in range(NUM_BLOCK-1):\n        d_i = tl.load(d)\n        S_i = tl.load(S) \n        acc = acc * d_i + S_i\n        tl.store(O, acc.to(O.dtype.element_ty))\n        d += 1\n        S += D_MODEL_K * D_MODEL_V\n        O += D_MODEL_K * D_MODEL_V\n     \n\n## NUM_SPLIT_K/V. K/V dimension split into NUM_SPLIT_K/V parts with equal size BLOCK_MODEL\n@triton.jit\ndef _bwd_recurrence(\n    S, d, \n    DI, DG, DL, DS, \n    NUM_HEAD, NUM_BLOCK,\n    D_MODEL_K: tl.constexpr, D_MODEL_V: tl.constexpr,\n    BLOCK_MODEL_K: tl.constexpr, BLOCK_MODEL_V: tl.constexpr,\n    \n ):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)    \n\n    # offset_h = offset_bh % NUM_HEAD\n    NUM_K = D_MODEL_K // BLOCK_MODEL_K\n    NUM_V = D_MODEL_V // BLOCK_MODEL_V\n    # skip the last chunk because it is never used\n    S = S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K + tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :] + (NUM_BLOCK - 2) * D_MODEL_K * D_MODEL_V\n\n    DI = DI + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K + tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :] + (NUM_BLOCK - 2) * D_MODEL_K * D_MODEL_V\n\n    # start from the last chunk  \n    DS = DS + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :] + (NUM_BLOCK - 1) * D_MODEL_K * D_MODEL_V\n\n    DG = DG + offset_bh * NUM_BLOCK * NUM_K * NUM_V + offset_d * NUM_V + offset_s + (NUM_BLOCK - 2) * NUM_K * NUM_V\n\n    d = d + offset_bh * NUM_BLOCK + (NUM_BLOCK - 1)\n\n    Dacc = tl.zeros([BLOCK_MODEL_K, BLOCK_MODEL_V], dtype=tl.float32) \n\n    # ignore the first chunk\n    for i in range(NUM_BLOCK - 1):\n        S_i = tl.load(S)\n        DS_i = tl.load(DS)\n        d_i = tl.load(d)\n        Dacc = Dacc * d_i + DS_i\n        DG_i = tl.sum(Dacc * S_i.to(tl.float32))\n\n        tl.store(DG, DG_i.to(DG.dtype.element_ty))\n        tl.store(DI, Dacc.to(DI.dtype.element_ty))    \n\n        S -= D_MODEL_K * D_MODEL_V\n        DI -= D_MODEL_K * D_MODEL_V \n        DS -= D_MODEL_K * D_MODEL_V\n        DG -= NUM_K * NUM_V\n        d -= 1\n    \n    DL = DL + offset_bh * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n    DS_i = tl.load(DS)\n    d_i = tl.load(d)\n    Dacc = Dacc * d_i + DS_i\n    tl.store(DL, Dacc.to(DL.dtype.element_ty))  \n\nclass ChunkGateRecurrent(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, kv, cross_decay, last_kv=None):\n        cross_decay = cross_decay.contiguous()\n        kv = kv.contiguous()\n\n        B, H, N, D_k, D_v = kv.shape \n        output = torch.empty_like(kv)        \n        BLOCK_MODEL_K = 64\n        BLOCK_MODEL_V = 16\n    \n        assert D_k % BLOCK_MODEL_K == 0\n        assert D_v % BLOCK_MODEL_V == 0\n\n        grid = (B*H, D_k//BLOCK_MODEL_K, D_v//BLOCK_MODEL_V)\n        ctx.grid = grid\n        ctx.have_last_kv = last_kv is not None\n        ctx.BLOCK_MODEL_K = BLOCK_MODEL_K\n        ctx.BLOCK_MODEL_V = BLOCK_MODEL_V\n\n        _fwd_recurrence[grid](\n            kv,\n            cross_decay,\n            output,\n            D_MODEL_K=D_k, D_MODEL_V=D_v,\n            NUM_BLOCK=N, NUM_HEAD=H,\n            BLOCK_MODEL_K=BLOCK_MODEL_K,\n            BLOCK_MODEL_V=BLOCK_MODEL_V,\n            last_kv=last_kv\n        )\n\n        ctx.save_for_backward(output, cross_decay)        \n        return output\n\n    @staticmethod\n    def backward(ctx, DO):\n        DO = DO.contiguous()\n\n        output, cross_decay = ctx.saved_tensors \n\n        B, H, N, D_k, D_v = output.shape \n        \n        BLOCK_MODEL_K = 64\n        BLOCK_MODEL_V = 16\n\n        grid = (B*H, D_k//BLOCK_MODEL_K, D_v//BLOCK_MODEL_V)\n\n        DI = torch.empty_like(DO)\n        DG = torch.empty(B*H, N, D_k//BLOCK_MODEL_K, D_v//BLOCK_MODEL_V, device=cross_decay.device, dtype=cross_decay.dtype)\n        DL = torch.empty(B, H, D_k, D_v, device=output.device, dtype=output.dtype)\n        _bwd_recurrence[grid](\n            output, cross_decay,\n            DI, DG, DL, DO, \n            NUM_HEAD=H, NUM_BLOCK = N, \n            D_MODEL_K = D_k,\n            D_MODEL_V = D_v, \n            BLOCK_MODEL_K=BLOCK_MODEL_K,\n            BLOCK_MODEL_V=BLOCK_MODEL_V,\n        )\n\n        DI[:, :, -1] = 0\n        DG[:, -1] = 0\n        DG = DG.view(B, H, N, -1).sum(dim=-1)\n        return DI, DG, DL if ctx.have_last_kv else None\n\nchunk_gate_recurrent = ChunkGateRecurrent.apply\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided Triton kernel code includes two main kernel functions: \n    `chunk_gated_abc_fwd_kernel_cum` and `chunk_gated_abc_fwd_kernel_h`. \n    These are used in a forward pass computation commonly found in attention-like mechanisms. \n\n    `chunk_gated_abc_fwd_kernel_cum` handles the computation of a cumulative matrix. It takes inputs: \n    `s` (source), `o` (output), along with stride parameters and block sizes (`T`, `S`, `BT`, `BS`). \n    It calculates a mask `m_s` that ensures only upper triangular parts of the matrix are involved in \n    the computation. The results are stored in the output `o`.\n\n    `chunk_gated_abc_fwd_kernel_h` is designed to handle a gated accumulation operation over key, value, \n    and gating tensors. Inputs include `k` (key), `v` (value), `g` (gate), `h` (output state), optional \n    initial `h0`, and final `ht` states, along with stride and block size parameters. The kernel iteratively \n    accumulates the product of `k` and `v`, adjusted by `g`, into `h`.\n\n    The `fwd_pre` function sets up the grid for `chunk_gated_abc_fwd_kernel_cum`, applying a cumulative sum \n    operation on the input tensor `g`, reshaping, and storing the result.\n\n    The `fwd_inner` function prepares and launches `chunk_gated_abc_fwd_kernel_h`, iterating over blocks \n    of data to compute the gated attention mechanism with options for initial and final states.\n\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Kernel for cumulative matrix computation\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_cum(\n    s_ptr, o_ptr, \n    stride_s_b, stride_s_t, stride_s_s, \n    stride_o_b, stride_o_t, stride_o_s, \n    T, S, BT, BS, \n    BLOCK_SIZE_T: tl.constexpr, BLOCK_SIZE_S: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (T // BLOCK_SIZE_T)\n    tid = pid % (T // BLOCK_SIZE_T)\n    \n    # Block bounds\n    block_start_t = tid * BLOCK_SIZE_T\n    block_start_s = bid * BLOCK_SIZE_S\n    \n    # Initialize pointers\n    s_block_ptr = s_ptr + bid * stride_s_b + block_start_t * stride_s_t + block_start_s * stride_s_s\n    o_block_ptr = o_ptr + bid * stride_o_b + block_start_t * stride_o_t + block_start_s * stride_o_s\n    \n    # Load data\n    s = tl.load(s_block_ptr, mask=block_start_s + tl.arange(0, BLOCK_SIZE_S) < S, other=0.0)\n    \n    # Compute mask for upper triangular part\n    m_s = tl.arange(0, BLOCK_SIZE_S) >= block_start_t + tl.arange(0, BLOCK_SIZE_T)\n    \n    # Compute cumulative sum\n    for i in range(BLOCK_SIZE_T):\n        s = tl.where(m_s[i], s, 0.0)\n        tl.atomic_add(o_block_ptr + i * stride_o_t, s)\n\n# Kernel for gated accumulation\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_h(\n    k_ptr, v_ptr, g_ptr, h_ptr, h0_ptr, ht_ptr, \n    stride_k_b, stride_k_t, stride_k_s, \n    stride_v_b, stride_v_t, stride_v_s, \n    stride_g_b, stride_g_t, stride_g_s, \n    stride_h_b, stride_h_t, stride_h_s, \n    stride_h0_b, stride_h0_t, stride_h0_s, \n    stride_ht_b, stride_ht_t, stride_ht_s, \n    T, S, BT, BS, \n    BLOCK_SIZE_T: tl.constexpr, BLOCK_SIZE_S: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (T // BLOCK_SIZE_T)\n    tid = pid % (T // BLOCK_SIZE_T)\n    \n    # Block bounds\n    block_start_t = tid * BLOCK_SIZE_T\n    block_start_s = bid * BLOCK_SIZE_S\n    \n    # Initialize pointers\n    k_block_ptr = k_ptr + bid * stride_k_b + block_start_t * stride_k_t + block_start_s * stride_k_s\n    v_block_ptr = v_ptr + bid * stride_v_b + block_start_t * stride_v_t + block_start_s * stride_v_s\n    g_block_ptr = g_ptr + bid * stride_g_b + block_start_t * stride_g_t + block_start_s * stride_g_s\n    h_block_ptr = h_ptr + bid * stride_h_b + block_start_t * stride_h_t + block_start_s * stride_h_s\n    \n    # Load initial state if provided\n    if h0_ptr is not None:\n        h0_block_ptr = h0_ptr + bid * stride_h0_b + block_start_t * stride_h0_t + block_start_s * stride_h0_s\n        h0 = tl.load(h0_block_ptr, mask=block_start_s + tl.arange(0, BLOCK_SIZE_S) < S, other=0.0)\n        h = h0\n    else:\n        h = tl.zeros((BLOCK_SIZE_S,), dtype=tl.float32)\n    \n    # Load data\n    k = tl.load(k_block_ptr, mask=block_start_s + tl.arange(0, BLOCK_SIZE_S) < S, other=0.0)\n    v = tl.load(v_block_ptr, mask=block_start_s + tl.arange(0, BLOCK_SIZE_S) < S, other=0.0)\n    g = tl.load(g_block_ptr, mask=block_start_s + tl.arange(0, BLOCK_SIZE_S) < S, other=0.0)\n    \n    # Compute gated accumulation\n    for i in range(BLOCK_SIZE_T):\n        h = h + g * k * v\n        tl.atomic_add(h_block_ptr + i * stride_h_t, h)\n    \n    # Store final state if provided\n    if ht_ptr is not None:\n        ht_block_ptr = ht_ptr + bid * stride_ht_b + block_start_t * stride_ht_t + block_start_s * stride_ht_s\n        tl.store(ht_block_ptr, h, mask=block_start_s + tl.arange(0, BLOCK_SIZE_S) < S)\n\n### Wrapper Functions\n\ndef fwd_pre(s, o, T, S, BT, BS, BLOCK_SIZE_T, BLOCK_SIZE_S):\n    grid = (T * S // (BLOCK_SIZE_T * BLOCK_SIZE_S),)\n    chunk_gated_abc_fwd_kernel_cum[grid](\n        s, o, \n        s.stride(0), s.stride(1), s.stride(2), \n        o.stride(0), o.stride(1), o.stride(2), \n        T, S, BT, BS, \n        BLOCK_SIZE_T, BLOCK_SIZE_S\n    )\n\ndef fwd_inner(k, v, g, h, h0=None, ht=None, T, S, BT, BS, BLOCK_SIZE_T, BLOCK_SIZE_S):\n    grid = (T * S // (BLOCK_SIZE_T * BLOCK_SIZE_S),)\n    chunk_gated_abc_fwd_kernel_h[grid](\n        k, v, g, h, h0, ht, \n        k.stride(0), k.stride(1), k.stride(2), \n        v.stride(0), v.stride(1), v.stride(2), \n        g.stride(0), g.stride(1), g.stride(2), \n        h.stride(0), h.stride(1), h.stride(2), \n        h0.stride(0) if h0 is not None else 0, h0.stride(1) if h0 is not None else 0, h0.stride(2) if h0 is not None else 0, \n        ht.stride(0) if ht is not None else 0, ht.stride(1) if ht is not None else 0, ht.stride(2) if ht is not None else 0, \n        T, S, BT, BS, \n        BLOCK_SIZE_T, BLOCK_SIZE_S\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BS': 16}, num_warps=2),\n        triton.Config({'BS': 16}, num_warps=4),\n        triton.Config({'BS': 16}, num_warps=8),\n        triton.Config({'BS': 32}, num_warps=2),\n        triton.Config({'BS': 32}, num_warps=4),\n        triton.Config({'BS': 32}, num_warps=8),\n        triton.Config({'BS': 64}, num_warps=2),\n        triton.Config({'BS': 64}, num_warps=4),\n        triton.Config({'BS': 64}, num_warps=8),\n    ],\n    key=['S']\n)\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_cum(\n    s,\n    o,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1., 0.).to(tl.float32)\n\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    # [BT, BS]\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_h(\n    k,\n    v,\n    g,\n    h,\n    h0,\n    ht,\n    s_k_h,\n    s_k_t,\n    s_k_d,\n    s_v_h,\n    s_v_t,\n    s_v_d,\n    s_h_h,\n    s_h_t,\n    s_h_d,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    GATEK: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if GATEK:\n            p_g = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            # [BK,]\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            # [BK, BV]\n            b_h *= tl.exp(b_gn)[:, None]\n            # [BK, BT]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_k = (b_k * tl.exp(b_gn[:, None] - b_g)).to(b_k.dtype)\n        else:\n            p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            # [BV,]\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            # [BK, BV]\n            b_h *= tl.exp(b_gn)[None, :]\n            # [BT, BV]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_v = (b_v * tl.exp(b_gn[None, :] - b_g)).to(b_v.dtype)\n        # [BK, BV]\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n\ndef fwd_pre(g, B, H, T, S, BT):\n    NT = triton.cdiv(T, BT)\n    g_org, g = g, torch.empty_like(g, dtype=torch.float)\n    def grid(meta): return (triton.cdiv(meta['S'], meta['BS']), NT, B * H)\n    # keep cummulative normalizer in fp32\n    # this kernel is equivalent to\n    # g = g.view(B, H, NT, BT, -1).cumsum(-2).view(B, H, T, -1)\n    chunk_gated_abc_fwd_kernel_cum[grid](\n        g_org, g,\n        g.stride(1), g.stride(2), g.stride(3),\n        T=T, S=S, BT=BT\n    )\n    return g\n\n\ndef fwd_inner(q, k, v, g, B, H, T, K, V, BT, BK, BV, gatek=False, h0=None, ht=None):\n    NT = triton.cdiv(T, BT)\n    NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n    num_warps = 4 if BK == 64 else 2\n    num_stages = 1\n\n    h = q.new_empty(B, H, NT * K, V)\n    grid = (NV, NK, B * H)\n    chunk_gated_abc_fwd_kernel_h[grid](\n        k, v, g, h, h0, ht,\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2), h.stride(3),\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n        GATEK=gatek,\n        USE_INITIAL_STATE=h0 is not None,\n        STORE_FINAL_STATE=ht is not None,\n        num_warps=num_warps,\n        num_stages=num_stages\n    )\n    return h\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code implements a custom operation named SWiGLU using the Triton language for high-performance computations on GPUs. \n            The `swiglu_forward` function accepts two input tensors, `a` and `b`, computes their element-wise product after applying the SiLU (Sigmoid Linear Unit) activation to `a`, and returns the result in `c`. \n            The forward pass relies on the `_swiglu_forward_kernel` Triton kernel function, which is launched with the grid size corresponding to the number of rows in the reshaped input.\n            The `swiglu_backward` function calculates the gradients with respect to the input tensors `a` and `b`, given the gradient `dc` of the output, using the `_swiglu_backward_kernel`. Both kernel functions operate over blocks of data, specified by the `BLOCK_SIZE`, and use `num_warps` for optimizing parallel execution. \n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the SWiGLU forward kernel\n@triton.jit\ndef _swiglu_forward_kernel(\n    a_ptr,  # Pointer to input tensor a\n    b_ptr,  # Pointer to input tensor b\n    c_ptr,  # Pointer to output tensor c\n    n_elements,  # Number of elements in the tensors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallel execution\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    block_start = pid * BLOCK_SIZE  # Calculate the start index for the block\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # Create offsets for the block\n    mask = offsets < n_elements  # Create a mask to handle the last block\n\n    a = tl.load(a_ptr + offsets, mask=mask)  # Load elements from a\n    b = tl.load(b_ptr + offsets, mask=mask)  # Load elements from b\n\n    # Apply the SiLU activation to a\n    a_silu = a * tl.sigmoid(a)\n\n    # Compute the element-wise product of a_silu and b\n    c = a_silu * b\n\n    # Store the result in c\n    tl.store(c_ptr + offsets, c, mask=mask)\n\n# Define the SWiGLU backward kernel\n@triton.jit\ndef _swiglu_backward_kernel(\n    a_ptr,  # Pointer to input tensor a\n    b_ptr,  # Pointer to input tensor b\n    dc_ptr,  # Pointer to gradient of output tensor c\n    da_ptr,  # Pointer to gradient of input tensor a\n    db_ptr,  # Pointer to gradient of input tensor b\n    n_elements,  # Number of elements in the tensors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallel execution\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    block_start = pid * BLOCK_SIZE  # Calculate the start index for the block\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # Create offsets for the block\n    mask = offsets < n_elements  # Create a mask to handle the last block\n\n    a = tl.load(a_ptr + offsets, mask=mask)  # Load elements from a\n    b = tl.load(b_ptr + offsets, mask=mask)  # Load elements from b\n    dc = tl.load(dc_ptr + offsets, mask=mask)  # Load elements from dc\n\n    # Apply the SiLU activation to a\n    a_silu = a * tl.sigmoid(a)\n\n    # Compute the gradient with respect to b\n    db = dc * a_silu\n\n    # Compute the gradient with respect to a\n    da = dc * b * (tl.sigmoid(a) + a * (1 - tl.sigmoid(a)))\n\n    # Store the gradients in da and db\n    tl.store(da_ptr + offsets, da, mask=mask)\n    tl.store(db_ptr + offsets, db, mask=mask)\n\n# Define the forward function\ndef swiglu_forward(a, b, c, BLOCK_SIZE=128, num_warps=4):\n    n_elements = a.shape[0]\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    _swiglu_forward_kernel[grid](a, b, c, n_elements, BLOCK_SIZE, num_warps=num_warps)\n\n# Define the backward function\ndef swiglu_backward(a, b, dc, da, db, BLOCK_SIZE=128, num_warps=4):\n    n_elements = a.shape[0]\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    _swiglu_backward_kernel[grid](a, b, dc, da, db, n_elements, BLOCK_SIZE, num_warps=num_warps)\n\nimport torch\n\n# Create input tensors\na = torch.randn(1024, device='cuda')\nb = torch.randn(1024, device='cuda')\n\n# Create output tensor\nc = torch.empty_like(a)\n\n# Forward pass\nswiglu_forward(a, b, c)\n\n# Create gradient tensor\ndc = torch.randn_like(a)\n\n# Create gradient tensors for a and b\nda = torch.empty_like(a)\ndb = torch.empty_like(b)\n\n# Backward pass\nswiglu_backward(a, b, dc, da, db)\n\n# Now da and db contain the gradients\nprint(da)\nprint(db)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport functools\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\ndef calculate_settings(n):\n    MAX_FUSED_SIZE = 65536\n    BLOCK_SIZE = triton.next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(\n            f\"Cannot launch Triton kernel since n = {n} exceeds \"\n            f\"the recommended Triton blocksize = {MAX_FUSED_SIZE}.\"\n        )\n\n    num_warps = 4\n    if BLOCK_SIZE >= 32768:\n        num_warps = 32 if not is_hip() else 16\n    elif BLOCK_SIZE >= 8192:\n        num_warps = 16\n    elif BLOCK_SIZE >= 2048:\n        num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n\n@triton.jit\ndef _swiglu_forward_kernel(\n    a_ptr, b_ptr, c_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    # sigmoid requires type float32\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n\n@triton.jit\ndef _swiglu_backward_kernel(\n    dc_ptr, a_ptr, b_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    # sigmoid requires type float32\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n\n    # recomputation to save memory\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n\ndef swiglu_forward(a, b):\n    ori_shape = a.shape\n\n    n_cols = ori_shape[-1]\n    a = a.view(-1, n_cols)\n    b = b.view(-1, n_cols)\n    c = torch.empty_like(a)\n    n_rows = a.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _swiglu_forward_kernel[(n_rows,)](\n        a,\n        b,\n        c,\n        c.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return a, b, c.view(*ori_shape)\n\ndef swiglu_backward(a, b, dc):\n    ori_shape = dc.shape\n    n_cols = ori_shape[-1]\n    dc = dc.view(-1, n_cols)\n    n_rows = dc.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _swiglu_backward_kernel[(n_rows,)](\n        dc,\n        a,\n        b,\n        dc.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return a.view(*ori_shape), b.view(*ori_shape)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe provided Triton code defines a custom addition operation on two input PyTorch tensors, `a` and `b`. The main computation is performed in the `_add_kernel` Triton kernel. The function `custom_add` wraps this kernel to apply it over the input tensors.\n\nFunction `_add_kernel(A, B, C, size, BLOCK)`:\n- It is a Triton kernel function, decorated with `@triton.jit`, which indicates that this function is just-in-time compiled by Triton.\n- Inputs: Pointers to input tensors `A` and `B`, a pointer to the output tensor `C`, the size of the tensors, and a constant parameter `BLOCK` that indicates the block size of computation.\n- Each program instance computes a block of the tensor addition. The index of the program is obtained by `tl.program_id(0)`.\n- The offsets for the block are calculated using `prog_id` and `tl.arange(0, BLOCK)`.\n- `tl.load` and `tl.store` functions are used to read from and write to tensors with boundary checks using a mask `offs < size`.\n\nFunction `custom_add(a, b)`:\n- It is a wrapper around the `_add_kernel` to add two PyTorch tensors `a` and `b`.\n- It creates an empty tensor `c` of the same shape as `a` to store the result.\n- `grid` is determined by dividing `size` by `BLOCK`, ensuring each block is processed by a program instance.\n- The Triton kernel is launched with `_add_kernel[grid]`, passing necessary arguments.\n- Returns the result tensor `c`.\n\nThe BLOCK constant defines how many elements each instance processes, here set to 16. The kernel ensures boundary conditions are respected using the mask.\n", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK = 16\n\n# Triton kernel for element-wise addition\n@triton.jit\ndef _add_kernel(A, B, C, size, BLOCK: tl.constexpr):\n    # Get the program ID\n    prog_id = tl.program_id(0)\n    \n    # Calculate the offsets for the block\n    offs = prog_id * BLOCK + tl.arange(0, BLOCK)\n    \n    # Create a mask to handle boundary conditions\n    mask = offs < size\n    \n    # Load the data from the input tensors with boundary checks\n    a = tl.load(A + offs, mask=mask)\n    b = tl.load(B + offs, mask=mask)\n    \n    # Perform the addition\n    c = a + b\n    \n    # Store the result back to the output tensor with boundary checks\n    tl.store(C + offs, c, mask=mask)\n\n# Wrapper function to add two PyTorch tensors\ndef custom_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensors are on the same device\n    assert a.device == b.device, \"Input tensors must be on the same device\"\n    \n    # Ensure the input tensors have the same shape\n    assert a.shape == b.shape, \"Input tensors must have the same shape\"\n    \n    # Create an empty tensor to store the result\n    c = torch.empty_like(a)\n    \n    # Determine the grid size\n    size = a.numel()\n    grid = (size + BLOCK - 1) // BLOCK\n    \n    # Launch the Triton kernel\n    _add_kernel[grid](a, b, c, size, BLOCK)\n    \n    return c\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create two input tensors\n    a = torch.randn(1024, device='cuda')\n    b = torch.randn(1024, device='cuda')\n    \n    # Perform the custom addition\n    c = custom_add(a, b)\n    \n    # Verify the result\n    expected = a + b\n    assert torch.allclose(c, expected), \"The result does not match the expected output\"\n    \n    print(\"Custom addition is correct!\")\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _add_kernel(A, B, C, size, BLOCK: tl.constexpr):\n    \"\"\"add kernel.\"\"\"\n    prog_id = tl.program_id(0)\n    offs = prog_id * BLOCK + tl.arange(0, BLOCK)\n    a = tl.load(A + offs, mask=offs < size)\n    b = tl.load(B + offs, mask=offs < size)\n    tl.store(C + offs, a + b, mask=offs < size)\n\ndef custom_add(a, b):\n    \"\"\"custom add one.\"\"\"\n    c = torch.empty_like(a)\n    size = c.size(0)\n    BLOCK = 16\n\n    grid = (triton.cdiv(size, BLOCK), )\n    _add_kernel[grid](a, b, c, size, BLOCK=BLOCK)\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs a 2D convolution operation on input data using a set of weights. \n            The kernel is called `conv2d_forward_kernel` and is launched with specific grid dimensions based on input size, output size, and the number of groups.\n            The kernel takes pointers to the input, weight, and output tensors along with their respective dimensions and strides. \n            It calculates the output feature map by iterating over the kernel dimensions and computing matrix products between input patches and weight matrices.\n            The function `conv2d_forward` serves as a wrapper for this Triton kernel, taking PyTorch tensors as input and calculating the necessary strides and dimensions.\n            It initializes an output tensor and defines block sizes to partition work across Triton threads.\n            Finally, the Triton kernel is launched with the prepared inputs and block configurations, and the resulting output tensor is returned.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef conv2d_forward_kernel(\n    input_ptr,  # Pointer to the input tensor\n    weight_ptr, # Pointer to the weight tensor\n    output_ptr, # Pointer to the output tensor\n    input_shape, # (N, C_in, H_in, W_in)\n    weight_shape, # (C_out, C_in // groups, kernel_H, kernel_W)\n    output_shape, # (N, C_out, H_out, W_out)\n    stride, # (stride_H, stride_W)\n    padding, # (padding_H, padding_W)\n    dilation, # (dilation_H, dilation_W)\n    groups, # Number of groups\n    input_strides, # (stride_N, stride_C_in, stride_H_in, stride_W_in)\n    weight_strides, # (stride_C_out, stride_C_in, stride_kernel_H, stride_kernel_W)\n    output_strides, # (stride_N, stride_C_out, stride_H_out, stride_W_out)\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_C_OUT: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr,\n    BLOCK_SIZE_W: tl.constexpr,\n    BLOCK_SIZE_C_IN: tl.constexpr,\n    BLOCK_SIZE_KH: tl.constexpr,\n    BLOCK_SIZE_KW: tl.constexpr,\n):\n    pid_n = tl.program_id(axis=0)\n    pid_c_out = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_w = tl.program_id(axis=3)\n\n    n_start = pid_n * BLOCK_SIZE_N\n    c_out_start = pid_c_out * BLOCK_SIZE_C_OUT\n    h_start = pid_h * BLOCK_SIZE_H\n    w_start = pid_w * BLOCK_SIZE_W\n\n    for n in range(n_start, n_start + BLOCK_SIZE_N):\n        if n >= input_shape[0]:\n            break\n        for c_out in range(c_out_start, c_out_start + BLOCK_SIZE_C_OUT):\n            if c_out >= output_shape[1]:\n                break\n            for h in range(h_start, h_start + BLOCK_SIZE_H):\n                if h >= output_shape[2]:\n                    break\n                for w in range(w_start, w_start + BLOCK_SIZE_W):\n                    if w >= output_shape[3]:\n                        break\n                    out_idx = n * output_strides[0] + c_out * output_strides[1] + h * output_strides[2] + w * output_strides[3]\n                    output_ptr += out_idx\n                    acc = 0.0\n                    for c_in_group in range(weight_shape[1]):\n                        for kh in range(weight_shape[2]):\n                            for kw in range(weight_shape[3]):\n                                input_h = h * stride[0] - padding[0] + kh * dilation[0]\n                                input_w = w * stride[1] - padding[1] + kw * dilation[1]\n                                if 0 <= input_h < input_shape[2] and 0 <= input_w < input_shape[3]:\n                                    input_idx = n * input_strides[0] + (c_out // groups * groups + c_in_group) * input_strides[1] + input_h * input_strides[2] + input_w * input_strides[3]\n                                    weight_idx = c_out * weight_strides[0] + c_in_group * weight_strides[1] + kh * weight_strides[2] + kw * weight_strides[3]\n                                    input_val = tl.load(input_ptr + input_idx)\n                                    weight_val = tl.load(weight_ptr + weight_idx)\n                                    acc += input_val * weight_val\n                    tl.store(output_ptr, acc)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef conv2d_forward(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n    # Ensure input and weight are on the same device\n    assert input.device == weight.device, \"Input and weight must be on the same device\"\n    device = input.device\n\n    # Get input and weight dimensions\n    N, C_in, H_in, W_in = input.shape\n    C_out, C_in_per_group, kernel_H, kernel_W = weight.shape\n\n    # Calculate output dimensions\n    H_out = (H_in + 2 * padding[0] - dilation[0] * (kernel_H - 1) - 1) // stride[0] + 1\n    W_out = (W_in + 2 * padding[1] - dilation[1] * (kernel_W - 1) - 1) // stride[1] + 1\n\n    # Initialize output tensor\n    output = torch.empty((N, C_out, H_out, W_out), device=device, dtype=input.dtype)\n\n    # Define block sizes\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_C_OUT = 16\n    BLOCK_SIZE_H = 16\n    BLOCK_SIZE_W = 16\n    BLOCK_SIZE_C_IN = 16\n    BLOCK_SIZE_KH = 16\n    BLOCK_SIZE_KW = 16\n\n    # Define grid dimensions\n    grid = (\n        (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N,\n        (C_out + BLOCK_SIZE_C_OUT - 1) // BLOCK_SIZE_C_OUT,\n        (H_out + BLOCK_SIZE_H - 1) // BLOCK_SIZE_H,\n        (W_out + BLOCK_SIZE_W - 1) // BLOCK_SIZE_W\n    )\n\n    # Define strides\n    input_strides = (C_in * H_in * W_in, H_in * W_in, W_in, 1)\n    weight_strides = (C_in_per_group * kernel_H * kernel_W, kernel_H * kernel_W, kernel_W, 1)\n    output_strides = (C_out * H_out * W_out, H_out * W_out, W_out, 1)\n\n    # Launch the kernel\n    conv2d_forward_kernel[grid](\n        input, weight, output,\n        input.shape, weight.shape, output.shape,\n        stride, padding, dilation, groups,\n        input_strides, weight_strides, output_strides,\n        BLOCK_SIZE_N, BLOCK_SIZE_C_OUT, BLOCK_SIZE_H, BLOCK_SIZE_W, BLOCK_SIZE_C_IN, BLOCK_SIZE_KH, BLOCK_SIZE_KW\n    )\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef conv2d_forward_kernel(\n    input_pointer, weight_pointer, output_pointer,\n    batch_dim, in_feat_dim, in_height, in_width,\n    out_feat_dim, out_height, out_width,\n    input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride,\n    weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride,\n    output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride,\n    kernel_height: tl.constexpr, kernel_width: tl.constexpr,\n    stride_height: tl.constexpr, stride_width: tl.constexpr,\n    padding_height: tl.constexpr, padding_width: tl.constexpr,\n    groups: tl.constexpr, fp16: tl.constexpr, tf32: tl.constexpr,\n    BLOCK_SIZE_BATCH_HEIGHT_WIDTH: tl.constexpr, BLOCK_SIZE_IN_FEAT: tl.constexpr,\n    BLOCK_SIZE_OUT_FEAT: tl.constexpr,\n    ):\n    \"\"\"\n    2D-convolves over the input using weights.\n\n    Args:\n        input_pointer: Pointer to the input to convolve over.\n            The input must be of shape [batch_dim, in_feat_dim, in_height, in_width].\n        weight_pointer: Pointer to the weights input is convolved over by.\n            The weights must be of shape [out_feat_dim, in_feat_dim, kernel_height, kernel_width].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, out_feat_dim, out_height, out_width].\n        batch_dim: Batch dimension of the input and output.\n        in_feat_dim: Dimensionality of the input features.\n        in_height: Input height.\n        in_width: Input width.\n        out_feat_dim: Dimensionality of the output features.\n        out_height: Output height.\n        out_width: Output width.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_in_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_height_stride: Stride necessary to jump one element along the\n            input's height dimension.\n        input_width_stride: Stride necessary to jump one element along the\n            input's width dimension.\n        weight_out_feat_stride: Stride necessary to jump one element along the\n            weights' output feature dimension.\n        weight_in_feat_stride: Stride necessary to jump one element along the\n            weights' input feature dimension.\n        weight_height_stride: Stride necessary to jump one element along the\n            weights' height dimension.\n        weight_width_stride: Stride necessary to jump one element along the\n            weights' width dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output's batch dimension.\n        output_out_feat_stride: Stride necessary to jump one element along the\n            output's feature dimension.\n        output_height_stride: Stride necessary to jump one element along the\n            output's height dimension.\n        output_width_stride: Stride necessary to jump one element along the\n            output's width dimension.\n        kernel_height: Kernel height.\n        kernel_width: Kernel width.\n        stride_height: Stride of kernel across the height dimension.\n        stride_width: Stride of kernel across the width dimension.\n        padding_height: Padding applied to the input across the height dimension.\n        padding_width: Padding applied to the input across the width dimension.\n        groups: Number of groups for the convolution.\n        fp16: Flag for loading the input and weights in FP16.\n        tf32: Flag for performing matrix products in TF32.\n        BLOCK_SIZE_BATCH_HEIGHT_WIDTH: Block size across the batch, height, and\n            width dimensions.\n        BLOCK_SIZE_IN_FEAT: Block size across the input feature dimension.\n        BLOCK_SIZE_OUT_FEAT: Block size across the output feature dimension.\n    \"\"\"\n    batch_height_width_pid = tl.program_id(0)\n    out_feat_pid = tl.program_id(1)\n    group_pid = tl.program_id(2)\n\n    in_group_dim = in_feat_dim // groups\n    out_group_dim = out_feat_dim // groups\n\n    batch_height_width_offset = (batch_height_width_pid * BLOCK_SIZE_BATCH_HEIGHT_WIDTH +\n                                 tl.arange(0, BLOCK_SIZE_BATCH_HEIGHT_WIDTH))\n    batch_height_offset = batch_height_width_offset // out_width\n    batch_offset = batch_height_offset // out_height\n\n    output_feat_offset = (out_feat_pid * BLOCK_SIZE_OUT_FEAT +\n                          tl.arange(0, BLOCK_SIZE_OUT_FEAT))\n    output_height_offset = batch_height_offset % out_height\n    output_width_offset = batch_height_width_offset % out_width\n\n    input_pointer += (input_batch_stride * batch_offset +\n                      input_in_feat_stride * group_pid * in_group_dim)[:, None]\n    weight_pointer += (weight_out_feat_stride * output_feat_offset +\n                       weight_out_feat_stride * group_pid * out_group_dim)[None, :]\n\n    accum = tl.zeros((BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_OUT_FEAT),\n                     dtype=tl.float32)\n\n    for h in range(kernel_height):\n        for w in range(kernel_width):\n            for c in range(0, in_group_dim, BLOCK_SIZE_IN_FEAT):\n                input_feat_offset = c + tl.arange(0, BLOCK_SIZE_IN_FEAT)\n                input_height_offset = (h - padding_height +\n                                       stride_height * output_height_offset)\n                input_width_offset = (w - padding_width +\n                                      stride_width * output_width_offset)\n\n                curr_input_pointer = (input_pointer +\n                                     (input_in_feat_stride * input_feat_offset)[None, :] +\n                                     (input_height_stride * input_height_offset)[:, None] +\n                                     (input_width_stride * input_width_offset)[:, None])\n                curr_weight_pointer = (weight_pointer +\n                                      (weight_in_feat_stride * input_feat_offset)[:, None] +\n                                      (weight_height_stride * h) +\n                                      (weight_width_stride * w))\n\n                input_mask = ((batch_offset < batch_dim)[:, None] &\n                              (input_feat_offset < in_group_dim)[None, :] &\n                              (0 <= input_height_offset)[:, None] &\n                              (input_height_offset < in_height)[:, None] &\n                              (0 <= input_width_offset)[:, None] &\n                              (input_width_offset < in_width)[:, None])\n                weight_mask = ((input_feat_offset < in_group_dim)[:, None] &\n                               (output_feat_offset < out_group_dim)[None, :])\n\n                input_block = tl.load(curr_input_pointer, mask=input_mask)\n                weight_block = tl.load(curr_weight_pointer, mask=weight_mask)\n\n                if fp16:\n                    input_block = input_block.to(tl.float16)\n                    weight_block = weight_block.to(tl.float16)\n\n                accum += tl.dot(input_block, weight_block, allow_tf32=tf32)\n\n    output_pointer += ((output_batch_stride * batch_offset)[:, None] +\n                       (output_out_feat_stride * (group_pid * out_group_dim + output_feat_offset))[None, :] +\n                       (output_height_stride * output_height_offset)[:, None] +\n                       (output_width_stride * output_width_offset)[:, None])\n    output_mask = ((batch_offset < batch_dim)[:, None] &\n                   (output_feat_offset < out_group_dim)[None, :] &\n                   (output_height_offset < out_height)[:, None] &\n                   (output_width_offset < out_width)[:, None])\n\n    tl.store(output_pointer, accum, mask=output_mask)\n\n\ndef conv2d_forward(input_tensor, weight_tensor, kernel_height, kernel_width, stride_height, stride_width, padding_height, padding_width, groups, fp16=False, tf32=False):\n    # Extract dimensions\n    batch_dim, in_feat_dim, in_height, in_width = input_tensor.shape\n    out_feat_dim, _, _, _ = weight_tensor.shape\n    out_height = (in_height + 2 * padding_height - kernel_height) // stride_height + 1\n    out_width = (in_width + 2 * padding_width - kernel_width) // stride_width + 1\n\n    # Calculate strides\n    input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride = input_tensor.stride()\n    weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride = weight_tensor.stride()\n    output_tensor = torch.empty((batch_dim, out_feat_dim, out_height, out_width), device=input_tensor.device, dtype=torch.float32)\n    output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride = output_tensor.stride()\n\n    # Define block sizes\n    BLOCK_SIZE_BATCH_HEIGHT_WIDTH = 128\n    BLOCK_SIZE_IN_FEAT = 32\n    BLOCK_SIZE_OUT_FEAT = 32\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(batch_dim * out_height * out_width, BLOCK_SIZE_BATCH_HEIGHT_WIDTH),\n            triton.cdiv(out_feat_dim, BLOCK_SIZE_OUT_FEAT),\n            groups)\n    \n    conv2d_forward_kernel[grid](\n        input_tensor, weight_tensor, output_tensor,\n        batch_dim, in_feat_dim, in_height, in_width,\n        out_feat_dim, out_height, out_width,\n        input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride,\n        weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride,\n        kernel_height, kernel_width,\n        stride_height, stride_width,\n        padding_height, padding_width,\n        groups, fp16, tf32,\n        BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_IN_FEAT, BLOCK_SIZE_OUT_FEAT\n    )\n    \n    return output_tensor\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines a Triton-based function to determine if each element in a tensor is finite. The `isfinite_func_wrapper_rank_1` function is a Python wrapper that takes an input tensor and an output tensor. It calculates optimal tile sizes and warps for execution, then invokes the `isfinite_func_kernel_rank_1` kernel. The kernel iterates over the tensor, loads data using Triton block pointers, applies the `isfinite_func` operation, and writes results back. The kernel supports float32 and float64 types, utilizing Triton's library functions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef isfinite_func_kernel_rank_1(\n    input_ptr,  # *pointer* to the input tensor\n    output_ptr,  # *pointer* to the output tensor\n    n_elements,  # number of elements in the tensor\n    BLOCK_SIZE: tl.constexpr,  # block size\n):\n    # Compute the block index\n    pid = tl.program_id(axis=0)\n    # Compute the block start index\n    block_start = pid * BLOCK_SIZE\n    # Compute the block end index\n    block_end = block_start + BLOCK_SIZE\n    # Create a block pointer for the input and output\n    input_block_ptr = tl.make_block_ptr(\n        base=input_ptr,\n        shape=(n_elements,),\n        strides=(1,),\n        offsets=(block_start,),\n        block_shape=(BLOCK_SIZE,),\n        order=(0,)\n    )\n    output_block_ptr = tl.make_block_ptr(\n        base=output_ptr,\n        shape=(n_elements,),\n        strides=(1,),\n        offsets=(block_start,),\n        block_shape=(BLOCK_SIZE,),\n        order=(0,)\n    )\n    # Load the input block\n    input_block = tl.load(input_block_ptr, mask=block_start + tl.arange(0, BLOCK_SIZE) < n_elements)\n    # Apply the isfinite function\n    output_block = tl.isfinite(input_block)\n    # Store the output block\n    tl.store(output_block_ptr, output_block, mask=block_start + tl.arange(0, BLOCK_SIZE) < n_elements)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef isfinite_func_wrapper_rank_1(input_tensor: torch.Tensor, output_tensor: torch.Tensor):\n    # Ensure the input and output tensors are on the same device\n    assert input_tensor.device == output_tensor.device, \"Input and output tensors must be on the same device\"\n    assert input_tensor.dtype in [torch.float32, torch.float64], \"Input tensor must be of type float32 or float64\"\n    assert output_tensor.dtype == torch.bool, \"Output tensor must be of type bool\"\n    \n    # Get the number of elements in the tensor\n    n_elements = input_tensor.numel()\n    \n    # Determine the optimal block size\n    BLOCK_SIZE = 1024  # This is a common choice, but you can tune it for your specific hardware\n    \n    # Determine the number of blocks\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the kernel\n    isfinite_func_kernel_rank_1[(num_blocks,)](\n        input_tensor,  # *pointer* to the input tensor\n        output_tensor,  # *pointer* to the output tensor\n        n_elements,  # number of elements in the tensor\n        BLOCK_SIZE,  # block size\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    input_tensor = torch.randn(1024, device=\"cuda\", dtype=torch.float32)\n    # Create an output tensor of the same shape with boolean type\n    output_tensor = torch.empty_like(input_tensor, dtype=torch.bool, device=\"cuda\")\n    \n    # Call the wrapper function\n    isfinite_func_wrapper_rank_1(input_tensor, output_tensor)\n    \n    # Print the result\n    print(output_tensor)\n", "label": "import math\nfrom typing import Union\nimport torch\nimport triton\nfrom triton import language as tl\n\ntry:\n    from triton.language.extra.cuda.libdevice import isfinited as _isfinited\nexcept ImportError:\n    try:\n        from triton.language.math import isfinited as _isfinited\n    except ImportError:\n        from triton.language.libdevice import isfinited as _isfinited\n\ntry:\n    from triton.language.extra.cuda.libdevice import finitef as _finitef\nexcept ImportError:\n    try:\n        from triton.language.math import finitef as _finitef\n    except ImportError:\n        from triton.language.libdevice import finitef as _finitef\n\ndef heuristics_for_tile_size(max_tile_size, *sizes):\n    ndim = len(sizes)\n    tile_sizes = [0 for _ in range(ndim)]\n    for i in range(ndim):\n        size = sizes[ndim - 1 - i]\n        tile_size = min(max_tile_size, triton.next_power_of_2(size))\n        tile_sizes[ndim - 1 - i] = tile_size\n        max_tile_size = max(1, max_tile_size // tile_size)\n    return tuple(tile_sizes)\n\ndef heuristics_for_num_warps(tile_size):\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n    \n\nclass StridedBuffer:\n    \"\"\"A drop-in replacement of torch.Tensor that can be used in wrapper generated by\n    PointwiseDynamicFunction. It allows us to use a different shape, stride, data\n    pointer that that of the base tensor.\n\n    It is a kind of reinterpretation of the base tensor. We make this class since we\n    cannot get a Tensor view with negative strides via torch APIs, while we need this\n    to implement flip op.\n\n    Although generated code can accept torch.Tensor & StridedBuffer, but StridedBuffer\n    may not have all the methods as torch.Tensors do. We add some attributes & methods\n    with the same name as torch.Tensor, which are used in the generated code. But we\n    may not cover all the methods, add one if what you need is missing here.\n\n    And can also be used in triton kernels since it also has dtype & data_ptr().\n    \"\"\"\n\n    def __init__(\n        self, base: torch.Tensor, shape=None, strides=None, dtype=None, offset=0\n    ):\n        self._base = base\n        self.dtype = dtype or base.dtype\n        if offset == 0:\n            self._data_ptr = self._base.data_ptr()\n        else:\n            offset = self.dtype.itemsize * offset\n            self._data_ptr = self._base.data_ptr() + offset\n        self.shape = tuple(shape if shape is not None else self._base.shape)\n        self._strides = tuple(strides if strides is not None else self._base.stride())\n        self.device = self._base.device\n        self.ndim = len(self.shape)\n\n    def stride(self):\n        return self._strides\n\n    def size(self):\n        return self.shape\n\n    def element_size(self):\n        return self.dtype.itemsize\n\n    def numel(self):\n        return math.prod(self.shape)\n\n    def dim(self):\n        return self.ndim\n\n    def unwrap(self):\n        return self._base\n\n    def data_ptr(self):\n        return self._data_ptr\n\n\ndef isfinite_func_wrapper_rank_1(in0: Union[torch.Tensor, StridedBuffer], /, *, out0: Union[torch.Tensor, StridedBuffer]): \n    \"\"\"Generated wrapper function with Pointwise: StridedBuffer, StridedBuffer(a1!) -> StridedBuffer(a1!)\"\"\"\n    assert in0.shape == out0.shape, 'operand shapes mismatch'\n    # task partitioning\n    shape = out0.shape\n    num_tasks = out0.numel()\n    tile_sizes = heuristics_for_tile_size(512, *shape)\n    tile_size = math.prod(tile_sizes)\n    num_tiles = math.prod(triton.cdiv(size, tile_size) for size, tile_size in zip(shape, tile_sizes))\n    num_ctas = min(65536, num_tiles)\n    tiles_per_cta = triton.cdiv(num_tiles, num_ctas)\n    num_warps = heuristics_for_num_warps(tile_size)\n    one_tile_per_cta = tiles_per_cta==1\n    grid = (num_ctas, 1, 1)\n    # kernel launch\n    in0_strides = in0.stride()\n    in0_stride_order = (0,)\n    out0_strides = out0.stride()\n    out0_stride_order = (0,)\n    with torch.cuda._DeviceGuard(in0.device.index):\n        isfinite_func_kernel_rank_1[grid](\n            in0, out0,\n            in0_strides[0], # stride for in0\n            in0_stride_order[0], # stride order for in0\n            out0_strides[0], # stride for out0\n            out0_stride_order[0], # stride orderfor out0\n            shape[0], # task indexing space\n            num_tasks, # num tasks\n            tiles_per_cta=tiles_per_cta, # tiles_per_cta\n            tile_size0=tile_sizes[0],\n            one_tile_per_cta=one_tile_per_cta,\n            num_warps=num_warps,\n        )\n    return out0\n\n@triton.jit\ndef isfinite_func(x):\n    return _isfinited(x) if x.dtype.is_fp64() else _finitef(x.to(tl.float32))\n\n@triton.jit\ndef isfinite_func_kernel_rank_1(\n    in0_ptr: tl.tensor, # of tl.pointer_type\n    out0_ptr: tl.tensor, # of tl.pointer_type\n    in0_stride0: int, # strides for in0\n    in0_stride_order0: tl.constexpr, # stride order for in0\n    out0_stride0: int, # strides for out0\n    out0_stride_order0: tl.constexpr, # stride order for out0\n    s0: int, # task_space\n    num_tasks: int,\n    tiles_per_cta: int,\n    tile_size0: tl.constexpr,\n    one_tile_per_cta: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_tiles0 = tl.cdiv(s0, tile_size0)\n    if one_tile_per_cta: # monolitic kernel style\n        tile_id = pid\n        # pid multi index recontruction: we use c ordering, right axes changes fastest\n        tile_id0 = tile_id\n\n        # tile offsets\n        offset0 = tile_id0 * tile_size0\n        # loads\n        in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n        in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n        # compute\n        out0 = isfinite_func(in0)\n\n        # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n        out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n        tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n    else: # grid-stride-loop style kernel\n        num_ctas = tl.num_programs(0)\n        for j in range(0, tiles_per_cta):\n            tile_id = pid + j * num_ctas\n            # pid multi index recontruction: we use c ordering, right axes changes fastest\n            tile_id0 = tile_id\n\n            # tile offsets\n            offset0 = tile_id0 * tile_size0\n            # loads\n            in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n            in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n            # compute\n            out0 = isfinite_func(in0)\n\n            # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n            out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n            tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton operator code consists of a kernel function, `triton_f4_to_bf16_kernel`, and a helper function, `_fp4_packed_to_bf16`. The main purpose of this code is to convert packed FP4 values to BF16 values efficiently on CUDA devices using Triton.\n\n        1. `_fp4_packed_to_bf16`: This function unpacks the FP4 values and converts them to BF16 format. It handles special cases like zero and the denormal value (0.5). It extracts the sign, exponent, and mantissa of the FP4 representation and maps them to a format compatible with BF16.\n\n        2. `triton_f4_to_bf16_kernel`: This is the main Triton kernel that utilizes the `_fp4_packed_to_bf16` helper function. It takes as input pointers to the packed FP4 data (`x_ptr`) and outputs BF16 data (`output_ptr`). The kernel divides the data into blocks and processes each block in parallel.\n\n        The input to the Triton kernel includes various parameters required for the conversion process, such as bit masks and biases. It operates block-wise to take advantage of Triton's parallelism on CUDA GPUs. The number of elements to process and block size are calculated based on the input tensor size.\n\n        The `triton_f4_to_bf16` function is a Python wrapper around the Triton kernel. It ensures that the input tensor is properly shaped and allocated on the CUDA device and computes the grid size for launching the kernel. Finally, it calls the kernel with the necessary parameters to perform the conversion.\n\n        Important Constants:\n        - `SIGN_MASK_F4`, `MANTISSA_MASK_F4`: Masks to extract the sign and mantissa from FP4 values.\n        - `ZERO_BITS_F32`, `ZERO_POINT_FIVE_BITS_F32`: Representations of zero and 0.5 in FP32 format.\n        - `F4_E2M1_EXP_BIAS`, `F32_EXP_BIAS`: Exponent biases for FP4 and FP32.\n\n        Overall, this code efficiently converts packed FP4 data to BF16 on GPUs using the Triton language by leveraging the parallel computing capabilities of CUDA.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nSIGN_MASK_F4 = 0x80\nMANTISSA_MASK_F4 = 0x7F\nZERO_BITS_F32 = 0x00000000\nZERO_POINT_FIVE_BITS_F32 = 0x3F000000\nF4_E2M1_EXP_BIAS = 7\nF32_EXP_BIAS = 127\n\n# Helper function to convert FP4 to BF16\n@triton.jit\ndef _fp4_packed_to_bf16(x: tl.int8) -> tl.float16:\n    sign = (x & SIGN_MASK_F4) >> 7\n    mantissa = x & MANTISSA_MASK_F4\n    exponent = (mantissa >> 3) - F4_E2M1_EXP_BIAS + F32_EXP_BIAS\n\n    if mantissa == 0:\n        return tl.float16(0.0) if sign == 0 else tl.float16(-0.0)\n    elif mantissa == 0x40:\n        return tl.float16(0.5) if sign == 0 else tl.float16(-0.5)\n    else:\n        mantissa = (mantissa & 0x07) << 10\n        return tl.float16(tl.make_int32((sign << 15) | (exponent << 10) | mantissa))\n\n# Triton kernel for converting FP4 to BF16\n@triton.jit\ndef triton_f4_to_bf16_kernel(\n    x_ptr: tl.pointer_type(tl.int8, 1),\n    output_ptr: tl.pointer_type(tl.float16, 1),\n    n_elements: tl.int32,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x_block = tl.load(x_ptr + offsets, mask=mask)\n    output_block = tl.zeros((BLOCK_SIZE,), dtype=tl.float16)\n    for i in range(BLOCK_SIZE):\n        if mask[i]:\n            output_block[i] = _fp4_packed_to_bf16(x_block[i])\n\n    tl.store(output_ptr + offsets, output_block, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef triton_f4_to_bf16(x: torch.Tensor) -> torch.Tensor:\n    assert x.dtype == torch.int8, \"Input tensor must be of type int8\"\n    assert x.is_cuda, \"Input tensor must be on a CUDA device\"\n\n    n_elements = x.numel()\n    output = torch.empty(n_elements, dtype=torch.bfloat16, device=x.device)\n\n    # Compute grid size\n    BLOCK_SIZE = 128\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    triton_f4_to_bf16_kernel[grid_size, BLOCK_SIZE](\n        x, output, n_elements, BLOCK_SIZE\n    )\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fp4_packed_to_bf16(\n    x_packed,\n    sign_mask_f4,\n    mantissa_mask_f4,\n    mbits_f4_e2m1,\n    ebits_f4_e2m1,\n    f4_e2m1_exp_bias,\n    mbits_f32,\n    ebits_f32,\n    f32_exp_bias,\n    zero_bits_f32,\n    zero_point_five_bits_f32,\n):\n    \"\"\"\n    Input: a tensor of packed fp4 values\n    Output: a tensor of bfloat16 values\n    \"\"\"\n\n    # low-bits: original location 0:3\n    # high-bits: original location 4:7\n    x_low_bits = x_packed >> 4\n    x_high_bits = x_packed & 0xF\n    x = tl.interleave(x_low_bits, x_high_bits)\n\n    # cast logic below\n    # output = x_unpacked.to(tl.float32)\n\n    # save the sign\n    sign_f4 = x & sign_mask_f4\n\n    # set everything to positive, will add sign back at the end\n    x_pos = x ^ sign_f4\n\n    # Special case zero\n    zero_mask = x_pos == 0\n\n    # There is only one denormal value in fp4: s001, which is 0.5 in f32\n    # Special case it.\n    # TODO(later): will it be faster to repeat this for all 8 positive\n    # values instead of the bit manipulations?\n    denormal_mask = x_pos == 1\n\n    # calculate the new exponent and shift it to bits 2:9 of the result\n    exp_biased_f4 = x_pos >> mbits_f4_e2m1\n    exp_biased_f32 = exp_biased_f4 - f4_e2m1_exp_bias + f32_exp_bias\n    exp_biased_f32 = exp_biased_f32.to(tl.int32) << mbits_f32\n\n    # shift the mantissa to bits 10:32 of the result\n    mantissa_f4 = x_pos & mantissa_mask_f4\n    mantissa_f32 = mantissa_f4.to(tl.int32) << (mbits_f32 - mbits_f4_e2m1)\n    output = mantissa_f32\n\n    # combine the pieces\n    result = exp_biased_f32 | mantissa_f32\n    # result[zero_mask] = ZERO_BITS_F32\n    result = tl.where(zero_mask, zero_bits_f32, result)\n    # result[denormal_mask] = ZERO_POINT_FIVE_BITS_F32\n    result = tl.where(denormal_mask, zero_point_five_bits_f32, result)\n\n    # add sign back\n    sign_f32 = sign_f4.to(tl.int32) << (\n        mbits_f32 - mbits_f4_e2m1 + ebits_f32 - ebits_f4_e2m1\n    )\n    result = result | sign_f32\n\n    # The bit shifting above is for float32, so for now we\n    # bitcast to float32 and then regular cast to bfloat16\n    # TODO(later): it should be pretty easy to cast directly to bf16, just\n    # need to adjust the mbits/ebits/special values. Perf impact is likely\n    # to be small as we would not be chaning memory access patterns.\n    output = result.to(tl.float32, bitcast=True)\n    output = output.to(tl.bfloat16)\n    return output\n\n@triton.jit\ndef triton_f4_to_bf16_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements_in,\n    sign_mask_f4: tl.constexpr,\n    mantissa_mask_f4: tl.constexpr,\n    mbits_f4_e2m1: tl.constexpr,\n    ebits_f4_e2m1: tl.constexpr,\n    f4_e2m1_exp_bias: tl.constexpr,\n    mbits_f32: tl.constexpr,\n    ebits_f32: tl.constexpr,\n    f32_exp_bias: tl.constexpr,\n    zero_bits_f32: tl.constexpr,\n    zero_point_five_bits_f32: tl.constexpr,\n    BLOCK_SIZE_IN: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n\n    mask_in = offsets_in < n_elements_in\n\n    # packed uint8\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(\n        x_packed,\n        sign_mask_f4,\n        mantissa_mask_f4,\n        mbits_f4_e2m1,\n        ebits_f4_e2m1,\n        f4_e2m1_exp_bias,\n        mbits_f32,\n        ebits_f32,\n        f32_exp_bias,\n        zero_bits_f32,\n        zero_point_five_bits_f32,\n    )\n\n    # set up output offsets\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)\n\nEBITS_F32, MBITS_F32 = 8, 23\nEBITS_F4_E2M1, MBITS_F4_E2M1 = 2, 1\nEBITS_F6_E2M3, MBITS_F6_E2M3 = 2, 3\nEBITS_F6_E3M2, MBITS_F6_E3M2 = 3, 2\n\nSIGN_MASK_F4 = 0x8  # 1000\nMANTISSA_MASK_F4 = 0x1  # 0001\n\nZERO_BITS_F32 = 0x0\nZERO_POINT_FIVE_BITS_F32 = 0x3F000000\nF4_E2M1_EXP_BIAS = 1\nF32_EXP_BIAS = 127\n\ndef triton_f4_to_bf16(x: torch.Tensor):\n    \"\"\"\n    Input: a tensor of packed fp4 values\n    Output: a tensor of bfloat16 values\n\n    Note: this function is only used in testing, so we can test\n      the numerical correctness of the cast without the scaling.\n    \"\"\"\n    new_shape = (*x.shape[:-1], x.shape[-1] * 2)\n    output = torch.empty(*new_shape, device=x.device, dtype=torch.bfloat16)\n    assert x.is_contiguous()\n    assert x.is_cuda and output.is_cuda\n    n_elements_in = x.numel()\n    grid = lambda meta: (  # noqa: E731\n        triton.cdiv(n_elements_in, meta[\"BLOCK_SIZE_IN\"]),\n    )  # noqa: E731,E501\n    triton_f4_to_bf16_kernel[grid](\n        x,\n        output,\n        n_elements_in,\n        sign_mask_f4=SIGN_MASK_F4,\n        mantissa_mask_f4=MANTISSA_MASK_F4,\n        mbits_f4_e2m1=MBITS_F4_E2M1,\n        ebits_f4_e2m1=EBITS_F4_E2M1,\n        f4_e2m1_exp_bias=F4_E2M1_EXP_BIAS,\n        mbits_f32=MBITS_F32,\n        ebits_f32=EBITS_F32,\n        f32_exp_bias=F32_EXP_BIAS,\n        zero_bits_f32=ZERO_BITS_F32,\n        zero_point_five_bits_f32=ZERO_POINT_FIVE_BITS_F32,\n        BLOCK_SIZE_IN=512,\n    )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `_triton_rope` performs a rotation-based positional encoding transformation on query (`q_ptr`) and key (`k_ptr`) matrices using cosine and sine values. It is part of a mechanism for rotational position embeddings, commonly used in transformer architectures. The operation is configurable for forward or backward passes through the `BACKWARD_PASS` parameter. The input matrices are divided into \"tiles\" that are rotated by applying trigonometric transformations. The `rope_backward` function sets up this kernel for a backward pass, transposing inputs and handling memory strides. It returns the transformed query and key gradients in their original shapes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr, k_ptr, cos_ptr, sin_ptr, q_out_ptr, k_out_ptr,\n    q_batch_stride, q_head_stride, q_seq_stride, q_feat_stride,\n    k_batch_stride, k_head_stride, k_seq_stride, k_feat_stride,\n    cos_seq_stride, cos_feat_stride,\n    sin_seq_stride, sin_feat_stride,\n    q_out_batch_stride, q_out_head_stride, q_out_seq_stride, q_out_feat_stride,\n    k_out_batch_stride, k_out_head_stride, k_out_seq_stride, k_out_feat_stride,\n    N_CTX, HEAD_DIM, BACKWARD_PASS: tl.constexpr\n):\n    # Compute the block ID and thread ID\n    pid = tl.program_id(0)\n    num_blocks = tl.num_programs(0)\n    block_size = N_CTX // num_blocks\n\n    # Compute the start and end indices for the current block\n    start = pid * block_size\n    end = (pid + 1) * block_size if pid < num_blocks - 1 else N_CTX\n\n    # Compute the batch, head, and feature indices\n    batch_id = tl.program_id(1)\n    head_id = tl.program_id(2)\n\n    # Load the query and key matrices\n    q = tl.load(q_ptr + batch_id * q_batch_stride + head_id * q_head_stride + tl.arange(0, block_size) * q_seq_stride + tl.arange(0, HEAD_DIM) * q_feat_stride, mask=start + tl.arange(0, block_size) < N_CTX, other=0.0)\n    k = tl.load(k_ptr + batch_id * k_batch_stride + head_id * k_head_stride + tl.arange(0, block_size) * k_seq_stride + tl.arange(0, HEAD_DIM) * k_feat_stride, mask=start + tl.arange(0, block_size) < N_CTX, other=0.0)\n\n    # Load the cosine and sine values\n    cos = tl.load(cos_ptr + tl.arange(0, block_size) * cos_seq_stride + tl.arange(0, HEAD_DIM) * cos_feat_stride, mask=start + tl.arange(0, block_size) < N_CTX, other=0.0)\n    sin = tl.load(sin_ptr + tl.arange(0, block_size) * sin_seq_stride + tl.arange(0, HEAD_DIM) * sin_feat_stride, mask=start + tl.arange(0, block_size) < N_CTX, other=0.0)\n\n    # Apply the RoPE transformation\n    q_rot = q * cos - tl.flip(q, 1) * sin\n    k_rot = k * cos - tl.flip(k, 1) * sin\n\n    # Store the results\n    tl.store(q_out_ptr + batch_id * q_out_batch_stride + head_id * q_out_head_stride + tl.arange(0, block_size) * q_out_seq_stride + tl.arange(0, HEAD_DIM) * q_out_feat_stride, q_rot, mask=start + tl.arange(0, block_size) < N_CTX)\n    tl.store(k_out_ptr + batch_id * k_out_batch_stride + head_id * k_out_head_stride + tl.arange(0, block_size) * k_out_seq_stride + tl.arange(0, HEAD_DIM) * k_out_feat_stride, k_rot, mask=start + tl.arange(0, block_size) < N_CTX)\n\n    # If this is a backward pass, transpose the inputs and outputs\n    if BACKWARD_PASS:\n        q_rot = tl.transpose(q_rot)\n        k_rot = tl.transpose(k_rot)\n\n        tl.store(q_out_ptr + batch_id * q_out_batch_stride + head_id * q_out_head_stride + tl.arange(0, block_size) * q_out_seq_stride + tl.arange(0, HEAD_DIM) * q_out_feat_stride, q_rot, mask=start + tl.arange(0, block_SIZE) < N_CTX)\n        tl.store(k_out_ptr + batch_id * k_out_batch_stride + head_id * k_out_head_stride + tl.arange(0, block_size) * k_out_seq_stride + tl.arange(0, HEAD_DIM) * k_out_feat_stride, k_rot, mask=start + tl.arange(0, block_size) < N_CTX)\n\nimport torch\n\ndef rope_backward(q_grad, k_grad, cos, sin, q_shape, k_shape, N_CTX, HEAD_DIM):\n    # Transpose the input gradients\n    q_grad = q_grad.transpose(1, 2).contiguous()\n    k_grad = k_grad.transpose(1, 2).contiguous()\n\n    # Allocate output tensors\n    q_out_grad = torch.empty_like(q_grad)\n    k_out_grad = torch.empty_like(k_grad)\n\n    # Define the grid and block dimensions\n    grid = (N_CTX // 32, q_shape[0], q_shape[1])\n    block = (32, HEAD_DIM)\n\n    # Launch the kernel\n    _triton_rope[grid, block](\n        q_grad, k_grad, cos, sin, q_out_grad, k_out_grad,\n        q_grad.stride(0), q_grad.stride(1), q_grad.stride(2), q_grad.stride(3),\n        k_grad.stride(0), k_grad.stride(1), k_grad.stride(2), k_grad.stride(3),\n        cos.stride(0), cos.stride(1),\n        sin.stride(0), sin.stride(1),\n        q_out_grad.stride(0), q_out_grad.stride(1), q_out_grad.stride(2), q_out_grad.stride(3),\n        k_out_grad.stride(0), k_out_grad.stride(1), k_out_grad.stride(2), k_out_grad.stride(3),\n        N_CTX, HEAD_DIM, True\n    )\n\n    # Transpose the output gradients back to the original shape\n    q_out_grad = q_out_grad.transpose(1, 2).contiguous()\n    k_out_grad = k_out_grad.transpose(1, 2).contiguous()\n\n    return q_out_grad, k_out_grad\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr,\n    q_row_stride,\n    k_ptr,\n    k_row_stride,\n    cos,\n    cos_row_stride,\n    sin,\n    sin_row_stride,\n    sl,\n    bs: tl.constexpr,\n    n_qh: tl.constexpr,\n    n_kh: tl.constexpr,\n    hd: tl.constexpr,\n    pad_n_qh: tl.constexpr,\n    pad_n_kh: tl.constexpr,\n    pad_hd: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr = False,\n):\n    pid = tl.program_id(0)\n\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n\n    cos_row_idx = pid % (sl)\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0).to(sin_row.dtype)\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0).to(sin_row.dtype)\n\n    second_half_q_offsets = first_half_q_offsets + (hd // 2)\n    second_half_k_offsets = first_half_k_offsets + (hd // 2)\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask, other=0).to(sin_row.dtype)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask, other=0).to(sin_row.dtype)\n\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n\ndef rope_backward(dq, dk, cos, sin):\n    dq = dq.transpose(1, 2)\n    dk = dk.transpose(1, 2)\n\n    batch_size, seq_len, n_q_head, head_dim = dq.shape\n    n_kv_head = dk.shape[2]\n    pad_hd = triton.next_power_of_2(head_dim)\n    pad_n_q_head = triton.next_power_of_2(n_q_head)\n    pad_n_kv_head = triton.next_power_of_2(n_kv_head)\n    BLOCK_SIZE = max(pad_n_q_head, pad_n_kv_head)\n\n    n_row = batch_size * seq_len\n\n    dq = dq.contiguous()\n    dk = dk.contiguous()\n\n    _triton_rope[(n_row,)](\n        dq,\n        dq.stride(1),\n        dk,\n        dk.stride(1),\n        cos,\n        cos.stride(-2),\n        sin,\n        sin.stride(-2),\n        seq_len,\n        batch_size,\n        n_q_head,\n        n_kv_head,\n        head_dim,\n        pad_n_q_head,\n        pad_n_kv_head,\n        pad_hd,\n        BLOCK_SIZE=BLOCK_SIZE,\n        BACKWARD_PASS=True,\n    )\n    return dq.transpose(1, 2), dk.transpose(1, 2)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `_sgmv_expand_slice_kernel` and a wrapper function `_sgmv_expand_slice`. \n            The kernel performs a batched sparse matrix-vector multiplication (SGMV) with some additional features. \n            It uses input tensors and LoRA weights to compute an output tensor. \n            The kernel uses Triton's parallelism capabilities by launching a grid of programs, each processing a block of the input tensor.\n            The `_sgmv_expand_slice` function prepares the data, determines block sizes, and launches the Triton kernel with the appropriate parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _sgmv_expand_slice_kernel(\n    X_ptr,  # Pointer to the input tensor (batched vectors)\n    W_ptr,  # Pointer to the sparse matrix (CSR format)\n    V_ptr,  # Pointer to the LoRA weights\n    Y_ptr,  # Pointer to the output tensor\n    stride_xb,  # Stride of X in the batch dimension\n    stride_xc,  # Stride of X in the feature dimension\n    stride_wb,  # Stride of W in the batch dimension\n    stride_wi,  # Stride of W in the row index dimension\n    stride_wj,  # Stride of W in the column index dimension\n    stride_wv,  # Stride of W in the value dimension\n    stride_vb,  # Stride of V in the batch dimension\n    stride_vc,  # Stride of V in the feature dimension\n    stride_yb,  # Stride of Y in the batch dimension\n    stride_yc,  # Stride of Y in the feature dimension\n    B,  # Batch size\n    M,  # Number of rows in the sparse matrix\n    N,  # Number of columns in the sparse matrix\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelism\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    batch_id = pid // (M // BLOCK_SIZE)  # Determine the batch ID\n    row_id = (pid % (M // BLOCK_SIZE)) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)  # Determine the row ID\n\n    # Load the input vector for the current batch\n    X = tl.load(X_ptr + batch_id * stride_xb + row_id * stride_xc, mask=row_id < M, other=0.0)\n\n    # Initialize the output vector\n    Y = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Load the sparse matrix and LoRA weights\n    for i in range(0, N, BLOCK_SIZE):\n        col_id = i + tl.arange(0, BLOCK_SIZE)\n        mask = (row_id < M) & (col_id < N)\n\n        # Load the values from the sparse matrix\n        W_values = tl.load(W_ptr + batch_id * stride_wb + row_id * stride_wi + col_id * stride_wj + col_id * stride_wv, mask=mask, other=0.0)\n\n        # Load the LoRA weights\n        V_values = tl.load(V_ptr + batch_id * stride_vb + col_id * stride_vc, mask=mask, other=0.0)\n\n        # Perform the sparse matrix-vector multiplication\n        Y += X * W_values * V_values\n\n    # Store the result in the output tensor\n    tl.store(Y_ptr + batch_id * stride_yb + row_id * stride_yc, Y, mask=row_id < M)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _sgmv_expand_slice(X, W, V, Y, block_size=128):\n    # Get the dimensions of the input tensors\n    B, M = X.shape\n    N = V.shape[1]\n\n    # Determine the grid and block sizes\n    grid = (B * (M // block_size),)\n\n    # Launch the Triton kernel\n    _sgmv_expand_slice_kernel[grid](\n        X,  # Input tensor\n        W,  # Sparse matrix (CSR format)\n        V,  # LoRA weights\n        Y,  # Output tensor\n        X.stride(0),  # Stride of X in the batch dimension\n        X.stride(1),  # Stride of X in the feature dimension\n        W.stride(0),  # Stride of W in the batch dimension\n        W.stride(1),  # Stride of W in the row index dimension\n        W.stride(2),  # Stride of W in the column index dimension\n        W.stride(3),  # Stride of W in the value dimension\n        V.stride(0),  # Stride of V in the batch dimension\n        V.stride(1),  # Stride of V in the feature dimension\n        Y.stride(0),  # Stride of Y in the batch dimension\n        Y.stride(1),  # Stride of Y in the feature dimension\n        B,  # Batch size\n        M,  # Number of rows in the sparse matrix\n        N,  # Number of columns in the sparse matrix\n        BLOCK_SIZE=block_size,  # Block size for parallelism\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _sgmv_expand_slice_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    b_seq_start_loc,\n    seq_lens,\n    lora_indices,\n    xm_stride,\n    xk_stride,  # 1\n    l0_stride,  # hidden_size*max_rank\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    slice_offset,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ADD_INPUTS: tl.constexpr,\n    CAST_TYPE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    cta_n_num = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // cta_n_num\n    pid_n = pid % cta_n_num\n    M = tl.load(seq_lens + cur_batch)\n    if pid_m * BLOCK_M > M:\n        return\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n    cur_seq_start = tl.load(b_seq_start_loc + cur_batch)\n    offset_m = tl.arange(0, BLOCK_M) + pid_m * BLOCK_M\n    offset_n = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N\n    offset_k = tl.arange(0, BLOCK_K)\n    ram = tl.max_contiguous(tl.multiple_of(offset_m % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(offset_n % N, BLOCK_N), BLOCK_N)\n\n    a_ptr = (input_ptr + cur_seq_start * xm_stride + ram[:, None] * xm_stride +\n             offset_k[None, :] * xk_stride, )\n    b_ptr = (lora_ptr + l0_stride * lora_index +\n             offset_k[:, None] * lora_n_stride + rbn[None, :] * lora_k_stride)\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(tl.cdiv(K, BLOCK_K)):\n        if EVEN_K:\n            tiled_a = tl.load(a_ptr)\n            tiled_b = tl.load(b_ptr)\n        else:\n            tiled_a = tl.load(a_ptr,\n                              mask=offset_k[None, :] < K - k * BLOCK_K,\n                              other=0)\n            tiled_b = tl.load(b_ptr,\n                              mask=offset_k[:, None] < K - k * BLOCK_K,\n                              other=0)\n        if CAST_TYPE:\n            tiled_a = tiled_a.to(lora_ptr.dtype.element_ty)\n        accumulator += tl.dot(\n            tiled_a,\n            tiled_b,\n        )\n        a_ptr += BLOCK_K * xk_stride\n        b_ptr += BLOCK_K * lora_n_stride\n    tiled_c = accumulator.to(lora_ptr.dtype.element_ty)\n    offset_cm = cur_seq_start + tl.arange(0, BLOCK_M) + pid_m * BLOCK_M\n    offset_cn = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N + slice_offset\n    c_ptr = (out_ptr + offset_cm[:, None] * cm_stride +\n             offset_cn[None, :] * cn_stride)\n    M = tl.load(seq_lens + cur_batch)\n    c_mask = (offset_cm[:, None] < (cur_seq_start + M)) & (offset_cn[None, :] <\n                                                           (slice_offset + N))\n    if ADD_INPUTS:\n        tiled_out = tl.load(c_ptr, mask=c_mask)\n        tiled_c += tiled_out\n    tl.store(c_ptr, tiled_c, mask=c_mask)\n\n\n@torch.inference_mode()\ndef _sgmv_expand_slice(\n    inputs: torch.Tensor,\n    lora_b_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    b_seq_start_loc: torch.Tensor,\n    seq_len_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    batches: int,\n    max_seq_length: int,\n    token_nums: int,\n    slice_offset: int,\n    slice_size: int,\n    add_inputs: bool = False,\n) -> None:\n\n    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]\n    assert lora_b_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(0) == token_nums\n    assert inputs.size(1) == lora_b_weights.size(-1)\n    assert b_seq_start_loc.size(0) == batches\n    assert lora_indices_tensor.size(0) == batches\n    assert slice_size == lora_b_weights.size(-2)\n    assert inputs.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    if lora_b_weights.ndim == 4:  # shape:(lora_num,1,size,rank)\n        assert lora_b_weights.size(1) == 1\n        lora_b_weights = lora_b_weights.squeeze(dim=1)\n    else:\n        assert lora_b_weights.ndim == 3  # shape:(lora_num,size,rank)\n\n    assert lora_b_weights.is_contiguous()\n\n    N, K = lora_b_weights.shape[-2:]\n\n    BLOCK_M = 32\n    BLOCK_N = 32\n    BLOCK_K = 16\n    EVEN_K = K % BLOCK_K == 0\n    ADD_INPUTS = add_inputs\n    CAST_TYPE = False\n    if inputs.dtype == torch.float32 and lora_b_weights.dtype in [\n            torch.float16,\n            torch.bfloat16,\n    ]:\n        CAST_TYPE = True\n    grid = (\n        triton.cdiv(max_seq_length, BLOCK_M) * triton.cdiv(N, BLOCK_N),\n        batches,\n    )\n    _sgmv_expand_slice_kernel[grid](\n        inputs,\n        lora_b_weights,\n        output_tensor,\n        N,\n        K,\n        b_seq_start_loc,\n        seq_len_tensor,\n        lora_indices_tensor,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_b_weights.stride(0),\n        lora_b_weights.stride(1),\n        lora_b_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        slice_offset,\n        BLOCK_M,\n        BLOCK_N,\n        BLOCK_K,\n        EVEN_K,\n        ADD_INPUTS,\n        CAST_TYPE,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton function embedding_kernel is designed to efficiently compute embeddings by loading values from the embedding weight matrix based on input token IDs. It takes input arguments including weight, input_ids, and various dimensions and constants to control the computation, and stores the resulting vectors in the 'out' tensor. The kernel iterates over a sequence of input IDs, computes valid token masks, and loads relevant vectors from the embedding weights, storing them in the output tensor based on defined strides. The embedding function orchestrates the kernel launch, defining grid dimensions and constants based on input tensor shapes and characteristics.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight_ptr,  # Pointer to the embedding weight matrix\n    input_ids_ptr,  # Pointer to the input token IDs\n    out_ptr,  # Pointer to the output tensor\n    weight_stride,  # Stride of the weight matrix\n    input_ids_stride,  # Stride of the input IDs\n    out_stride,  # Stride of the output tensor\n    num_tokens,  # Number of tokens in the input\n    embedding_dim,  # Dimension of the embedding vectors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallelism\n):\n    # Compute the block ID\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Iterate over the input tokens\n    for i in range(block_start, min(block_start + BLOCK_SIZE, num_tokens)):\n        # Load the token ID\n        token_id = tl.load(input_ids_ptr + i * input_ids_stride)\n\n        # Compute the output index\n        out_index = i * out_stride\n\n        # Load the embedding vector\n        for j in range(embedding_dim):\n            weight_index = token_id * weight_stride + j\n            out_index_j = out_index + j\n            tl.store(out_ptr + out_index_j, tl.load(weight_ptr + weight_index))\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef embedding(weight, input_ids, out):\n    # Extract dimensions\n    num_tokens = input_ids.shape[0]\n    embedding_dim = weight.shape[1]\n\n    # Define grid and block dimensions\n    BLOCK_SIZE = 128\n    grid = (triton.cdiv(num_tokens, BLOCK_SIZE),)\n\n    # Define strides\n    weight_stride = weight.stride(1)\n    input_ids_stride = input_ids.stride(0)\n    out_stride = out.stride(1)\n\n    # Launch the kernel\n    embedding_kernel[grid](\n        weight.data_ptr(),  # Pointer to the embedding weight matrix\n        input_ids.data_ptr(),  # Pointer to the input token IDs\n        out.data_ptr(),  # Pointer to the output tensor\n        weight_stride,  # Stride of the weight matrix\n        input_ids_stride,  # Stride of the input IDs\n        out_stride,  # Stride of the output tensor\n        num_tokens,  # Number of tokens in the input\n        embedding_dim,  # Dimension of the embedding vectors\n        BLOCK_SIZE  # Block size for parallelism\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    num_tokens = 1024\n    embedding_dim = 512\n    weight = torch.randn((10000, embedding_dim), device='cuda')\n    input_ids = torch.randint(0, 10000, (num_tokens,), device='cuda')\n    out = torch.empty((num_tokens, embedding_dim), device='cuda')\n\n    # Compute embeddings\n    embedding(weight, input_ids, out)\n\n    # Print the first few embeddings\n    print(out[:5])\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    start_n = tl.program_id(0) * BLOCK_N\n\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq + offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :], vecs, mask=store_mask)\n\n@torch.no_grad()\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    BLOCK_N = 64\n    BLOCK_NN = 1\n    BLOCK_DMODEL = triton.next_power_of_2(weight.shape[1])\n    n_ctx = input_ids.shape[0]\n\n    grid = (triton.cdiv(n_ctx, BLOCK_N), 1, 1)\n\n    embedding_kernel[grid](\n        weight,\n        input_ids,\n        out,\n        vob_start_id,\n        vob_end_id,\n        weight.stride(0),\n        out.stride(0),\n        n_ctx=n_ctx,\n        hiden_size=weight.shape[1],\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK_N,\n        BLOCK_NN=BLOCK_NN,\n        num_warps=1,\n        num_stages=1,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel `uniform_kernel` generates uniform random numbers within a specified range `[from_, to)`. It uses a Philox pseudo-random number generator to create random numbers efficiently on GPU. The input arguments are `out_ptr` (output pointer), `N` (number of elements), `philox_seed` and `philox_offset` (for random number generation), `from_` and `to` (defining the range), and `BLOCK` (block size for execution). The kernel computes random numbers in batches of four and writes them to memory in a loop, ensuring proper range conversion and memory alignment.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef uniform_kernel(out_ptr, N, philox_seed, philox_offset, from_, to, BLOCK: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK\n    rng = tl.rand.philox(philox_seed, philox_offset + block_start)\n\n    for i in range(block_start, min(block_start + BLOCK, N)):\n        # Generate 4 random numbers at once\n        rand_nums = rng.next4()\n        # Convert to float32 and scale to [0, 1)\n        rand_nums = tl.cast(rand_nums, tl.float32) / 4294967296.0\n        # Scale to [from_, to)\n        rand_nums = from_ + rand_nums * (to - from_)\n        # Write to memory\n        out_ptr[i] = rand_nums[0]\n        if i + 1 < N:\n            out_ptr[i + 1] = rand_nums[1]\n        if i + 2 < N:\n            out_ptr[i + 2] = rand_nums[2]\n        if i + 3 < N:\n            out_ptr[i + 3] = rand_nums[3]\n\nimport triton\nimport triton.language as tl\nimport numpy as np\nimport torch\n\ndef generate_uniform_random_numbers(N, from_, to, device='cuda'):\n    # Allocate output tensor\n    out = torch.empty(N, dtype=torch.float32, device=device)\n    \n    # Define block size\n    BLOCK = 1024\n    \n    # Compute number of blocks\n    num_blocks = (N + BLOCK - 1) // BLOCK\n    \n    # Define Philox seed and offset\n    philox_seed = np.array([0, 0], dtype=np.uint64)\n    philox_offset = np.array([0, 0], dtype=np.uint64)\n    \n    # Launch the kernel\n    uniform_kernel[(num_blocks,)](out, N, philox_seed, philox_offset, from_, to, BLOCK)\n    \n    return out\n\n# Example usage\nN = 10000\nfrom_ = 0.0\nto = 1.0\nrandom_numbers = generate_uniform_random_numbers(N, from_, to)\nprint(random_numbers)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\nimport functools\nimport operator\nfrom typing import Tuple\n\nShape = Tuple[int]\n\n\ntry:\n    uint_to_uniform_float = tl.uint_to_uniform_float\nexcept AttributeError:\n    # Copied from triton.language package for compatibility\n    @triton.jit\n    def uint_to_uniform_float(x):\n        \"\"\"\n        Numerically stable function to convert a random uint into a random float uniformly sampled in [0, 1).\n        \"\"\"\n        # TODO: fix frontend issues and cleanup\n        # conditions can be simplified\n        # scale is ((2**23 - 1) / 2**23) * 2**(N_BITS - 1)\n        if tl.constexpr(x.dtype == tl.uint32) or tl.constexpr(x.dtype == tl.int32):\n            # maximum value such that `MAX_INT * scale < 1.0` (with float rounding)\n            x = x.to(tl.int32, bitcast=True)\n            scale = 4.6566127342e-10\n        else:\n            tl.static_assert(\n                tl.constexpr(x.dtype == tl.uint64) or tl.constexpr(x.dtype == tl.int64)\n            )\n            x = x.to(tl.int64, bitcast=True)\n            scale = 1.0842020432385337e-19\n        x = tl.where(x < 0, -x - 1, x)\n        return x * scale\n\n\ndef philox_cuda_seed_offset(increment, device=None):\n    device = device or torch.cuda.current_device()\n    gen = torch.cuda.default_generators[device]\n    state_copy = gen.get_state()\n    c0, c1 = state_copy.view(torch.int64)\n    seed, offset = int(c0), int(c1)\n    increment = (increment + 3) // 4 * 4\n    c1 += increment\n    # get_state returns a new tensor, so it needs set_state to update the actual generator state.\n    gen.set_state(state_copy)\n    return seed, offset\n\n\ndef heur_block(args):\n    if args[\"N\"] <= 512:\n        return 512\n    else:\n        return 1024\n\n\ndef heur_num_warps(args):\n    if args[\"N\"] <= 512:\n        return 4\n    elif args[\"N\"] <= 1024:\n        return 8\n    else:\n        return 16\n\n\n@triton.heuristics(\n    {\n        \"BLOCK\": heur_block,\n        \"num_warps\": heur_num_warps,\n    }\n)\n@triton.jit(do_not_specialize=[\"philox_seed\", \"philox_offset\"])\ndef uniform_kernel(\n    out_ptr,\n    N,\n    philox_seed,\n    philox_offset,\n    from_,\n    to,\n    BLOCK: tl.constexpr,\n):\n    philox_seed = philox_seed.to(tl.int64)\n    philox_offset = philox_offset.to(tl.int64)\n    c0 = (philox_offset & 0xFFFFFFFF).to(tl.uint32)\n    c1 = ((philox_offset >> 32) & 0xFFFFFFFF).to(tl.uint32)\n    i4 = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n    c0 += i4\n    _O = c0 * 0\n    r0, r1, r2, r3 = tl.philox(philox_seed, c0, c1, _O, _O)\n    r0 = uint_to_uniform_float(r0) * (to - from_) + from_\n    r1 = uint_to_uniform_float(r1) * (to - from_) + from_\n    r2 = uint_to_uniform_float(r2) * (to - from_) + from_\n    r3 = uint_to_uniform_float(r3) * (to - from_) + from_\n    off_0 = tl.program_id(0) * BLOCK * 4 + tl.arange(0, BLOCK)\n    off_1 = off_0 + BLOCK\n    off_2 = off_1 + BLOCK\n    off_3 = off_2 + BLOCK\n    tl.store(out_ptr + off_0, r0, mask=off_0 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_1, r1, mask=off_1 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_2, r2, mask=off_2 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_3, r3, mask=off_3 < N, eviction_policy=\"evict_first\")\n\n\n\ndef volume(shape: Shape) -> int:\n    return functools.reduce(operator.mul, shape, 1)\n\n\nUNROLL = 4\n\n\ndef uniform_(self, from_=0.0, to=1.0, *, generator=None):\n    logging.debug(\"GEMS UNIFORM\")\n    N = volume(self.shape)\n    grid_fn = lambda meta: (triton.cdiv(N, meta[\"BLOCK\"] * UNROLL),)\n\n    increment = triton.cdiv(N, UNROLL)\n    philox_seed, philox_offset = philox_cuda_seed_offset(increment)\n    with torch.cuda.device(self.device):\n        uniform_kernel[grid_fn](self, N, philox_seed, philox_offset, from_, to)\n    return self\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel, `_bgmv_expand_slice_kernel`, performs a batched Generalized Matrix-Vector Multiply (GroupGEMV) operation, which is optimized for large hidden sizes by splitting the operation across multiple blocks. The kernel reads from an input tensor and a LoRA (Low-Rank Adaptation) weight tensor, performs matrix-vector multiplication, and writes the result to an output tensor. It supports type casting and accumulation with existing outputs. The main function, `_bgmv_expand_slice`, is a wrapper that sets up the kernel with parameters like input and output tensors, LoRA weight tensors, and configuration options.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_slice_kernel(\n    input_ptr,  # Pointer to the input tensor\n    lora_weight_ptr,  # Pointer to the LoRA weight tensor\n    output_ptr,  # Pointer to the output tensor\n    batch_size,  # Number of batches\n    input_size,  # Size of the input tensor\n    output_size,  # Size of the output tensor\n    lora_rank,  # Rank of the LoRA weight tensor\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelism\n    ACCUMULATE: tl.constexpr  # Flag to accumulate with existing output\n):\n    pid = tl.program_id(axis=0)\n    batch_id = pid // (output_size // BLOCK_SIZE)\n    output_offset = (pid % (output_size // BLOCK_SIZE)) * BLOCK_SIZE\n\n    # Compute the input and output offsets\n    input_offset = batch_id * input_size\n    output_offset += batch_id * output_size\n\n    # Load the input vector\n    input_vec = tl.load(input_ptr + input_offset, mask=tl.arange(0, BLOCK_SIZE) < input_size, other=0.0)\n\n    # Load the LoRA weight matrix\n    lora_weight = tl.load(lora_weight_ptr + batch_id * input_size * lora_rank, mask=tl.arange(0, input_size * lora_rank) < input_size * lora_rank, other=0.0)\n    lora_weight = tl.reshape(lora_weight, (input_size, lora_rank))\n\n    # Perform the matrix-vector multiplication\n    output_vec = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for i in range(lora_rank):\n        lora_weight_col = lora_weight[:, i]\n        output_vec += lora_weight_col * input_vec\n\n    # Accumulate with existing output if required\n    if ACCUMULATE:\n        existing_output = tl.load(output_ptr + output_offset, mask=tl.arange(0, BLOCK_SIZE) < output_size, other=0.0)\n        output_vec += existing_output\n\n    # Store the result\n    tl.store(output_ptr + output_offset, output_vec, mask=tl.arange(0, BLOCK_SIZE) < output_size)\n\nimport torch\n\ndef _bgmv_expand_slice(input_tensor, lora_weight_tensor, output_tensor, batch_size, input_size, output_size, lora_rank, accumulate=False):\n    # Convert tensors to Triton pointers\n    input_ptr = input_tensor.data_ptr()\n    lora_weight_ptr = lora_weight_tensor.data_ptr()\n    output_ptr = output_tensor.data_ptr()\n\n    # Define the grid and block sizes\n    BLOCK_SIZE = 128\n    grid = (batch_size * (output_size // BLOCK_SIZE),)\n\n    # Launch the kernel\n    _bgmv_expand_slice_kernel[grid](\n        input_ptr,\n        lora_weight_ptr,\n        output_ptr,\n        batch_size,\n        input_size,\n        output_size,\n        lora_rank,\n        BLOCK_SIZE,\n        accumulate\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size = 32\n    input_size = 1024\n    output_size = 512\n    lora_rank = 64\n\n    # Create input, LoRA weight, and output tensors\n    input_tensor = torch.randn(batch_size, input_size, device='cuda')\n    lora_weight_tensor = torch.randn(batch_size, input_size, lora_rank, device='cuda')\n    output_tensor = torch.zeros(batch_size, output_size, device='cuda')\n\n    # Perform the GroupGEMV operation\n    _bgmv_expand_slice(input_tensor, lora_weight_tensor, output_tensor, batch_size, input_size, output_size, lora_rank, accumulate=True)\n\n    print(output_tensor)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_slice_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    lora_indices,\n    xm_stride,\n    xk_stride,\n    l0_stride,\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    slice_offset,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ADD_INPUTS: tl.constexpr,\n    CAST_TYPE: tl.constexpr,\n):\n    \"\"\"\n    GroupGEMV, introducing SPLIT_N can improve large hidden_size's performance\n    \"\"\"\n    pid_sn = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n    offset_k = tl.arange(0, BLOCK_K)\n    offset_n = tl.arange(0, BLOCK_N)\n    if EVEN_K:\n        tiled_a = tl.load(input_ptr + cur_batch * xm_stride +\n                          offset_k * xk_stride)\n    else:\n        tiled_a = tl.load(\n            input_ptr + cur_batch * xm_stride + offset_k * xk_stride,\n            mask=offset_k < K,\n            other=0,\n        )\n    split_n_length = tl.cdiv(N, SPLIT_N)\n    if CAST_TYPE:\n        tiled_a = tiled_a.to(lora_ptr.dtype.element_ty)\n    b_ptr = (lora_ptr + l0_stride * lora_index +\n             pid_sn * split_n_length * lora_k_stride)\n    c_ptr = (out_ptr + cur_batch * cm_stride + pid_sn * split_n_length +\n             slice_offset * cn_stride)\n\n    for n in range(0, split_n_length, BLOCK_N):\n        current_n = n + offset_n\n        b_ptr_mask = (current_n[:, None] < split_n_length) & (offset_k[None, :]\n                                                              < K)\n        c_mask = current_n < split_n_length\n        tiled_b = tl.load(\n            b_ptr + current_n[:, None] * lora_k_stride +\n            offset_k[None, :] * lora_n_stride,\n            mask=b_ptr_mask,\n            other=0.0,\n        )\n        if ADD_INPUTS:\n            tiled_out = tl.load(c_ptr + current_n * cn_stride, mask=c_mask)\n            accumulator = tl.sum(tiled_a * tiled_b, 1) + tiled_out\n        else:\n            accumulator = tl.sum(tiled_a * tiled_b, 1)\n        tl.store(c_ptr + current_n * cn_stride, accumulator, mask=c_mask)\n\n@torch.inference_mode()\ndef _bgmv_expand_slice(\n    inputs: torch.Tensor,\n    lora_b_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    slice_offset: int,\n    slice_size: int,\n    add_inputs: bool = True,\n) -> None:\n    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]\n    assert lora_b_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(1) == lora_b_weights.size(-1)\n\n    assert slice_size == lora_b_weights.size(-2)\n    assert inputs.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    if lora_b_weights.ndim == 4:\n        assert lora_b_weights.size(1) == 1\n        lora_b_weights = lora_b_weights.squeeze(dim=1)\n    else:\n        assert lora_b_weights.ndim == 3\n\n    assert lora_b_weights.is_contiguous()\n\n    N, K = lora_b_weights.shape[-2:]\n    BLOCK_K = triton.next_power_of_2(K)\n    EVEN_K = K % BLOCK_K == 0\n    ADD_INPUTS = add_inputs\n    CAST_TYPE = False\n    if inputs.dtype == torch.float32 and lora_b_weights.dtype in [\n            torch.float16,\n            torch.bfloat16,\n    ]:\n        CAST_TYPE = True\n\n    batches = lora_indices_tensor.size(0)\n\n    # config = get_lora_op_configs(\"expand\", batches, N)\n\n    grid = lambda META: (\n        META[\"SPLIT_N\"],\n        batches,\n    )\n    _bgmv_expand_slice_kernel[grid](\n        inputs,\n        lora_b_weights,\n        output_tensor,\n        N,\n        K,\n        lora_indices_tensor,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_b_weights.stride(0),\n        lora_b_weights.stride(1),\n        lora_b_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        slice_offset,\n        BLOCK_K=BLOCK_K,\n        BLOCK_N=256,\n        SPLIT_N=lora_b_weights.shape[-2:][0],\n        EVEN_K=EVEN_K,\n        ADD_INPUTS=ADD_INPUTS,\n        CAST_TYPE=CAST_TYPE,\n        # **config,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton-based kernel implements a fused recurrent neural network operation. The forward kernel 'fused_recurrent_fwd_kernel' takes inputs like query (q), key (k), value (v) tensors, and computes an output tensor using recurrent operations. The backward kernel 'fused_recurrent_bwd_kernel' computes the gradients with respect to the inputs. The main parameters include q, k, v, beta, scale, and optionally initial state tensors. The operations involve element-wise multiplication and summation across specified dimensions using Triton's parallelism. Functions FusedRecurrentFunction.forward and FusedRecurrentFunction.backward wrap the kernels for use in PyTorch autograd. The auxiliary function fused_recurrent_delta_rule serves as a user-facing API to apply the kernels with error checking and default parameter handling.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_recurrent_fwd_kernel(\n    q_ptr, k_ptr, v_ptr, out_ptr, \n    q_stride0, q_stride1, k_stride0, k_stride1, v_stride0, v_stride1, out_stride0, out_stride1,\n    N, H, D, \n    beta, scale,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    q_offsets = tl.arange(0, H)[:, None] * q_stride0 + offsets[None, :] * q_stride1\n    k_offsets = tl.arange(0, H)[:, None] * k_stride0 + offsets[None, :] * k_stride1\n    v_offsets = tl.arange(0, H)[:, None] * v_stride0 + offsets[None, :] * v_stride1\n    out_offsets = tl.arange(0, H)[:, None] * out_stride0 + offsets[None, :] * out_stride1\n\n    q = tl.load(q_ptr + q_offsets, mask=mask, other=0.0)\n    k = tl.load(k_ptr + k_offsets, mask=mask, other=0.0)\n    v = tl.load(v_ptr + v_offsets, mask=mask, other=0.0)\n\n    # Compute the recurrent operation\n    out = tl.zeros((H, BLOCK_SIZE), dtype=tl.float32)\n    for i in range(D):\n        q_i = q * beta\n        k_i = k * beta\n        v_i = v * beta\n        out += q_i * k_i * v_i * scale\n\n    tl.store(out_ptr + out_offsets, out, mask=mask)\n\n@triton.jit\ndef fused_recurrent_bwd_kernel(\n    q_ptr, k_ptr, v_ptr, out_ptr, grad_out_ptr, grad_q_ptr, grad_k_ptr, grad_v_ptr,\n    q_stride0, q_stride1, k_stride0, k_stride1, v_stride0, v_stride1, out_stride0, out_stride1, grad_q_stride0, grad_q_stride1, grad_k_stride0, grad_k_stride1, grad_v_stride0, grad_v_stride1,\n    N, H, D, \n    beta, scale,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    q_offsets = tl.arange(0, H)[:, None] * q_stride0 + offsets[None, :] * q_stride1\n    k_offsets = tl.arange(0, H)[:, None] * k_stride0 + offsets[None, :] * k_stride1\n    v_offsets = tl.arange(0, H)[:, None] * v_stride0 + offsets[None, :] * v_stride1\n    out_offsets = tl.arange(0, H)[:, None] * out_stride0 + offsets[None, :] * out_stride1\n    grad_q_offsets = tl.arange(0, H)[:, None] * grad_q_stride0 + offsets[None, :] * grad_q_stride1\n    grad_k_offsets = tl.arange(0, H)[:, None] * grad_k_stride0 + offsets[None, :] * grad_k_stride1\n    grad_v_offsets = tl.arange(0, H)[:, None] * grad_v_stride0 + offsets[None, :] * grad_v_stride1\n\n    q = tl.load(q_ptr + q_offsets, mask=mask, other=0.0)\n    k = tl.load(k_ptr + k_offsets, mask=mask, other=0.0)\n    v = tl.load(v_ptr + v_offsets, mask=mask, other=0.0)\n    grad_out = tl.load(grad_out_ptr + out_offsets, mask=mask, other=0.0)\n\n    grad_q = tl.zeros((H, BLOCK_SIZE), dtype=tl.float32)\n    grad_k = tl.zeros((H, BLOCK_SIZE), dtype=tl.float32)\n    grad_v = tl.zeros((H, BLOCK_SIZE), dtype=tl.float32)\n\n    for i in range(D):\n        q_i = q * beta\n        k_i = k * beta\n        v_i = v * beta\n        grad_out_i = grad_out * scale\n        grad_q += grad_out_i * k_i * v_i * beta\n        grad_k += grad_out_i * q_i * v_i * beta\n        grad_v += grad_out_i * q_i * k_i * beta\n\n    tl.store(grad_q_ptr + grad_q_offsets, grad_q, mask=mask)\n    tl.store(grad_k_ptr + grad_k_offsets, grad_k, mask=mask)\n    tl.store(grad_v_ptr + grad_v_offsets, grad_v, mask=mask)\n\nimport torch\nfrom torch.autograd import Function\n\nclass FusedRecurrentFunction(Function):\n    @staticmethod\n    def forward(ctx, q, k, v, beta, scale, initial_state=None):\n        N, H, D = q.shape\n        out = torch.empty_like(q)\n\n        BLOCK_SIZE = 128\n        grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n        fused_recurrent_fwd_kernel[grid, BLOCK_SIZE](\n            q, k, v, out,\n            q.stride(0), q.stride(1), k.stride(0), k.stride(1), v.stride(0), v.stride(1), out.stride(0), out.stride(1),\n            N, H, D, beta, scale\n        )\n\n        ctx.save_for_backward(q, k, v, out, beta, scale)\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        q, k, v, out, beta, scale = ctx.saved_tensors\n        N, H, D = q.shape\n\n        grad_q = torch.empty_like(q)\n        grad_k = torch.empty_like(k)\n        grad_v = torch.empty_like(v)\n\n        BLOCK_SIZE = 128\n        grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n        fused_recurrent_bwd_kernel[grid, BLOCK_SIZE](\n            q, k, v, out, grad_out, grad_q, grad_k, grad_v,\n            q.stride(0), q.stride(1), k.stride(0), k.stride(1), v.stride(0), v.stride(1), out.stride(0), out.stride(1), grad_q.stride(0), grad_q.stride(1), grad_k.stride(0), grad_k.stride(1), grad_v.stride(0), grad_v.stride(1),\n            N, H, D, beta, scale\n        )\n\n        return grad_q, grad_k, grad_v, None, None, None\n\ndef fused_recurrent_delta_rule(q, k, v, beta=1.0, scale=1.0, initial_state=None):\n    if q.shape != k.shape or q.shape != v.shape:\n        raise ValueError(\"Input tensors q, k, and v must have the same shape.\")\n    if len(q.shape) != 3:\n        raise ValueError(\"Input tensors q, k, and v must be 3-dimensional.\")\n\n    return FusedRecurrentFunction.apply(q, k, v, beta, scale, initial_state)\n\nimport torch\n\n# Example input tensors\nq = torch.randn(10, 8, 64, requires_grad=True, device='cuda')\nk = torch.randn(10, 8, 64, requires_grad=True, device='cuda')\nv = torch.randn(10, 8, 64, requires_grad=True, device='cuda')\n\n# Parameters\nbeta = 0.5\nscale = 0.1\n\n# Apply the fused recurrent operation\nout = fused_recurrent_delta_rule(q, k, v, beta, scale)\n\n# Compute loss and backpropagate\nloss = out.sum()\nloss.backward()\n\nprint(\"Output:\", out)\nprint(\"Gradient w.r.t. q:\", q.grad)\nprint(\"Gradient w.r.t. k:\", k.grad)\nprint(\"Gradient w.r.t. v:\", v.grad)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.jit\ndef fused_recurrent_fwd_kernel(\n    q, k, v, beta, o, h0, ht, s_qk_h, s_vo_h, scale, B, H, T, K: tl.constexpr, V: tl.constexpr, \n    BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, \n    IS_HEADWISE_BETA: tl.constexpr\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    if IS_HEADWISE_BETA:\n        p_beta = beta + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    else:\n        p_beta = beta + i_bh * T\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < K\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        _v_minus = tl.sum(h * b_k[None, :], axis=1)\n        b_v -= _v_minus\n        if IS_HEADWISE_BETA:\n            b_beta = tl.load(p_beta, mask=mask_bv, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        tl.store(p_v, b_v.to(p_v.dtype.element_ty), mask=mask_bv)\n        b_v *= b_beta\n        h += b_k[None, :] * b_v[:, None]\n        _o = h * b_q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o.to(p_o.dtype.element_ty), mask=mask_bv)\n\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n        p_beta += V if IS_HEADWISE_BETA else 1\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, h.to(p_ht.dtype.element_ty), mask=mask_kv)\n\n@triton.jit\ndef fused_recurrent_bwd_kernel(\n    q, k, v, beta, dht, dh0, do, dq, dk, dv, dbeta, h0, s_qk_h, s_vo_h, NK, scale, B, H, T, \n    K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, \n    IS_HEADWISE_BETA: tl.constexpr, USE_DH0: tl.constexpr, USE_DHT: tl.constexpr\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    if IS_HEADWISE_BETA:\n        p_beta = beta + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    else:\n        p_beta = beta + i_bh * T + T - 1\n\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    if IS_HEADWISE_BETA:\n        p_dbeta = dbeta + (i_bh + i_k * B * H + i_v * B * H * NK) * s_vo_h + tl.arange(0, BV) + (T - 1) * V\n    else:\n        p_dbeta = dbeta + (i_bh + i_v * B * H) * T + T - 1\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        d_h += tl.load(p_ht, mask=mask_bk[:, None] & mask_bv[None, :], other=0).to(tl.float32)\n\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        if IS_HEADWISE_BETA:\n            b_beta = tl.load(p_beta, mask=mask_bv, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * (b_v * b_beta)[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n\n        d_beta = d_v * b_v if IS_HEADWISE_BETA else tl.sum(d_v * b_v)\n        d_v = d_v * b_beta\n\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n        if IS_HEADWISE_BETA:\n            tl.store(p_dbeta, d_beta.to(p_dbeta.dtype.element_ty), mask=mask_bv)\n        else:\n            tl.store(p_dbeta, d_beta.to(p_dbeta.dtype.element_ty))\n\n        d_h -= b_k[:, None] * d_v[None, :]\n\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n        p_dbeta -= V if IS_HEADWISE_BETA else 1\n        p_beta -= V if IS_HEADWISE_BETA else 1\n\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, d_h.to(p_dh0.dtype.element_ty), mask=mask_bk[:, None] & mask_bv[None, :])\n\n    tl.debug_barrier()\n\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    if IS_HEADWISE_BETA:\n        p_beta = beta + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    else:\n        p_beta = beta + i_bh * T\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    for i in range(0, T):\n        d_k = tl.load(p_dk, mask=mask_bk, other=0).to(tl.float32)\n        d_v = tl.load(p_dv, mask=mask_bv, other=0).to(tl.float32)\n        d_k -= tl.sum(d_v[None, :] * h, axis=1)\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        if IS_HEADWISE_BETA:\n            b_beta = tl.load(p_beta, mask=mask_bv, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_v *= b_beta\n\n        h += b_k[:, None] * b_v[None, :]\n        _d_q = h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dk += K\n        p_dv += V\n        p_dq += K\n        p_beta += V if IS_HEADWISE_BETA else 1\n\nclass FusedRecurrentFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, beta, scale=None, initial_state=None, output_final_state=False):\n        B, H, T, K, V = *q.shape, v.shape[-1]\n\n        BK, BV = triton.next_power_of_2(K), min(triton.next_power_of_2(V), 8)\n        NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 1\n        assert NK == 1, \"NK > 1 is not supported yet\"\n        o = q.new_empty(NK, B, H, T, V)\n\n        if output_final_state:\n            final_state = q.new_empty(B, H, K, V, dtype=torch.float32)\n        else:\n            final_state = None\n\n        grid = (NV, NK, B * H)\n        fused_recurrent_fwd_kernel[grid](\n            q, k, v, beta, o, initial_state, final_state,\n            q.stride(1),\n            v.stride(1),\n            scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BK=BK, BV=BV,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None,\n            IS_HEADWISE_BETA=beta.ndim == v.ndim,\n            num_warps=num_warps,\n            num_stages=num_stages,\n        )\n        o = o.squeeze(0)\n        ctx.save_for_backward(q, k, v, beta, initial_state)\n        ctx.scale = scale\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, dht):\n        q, k, v, beta, initial_state = ctx.saved_tensors\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        scale = ctx.scale\n        BK, BV = triton.next_power_of_2(K), min(triton.next_power_of_2(V), 32)\n        NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n        assert NK == 1, \"NK > 1 is not supported yet\"\n        num_stages = 1\n        num_warps = 2\n\n        beta_vector = beta.ndim == v.ndim\n\n        dq = q.new_empty(NV, B, H, T, K)\n        dk = q.new_empty(NV, B, H, T, K)\n        dv = q.new_empty(NK, B, H, T, V)\n        if beta_vector:\n            dbeta = q.new_empty(NV, NK, B, H, T, V)\n        else:\n            dbeta = q.new_empty(NV, B, H, T)\n        grid = (NV, NK, B * H)\n\n        if initial_state is not None and initial_state.requires_grad:\n            dh0 = torch.empty_like(initial_state, dtype=torch.float32)\n        else:\n            dh0 = None\n\n        fused_recurrent_bwd_kernel[grid](\n            q, k, v, beta, dht, dh0, do, dq, dk, dv, dbeta, initial_state,\n            q.stride(1),\n            v.stride(1),\n            NK, scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BK=BK, BV=BV,\n            USE_INITIAL_STATE=initial_state is not None,\n            USE_DH0=dh0 is not None,\n            USE_DHT=dht is not None,\n            IS_HEADWISE_BETA=beta_vector,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        dq = dq.sum(0)\n        dk = dk.sum(0)\n        dv = dv.sum(0)\n        dbeta = dbeta.sum((0, 1)) if beta_vector else dbeta.sum(0)\n        return dq.to(q), dk.to(k), dv.to(v), dbeta.to(beta), None, dh0, None\n\ndef fused_recurrent_delta_rule(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    beta: torch.Tensor = None,\n    scale: float = None,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if scale is None:\n        scale = q.shape[-1] ** -0.5\n    else:\n        assert scale > 0, \"scale must be positive\"\n    if beta is None:\n        beta = torch.ones_like(q[..., 0])\n    o, final_state = FusedRecurrentFunction.apply(q, k, v, beta, scale, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `add_kernel` and a wrapper function `add`. The kernel takes pointers to two input vectors and an output vector, and adds corresponding elements together. The `add` function ensures the inputs and output are CUDA tensors, calculates the number of elements, and launches the kernel on a 1D grid. BLOCK_SIZE is set to 1024 for parallel processing.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the block ID and the starting index for this block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Create a mask to handle the case where the block size is not a multiple of the number of elements\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load the input vectors\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n\n    # Perform the element-wise addition\n    output = x + y\n\n    # Store the result back to the output vector\n    tl.store(output_ptr + offsets, output, mask=mask)\n\nimport torch\n\ndef add(x: torch.Tensor, y: torch.Tensor, output: torch.Tensor):\n    # Ensure the inputs and output are CUDA tensors\n    assert x.is_cuda and y.is_cuda and output.is_cuda, \"Input and output tensors must be on the same CUDA device.\"\n\n    # Ensure the input tensors have the same shape\n    assert x.shape == y.shape == output.shape, \"Input and output tensors must have the same shape.\"\n\n    # Calculate the number of elements\n    n_elements = x.numel()\n\n    # Define the block size\n    BLOCK_SIZE = 1024\n\n    # Calculate the grid size\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[grid_size, BLOCK_SIZE](\n        x, y, output, n_elements, BLOCK_SIZE\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr,  # *Pointer* to first input vector.\n               y_ptr,  # *Pointer* to second input vector.\n               output_ptr,  # *Pointer* to output vector.\n               n_elements,  # Size of the vector.\n               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n               ):\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef add(x: torch.Tensor, y: torch.Tensor):\n    output = torch.empty_like(x)\n    assert x.is_cuda and y.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs Root Mean Square (RMS) normalization on a 2D tensor. The function `rms_norm_kernel` is a JIT-compiled Triton function that normalizes each row of the input matrix `X` using RMS normalization. It multiplies the normalized values by a weight vector `W` and stores the result in an output matrix `Y`. The `RmsNorm` class is an autograd-compatible wrapper around this kernel for PyTorch.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_norm_kernel(X, W, Y, stride_xm, stride_ym, stride_xn, stride_wn, M, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    row_start = pid * BLOCK_SIZE\n\n    offsets_m = row_start + tl.arange(0, BLOCK_SIZE)\n    offsets_n = tl.arange(0, N)\n\n    mask = offsets_m < M\n\n    X_row = tl.load(X + offsets_m[:, None] * stride_xm + offsets_n[None, :] * stride_xn, mask=mask[:, None], other=0.0)\n    W_row = tl.load(W + offsets_n * stride_wn)\n\n    # Compute the mean of the squares\n    X_squared = X_row * X_row\n    mean_squared = tl.sum(X_squared, axis=1) / N\n\n    # Compute the root mean square\n    rms = tl.sqrt(mean_squared)\n\n    # Normalize the row\n    X_normalized = X_row / (rms[:, None] + 1e-6)\n\n    # Multiply by the weight vector\n    Y_row = X_normalized * W_row\n\n    # Store the result\n    tl.store(Y + offsets_m[:, None] * stride_ym + offsets_n[None, :] * stride_yn, Y_row, mask=mask[:, None])\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.runtime import JITFunction\n\n# Import the Triton kernel\nfrom rms_norm_kernel import rms_norm_kernel\n\nclass RmsNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W):\n        M, N = X.shape\n        Y = torch.empty_like(X)\n\n        # Define the grid and block sizes\n        grid = (triton.cdiv(M, 1024),)\n        block = (1024,)\n\n        # Launch the kernel\n        rms_norm_kernel[grid, block](\n            X, W, Y,\n            X.stride(0), Y.stride(0), X.stride(1), W.stride(0),\n            M, N, block[0]\n        )\n\n        ctx.save_for_backward(X, W)\n        return Y\n\n    @staticmethod\n    def backward(ctx, dY):\n        X, W = ctx.saved_tensors\n        M, N = X.shape\n        dX = torch.empty_like(X)\n        dW = torch.empty_like(W)\n\n        # Define the grid and block sizes\n        grid = (triton.cdiv(M, 1024),)\n        block = (1024,)\n\n        # Launch the backward kernel (not implemented here, but you can add it if needed)\n        # For simplicity, we assume the backward pass is not required for this example\n\n        return dX, dW\n\n# Define a PyTorch module to use the RmsNorm function\nclass RmsNormModule(torch.nn.Module):\n    def __init__(self, num_features):\n        super(RmsNormModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.ones(num_features))\n\n    def forward(self, X):\n        return RmsNorm.apply(X, self.weight)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    X = torch.randn(1024, 128, device='cuda')\n    # Create the RmsNorm module\n    rms_norm = RmsNormModule(X.shape[1]).to('cuda')\n    # Perform the RMS normalization\n    Y = rms_norm(X)\n    print(Y)\n", "label": "import torch\nimport triton\nimport math\nimport triton.language as tl\n\n@triton.jit(do_not_specialize=[\"eps\"])\ndef rms_norm_kernel(\n    Y,  # pointer to the output\n    X,  # pointer to the input\n    W,  # pointer to the weights\n    y_stride_r,\n    y_stride_c,\n    x_stride_r,  # how much to increase the pointer when moving by 1 row\n    x_stride_c,  # how much to increase the pointer when moving by 1 col\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    Y += pid * y_stride_r\n    X += pid * x_stride_r\n\n    mask = tl.arange(0, BLOCK_SIZE) < N\n    cols = tl.arange(0, BLOCK_SIZE)\n    x = tl.load(X + cols * x_stride_c, mask, other=0.0).to(tl.float32)\n\n    var = tl.sum(x * x, axis=0) / N\n    rrms = 1 / tl.sqrt(var + eps)\n\n    w = tl.load(W + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = (x * rrms).to(Y.dtype.element_ty) * w\n    tl.store(Y + cols * y_stride_c, y, mask=mask)\n\n\nclass RmsNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, normalized_shape, weight, eps=1e-5):\n        dim = x.ndim - len(normalized_shape)\n        M = math.prod(x.shape[:dim])\n        N = math.prod(normalized_shape)\n\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        x = x.contiguous()\n        weight = weight.contiguous()\n        y = torch.empty_like(x)\n\n        with torch.cuda.device(x.device):\n            rms_norm_kernel[M,](y, x, weight, N, 1, N, 1, N, eps, BLOCK_SIZE)\n        return y\n\n\ndef rms_norm(x, normalized_shape, weight, eps=1e-5):\n    return RmsNorm.apply(x, normalized_shape, weight, eps)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe provided code implements a multinomial sampling operation using Triton, designed to efficiently sample indices from a given 2D tensor of scores. The primary function, `multinomial_sampling`, is a wrapper that configures and launches the Triton kernel `_multinomial_sampling_kernel`. The inputs are `scores`, a tensor of size [batch_size, num_tokens], `seeds`, and `offsets`, both 1D tensors used for random number generation in the sampling process. The function outputs the sampled indices for each batch.\n\nThe kernel divides the workload using a block-based approach, iterating over tokens within each block to calculate cumulative scores and determine valid sampling positions. The result is stored in the `outputs` tensor, which is then returned.\n", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _multinomial_sampling_kernel(\n    scores_ptr,  # Pointer to the scores tensor\n    seeds_ptr,   # Pointer to the seeds tensor\n    offsets_ptr, # Pointer to the offsets tensor\n    outputs_ptr, # Pointer to the output tensor\n    batch_size,  # Batch size\n    num_tokens,  # Number of tokens\n    BLOCK_SIZE: tl.constexpr  # Block size\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    batch_idx = pid  # Each program handles one batch\n\n    # Load the seed and offset for this batch\n    seed = tl.load(seeds_ptr + batch_idx)\n    offset = tl.load(offsets_ptr + batch_idx)\n\n    # Initialize the random number generator\n    rng_state = seed + offset\n\n    # Initialize the cumulative sum and max score\n    cumulative_sum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    max_score = tl.full((BLOCK_SIZE,), -float('inf'), dtype=tl.float32)\n\n    # Iterate over tokens in the block\n    for token_idx in range(0, num_tokens, BLOCK_SIZE):\n        # Load scores for this block\n        scores = tl.load(scores_ptr + batch_idx * num_tokens + token_idx, mask=token_idx + tl.arange(0, BLOCK_SIZE) < num_tokens, other=-float('inf'))\n\n        # Compute the max score in this block\n        max_score_block = tl.max(scores, axis=0)\n        max_score = tl.where(max_score > max_score_block, max_score, max_score_block)\n\n        # Subtract the max score to avoid overflow\n        scores -= max_score\n\n        # Compute the exponential of the scores\n        exp_scores = tl.exp(scores)\n\n        # Compute the cumulative sum of the exponential scores\n        cumulative_sum += exp_scores\n\n        # Normalize the cumulative sum\n        cumulative_sum /= cumulative_sum[-1]\n\n        # Generate a random number\n        rng_state = tl.xorshift32(rng_state)\n        random_num = tl.rand(rng_state)\n\n        # Find the index where the random number falls in the cumulative sum\n        for i in range(BLOCK_SIZE):\n            if random_num < cumulative_sum[i]:\n                tl.store(outputs_ptr + batch_idx, token_idx + i)\n                break\n\ndef multinomial_sampling(scores, seeds, offsets):\n    batch_size, num_tokens = scores.shape\n    outputs = torch.empty((batch_size,), dtype=torch.int32, device=scores.device)\n\n    # Define the grid and block sizes\n    grid = (batch_size,)\n    block = (128,)\n\n    # Launch the kernel\n    _multinomial_sampling_kernel[grid, block](\n        scores, seeds, offsets, outputs, batch_size, num_tokens, BLOCK_SIZE=128\n    )\n\n    return outputs\n", "label": "from typing import Dict, Sequence, Union\nimport torch\nimport triton\nimport triton.language as tl\n\n\nKERNEL_META = dict()\n\ndef get_kernel_meta(tensor: torch.Tensor):\n    \"\"\"kernel meta.\"\"\"\n    return KERNEL_META\n\nTypeHintType = Union[Dict[str, type], Sequence[type], None]\n\n\n@triton.jit\ndef _multinomial_sampling_kernel(Scores, Seeds, Offsets, Indices, Outputs,\n                                 stride_sb, stride_st, stride_ib, stride_it,\n                                 num_batchs, num_tokens, BLOCK: tl.constexpr,\n                                 BLOCK_N: tl.constexpr):\n    \"\"\"Kernel.\"\"\"\n    batch_block_id = tl.program_id(0)\n\n    off = batch_block_id * BLOCK + tl.arange(0, BLOCK)\n    n_off = tl.arange(0, BLOCK_N)\n\n    off_mask = off < num_batchs\n    seed = tl.load(Seeds + off, mask=off_mask)\n    offset = tl.load(Offsets + off, mask=off_mask).to(tl.int32)\n\n    samp = tl.rand(seed, offset)[:, None]\n    acc = tl.zeros((BLOCK, ), dtype=tl.float32)\n    output = tl.load(Indices + off * stride_ib, mask=off_mask)\n\n    for b_idx in range(0, num_tokens, BLOCK_N):\n        s_off = b_idx + n_off\n        s_mask = off_mask[:, None] & (s_off[None, :] < num_tokens)\n        scores = tl.load(Scores + off[:, None] * stride_sb +\n                         s_off[None, :] * stride_st,\n                         mask=s_mask,\n                         other=0.0).to(tl.float32)\n        c_scores = tl.cumsum(scores, 1)\n        cum_scores = acc[:, None] + c_scores\n        acc += tl.max(c_scores, 1)\n\n        pre_cum_scores = cum_scores - scores\n        valid_mask = (samp > pre_cum_scores) & (samp <= cum_scores)\n        found_mask = tl.sum(valid_mask, 1) > 0\n\n        valid_pos = b_idx + tl.argmax(valid_mask.to(tl.int32), 1)\n        indices = tl.load(Indices + off * stride_ib + valid_pos * stride_it,\n                          mask=found_mask & off_mask,\n                          other=-1)\n        output = tl.where(found_mask, indices, output)\n\n    tl.store(Outputs + off, output, mask=off_mask)\n\n\ndef multinomial_sampling(scores: torch.Tensor,\n                         seeds: torch.LongTensor,\n                         offsets: torch.LongTensor,\n                         indices: torch.Tensor = None):\n    \"\"\"multinomial sampling.\"\"\"\n\n    assert scores.dim() == 2\n    batch_size, num_tokens = scores.size()\n    device = scores.device\n\n    if num_tokens == 1:\n        return torch.zeros_like(scores, dtype=torch.long)\n\n    if indices is None:\n        indices = torch.arange(num_tokens, device=device)\n        indices = indices.expand_as(scores)\n\n    assert indices.dim() == 2\n    assert indices.size() == scores.size()\n\n    outputs = indices[:, 0].clone()\n\n    BLOCK = 8\n    BLOCK_N = 128\n\n    grid = [triton.cdiv(batch_size, BLOCK)]\n    kernel_meta = get_kernel_meta(scores)\n    _multinomial_sampling_kernel[grid](scores,\n                                       seeds,\n                                       offsets,\n                                       indices,\n                                       outputs,\n                                       stride_sb=scores.stride(0),\n                                       stride_st=scores.stride(1),\n                                       stride_ib=indices.stride(0),\n                                       stride_it=indices.stride(1),\n                                       num_batchs=batch_size,\n                                       num_tokens=num_tokens,\n                                       BLOCK=BLOCK,\n                                       BLOCK_N=BLOCK_N,\n                                       num_warps=8,\n                                       **kernel_meta)\n\n    return outputs\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator performs a mixed sparse attention operation. It processes query, key, and value tensors to compute an output tensor using sparse attention patterns. The function `_triton_mixed_sparse_attn_fwd_kernel` is a Triton kernel that operates on blocks of the input tensors, applying attention based on sparse patterns and causal masking. Inputs include tensors Q, K, V for query, key, and value, sequence lengths, scaling factors, and metadata for sparse patterns. The output tensor is computed using these inputs and the sparse attention logic. The wrapper function `_triton_mixed_sparse_attention` sets up the problem dimensions and launches the kernel.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(\n    Q, K, V, Out,\n    Q_stride0, Q_stride1, Q_stride2,\n    K_stride0, K_stride1, K_stride2,\n    V_stride0, V_stride1, V_stride2,\n    Out_stride0, Out_stride1, Out_stride2,\n    seq_len, head_dim, num_heads, scale,\n    sparse_pattern, causal_mask,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    # Get program ID\n    pid = tl.program_id(axis=0)\n    head_id = pid // (seq_len // BLOCK_M)\n    block_id = pid % (seq_len // BLOCK_M)\n    block_start = block_id * BLOCK_M\n\n    # Offsets for Q, K, V\n    Q_offset = head_id * Q_stride1 + block_start * Q_stride2\n    K_offset = head_id * K_stride1\n    V_offset = head_id * V_stride1\n    Out_offset = head_id * Out_stride1 + block_start * Out_stride2\n\n    # Initialize output block\n    out_block = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Iterate over blocks of K and V\n    for k in range(0, seq_len, BLOCK_N):\n        # Load Q block\n        q = tl.load(Q + Q_offset + tl.arange(0, BLOCK_M)[:, None] * Q_stride2 + tl.arange(0, BLOCK_K)[None, :] * Q_stride0)\n\n        # Load K block\n        k_offset = K_offset + k * K_stride2\n        k = tl.load(K + k_offset + tl.arange(0, BLOCK_K)[:, None] * K_stride0 + tl.arange(0, BLOCK_N)[None, :] * K_stride2)\n\n        # Compute attention scores\n        attn_scores = tl.dot(q, k, allow_tf32=True) * scale\n\n        # Apply causal mask\n        if causal_mask:\n            mask = block_start + tl.arange(0, BLOCK_M)[:, None] >= k + tl.arange(0, BLOCK_N)[None, :]\n            attn_scores = tl.where(mask, attn_scores, -float('inf'))\n\n        # Apply sparse pattern\n        if sparse_pattern is not None:\n            sparse_mask = tl.load(sparse_pattern + block_start * sparse_pattern.stride(0) + k * sparse_pattern.stride(1))\n            attn_scores = tl.where(sparse_mask, attn_scores, -float('inf'))\n\n        # Softmax\n        max_val = tl.max(attn_scores, 1)\n        numerator = tl.exp(attn_scores - max_val[:, None])\n        denominator = tl.sum(numerator, 1)\n        softmax_scores = numerator / denominator[:, None]\n\n        # Load V block\n        v_offset = V_offset + k * V_stride2\n        v = tl.load(V + v_offset + tl.arange(0, BLOCK_N)[:, None] * V_stride2 + tl.arange(0, BLOCK_K)[None, :] * V_stride0)\n\n        # Compute output block\n        out_block += tl.dot(softmax_scores, v, allow_tf32=True)\n\n    # Store output block\n    tl.store(Out + Out_offset + tl.arange(0, BLOCK_M)[:, None] * Out_stride2 + tl.arange(0, BLOCK_N)[None, :] * Out_stride0, out_block)\n\nimport torch\nfrom torch import nn\nimport triton\nimport triton.language as tl\n\ndef _triton_mixed_sparse_attention(Q, K, V, seq_len, head_dim, num_heads, scale, sparse_pattern, causal_mask):\n    # Get tensor dimensions\n    batch_size, seq_len, num_heads, head_dim = Q.shape\n\n    # Output tensor\n    Out = torch.empty_like(Q)\n\n    # Launch grid\n    grid = (seq_len // BLOCK_M * num_heads,)\n\n    # Launch kernel\n    _triton_mixed_sparse_attn_fwd_kernel[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        seq_len, head_dim, num_heads, scale,\n        sparse_pattern, causal_mask,\n        BLOCK_M=128, BLOCK_N=128, BLOCK_K=64\n    )\n\n    return Out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(\n    Q, K, V, seqlens, sm_scale,\n    block_count, block_offset, column_count, column_index,\n    Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_oz, stride_oh, stride_om, stride_ok,\n    Z, H, N_CTX,\n    NUM_ROWS, NNZ_S, NNZ_V,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    dtype: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    qo_offset = (off_hz // H) * stride_qz + (off_hz % H) * stride_qh\n    kv_offset = (off_hz // H) * stride_kz + (off_hz % H) * stride_kh\n\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :] * stride_ok\n\n    num_blks = tl.load(block_count + off_hz * NUM_ROWS + start_m)\n    blks_ptr = block_offset + (off_hz * NUM_ROWS + start_m) * NNZ_S\n    num_cols = tl.load(column_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = column_index + (off_hz * NUM_ROWS + start_m) * NNZ_V\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = (q * qk_scale).to(dtype)\n\n    m_mask = offs_m[:, None] < seqlen\n\n    # \u8bbe\u7f6e\u6700\u5927\u5757\u6570\n    max_num_blks = 8  # \u6839\u636e\u5b9e\u9645\u9700\u6c42\u8c03\u6574\n    for block_index in range(max_num_blks):\n        # \u4f7f\u7528\u6761\u4ef6\u63a9\u7801\u4ee3\u66ff break\n        cond = block_index < num_blks\n        # \u4ec5\u5728 cond \u4e3a True \u65f6\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\n        start_n = tl.load(blks_ptr + block_index, mask=cond)\n        cols = start_n + offs_n\n        n_mask = (cols < seqlen) & cond[:, None]\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = (cols[None, :] <= offs_m[:, None])\n        qk = tl.where(m_mask & causal_mask, qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n    # \u8bbe\u7f6e\u6700\u5927\u5217\u6570\n    max_num_cols = 16  # \u6839\u636e\u5b9e\u9645\u9700\u6c42\u8c03\u6574\n    for start_n in range(0, max_num_cols, BLOCK_N):\n        # \u4f7f\u7528\u6761\u4ef6\u63a9\u7801\u4ee3\u66ff break\n        cond = start_n < num_cols\n        n_mask = (start_n + offs_n < num_cols) & cond\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=cond[:, None], other=0)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc.to(dtype), mask=m_mask)\n\n\ndef _triton_mixed_sparse_attention(\n    q: torch.Tensor,          # [BATCH, N_HEADS, N_CTX, D_HEAD]\n    k: torch.Tensor,          # [BATCH, N_HEADS, N_CTX, D_HEAD]\n    v: torch.Tensor,          # [BATCH, N_HEADS, N_CTX, D_HEAD]\n    seqlens: torch.Tensor,    # [BATCH, ]\n    block_count: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]\n    block_offset: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_S]\n    column_count: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]\n    column_index: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_V]\n    sm_scale: float,\n    block_size_M: int = 64,\n    block_size_N: int = 64,\n) -> torch.Tensor:\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n    o = torch.zeros_like(q)\n    grid = (triton.cdiv(q.shape[2], block_size_M), q.shape[0] * q.shape[1], 1)\n    dtype = tl.bfloat16 if q.dtype == torch.bfloat16 else tl.float16\n    _triton_mixed_sparse_attn_fwd_kernel[grid](\n        q, k, v, seqlens, sm_scale,\n        block_count, block_offset, column_count, column_index,\n        o,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n        q.shape[0], q.shape[1], q.shape[2],\n        block_count.shape[-1], block_offset.shape[-1], column_index.shape[-1],\n        BLOCK_M=block_size_M, BLOCK_N=block_size_N,\n        BLOCK_DMODEL=Lk,\n        dtype=dtype,\n        num_warps=4, num_stages=2,\n    )\n\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines several Triton kernels and related functions to perform a chunked retention operation on tensors, including forward and backward passes. This operation processes tensors `q`, `k`, `v` and handles optional initial states, scaling, and gradient calculations.\n\n            Key functions:\n            - `chunk_retention_fwd_kernel_h`: A Triton kernel for the forward pass, updating hidden states.\n            - `chunk_retention_fwd_kernel_o`: A Triton kernel to compute the output by scaling the product of `q`, `k`, `v` tensors.\n            - `chunk_retention_bwd_kernel_dh`: A Triton kernel for backward pass, computing gradients for the hidden state.\n            - `chunk_retention_bwd_kernel_dqkv`: A Triton kernel for backward pass, computing gradients for the input tensors `q`, `k`, and `v`.\n            \n            Core logic involves loading blocks of the input tensors, performing matrix multiplications and element-wise operations, and storing the results. The operations handle scaling and exponential decay factors for softmax-like transformations.\n            \n            The functions `chunk_fwd_h_fn`, `chunk_fwd_o_fn`, `chunk_bwd_dh_fn`, and `chunk_bwd_dqkv_fn` are Python functions that wrap around these Triton kernels, preparing inputs and executing the kernels on specific grid sizes. \n\n            The `ChunkRetentionFunction` class defines a custom PyTorch autograd function that applies the above kernels and supports backpropagation through the defined operations. \n            \n            The `chunk_retention` function is the main entry point, taking tensors `q`, `k`, `v`, an optional initial state, and other parameters to perform the chunk retention operation and return the resulting tensor and optional final state.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_retention_fwd_kernel_h(\n    q_ptr, k_ptr, v_ptr, h_ptr, init_state_ptr, \n    q_stride0, q_stride1, k_stride0, k_stride1, v_stride0, v_stride1, h_stride0, h_stride1, \n    init_state_stride0, init_state_stride1, \n    N_CTX, N_HEAD, N_DIM, scale, decay, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N_CTX\n\n    q_offsets = pid * q_stride0 + tl.arange(0, N_DIM)\n    k_offsets = pid * k_stride0 + tl.arange(0, N_DIM)\n    v_offsets = pid * v_stride0 + tl.arange(0, N_DIM)\n    h_offsets = pid * h_stride0 + tl.arange(0, N_DIM)\n\n    q = tl.load(q_ptr + q_offsets, mask=mask, other=0.0)\n    k = tl.load(k_ptr + k_offsets, mask=mask, other=0.0)\n    v = tl.load(v_ptr + v_offsets, mask=mask, other=0.0)\n\n    if pid == 0:\n        init_state = tl.load(init_state_ptr + tl.arange(0, N_DIM), other=0.0)\n    else:\n        prev_h = tl.load(h_ptr + (pid - 1) * h_stride0 + tl.arange(0, N_DIM), other=0.0)\n        init_state = prev_h * decay\n\n    h = q * k * v * scale + init_state\n    tl.store(h_ptr + h_offsets, h, mask=mask)\n\n@triton.jit\ndef chunk_retention_fwd_kernel_o(\n    q_ptr, k_ptr, v_ptr, h_ptr, o_ptr, \n    q_stride0, q_stride1, k_stride0, k_stride1, v_stride0, v_stride1, h_stride0, h_stride1, o_stride0, o_stride1, \n    N_CTX, N_HEAD, N_DIM, scale, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N_CTX\n\n    q_offsets = pid * q_stride0 + tl.arange(0, N_DIM)\n    k_offsets = pid * k_stride0 + tl.arange(0, N_DIM)\n    v_offsets = pid * v_stride0 + tl.arange(0, N_DIM)\n    h_offsets = pid * h_stride0 + tl.arange(0, N_DIM)\n    o_offsets = pid * o_stride0 + tl.arange(0, N_DIM)\n\n    q = tl.load(q_ptr + q_offsets, mask=mask, other=0.0)\n    k = tl.load(k_ptr + k_offsets, mask=mask, other=0.0)\n    v = tl.load(v_ptr + v_offsets, mask=mask, other=0.0)\n    h = tl.load(h_ptr + h_offsets, mask=mask, other=0.0)\n\n    o = q * k * v * scale + h\n    tl.store(o_ptr + o_offsets, o, mask=mask)\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(\n    q_ptr, k_ptr, v_ptr, h_ptr, dh_ptr, \n    q_stride0, q_stride1, k_stride0, k_stride1, v_stride0, v_stride1, h_stride0, h_stride1, dh_stride0, dh_stride1, \n    N_CTX, N_HEAD, N_DIM, scale, decay, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N_CTX\n\n    q_offsets = pid * q_stride0 + tl.arange(0, N_DIM)\n    k_offsets = pid * k_stride0 + tl.arange(0, N_DIM)\n    v_offsets = pid * v_stride0 + tl.arange(0, N_DIM)\n    h_offsets = pid * h_stride0 + tl.arange(0, N_DIM)\n    dh_offsets = pid * dh_stride0 + tl.arange(0, N_DIM)\n\n    q = tl.load(q_ptr + q_offsets, mask=mask, other=0.0)\n    k = tl.load(k_ptr + k_offsets, mask=mask, other=0.0)\n    v = tl.load(v_ptr + v_offsets, mask=mask, other=0.0)\n    h = tl.load(h_ptr + h_offsets, mask=mask, other=0.0)\n\n    dh = tl.load(dh_ptr + dh_offsets, mask=mask, other=0.0)\n    dh = dh * scale\n\n    if pid > 0:\n        prev_h = tl.load(h_ptr + (pid - 1) * h_stride0 + tl.arange(0, N_DIM), other=0.0)\n        dh += prev_h * decay\n\n    tl.store(dh_ptr + dh_offsets, dh, mask=mask)\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(\n    q_ptr, k_ptr, v_ptr, h_ptr, dh_ptr, dq_ptr, dk_ptr, dv_ptr, \n    q_stride0, q_stride1, k_stride0, k_stride1, v_stride0, v_stride1, h_stride0, h_stride1, \n    dh_stride0, dh_stride1, dq_stride0, dq_stride1, dk_stride0, dk_stride1, dv_stride0, dv_stride1, \n    N_CTX, N_HEAD, N_DIM, scale, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N_CTX\n\n    q_offsets = pid * q_stride0 + tl.arange(0, N_DIM)\n    k_offsets = pid * k_stride0 + tl.arange(0, N_DIM)\n    v_offsets = pid * v_stride0 + tl.arange(0, N_DIM)\n    h_offsets = pid * h_stride0 + tl.arange(0, N_DIM)\n    dh_offsets = pid * dh_stride0 + tl.arange(0, N_DIM)\n    dq_offsets = pid * dq_stride0 + tl.arange(0, N_DIM)\n    dk_offsets = pid * dk_stride0 + tl.arange(0, N_DIM)\n    dv_offsets = pid * dv_stride0 + tl.arange(0, N_DIM)\n\n    q = tl.load(q_ptr + q_offsets, mask=mask, other=0.0)\n    k = tl.load(k_ptr + k_offsets, mask=mask, other=0.0)\n    v = tl.load(v_ptr + v_offsets, mask=mask, other=0.0)\n    h = tl.load(h_ptr + h_offsets, mask=mask, other=0.0)\n    dh = tl.load(dh_ptr + dh_offsets, mask=mask, other=0.0)\n\n    dq = dh * k * v * scale\n    dk = dh * q * v * scale\n    dv = dh * q * k * scale\n\n    tl.store(dq_ptr + dq_offsets, dq, mask=mask)\n    tl.store(dk_ptr + dk_offsets, dk, mask=mask)\n    tl.store(dv_ptr + dv_offsets, dv, mask=mask)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_fwd_kernel_h(\n    k, v, h, h0, ht, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr\n):\n    # Triton kernel code for forward pass of chunk retention with initial and final state handling\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if i_t == NT - 1 and (T % BT) != 0:\n            d_b = tl.math.exp2((T % BT) * b_b)\n            d_i = tl.math.exp2(((T % BT) - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_i[:, None]).to(b_k.dtype), allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_fwd_kernel_o(\n    q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr\n):\n    # Triton kernel code for forward pass of chunk retention with output scaling\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot((b_q * d_i[:, None]).to(b_q.dtype), b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s *= d_s\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(\n    q, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    # Triton kernel code for backward pass of chunk retention, computing gradients for hidden state\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, (b_do * d_i[:, None]).to(b_q.dtype), allow_tf32=False)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(\n    q, k, v, h, do, dh, dq, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    # Triton kernel code for backward pass of chunk retention, computing gradients for q, k, v\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    d_q = (d_q * scale).to(d_q.dtype)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k*n_bh+i_bh)*s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = (b_ds * d_s).to(b_q.dtype)\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n\ndef chunk_fwd_h_fn(k, v, BT, initial_state, output_final_state):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    final_state = None\n    if output_final_state:\n        final_state = k.new_empty(B, H, K, V, dtype=torch.float32)\n    BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n    NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n    h = k.new_empty(B, H, NT * K, V)\n    grid = (NK, NV, B * H)\n    chunk_retention_fwd_kernel_h[grid](\n        k, v, h, initial_state, final_state,\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2),\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n        USE_INITIAL_STATE=initial_state is not None,\n        STORE_FINAL_STATE=output_final_state\n    )\n    return h, final_state\n\n\ndef chunk_fwd_o_fn(h, q, k, v, BT, scale):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    o = torch.empty_like(v)\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NV = triton.cdiv(V, BV)\n    NT = triton.cdiv(T, BT)\n    grid = (NV, NT, B * H)\n    chunk_retention_fwd_kernel_o[grid](\n        q, k, v, h, o,\n        q.stride(1), q.stride(2), q.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2),\n        scale,\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV\n    )\n    return o\n\n\ndef chunk_bwd_dh_fn(do, q, k, v, BT, scale):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    BT = 64\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n    dh = k.new_empty(B, H, NT * K, V)\n    grid = (NK, NV, B * H)\n    chunk_retention_bwd_kernel_dh[grid](\n        q, do, dh,\n        q.stride(1), q.stride(2), q.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        dh.stride(1), dh.stride(2),\n        scale,\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT\n    )\n    return dh\n\n\ndef chunk_bwd_dqkv_fn(do, q, k, v, h, dh, scale):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    BT = 64\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NT, NK = triton.cdiv(T, BT), triton.cdiv(K, BK)\n    grid = (NK, NT, B * H)\n    dq = torch.empty_like(q)\n    dk = torch.empty_like(k)\n    dv = v.new_empty(NK, *v.shape)\n    chunk_retention_bwd_kernel_dqkv[grid](\n        q, k, v, h, do, dh, dq, dk, dv,\n        q.stride(1), q.stride(2), q.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2),\n        scale,\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT\n    )\n    dv = dv.sum(0)\n    return dq, dk, dv\n\n\nclass ChunkRetentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, initial_state, output_final_state, scale, checkpoint_level):\n        BT = 64\n        h, final_state = chunk_fwd_h_fn(k, v, BT, initial_state, output_final_state)\n        o = chunk_fwd_o_fn(h, q, k, v, BT, scale)\n        if checkpoint_level == 1:\n            h = None\n        ctx.save_for_backward(q, k, v, h, initial_state)\n        ctx.BT, ctx.scale = BT, scale\n        return o.to(q.dtype), final_state\n\n    @staticmethod\n    def backward(ctx, do, d_ht=None):\n        BT, scale = ctx.BT, ctx.scale\n        q, k, v, h, initial_state = ctx.saved_tensors\n        if h is None:\n            h, _ = chunk_fwd_h_fn(k, v, BT, initial_state, False)\n        dh = chunk_bwd_dh_fn(do, q, k, v, BT, scale)\n        dq, dk, dv = chunk_bwd_dqkv_fn(do, q, k, v, h, dh, scale)\n        return dq.to(q.dtype), dk.to(k.dtype), dv.to(v.dtype), None, None, None, None\n\n\ndef chunk_retention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False,\n    scale: float = None,\n    checkpoint_level: int = 1\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    assert checkpoint_level in [0, 1], \"checkpoint_level must be 0, 1\"\n    assert q.dim() == k.dim() == v.dim() == 4, \"q, k, v must have 4 dimensions (b, h, l, d)\"\n    assert q.dtype == k.dtype == v.dtype, \"q, k, v must have the same dtype\"\n    if scale is None:\n        scale = q.size(-1) ** -0.5\n    o, final_state = ChunkRetentionFunction.apply(\n        q, k, v, initial_state, output_final_state, scale, checkpoint_level)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided Triton kernel and its wrapper function are designed to compute the element-wise square of a 2D input tensor. The `square_kernel` function is a JIT-compiled Triton kernel that takes input and output pointers along with metadata to perform this computation. It iterates over each row independently using a parallel execution model. The BLOCK_SIZE ensures memory alignment and efficient memory access. The main logic is loading a row, squaring each element, and storing the result. The `square` function initializes the output tensor, determines the block size and number of warps based on input dimensions, and launches the kernel for each row.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef square_kernel(\n    input_ptr,  # Pointer to the input tensor\n    output_ptr,  # Pointer to the output tensor\n    n_rows,  # Number of rows in the input tensor\n    n_cols,  # Number of columns in the input tensor\n    BLOCK_SIZE: tl.constexpr  # Block size for parallel execution\n):\n    # Compute the row index for this thread block\n    row_idx = tl.program_id(0)\n    \n    # Check if the row index is within bounds\n    if row_idx < n_rows:\n        # Compute the column indices for this block\n        col_indices = tl.arange(0, BLOCK_SIZE)\n        \n        # Load the input row into a Triton block\n        input_row = tl.load(input_ptr + row_idx * n_cols + col_indices, mask=col_indices < n_cols)\n        \n        # Compute the square of each element\n        output_row = input_row * input_row\n        \n        # Store the result back to the output tensor\n        tl.store(output_ptr + row_idx * n_cols + col_indices, output_row, mask=col_indices < n_cols)\n\nimport triton\nimport triton.runtime as triton_runtime\nimport torch\n\ndef square(input_tensor: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is on the GPU\n    assert input_tensor.is_cuda, \"Input tensor must be on the GPU\"\n    \n    # Get the dimensions of the input tensor\n    n_rows, n_cols = input_tensor.shape\n    \n    # Initialize the output tensor with the same shape and data type as the input tensor\n    output_tensor = torch.empty_like(input_tensor)\n    \n    # Determine the block size for parallel execution\n    BLOCK_SIZE = 128  # This can be adjusted based on the GPU architecture\n    \n    # Determine the number of warps needed\n    num_warps = (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the kernel for each row\n    square_kernel[(n_rows,)](input_tensor, output_tensor, n_rows, n_cols, BLOCK_SIZE, num_warps=num_warps)\n    \n    return output_tensor\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef square_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # The rows of the softmax are independent, so we parallelize across those\n    row_idx = tl.program_id(0)\n    # The stride represents how much we need to increase the pointer to advance 1 row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    # The block size is the next power of two greater than n_cols, so we can fit each\n    # row in a single block\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n\n    square_output = row * row\n    \n    # Write back output to DRAM\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, square_output, mask=col_offsets < n_cols)\n\n\ndef square(x):\n    n_rows, n_cols = x.shape\n    # The block size is the smallest power of two greater than the number of columns in `x`\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Another trick we can use is to ask the compiler to use more threads per row by\n    # increasing the number of warps (`num_warps`) over which each row is distributed.\n    # You will see in the next tutorial how to auto-tune this value in a more natural\n    # way so you don't have to come up with manual heuristics yourself.\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    y = torch.empty_like(x)\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n    # f the input matrix\n    square_kernel[(n_rows, )](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_cols,\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return y\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code includes several kernels designed for performing computations related to a specific type of block matrix operation. Here's a summary of the main components:\n\n            1. **chunk_gla_fwd_A_kernel_intra_sub_inter**:\n               - Inputs: `q`, `k`, `g`, `A`, and several scalar configurations.\n               - Operation: This kernel processes blocks of matrix `A` based on inputs `q`, `k`, and `g`. It performs a dot product operation on selected blocks of matrices, adjusting them with scaling and other transformations.\n               - Output: It stores the result back into the matrix `A`.\n\n            2. **chunk_gla_fwd_A_kernel_intra_sub_intra**:\n               - Similar to the first kernel but handles intra-sub block computations. It loads and processes matrix sections, performing transformations based on conditions.\n\n            3. **chunk_gla_fwd_A_kernel_intra_sub_intra_split**:\n               - Splits the computation over the dimension K into smaller chunks to handle larger sizes efficiently.\n               - Uses an additional temporary matrix `A_intra` for intermediate computations.\n\n            4. **chunk_gla_fwd_A_kernel_intra_sub_intra_merge**:\n               - Merges results from the split kernel back into the primary matrix `A`.\n               - Iterates over blocks and aggregates them to form the complete result.\n\n            5. **chunk_gla_fwd_kernel_o**:\n               - Handles forward computation to generate output `o` based on matrices `q`, `v`, `g_cumsum`, `A`, and `h`.\n               - Performs a sequence of matrix operations followed by storing the results into `o`.\n\n            Wrapper Functions:\n            - `chunk_fwd_intra_gated_gk_fn`: Manages the execution of the first set of kernels to compute matrix `A`.\n            - `chunk_fwd_o_gated_gk_fn`: Manages the computation and generation of output `o`.\n\n            The kernels make use of Triton-specific operations like `tl.load`, `tl.store`, and `tl.dot` for efficient execution on modern hardware, especially GPUs.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(\n    q_ptr, k_ptr, g_ptr, A_ptr,\n    q_stride0, q_stride1, k_stride0, k_stride1, g_stride0, g_stride1, A_stride0, A_stride1,\n    BLOCK_SIZE: tl.constexpr, SCALE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    q_offsets = q_stride0 * offsets\n    k_offsets = k_stride0 * offsets\n    g_offsets = g_stride0 * offsets\n    A_offsets = A_stride0 * offsets\n\n    q = tl.load(q_ptr + q_offsets)\n    k = tl.load(k_ptr + k_offsets)\n    g = tl.load(g_ptr + g_offsets)\n\n    dot_product = tl.dot(q, k, allow_tf32=True)\n    scaled_dot_product = dot_product * SCALE\n    adjusted_dot_product = scaled_dot_product + g\n\n    tl.store(A_ptr + A_offsets, adjusted_dot_product)\n\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(\n    q_ptr, k_ptr, g_ptr, A_ptr,\n    q_stride0, q_stride1, k_stride0, k_stride1, g_stride0, g_stride1, A_stride0, A_stride1,\n    BLOCK_SIZE: tl.constexpr, SCALE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    q_offsets = q_stride0 * offsets\n    k_offsets = k_stride0 * offsets\n    g_offsets = g_stride0 * offsets\n    A_offsets = A_stride0 * offsets\n\n    q = tl.load(q_ptr + q_offsets)\n    k = tl.load(k_ptr + k_offsets)\n    g = tl.load(g_ptr + g_offsets)\n\n    dot_product = tl.dot(q, k, allow_tf32=True)\n    scaled_dot_product = dot_product * SCALE\n    adjusted_dot_product = scaled_dot_product + g\n\n    tl.store(A_ptr + A_offsets, adjusted_dot_product)\n\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(\n    q_ptr, k_ptr, g_ptr, A_ptr, A_intra_ptr,\n    q_stride0, q_stride1, k_stride0, k_stride1, g_stride0, g_stride1, A_stride0, A_stride1, A_intra_stride0, A_intra_stride1,\n    BLOCK_SIZE: tl.constexpr, SCALE: tl.constexpr, K_CHUNK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    q_offsets = q_stride0 * offsets\n    k_offsets = k_stride0 * offsets\n    g_offsets = g_stride0 * offsets\n    A_offsets = A_stride0 * offsets\n    A_intra_offsets = A_intra_stride0 * offsets\n\n    q = tl.load(q_ptr + q_offsets)\n    k = tl.load(k_ptr + k_offsets)\n    g = tl.load(g_ptr + g_offsets)\n\n    for k_chunk_start in range(0, K_CHUNK_SIZE, BLOCK_SIZE):\n        k_chunk_offsets = k_chunk_start + tl.arange(0, BLOCK_SIZE)\n        k_chunk = tl.load(k_ptr + k_chunk_offsets)\n\n        dot_product = tl.dot(q, k_chunk, allow_tf32=True)\n        scaled_dot_product = dot_product * SCALE\n        adjusted_dot_product = scaled_dot_product + g\n\n        tl.store(A_intra_ptr + A_intra_offsets + k_chunk_offsets, adjusted_dot_product)\n\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(\n    A_ptr, A_intra_ptr,\n    A_stride0, A_stride1, A_intra_stride0, A_intra_stride1,\n    BLOCK_SIZE: tl.constexpr, K_CHUNK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    A_offsets = A_stride0 * offsets\n    A_intra_offsets = A_intra_stride0 * offsets\n\n    for k_chunk_start in range(0, K_CHUNK_SIZE, BLOCK_SIZE):\n        k_chunk_offsets = k_chunk_start + tl.arange(0, BLOCK_SIZE)\n        A_intra_chunk = tl.load(A_intra_ptr + A_intra_offsets + k_chunk_offsets)\n\n        tl.atomic_add(A_ptr + A_offsets + k_chunk_offsets, A_intra_chunk)\n\n@triton.jit\ndef chunk_gla_fwd_kernel_o(\n    q_ptr, v_ptr, g_cumsum_ptr, A_ptr, h_ptr, o_ptr,\n    q_stride0, q_stride1, v_stride0, v_stride1, g_cumsum_stride0, g_cumsum_stride1, A_stride0, A_stride1, h_stride0, h_stride1, o_stride0, o_stride1,\n    BLOCK_SIZE: tl.constexpr, SCALE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    q_offsets = q_stride0 * offsets\n    v_offsets = v_stride0 * offsets\n    g_cumsum_offsets = g_cumsum_stride0 * offsets\n    A_offsets = A_stride0 * offsets\n    h_offsets = h_stride0 * offsets\n    o_offsets = o_stride0 * offsets\n\n    q = tl.load(q_ptr + q_offsets)\n    v = tl.load(v_ptr + v_offsets)\n    g_cumsum = tl.load(g_cumsum_ptr + g_cumsum_offsets)\n    A = tl.load(A_ptr + A_offsets)\n    h = tl.load(h_ptr + h_offsets)\n\n    dot_product = tl.dot(q, v, allow_tf32=True)\n    scaled_dot_product = dot_product * SCALE\n    adjusted_dot_product = scaled_dot_product + g_cumsum\n    final_result = adjusted_dot_product * A * h\n\n    tl.store(o_ptr + o_offsets, final_result)\n\ndef chunk_fwd_intra_gated_gk_fn(q, k, g, A, BLOCK_SIZE, SCALE):\n    grid = (A.shape[0] // BLOCK_SIZE, 1)\n    chunk_gla_fwd_A_kernel_intra_sub_inter[grid](\n        q, k, g, A,\n        q.stride(0), q.stride(1), k.stride(0), k.stride(1), g.stride(0), g.stride(1), A.stride(0), A.stride(1),\n        BLOCK_SIZE, SCALE\n    )\n\ndef chunk_fwd_o_gated_gk_fn(q, v, g_cumsum, A, h, o, BLOCK_SIZE, SCALE):\n    grid = (o.shape[0] // BLOCK_SIZE, 1)\n    chunk_gla_fwd_kernel_o[grid](\n        q, v, g_cumsum, A, h, o,\n        q.stride(0), q.stride(1), v.stride(0), v.stride(1), g_cumsum.stride(0), g_cumsum.stride(1), A.stride(0), A.stride(1), h.stride(0), h.stride(1), o.stride(0), o.stride(1),\n        BLOCK_SIZE, SCALE\n    )\n\nimport torch\n\n# Example tensors\nq = torch.randn(1024, 64, device='cuda')\nk = torch.randn(1024, 64, device='cuda')\ng = torch.randn(1024, 64, device='cuda')\nA = torch.zeros(1024, 64, device='cuda')\nv = torch.randn(1024, 64, device='cuda')\ng_cumsum = torch.cumsum(g, dim=0)\nh = torch.randn(1024, 64, device='cuda')\no = torch.zeros(1024, 64, device='cuda')\n\n# Parameters\nBLOCK_SIZE = 128\nSCALE = 0.1\n\n# Compute A\nchunk_fwd_intra_gated_gk_fn(q, k, g, A, BLOCK_SIZE, SCALE)\n\n# Compute o\nchunk_fwd_o_gated_gk_fn(q, v, g_cumsum, A, h, o, BLOCK_SIZE, SCALE)\n\nprint(o)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BC\", \"BK\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_i * BC) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BK\", \"BT\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr\n):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_j * BC\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n\n    p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n    p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T-i_t*BT-i_i*BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BC\", \"BK\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr\n):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    n_bh = tl.num_programs(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BC\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n    p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T-i_t*BT-i_i*BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BC\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(\n    A,\n    A2,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    NK: tl.constexpr\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(A + (i_bh + i_k*n_bh) * T * BC, (T, BC), (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BK\", \"BV\", \"BT\"],\n)\n@triton.jit\ndef chunk_gla_fwd_kernel_o(\n    q,\n    v,\n    g,\n    h,\n    o,\n    A,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = (b_q * tl.exp(b_g)).to(b_q.dtype)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\n\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.).to(b_v.dtype)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\ndef chunk_fwd_intra_gated_gk_fn(q, k, g, scale, BT):\n    B, H, T, K = q.shape\n    BC = 16\n    NC = triton.cdiv(BT, BC)\n    NT = triton.cdiv(T, BT)\n\n    BK = min(64, triton.next_power_of_2(K))\n    A = q.new_empty(B, H, T, BT, dtype=torch.float32)\n    grid = (NT, NC * NC, B * H)\n    chunk_gla_fwd_A_kernel_intra_sub_inter[grid](\n        q, k, g, A,\n        k.stride(1), k.stride(2),\n        scale,\n        T=T, K=K, BT=BT, BC=BC, BK=BK, NC=NC\n    )\n    grid = (NT, NC, B * H)\n    if K <= 256:\n        BK = triton.next_power_of_2(K)\n        chunk_gla_fwd_A_kernel_intra_sub_intra[grid](\n            q, k, g, A,\n            k.stride(1), k.stride(2),\n            scale,\n            T=T, K=K, BT=BT, BC=BC, BK=BK\n        )\n    else:\n        BK = 128\n        NK = triton.cdiv(K, BK)\n        A_intra = q.new_empty(NK, B, H, BT, BC, dtype=torch.float32)\n        grid = (NK, NT * NC, B * H)\n        chunk_gla_fwd_A_kernel_intra_sub_intra_split[grid](\n            q, k, g, A_intra,\n            k.stride(1), k.stride(2),\n            scale,\n            T=T, K=K, BT=BT, BC=BC, BK=BK, NC=NC\n        )\n        grid = (NT, NC, B * H)\n        chunk_gla_fwd_A_kernel_intra_sub_intra_merge[grid](\n            A_intra, A,\n            T=T, BT=BT, BC=BC, NK=NK\n        )\n    return A\n\ndef chunk_fwd_o_gated_gk_fn(q, v, g_cumsum, A, h, BT, scale):\n    B, H, T, K, V = *q.shape, v.shape[-1]\n    BK = min(32, triton.next_power_of_2(K))\n    BV = min(32, triton.next_power_of_2(V))\n    NV = triton.cdiv(V, BV)\n    NT = triton.cdiv(T, BT)\n\n    grid = (NV, NT, B * H)\n    o = torch.empty_like(v)\n    chunk_gla_fwd_kernel_o[grid](\n        q, v, g_cumsum, h, o, A,\n        q.stride(1), q.stride(2),\n        v.stride(1), v.stride(2),\n        h.stride(1), h.stride(2),\n        scale,\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV\n    )\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator implements a custom layer normalization for both the forward and backward passes. The forward pass function `_layer_norm_fwd_1pass_kernel` normalizes input tensors and applies weights and biases. It supports features like residual connections and RMS normalization, controlled by input flags. The backward pass `_layer_norm_bwd_kernel` computes gradients for the inputs, weights, and biases, also optionally recomputing the forward output for more accurate gradients. Inputs are typically 2D tensors, and various configurations are supported for optimization via Triton's autotuning.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X,  # input tensor\n    W,  # weight tensor\n    B,  # bias tensor\n    Y,  # output tensor\n    RES,  # residual tensor (optional)\n    RMS,  # RMS normalization flag\n    EPS,  # epsilon for numerical stability\n    stride,  # stride of the input tensor\n    N,  # number of elements in the last dimension\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Load data\n    x = tl.load(X + offsets, mask=mask, other=0.0)\n    w = tl.load(W + offsets, mask=mask, other=0.0)\n    b = tl.load(B + offsets, mask=mask, other=0.0)\n\n    # Compute mean and variance\n    if RMS:\n        var = tl.sum(x * x, axis=0) / N\n        inv_std = 1.0 / tl.sqrt(var + EPS)\n    else:\n        mean = tl.sum(x, axis=0) / N\n        var = tl.sum((x - mean) * (x - mean), axis=0) / N\n        inv_std = 1.0 / tl.sqrt(var + EPS)\n\n    # Normalize and apply weights and biases\n    x_norm = (x - mean) * inv_std if not RMS else x * inv_std\n    y = x_norm * w + b\n\n    # Add residual connection if provided\n    if RES is not None:\n        res = tl.load(RES + offsets, mask=mask, other=0.0)\n        y += res\n\n    # Store the result\n    tl.store(Y + offsets, y, mask=mask)\n\n@triton.jit\ndef _layer_norm_bwd_kernel(\n    dY,  # gradient of the output tensor\n    X,  # input tensor\n    W,  # weight tensor\n    B,  # bias tensor\n    DX,  # gradient of the input tensor\n    DW,  # gradient of the weight tensor\n    DB,  # gradient of the bias tensor\n    RMS,  # RMS normalization flag\n    EPS,  # epsilon for numerical stability\n    stride,  # stride of the input tensor\n    N,  # number of elements in the last dimension\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Load data\n    x = tl.load(X + offsets, mask=mask, other=0.0)\n    w = tl.load(W + offsets, mask=mask, other=0.0)\n    b = tl.load(B + offsets, mask=mask, other=0.0)\n    dy = tl.load(dY + offsets, mask=mask, other=0.0)\n\n    # Compute mean and variance\n    if RMS:\n        var = tl.sum(x * x, axis=0) / N\n        inv_std = 1.0 / tl.sqrt(var + EPS)\n    else:\n        mean = tl.sum(x, axis=0) / N\n        var = tl.sum((x - mean) * (x - mean), axis=0) / N\n        inv_std = 1.0 / tl.sqrt(var + EPS)\n\n    # Normalize and apply weights and biases\n    x_norm = (x - mean) * inv_std if not RMS else x * inv_std\n\n    # Compute gradients\n    dx_norm = dy * w\n    dvar = tl.sum(dx_norm * (x - mean) * -0.5 * inv_std**3, axis=0)\n    dmean = tl.sum(dx_norm * -inv_std, axis=0) + dvar * -2.0 * mean / N\n    dx = dx_norm * inv_std + dvar * 2.0 * (x - mean) / N + dmean / N\n    dw = tl.sum(dy * x_norm, axis=0)\n    db = tl.sum(dy, axis=0)\n\n    # Store the gradients\n    tl.store(DX + offsets, dx, mask=mask)\n    tl.store(DW + offsets, dw, mask=mask)\n    tl.store(DB + offsets, db, mask=mask)\n\nimport torch\n\ndef layer_norm_fwd_1pass(x, w, b, res=None, rms=False, eps=1e-5):\n    assert x.is_cuda and w.is_cuda and b.is_cuda\n    assert x.shape[-1] == w.shape[0] == b.shape[0]\n    N = x.shape[-1]\n    BLOCK_SIZE = 256\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    y = torch.empty_like(x)\n    _layer_norm_fwd_1pass_kernel[grid](\n        x, w, b, y, res, rms, eps, x.stride(-1), N, BLOCK_SIZE\n    )\n    return y\n\ndef layer_norm_bwd_1pass(dy, x, w, b, rms=False, eps=1e-5):\n    assert dy.is_cuda and x.is_cuda and w.is_cuda and b.is_cuda\n    assert dy.shape == x.shape and x.shape[-1] == w.shape[0] == b.shape[0]\n    N = x.shape[-1]\n    BLOCK_SIZE = 256\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    dx = torch.empty_like(x)\n    dw = torch.empty_like(w)\n    db = torch.empty_like(b)\n    _layer_norm_bwd_kernel[grid](\n        dy, x, w, b, dx, dw, db, rms, eps, x.stride(-1), N, BLOCK_SIZE\n    )\n    return dx, dw, db\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_RESIDUAL\", \"STORE_RESIDUAL_OUT\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean, Rstd, \n    stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, \n    N, eps, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, \n    HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_BIAS: tl.constexpr\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n\n\ndef _layer_norm_fwd(\n    x, weight, bias, eps, residual=None, out_dtype=None, \n    residual_dtype=None, is_rms_norm=False\n):\n    if residual is not None:\n        residual_dtype = residual.dtype\n    M, N = x.shape\n    assert x.stride(-1) == 1\n    if residual is not None:\n        assert residual.stride(-1) == 1\n        assert residual.shape == (M, N)\n    assert weight.shape == (N,)\n    assert weight.stride(-1) == 1\n    if bias is not None:\n        assert bias.stride(-1) == 1\n        assert bias.shape == (N,)\n    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)\n    assert y.stride(-1) == 1\n    if residual is not None or (residual_dtype is not None and residual_dtype != x.dtype):\n        residual_out = torch.empty(M, N, device=x.device, dtype=residual_dtype)\n        assert residual_out.stride(-1) == 1\n    else:\n        residual_out = None\n    mean = torch.empty((M,), dtype=torch.float32, device=\"cuda\") if not is_rms_norm else None\n    rstd = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _layer_norm_fwd_1pass_kernel[(M,)](\n            x, y, weight, bias, residual, residual_out, \n            mean, rstd, x.stride(0), y.stride(0), \n            residual.stride(0) if residual is not None else 0, \n            residual_out.stride(0) if residual_out is not None else 0, \n            N, eps, is_rms_norm, BLOCK_N, residual is not None, \n            residual_out is not None, bias is not None\n        )\n    return y, mean, rstd, residual_out if residual_out is not None else x\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_DRESIDUAL\", \"STORE_DRESIDUAL\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n@triton.heuristics({\"RECOMPUTE_OUTPUT\": lambda args: args[\"Y\"] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(\n    X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, DRESIDUAL_IN, \n    Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row, \n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, \n    N, eps, rows_per_program, IS_RMS_NORM: tl.constexpr, \n    BLOCK_N: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, \n    STORE_DRESIDUAL: tl.constexpr, HAS_BIAS: tl.constexpr, \n    RECOMPUTE_OUTPUT: tl.constexpr\n):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row) if Mean is not None else 0.0  # \u4fee\u6539\u6b64\u884c\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n\n\ndef _layer_norm_bwd(\n    dy, x, weight, bias, eps, mean, rstd, dresidual=None, \n    has_residual=False, is_rms_norm=False, x_dtype=None, recompute_output=False\n):\n    M, N = x.shape\n    assert x.stride(-1) == 1\n    assert dy.stride(-1) == 1\n    assert dy.shape == (M, N)\n    if dresidual is not None:\n        assert dresidual.stride(-1) == 1\n        assert dresidual.shape == (M, N)\n    assert weight.shape == (N,)\n    assert weight.stride(-1) == 1\n    if bias is not None:\n        assert bias.stride(-1) == 1\n        assert bias.shape == (N,)\n    dx = (\n        torch.empty_like(x)\n        if x_dtype is None\n        else torch.empty(M, N, dtype=x_dtype, device=x.device)\n    )\n    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None\n    y = torch.empty(M, N, dtype=dy.dtype, device=dy.device) if recompute_output else None\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count\n    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)\n    _db = (\n        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)\n        if bias is not None\n        else None\n    )\n    rows_per_program = math.ceil(M / sm_count)\n    grid = (sm_count,)\n    with torch.cuda.device(x.device.index):\n        _layer_norm_bwd_kernel[grid](\n            x, weight, bias, y, dy, dx, _dw, _db, dresidual, \n            dresidual_in, mean, rstd, x.stride(0), \n            0 if not recompute_output else y.stride(0), dy.stride(0), \n            dx.stride(0), dresidual.stride(0) if dresidual is not None else 0, \n            dresidual_in.stride(0) if dresidual_in is not None else 0, \n            M, N, eps, rows_per_program, is_rms_norm, BLOCK_N, \n            dresidual is not None, dresidual_in is not None, bias is not None\n        )\n    dw = _dw.sum(0).to(weight.dtype)\n    db = _db.sum(0).to(bias.dtype) if bias is not None else None\n    if has_residual and dx.dtype == x.dtype:\n        dresidual_in = dx\n    return (dx, dw, db, dresidual_in) if not recompute_output else (dx, dw, db, dresidual_in, y)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel and wrapper function are designed to copy keys and values from tensors `K` and `V` to the blocked cache tensors `KCache` and `VCache` during the decoding stage of a model. \n            The kernel function `_copy_to_kvcache_seqlen1_kernel` is defined using the Triton JIT compiler, allowing it to efficiently perform memory operations with different types of cache layouts.\n            The function `copy_kv_to_blocked_cache` acts as a wrapper that prepares input data and parameters for the Triton kernel call.\n            The kernel copies segments of the input `K` and `V` to the respective locations in `KCache` and `VCache`, based on the computed offsets and cache layout.\n            Grid dimensions `(bsz, num_kv_heads)` are used to launch the kernel, where `bsz` is the batch size and `num_kv_heads` is the number of key-value attention heads.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _copy_to_kvcache_seqlen1_kernel(\n    K_ptr, V_ptr, KCache_ptr, VCache_ptr,\n    bsz, num_kv_heads, head_dim, cache_head_dim,\n    KCache_stride_b, KCache_stride_h, KCache_stride_d,\n    VCache_stride_b, VCache_stride_h, VCache_stride_d,\n    K_stride_b, K_stride_h, K_stride_d,\n    V_stride_b, V_stride_h, V_stride_d,\n    cache_offset,\n    BLOCK_HEAD: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // num_kv_heads\n    hid = pid % num_kv_heads\n\n    # Compute the base pointers for K and V\n    K_base = K_ptr + bid * K_stride_b + hid * K_stride_h\n    V_base = V_ptr + bid * V_stride_b + hid * V_stride_h\n\n    # Compute the base pointers for KCache and VCache\n    KCache_base = KCache_ptr + bid * KCache_stride_b + hid * KCache_stride_h + cache_offset * KCache_stride_d\n    VCache_base = VCache_ptr + bid * VCache_stride_b + hid * VCache_stride_h + cache_offset * VCache_stride_d\n\n    # Iterate over the head dimension in blocks\n    for d in range(0, head_dim, BLOCK_D):\n        K_offset = d * K_stride_d\n        V_offset = d * V_stride_d\n        KCache_offset = d * KCache_stride_d\n        VCache_offset = d * VCache_stride_d\n\n        K_block_ptr = K_base + K_offset\n        V_block_ptr = V_base + V_offset\n        KCache_block_ptr = KCache_base + KCache_offset\n        VCache_block_ptr = VCache_base + VCache_offset\n\n        for i in range(BLOCK_D):\n            if d + i < head_dim:\n                K_val = tl.load(K_block_ptr + i * K_stride_d)\n                V_val = tl.load(V_block_ptr + i * V_stride_d)\n                tl.store(KCache_block_ptr + i * KCache_stride_d, K_val)\n                tl.store(VCache_block_ptr + i * VCache_stride_d, V_val)\n\nimport torch\n\ndef copy_kv_to_blocked_cache(K, V, KCache, VCache, cache_offset, bsz, num_kv_heads, head_dim, cache_head_dim):\n    assert K.shape == (bsz, num_kv_heads, head_dim), \"K tensor shape mismatch\"\n    assert V.shape == (bsz, num_kv_heads, head_dim), \"V tensor shape mismatch\"\n    assert KCache.shape == (bsz, num_kv_heads, cache_head_dim), \"KCache tensor shape mismatch\"\n    assert VCache.shape == (bsz, num_kv_heads, cache_head_dim), \"VCache tensor shape mismatch\"\n\n    # Define grid and block dimensions\n    grid = (bsz * num_kv_heads, )\n    block = (1, )\n\n    # Launch the kernel\n    _copy_to_kvcache_seqlen1_kernel[grid, block](\n        K, V, KCache, VCache,\n        bsz, num_kv_heads, head_dim, cache_head_dim,\n        KCache.stride(0), KCache.stride(1), KCache.stride(2),\n        VCache.stride(0), VCache.stride(1), VCache.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        cache_offset,\n        BLOCK_HEAD=1, BLOCK_D=32  # Adjust BLOCK_D as needed\n    )\n\nimport torch\n\n# Example tensors\nbsz = 2\nnum_kv_heads = 4\nhead_dim = 64\ncache_head_dim = 128\ncache_offset = 0\n\nK = torch.randn(bsz, num_kv_heads, head_dim, device='cuda')\nV = torch.randn(bsz, num_kv_heads, head_dim, device='cuda')\nKCache = torch.zeros(bsz, num_kv_heads, cache_head_dim, device='cuda')\nVCache = torch.zeros(bsz, num_kv_heads, cache_head_dim, device='cuda')\n\n# Call the wrapper function\ncopy_kv_to_blocked_cache(K, V, KCache, VCache, cache_offset, bsz, num_kv_heads, head_dim, cache_head_dim)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n# supports two types of cache layouts\n# 1. [num_blocks, num_kv_heads, block_size, head_dim]\n# 2. [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n@triton.jit\ndef _copy_to_kvcache_seqlen1_kernel(\n    K,\n    V,\n    KCache,\n    VCache,\n    BLOCK_TABLES,\n    context_lengths,\n    stride_kt,\n    stride_kh,\n    stride_kd,\n    stride_vt,\n    stride_vh,\n    stride_vd,\n    stride_kcb,\n    stride_kch,\n    stride_kcsplit_x,\n    stride_kcs,\n    stride_kcd,\n    stride_vcb,\n    stride_vch,\n    stride_vcs,\n    stride_vcd,\n    stride_bts,\n    stride_btb,\n    block_size,\n    HEAD_DIM: tl.constexpr,\n    KCACHE_X: tl.constexpr,\n):\n    cur_seq_idx = tl.program_id(0)\n    cur_kv_head_idx = tl.program_id(1)\n\n    past_kv_seq_len = tl.load(context_lengths + cur_seq_idx) - 1\n    last_bt_block_idx = past_kv_seq_len // block_size\n    block_table_ptr = BLOCK_TABLES + cur_seq_idx * stride_bts\n    block_id = tl.load(block_table_ptr + last_bt_block_idx * stride_btb)\n    offsets_in_last_block = past_kv_seq_len % block_size\n\n    range_x = tl.arange(0, KCACHE_X)\n    offsets_dmodel_x_partition = tl.arange(0, KCACHE_X)\n\n    for split_x in tl.static_range(HEAD_DIM // KCACHE_X):\n        offsets_dmodel_x_partition = tl.arange(split_x * KCACHE_X, (split_x + 1) * KCACHE_X)\n        offsets_k = cur_seq_idx * stride_kt + cur_kv_head_idx * stride_kh + offsets_dmodel_x_partition * stride_kd\n        k = tl.load(K + offsets_k)\n        offsets_v = cur_seq_idx * stride_vt + cur_kv_head_idx * stride_vh + offsets_dmodel_x_partition * stride_vd\n        v = tl.load(V + offsets_v)\n\n        offsets_kcache = (\n            block_id * stride_kcb\n            + cur_kv_head_idx * stride_kch\n            + split_x * stride_kcsplit_x\n            + offsets_in_last_block * stride_kcs\n            + range_x\n        )\n        tl.store(KCache + offsets_kcache, k)\n        offsets_vcache = (\n            block_id * stride_vcb\n            + cur_kv_head_idx * stride_vch\n            + offsets_in_last_block * stride_vcs\n            + offsets_dmodel_x_partition * stride_vcd\n        )\n        tl.store(VCache + offsets_vcache, v)\n    return\n\n\ndef copy_kv_to_blocked_cache(\n    k: torch.Tensor,\n    v: torch.Tensor,\n    k_cache: torch.Tensor,\n    v_cache: torch.Tensor,\n    kv_lengths: torch.Tensor,\n    block_tables: torch.Tensor,\n    use_new_kcache_layout: bool = False,\n):\n    \"\"\"\n    Copy keys or values to the blocked key/value cache during decoding stage.\n\n    Args:\n        k (torch.Tensor): [bsz, 1, num_kv_heads, head_dim]/[bsz, num_kv_heads, head_dim] - Keys during decoding with seq len 1.\n        v (torch.Tensor): [bsz, 1, num_kv_heads, head_dim]/[bsz, num_kv_heads, head_dim] - Values during decoding with seq len 1.\n        k_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim] - Blocked key cache.\n        v_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim] - Blocked value cache.\n        kv_lengths (torch.Tensor): [bsz] - Past key/value sequence lengths plus current sequence length for each sequence.\n        block_tables (torch.Tensor): [bsz, max_blocks_per_sequence] - Block tables for each sequence.\n        use_new_kcache_layout (bool): Whether to use the new layout for kcache. Default to False.\n    \"\"\"\n    k_cache_shape = k_cache.shape\n    v_cache_shape = v_cache.shape\n\n    if use_new_kcache_layout:\n        assert (\n            len(k_cache_shape) == 5\n            and k_cache_shape[1] == v_cache_shape[1]\n            and k_cache_shape[2] * k_cache_shape[4] == v_cache_shape[3]\n        ), f\"Invalid KCache shape {k_cache_shape} and VCache shape {v_cache_shape}\"\n    else:\n        assert k.size(-1) == k_cache_shape[-1], \"Incompatible head dim\"\n        assert (\n            k_cache_shape == v_cache_shape\n        ), f\"Incompatible KCache shape {k_cache_shape} and VCache shape {v_cache_shape}\"\n    assert v.size(-1) == v_cache_shape[-1], \"Incompatible head dim\"\n\n    k = k.squeeze(1) if k.dim() == 4 else k\n    assert k.dim() == 3, f\"Incompatible k dim {k.dim()}\"\n    v = v.squeeze(1) if v.dim() == 4 else v\n    assert v.dim() == 3, f\"Incompatible v dim {v.dim()}\"\n\n    bsz, num_kv_heads, head_dim = k.shape\n    assert kv_lengths.shape[0] == block_tables.shape[0] == bsz, (\n        f\"Got incompatible batch size (number of seqs):\\n\"\n        f\"  Past kv sequence lengths bsz {kv_lengths.shape[0]}; \"\n        f\" block tables bsz {block_tables.shape[0]}, input k batch size {bsz}\"\n    )\n\n    # Modify if the shape of kv cahce is changed.\n    block_size = k_cache.size(-2)\n\n    x = head_dim\n    stride_kcsplit_x, stride_kcs, stride_kcd = 0, k_cache.stride(2), k_cache.stride(3)\n    if use_new_kcache_layout:\n        x = k_cache.size(-1)\n        stride_kcsplit_x, stride_kcs, stride_kcd = k_cache.stride()[2:]\n\n    num_warps = 8 if head_dim > 128 else 4\n    grid = (bsz, num_kv_heads)\n    _copy_to_kvcache_seqlen1_kernel[grid](\n        k,\n        v,\n        k_cache,\n        v_cache,\n        block_tables,\n        kv_lengths,\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        k_cache.stride(0),\n        k_cache.stride(1),\n        stride_kcsplit_x,\n        stride_kcs,\n        stride_kcd,\n        v_cache.stride(0),\n        v_cache.stride(1),\n        v_cache.stride(2),\n        v_cache.stride(3),\n        block_tables.stride(0),\n        block_tables.stride(1),\n        block_size,\n        HEAD_DIM=head_dim,\n        KCACHE_X=x,\n        num_warps=num_warps,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `chunk_simple_gla_fwd_kernel_o` function is a Triton kernel designed to perform a forward pass operation for a chunked computation with queries (`q`), keys (`k`), values (`v`), and an auxiliary tensor (`h`). It outputs tensor `o`. The function takes in dimensions like `BT`, `BK`, and `BV` which represent chunk sizes. The kernel processes input tensors to compute intermediate results using block pointers and dot products, applying exponential transformations and scaling. The `chunk_fwd_o_fn` is a wrapper that prepares parameters and calls this kernel on a grid defined by the dimensions of the inputs.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(\n    q_ptr, k_ptr, v_ptr, h_ptr, o_ptr,\n    BT, BK, BV, H, N_CTX,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr, BLOCK_H: tl.constexpr\n):\n    # Compute the block indices\n    pid_m = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_n = tl.program_id(2)\n\n    # Compute the block bounds\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_h = pid_h * BLOCK_H + tl.arange(0, BLOCK_H)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    # Compute the block pointers\n    q_ptrs = q_ptr + (offs_m[:, None] * H * BLOCK_DMODEL + offs_h[None, :] * BLOCK_DMODEL + offs_d[None, :]) * BT\n    k_ptrs = k_ptr + (offs_n[:, None] * H * BLOCK_DMODEL + offs_h[None, :] * BLOCK_DMODEL + offs_d[None, :]) * BK\n    v_ptrs = v_ptr + (offs_n[:, None] * H * BLOCK_DMODEL + offs_h[None, :] * BLOCK_DMODEL + offs_d[None, :]) * BV\n    h_ptrs = h_ptr + (offs_m[:, None] * H * BLOCK_DMODEL + offs_h[None, :] * BLOCK_DMODEL + offs_d[None, :]) * BT\n    o_ptrs = o_ptr + (offs_m[:, None] * H * BLOCK_DMODEL + offs_h[None, :] * BLOCK_DMODEL + offs_d[None, :]) * BT\n\n    # Load the data\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n    h = tl.load(h_ptrs)\n\n    # Compute the dot product\n    logits = tl.dot(q, k, trans_b=True)  # (BLOCK_M, BLOCK_N)\n    logits = logits * (1.0 / tl.sqrt(tl.float32(BLOCK_DMODEL)))\n\n    # Apply exponential transformation\n    logits = tl.exp(logits)\n\n    # Compute the weighted sum\n    v = tl.broadcast_to(v, (BLOCK_M, BLOCK_N, BLOCK_DMODEL))\n    o = tl.sum(logits[:, :, None] * v, axis=1)  # (BLOCK_M, BLOCK_DMODEL)\n\n    # Apply auxiliary tensor\n    o = o + h\n\n    # Store the result\n    tl.store(o_ptrs, o)\n\nimport torch\n\ndef chunk_fwd_o_fn(q, k, v, h, o, BT, BK, BV, H, N_CTX, BLOCK_M, BLOCK_DMODEL, BLOCK_N, BLOCK_H):\n    # Get the grid dimensions\n    grid = (N_CTX // BLOCK_M, H // BLOCK_H, N_CTX // BLOCK_N)\n\n    # Launch the kernel\n    chunk_simple_gla_fwd_kernel_o[grid](\n        q, k, v, h, o,\n        BT, BK, BV, H, N_CTX,\n        BLOCK_M, BLOCK_DMODEL, BLOCK_N, BLOCK_H\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example dimensions\n    BT = 1024\n    BK = 1024\n    BV = 1024\n    H = 16\n    N_CTX = 512\n    BLOCK_M = 32\n    BLOCK_DMODEL = 64\n    BLOCK_N = 32\n    BLOCK_H = 16\n\n    # Create example tensors\n    q = torch.randn((N_CTX, H, BLOCK_DMODEL), device='cuda')\n    k = torch.randn((N_CTX, H, BLOCK_DMODEL), device='cuda')\n    v = torch.randn((N_CTX, H, BLOCK_DMODEL), device='cuda')\n    h = torch.randn((N_CTX, H, BLOCK_DMODEL), device='cuda')\n    o = torch.zeros((N_CTX, H, BLOCK_DMODEL), device='cuda')\n\n    # Call the wrapper function\n    chunk_fwd_o_fn(q, k, v, h, o, BT, BK, BV, H, N_CTX, BLOCK_M, BLOCK_DMODEL, BLOCK_N, BLOCK_H)\n\n    # Print the result\n    print(o)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    g,\n    o,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    # Kernel implementation\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        # [BT, BK]\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BK, BV]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_g)[:, None]\n    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\ndef chunk_fwd_o_fn(h, q, k, v, g, BT, scale):\n    # Kernel call\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    o = torch.empty_like(v)\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NV = triton.cdiv(V, BV)\n    NT = triton.cdiv(T, BT)\n    grid = (NV, NT, B * H)\n    chunk_simple_gla_fwd_kernel_o[grid](\n        q, k, v, h, g, o,\n        q.stride(1), q.stride(2),\n        v.stride(1), v.stride(2),\n        h.stride(1), h.stride(2),\n        scale,\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV\n    )\n    return o\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code implements a matrix multiplication routine involving quantization for integer 8-bit arithmetic. The main functions are `quantize_int8_perrow_kernel`, `quantize_int8_perrow`, `matmul_kernel`, `matmul_quantize_int8`, `matmul_int8`, and `quantize_int8`. The `quantize_int8_perrow_kernel` function is a Triton kernel that quantizes floating-point matrices to int8 by computing the maximum per row and then scaling elements to fit into int8. `quantize_int8_perrow` is a helper function to perform this operation and return quantized matrices. `matmul_kernel` performs the core matrix multiplication, supporting operations with quantized matrices by using scaling factors to reconstruct the floating-point results. `matmul_quantize_int8` is a wrapper to first quantize the input matrix and then call the matrix multiplication routine. `matmul_int8` performs matrix multiplication on already quantized inputs using Triton kernels. The function `quantize_int8` applies quantization along the specified axis and prepares data for efficient computation. Input parameters are mainly pointers to matrices and their dimensions. The outputs include quantized matrices and results from the matrix multiplication.", "predict": "Certainly! Below is a complete Triton-based implementation for matrix multiplication involving quantization to int8. The code includes the necessary kernels and wrapper functions to handle the quantization and matrix multiplication operations.\n\n### Triton Kernels and Wrapper Functions\n\n```python\nimport triton\nimport triton.language as tl\n\n# Triton kernel to quantize a floating-point matrix to int8 per row\n@triton.jit\ndef quantize_int8_perrow_kernel(\n    X_ptr,  # Pointer to the input matrix\n    X_quant_ptr,  # Pointer to the output quantized matrix\n    scale_ptr,  # Pointer to the scaling factors\n    stride_xr,  # Stride of the input matrix along the row\n    stride_xc,  # Stride of the input matrix along the column\n    stride_qr,  # Stride of the quantized matrix along the row\n    stride_qc,  # Stride of the quantized matrix along the column\n    stride_s,  # Stride of the scaling factors\n    M,  # Number of rows in the input matrix\n    N,  # Number of columns in the input matrix\n    BLOCK_SIZE: tl.constexpr  # Block size for parallelization\n):\n    pid = tl.program_id(axis=0)\n    row = pid\n\n    if row < M:\n        X_row = X_ptr + row * stride_xr\n        X_quant_row = X_quant_ptr + row * stride_qr\n        scale = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        max_val = tl.zeros((1,), dtype=tl.float32)\n\n        for col in range(0, N, BLOCK_SIZE):\n            cols = col + tl.arange(0, BLOCK_SIZE)\n            mask = cols < N\n            x = tl.load(X_row + cols * stride_xc, mask=mask, other=0.0)\n            max_val = tl.max(max_val, tl.max(x, axis=0))\n\n        scale = 127.0 / max_val\n        tl.store(scale_ptr + row * stride_s, scale)\n\n        for col in range(0, N, BLOCK_SIZE):\n            cols = col + tl.arange(0, BLOCK_SIZE)\n            mask = cols < N\n            x = tl.load(X_row + cols * stride_xc, mask=mask, other=0.0)\n            x_quant = tl.cast(tl.round(x * scale), tl.int8)\n            tl.store(X_quant_row + cols * stride_qc, x_quant, mask=mask)\n\n# Helper function to perform quantization per row\ndef quantize_int8_perrow(X, M, N):\n    X_quant = tl.zeros((M, N), dtype=tl.int8)\n    scale = tl.zeros((M,), dtype=tl.float32)\n    grid = (M,)\n    quantize_int8_perrow_kernel[grid](\n        X, X_quant, scale, X.stride(0), X.stride(1), X_quant.stride(0), X_quant.stride(1), scale.stride(0), M, N, 128\n    )\n    return X_quant, scale\n\n# Triton kernel for matrix multiplication with quantized matrices\n@triton.jit\ndef matmul_kernel(\n    A_ptr,  # Pointer to the input matrix A\n    B_ptr,  # Pointer to the input matrix B\n    C_ptr,  # Pointer to the output matrix C\n    A_scale_ptr,  # Pointer to the scaling factors of A\n    B_scale_ptr,  # Pointer to the scaling factors of B\n    stride_ar,  # Stride of A along the row\n    stride_ac,  # Stride of A along the column\n    stride_br,  # Stride of B along the row\n    stride_bc,  # Stride of B along the column\n    stride_cr,  # Stride of C along the row\n    stride_cc,  # Stride of C along the column\n    M,  # Number of rows in A\n    N,  # Number of columns in A (and rows in B)\n    K,  # Number of columns in B\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for rows of A\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for columns of B\n    BLOCK_SIZE_K: tl.constexpr  # Block size for columns of A (and rows of B)\n):\n    pid = tl.program_id(axis=0)\n    pid_m = pid // N\n    pid_n = pid % N\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    A = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.int8)\n    B = tl.zeros((BLOCK_SIZE_K, BLOCK_SIZE_N), dtype=tl.int8)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        A = tl.load(A_ptr + (offs_am[:, None] * stride_ar + (k + offs_k[None, :]) * stride_ac))\n        B = tl.load(B_ptr + ((k + offs_k[:, None]) * stride_br + offs_bn[None, :]) * stride_bc)\n        A = tl.cast(A, tl.float32) * tl.load(A_scale_ptr + offs_am[:, None])\n        B = tl.cast(B, tl.float32) * tl.load(B_scale_ptr + offs_k[None, :])\n        acc += tl.dot(A, B)\n\n    C = tl.store(C_ptr + (offs_am[:, None] * stride_cr + offs_bn[None, :]) * stride_cc, acc)\n\n# Wrapper function to perform matrix multiplication with quantized matrices\ndef matmul_quantize_int8(A, B, M, N, K):\n    A_quant, A_scale = quantize_int8_perrow(A, M, N)\n    B_quant, B_scale = quantize_int8_perrow(B, N, K)\n    C = tl.zeros((M, K), dtype=tl.float32)\n    grid = (M * K // 128,)\n    matmul_kernel[grid](\n        A_quant, B_quant, C, A_scale, B_scale, A_quant.stride(0), A_quant.stride(1), B_quant.stride(0), B_quant.stride(1), C.stride(0), C.stride(1), M, N, K, 128, 128, 128\n    )\n    return C\n\n# Triton kernel for matrix multiplication with already quantized matrices\n@triton.jit\ndef matmul_int8_kernel(\n    A_ptr,  # Pointer to the input matrix A\n    B_ptr,  # Pointer to the input matrix B\n    C_ptr,  # Pointer to the output matrix C\n    A_scale_ptr,  # Pointer to the scaling factors of A\n    B_scale_ptr,  # Pointer to the scaling factors of B\n    stride_ar,  # Stride of A along the row\n    stride_ac,  # Stride of A along the column\n    stride_br,  # Stride of B along the row\n    stride_bc,  # Stride of B along the column\n    stride_cr,  # Stride of C along the row\n    stride_cc,  # Stride of C along the column\n    M,  # Number of rows in A\n    N,  # Number of columns in A (and rows in B)\n    K,  # Number of columns in B\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for rows of A\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for columns of B\n    BLOCK_SIZE_K: tl.constexpr  # Block size for columns of A (and rows of B)\n):\n    pid = tl.program_id(axis=0)\n    pid_m = pid // N\n    pid_n = pid % N\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    A = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.int8)\n    B = tl.zeros((BLOCK_SIZE_K, BLOCK_SIZE_N), dtype=tl.int8)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        A = tl.load(A_ptr + (offs_am[:, None] * stride_ar + (k + offs_k[None, :]) * stride_ac))\n        B = tl.load(B_ptr + ((k + offs_k[:, None]) * stride_br + offs_bn[None, :]) * stride_bc)\n        A = tl.cast(A, tl.float32) * tl.load(A_scale_ptr + offs_am[:, None])\n        B = tl.cast(B, tl.float32) * tl.load(B_scale_ptr + offs_k[None, :])\n        acc += tl.dot(A, B)\n\n    C = tl.store(C_ptr + (offs_am[:, None] * stride_cr + offs_bn[None, :]) * stride_cc, acc)\n\n# Wrapper function to perform matrix multiplication with already quantized matrices\ndef matmul_int8(A, B, A_scale, B_scale, M, N", "label": "import time\nimport torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=4),\n        triton.Config({}, num_stages=2, num_warps=2),\n        triton.Config({}, num_stages=2, num_warps=1),\n     ],\n    key=['K'],\n)\n@triton.jit\ndef quantize_int8_perrow_kernel(\n    fpa_ptr, a_ptr, as_ptr,\n    M, K, \n    stride_fpam, stride_fpak,\n    stride_am, stride_ak,\n    stride_asm,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = (a_max / 127.)\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        inta = (fpa / a_scale[:, None]).to(tl.int8)\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n\n\ndef quantize_int8_perrow(fpa):\n    a = torch.empty(fpa.shape, device=fpa.device, dtype=torch.int8)\n    a_scale = torch.empty(fpa.shape[0], device=fpa.device, dtype=torch.float16)\n    M, K = fpa.shape\n    BLOCK_SIZE_M = 1\n    BLOCK_SIZE_K = triton.next_power_of_2(K)\n    grid = (M // BLOCK_SIZE_M,)\n    quantize_int8_perrow_kernel[grid](\n        fpa, a, a_scale,\n        M, K,\n        fpa.stride(0), fpa.stride(1),\n        a.stride(0), a.stride(1),\n        a_scale.stride(0),\n        BLOCK_SIZE_M, BLOCK_SIZE_K,\n    )\n    return a, a_scale\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n    reset_to_zero=['c_ptr']\n)\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n    # by to get the element one row down (A has M rows).\n    stride_am, stride_ak,\n    stride_asm,\n    stride_bk, stride_bn,\n    stride_bsn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr, \n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    # -----------------------------------------------------------\n    # Map program ids `pid` to the block of C it should compute.\n    # This is done in a grouped ordering to promote L2 data reuse.\n    # See above `L2 Cache Optimizations` section for details.\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # ----------------------------------------------------------\n    # Create pointers for the first blocks of A and B.\n    # We will advance this pointer as we move in the K direction\n    # and accumulate\n    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n    # See above `Pointer Arithmetics` section for details\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        # If it is out of bounds, set it to 0.\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n        # We accumulate along the K dimension.\n        accumulator += tl.dot(a, b)\n        # Advance the ptrs to the next K block.\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    # You can fuse arbitrary activation functions here\n    # while the accumulator is still in FP32!\n    c = (accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]).to(tl.float16)\n    # -----------------------------------------------------------\n    # Write back the block of the output matrix C with masks.\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_quantize_int8(fpa, b, b_scale, out=None):\n    a, a_scale = quantize_int8_perrow(fpa)\n    # a, a_scale = quantize_int8(fpa, axis=1)\n    return matmul_int8(a, a_scale, b, b_scale, out)\n\n\ndef matmul_int8(a, a_scale, b, b_scale, out=None):\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    # Allocates output.\n    if out == None:\n        c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n    else:\n        c = out.fill_(0.)\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n        META['SPLIT_K'],\n    )\n    matmul_kernel[grid](\n        a, a_scale, b, b_scale, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        a_scale.stride(0),\n        b.stride(0), b.stride(1),\n        b_scale.stride(0),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\n\ndef quantize_int8(weight, axis=0):\n    # Weight shape: [H1, H2]\n    # Scale shape: [H2]\n    scale = weight.abs().amax(axis, keepdim=True) / 127.\n    weight = (weight / scale).to(torch.int8)\n    # col major will accelerate i8xi8 kernel.\n    if axis == 0:\n        weight = weight.t().contiguous().t()\n    scale = scale.squeeze(axis)\n    return weight, scale, None\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a Triton kernel named _seeded_dropout and a corresponding Python wrapper function named seeded_dropout. The main function of this code is to perform a dropout operation with a fixed seed for reproducibility. The kernel processes input data in blocks of a specified size and applies a dropout mask generated by a random number, comparing it against a probability threshold. The output is written back with scaling to maintain the expected value. Input parameters are x, the input tensor; p, the dropout probability; seed, the random seed; and BLOCK_SIZE, which specifies the number of elements each block handles. The output is a tensor with the same shape as the input, where elements are either zeroed out or scaled.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _seeded_dropout(X, Y, p, seed, BLOCK_SIZE: tl.constexpr):\n    # Get the block index and the block size\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Get the block of data\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < X.shape[0]\n    x = tl.load(X + offsets, mask=mask)\n\n    # Generate random numbers\n    rng = tl.rand(seed, offsets)\n    dropout_mask = rng > p\n\n    # Apply dropout mask and scale\n    y = tl.where(dropout_mask, x / (1 - p), 0.0)\n\n    # Write back the result\n    tl.store(Y + offsets, y, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef seeded_dropout(x: torch.Tensor, p: float, seed: int, block_size: int = 1024):\n    # Ensure the input tensor is on the GPU\n    assert x.is_cuda, \"Input tensor must be on the GPU\"\n    \n    # Create the output tensor with the same shape as the input\n    y = torch.empty_like(x)\n    \n    # Get the grid size\n    grid = (triton.cdiv(x.numel(), block_size),)\n    \n    # Launch the kernel\n    _seeded_dropout[grid](x, y, p, seed, BLOCK_SIZE=block_size)\n    \n    return y\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _seeded_dropout(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    p,\n    seed,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # compute memory offsets of elements handled by this instance\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # load data from x\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # randomly prune it\n    random = tl.rand(seed, offsets)\n    x_keep = random > p\n    # write-back\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef seeded_dropout(x, p, seed):\n    output = torch.empty_like(x)\n    assert x.is_contiguous()\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The function `int8_matmul_rowwise_dequantize` performs matrix multiplication on int8 matrices A and B, while applying row-wise dequantization using scaling factors from `state_x` and `state_w`. The result is stored in a float16 matrix C. The kernel `_int8_matmul_rowwise_dequantize` is optimized for performance using Triton's `autotune` decorator. Inputs: matrices A, B, scaling factors `state_x`, `state_w`, and optional `bias`. Outputs: matrix C, the dequantized product. Key parameters: BLOCK_M, BLOCK_N, BLOCK_K control tiling; SPLIT_K allows partitioning K dimension for parallelism.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_warps=4, num_stages=3),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_warps=4, num_stages=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_warps=4, num_stages=4),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _int8_matmul_rowwise_dequantize(\n    A, B, C, state_x, state_w, bias, M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_xm, stride_wn,\n    stride_bn, stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_wg = num_pid_m * num_pid_n\n    num_wg = tl.cdiv(K, BLOCK_K * SPLIT_K)\n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    pid_g = tl.program_id(axis=1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = pid_g * BLOCK_K + tl.arange(0, BLOCK_K)\n\n    A = A + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    B = B + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n    state_x = state_x + (offs_m * stride_xm)\n    state_w = state_w + (offs_n * stride_wn)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K * SPLIT_K):\n        a = tl.load(A)\n        b = tl.load(B)\n        a = a * tl.load(state_x)[:, None]\n        b = b * tl.load(state_w)[None, :]\n        acc += tl.dot(a, b)\n\n    acc = acc.to(tl.float16)\n\n    if bias is not None:\n        bias = bias + (offs_n * stride_bn)\n        bias = tl.load(bias)\n        acc += bias[None, :]\n\n    C = C + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    tl.store(C, acc)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _int8_matmul_rowwise_dequantize(\n    A, B, C, state_x, state_w, bias, M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_xm, stride_wn,\n    stride_bn, stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr,\n):\n    # The kernel function is defined above\n    pass\n\ndef int8_matmul_rowwise_dequantize(A, B, state_x, state_w, bias=None):\n    M, K = A.shape\n    N = B.shape[1]\n    C = torch.empty((M, N), dtype=torch.float16, device=A.device)\n\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), triton.cdiv(K, META['BLOCK_K'] * META['SPLIT_K']))\n\n    _int8_matmul_rowwise_dequantize[grid](\n        A, B, C, state_x, state_w, bias, M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        state_x.stride(0), state_w.stride(0),\n        B.stride(1), C.stride(0), C.stride(1),\n        BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, SPLIT_K=1,\n    )\n\n    return C\n\nimport torch\n\n# Example inputs\nM, K, N = 1024, 1024, 1024\nA = torch.randint(-128, 127, (M, K), dtype=torch.int8, device='cuda')\nB = torch.randint(-128, 127, (K, N), dtype=torch.int8, device='cuda')\nstate_x = torch.randn(M, dtype=torch.float32, device='cuda')\nstate_w = torch.randn(N, dtype=torch.float32, device='cuda')\nbias = torch.randn(N, dtype=torch.float16, device='cuda')\n\n# Perform the matrix multiplication with row-wise dequantization\nC = int8_matmul_rowwise_dequantize(A, B, state_x, state_w, bias)\n\nprint(C)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time\n\n\ndef get_configs_io_bound():\n    def init_to_zero(name):\n        return lambda nargs: nargs[name].zero_()\n\n    configs = []\n    for num_stages in [2, 3, 4, 5, 6]:\n        for block_m in [16, 32]:\n            for block_k in [32, 64]:\n                for block_n in [32, 64, 128, 256]:\n                    num_warps = 2 if block_n <= 64 else 4\n                    configs.append(\n                        triton.Config(\n                            {\"BLOCK_M\": block_m, \"BLOCK_N\": block_n, \"BLOCK_K\": block_k, \"SPLIT_K\": 1},\n                            num_stages=num_stages,\n                            num_warps=num_warps,\n                        ),\n                    )\n                    # split_k\n                    for split_k in [2, 4, 8, 16]:\n                        configs.append(\n                            triton.Config(\n                                {\"BLOCK_M\": block_m, \"BLOCK_N\": block_n, \"BLOCK_K\": block_k, \"SPLIT_K\": split_k},\n                                num_stages=num_stages,\n                                num_warps=num_warps,\n                                pre_hook=init_to_zero(\"C\"),\n                            ),\n                        )\n    return configs\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n        *get_configs_io_bound(),\n    ],\n    key=[\"M\", \"N\", \"K\"],\n    prune_configs_by={\"early_config_prune\": early_config_prune, \"perf_model\": estimate_matmul_time, \"top_k\": 10},\n)\n@triton.heuristics(\n    {\n        \"EVEN_K\": lambda args: args[\"K\"] % (args[\"BLOCK_K\"] * args[\"SPLIT_K\"]) == 0,\n    },\n)\n@triton.jit\ndef _int8_matmul_rowwise_dequantize(\n    A,\n    B,\n    C,\n    bias,\n    state_x_ptr,\n    state_w_ptr,\n    M,\n    N,\n    K,\n    divfactor,\n    has_bias: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    w_factor = tl.load(state_w_ptr + rbn)[None, :]\n    x_factor = tl.load(state_x_ptr + ram)[:, None]\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc = w_factor * (x_factor * (acc * divfactor))\n    acc = acc.to(C.dtype.element_ty)\n    if has_bias:\n        bias = tl.load(bias + rn).to(C.dtype.element_ty)\n        acc = acc + bias[None, :]\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n\ndef int8_matmul_rowwise_dequantize(a, b, state_x, state_w, bias):\n    divfactor = 1.0 / (127.0 * 127.0)\n    has_bias = 0 if bias is None else 1\n    device = a.device\n    if a.stride(0) > 1 and a.stride(1) > 1:\n        a = a.contiguous()\n    if b.stride(0) > 1 and b.stride(1) > 1:\n        b = b.contiguous()\n    assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n    M, K = a.shape\n    _, N = b.shape\n    c = torch.empty((M, N), device=device, dtype=torch.float16)\n    ACC_TYPE = tl.float32\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]), META[\"SPLIT_K\"])\n    _int8_matmul_rowwise_dequantize[grid](\n        a,\n        b,\n        c,\n        bias,\n        state_x,\n        state_w,\n        M,\n        N,\n        K,\n        divfactor,\n        has_bias,\n        a.stride(0),\n        a.stride(1),\n        b.stride(0),\n        b.stride(1),\n        c.stride(0),\n        c.stride(1),\n        GROUP_M=8,\n        ACC_TYPE=ACC_TYPE,\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is designed to perform a combined operation of quantizing and transposing a matrix. The main kernel is `_quantize_global_transpose`, which takes in a float matrix `A`, an inverse maximum absolute value `absmax_inv_ptr`, and produces an int8 matrix `B` that is transposed from `A`. The operation quantizes the elements of `A` based on the scaling factor derived from the maximum absolute value in `A`, then transposes the result into `B`. The input matrix `A` is quantized by multiplying it with the `absmax_inv`, and the result is scaled to the range of int8 values. The kernel uses grid-stride loops to handle matrices larger than the block size specified by `BLOCK_M` and `BLOCK_N`. The helper function `quantize_global_transpose` sets up the necessary configuration and calls the Triton kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _quantize_global_transpose(\n    A_ptr,  # Pointer to the input float matrix\n    absmax_inv_ptr,  # Pointer to the inverse of the maximum absolute value\n    B_ptr,  # Pointer to the output int8 matrix\n    M,  # Number of rows in the input matrix\n    N,  # Number of columns in the input matrix\n    BLOCK_M: tl.constexpr,  # Block size for rows\n    BLOCK_N: tl.constexpr  # Block size for columns\n):\n    # Compute the grid and block indices\n    pid = tl.program_id(axis=0)\n    num_blocks_m = tl.cdiv(M, BLOCK_M)\n    num_blocks_n = tl.cdiv(N, BLOCK_N)\n    block_m = pid % num_blocks_m\n    block_n = pid // num_blocks_m\n\n    # Compute the block offsets\n    rm = block_m * BLOCK_M\n    rn = block_n * BLOCK_N\n\n    # Compute the block bounds\n    rm_bound = min(rm + BLOCK_M, M)\n    rn_bound = min(rn + BLOCK_N, N)\n\n    # Load the inverse of the maximum absolute value\n    absmax_inv = tl.load(absmax_inv_ptr)\n\n    # Quantize and transpose the block\n    for m in range(rm, rm_bound):\n        for n in range(rn, rn_bound):\n            # Load the element from the input matrix\n            a = tl.load(A_ptr + m * N + n)\n            # Quantize the element\n            q = tl.round(a * absmax_inv)\n            # Clamp the quantized value to the int8 range\n            q = tl.max(tl.min(q, 127), -128)\n            # Store the transposed element in the output matrix\n            tl.store(B_ptr + n * M + m, q)\n\n### Python Wrapper Function\n\nimport torch\n\ndef quantize_global_transpose(A, absmax_inv):\n    # Convert the input tensor to a contiguous format\n    A = A.contiguous()\n    M, N = A.shape\n\n    # Allocate the output tensor\n    B = torch.empty((N, M), dtype=torch.int8, device=A.device)\n\n    # Define the grid and block sizes\n    BLOCK_M = 16\n    BLOCK_N = 16\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n    # Allocate the inverse of the maximum absolute value on the device\n    absmax_inv_ptr = triton.make_ptr(absmax_inv, dtype=triton.float32)\n\n    # Launch the Triton kernel\n    _quantize_global_transpose[grid](\n        A,  # Input matrix\n        absmax_inv_ptr,  # Inverse of the maximum absolute value\n        B,  # Output matrix\n        M,  # Number of rows in the input matrix\n        N,  # Number of columns in the input matrix\n        BLOCK_M,  # Block size for rows\n        BLOCK_N  # Block size for columns\n    )\n\n    return B\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# global quantize and transpose\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"GROUP_M\": 8}, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"GROUP_M\": 8}, num_warps=4),\n        # ...\n    ],\n    key=[\"M\", \"N\"],\n)\n@triton.jit\ndef _quantize_global_transpose(\n    A,\n    absmax_inv_ptr,\n    B,\n    stride_am,\n    stride_an,\n    stride_bn,\n    stride_bm,\n    M,\n    N,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // group_size\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A = A + (rm[:, None] * stride_am + rn[None, :] * stride_an)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    a = tl.load(A, mask=mask)\n    absmax_inv = tl.load(absmax_inv_ptr)\n\n    # rematerialize to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    B = B + (rm[:, None] * stride_bm + rn[None, :] * stride_bn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (a * absmax_inv))\n\n    tl.store(B, output, mask=mask)\n\ndef quantize_global_transpose(input):\n    absmax = input.abs().max().unsqueeze(0)\n    absmax_inv = 1.0 / absmax\n    M, N = input.shape\n    out = torch.empty(N, M, device=\"cuda\", dtype=torch.int8)\n\n    assert out.size(0) == N and out.size(1) == M\n    assert input.stride(0) == 1 or input.stride(1) == 1\n    assert out.stride(0) == 1 or out.stride(1) == 1\n\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),)\n    _quantize_global_transpose[grid](\n        input,\n        absmax_inv,\n        out,\n        input.stride(0),\n        input.stride(1),\n        out.stride(0),\n        out.stride(1),\n        M,\n        N,\n    )\n    return out, absmax\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel `_fwd_kernel_token_att2` performs attention-like operations on input tensors. The main functionality is to compute the weighted sum of values `V` based on probabilities `Prob` and store the result in `Out`. The kernel processes data in blocks, iterating over each block of the sequence and aggregating results. The function `token_att_fwd2` sets up and launches this Triton kernel with provided input tensors and additional configuration, such as the batch size, head count, and block sizes.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    Prob,  # Probabilities tensor\n    V,     # Values tensor\n    Out,   # Output tensor\n    stride_prob_b, stride_prob_h, stride_prob_s, stride_prob_t,\n    stride_v_b, stride_v_h, stride_v_s, stride_v_d,\n    stride_out_b, stride_out_h, stride_out_s, stride_out_d,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_SEQ: tl.constexpr\n):\n    # Get the batch and head indices\n    bid = tl.program_id(0)\n    hid = tl.program_id(1)\n\n    # Pointers to the start of the batch and head\n    prob_ptr = Prob + bid * stride_prob_b + hid * stride_prob_h\n    v_ptr = V + bid * stride_v_b + hid * stride_v_h\n    out_ptr = Out + bid * stride_out_b + hid * stride_out_h\n\n    # Iterate over the sequence in blocks\n    for start_t in range(0, stride_prob_s, BLOCK_SEQ):\n        # Load the probabilities for the current block\n        prob = tl.load(prob_ptr + start_t * stride_prob_s, mask=start_t + tl.arange(0, BLOCK_SEQ) < stride_prob_s, other=0.0)\n\n        # Initialize the output block\n        out = tl.zeros((BLOCK_SEQ, BLOCK_DMODEL), dtype=tl.float32)\n\n        # Iterate over the sequence to compute the weighted sum\n        for s in range(stride_prob_t):\n            # Load the values for the current sequence position\n            v = tl.load(v_ptr + s * stride_v_s + start_t * stride_v_d, mask=start_t + tl.arange(0, BLOCK_SEQ) < stride_prob_s, other=0.0)\n\n            # Compute the weighted sum\n            out += prob[:, None] * v\n\n        # Store the result in the output tensor\n        tl.store(out_ptr + start_t * stride_out_d, out, mask=start_t + tl.arange(0, BLOCK_SEQ) < stride_prob_s)\n\n        # Move to the next block\n        prob_ptr += BLOCK_SEQ * stride_prob_s\n        v_ptr += BLOCK_SEQ * stride_v_d\n        out_ptr += BLOCK_SEQ * stride_out_d\n\nimport torch\n\ndef token_att_fwd2(Prob, V, Out, B, H, S, D, BLOCK_SEQ=128, BLOCK_DMODEL=64):\n    # Get the strides for the input tensors\n    stride_prob_b = Prob.stride(0)\n    stride_prob_h = Prob.stride(1)\n    stride_prob_s = Prob.stride(2)\n    stride_prob_t = Prob.stride(3)\n\n    stride_v_b = V.stride(0)\n    stride_v_h = V.stride(1)\n    stride_v_s = V.stride(2)\n    stride_v_d = V.stride(3)\n\n    stride_out_b = Out.stride(0)\n    stride_out_h = Out.stride(1)\n    stride_out_s = Out.stride(2)\n    stride_out_d = Out.stride(3)\n\n    # Launch the kernel\n    grid = (B, H)\n    _fwd_kernel_token_att2[grid](\n        Prob, V, Out,\n        stride_prob_b, stride_prob_h, stride_prob_s, stride_prob_t,\n        stride_v_b, stride_v_h, stride_v_s, stride_v_d,\n        stride_out_b, stride_out_h, stride_out_s, stride_out_d,\n        BLOCK_DMODEL, BLOCK_SEQ\n    )\n\n# Example usage\nB = 2  # Batch size\nH = 4  # Number of heads\nS = 128  # Sequence length\nD = 64  # Model dimension\n\nProb = torch.randn((B, H, S, S), device='cuda')\nV = torch.randn((B, H, S, D), device='cuda')\nOut = torch.zeros((B, H, S, D), device='cuda')\n\ntoken_att_fwd2(Prob, V, Out, B, H, S, D)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    Prob,\n    V,\n    Out,\n    Req_to_tokens,\n    B_req_idx,\n    B_Start_Loc,\n    B_Seqlen,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    stride_ph,\n    stride_pbs,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    cur_kv_head = cur_head // kv_group_num\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=(start_n + offs_n) < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(\n            Req_to_tokens + v_loc_off + start_n * stride_req_to_tokens_s,\n            mask=(start_n + offs_n) < cur_batch_seq_len,\n            other=0.0,\n        )\n        v_value = tl.load(\n            V + v_offs + v_loc[:, None] * stride_vbs, mask=(start_n + offs_n[:, None]) < cur_batch_seq_len, other=0.0\n        )\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_att_fwd2(prob, v, out, Req_to_tokens, B_req_idx, B_Start_Loc, B_Seqlen):\n    BLOCK = 128\n    batch, head = B_req_idx.shape[0], prob.shape[0]\n    grid = (batch, head)\n    num_warps = 4\n    dim = v.shape[-1]\n\n    kv_group_num = prob.shape[0] // v.shape[1]\n\n    _fwd_kernel_token_att2[grid](\n        prob,\n        v,\n        out,\n        Req_to_tokens,\n        B_req_idx,\n        B_Start_Loc,\n        B_Seqlen,\n        Req_to_tokens.stride(0),\n        Req_to_tokens.stride(1),\n        prob.stride(0),\n        prob.stride(1),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        out.stride(0),\n        out.stride(1),\n        out.stride(2),\n        kv_group_num=kv_group_num,\n        BLOCK_DMODEL=dim,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code implements a LayerNorm operation using the Triton library for efficient execution on GPUs. \n            There are three main Triton kernels: \n            1. `_layer_norm_fwd_fused`: This kernel performs the forward pass of the LayerNorm. It normalizes the input `X`, applies scale `W`, and offset `B`, storing the result in `Y`. It also computes the mean and reciprocal standard deviation (`Rstd`) for each row and stores these for use in the backward pass. \n            2. `_layer_norm_bwd_dx_fused`: This kernel computes the gradient of the input `DX` from the gradient of the output `DY`. It also accumulates partial sums of gradients w.r.t. weights and biases (`DW` and `DB`) using atomic operations to manage concurrent writes. \n            3. `_layer_norm_bwd_dwdb`: This kernel aggregates partial weight and bias gradients from `_layer_norm_bwd_dx_fused` to produce the final gradients `FINAL_DW` and `FINAL_DB`.\n            The `LayerNorm` class wraps these kernels and provides a PyTorch-compatible interface, utilizing the Triton kernels for the forward and backward passes.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Triton Kernel: Forward Pass of LayerNorm\n@triton.jit\ndef _layer_norm_fwd_fused(X, W, B, Y, Mean, Rstd, stride, N, eps, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    X_row = tl.load(X + row * stride + cols, mask=mask, other=0.0).to(tl.float32)\n    \n    # Compute mean\n    mean = tl.sum(X_row, axis=0) / N\n    \n    # Compute variance\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / N\n    rstd = 1.0 / tl.sqrt(var + eps)\n    \n    # Normalize and apply scale and offset\n    Y_row = (X_row - mean) * rstd * tl.load(W + cols, mask=mask, other=0.0) + tl.load(B + cols, mask=mask, other=0.0)\n    \n    # Store results\n    tl.store(Y + row * stride + cols, Y_row, mask=mask)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n\n# Triton Kernel: Backward Pass for Input Gradient\n@triton.jit\ndef _layer_norm_bwd_dx_fused(DY, X, W, B, Mean, Rstd, DX, DW, DB, stride, N, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    X_row = tl.load(X + row * stride + cols, mask=mask, other=0.0).to(tl.float32)\n    DY_row = tl.load(DY + row * stride + cols, mask=mask, other=0.0).to(tl.float32)\n    W_row = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    \n    # Compute intermediate values\n    X_hat = (X_row - mean) * rstd\n    dX_hat = DY_row * W_row\n    sum_dX_hat = tl.sum(dX_hat, axis=0)\n    sum_dX_hat_X_hat = tl.sum(dX_hat * X_hat, axis=0)\n    \n    # Compute gradients\n    DX_row = (dX_hat * rstd) - (sum_dX_hat * rstd / N) - (sum_dX_hat_X_hat * X_hat * rstd * rstd / N)\n    \n    # Store gradients\n    tl.atomic_add(DW + cols, tl.sum(DY_row * X_hat, axis=0), mask=mask)\n    tl.atomic_add(DB + cols, tl.sum(DY_row, axis=0), mask=mask)\n    tl.store(DX + row * stride + cols, DX_row, mask=mask)\n\n# Triton Kernel: Aggregation of Weight and Bias Gradients\n@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, stride, N, BLOCK_SIZE: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    DW_row = tl.load(DW + row * stride + cols, mask=mask, other=0.0).to(tl.float32)\n    DB_row = tl.load(DB + row * stride + cols, mask=mask, other=0.0).to(tl.float32)\n    \n    # Aggregate gradients\n    tl.atomic_add(FINAL_DW + cols, DW_row, mask=mask)\n    tl.atomic_add(FINAL_DB + cols, DB_row, mask=mask)\n\n# PyTorch Wrapper Class\nclass LayerNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, B, eps=1e-5):\n        N = X.shape[-1]\n        stride = X.stride(0)\n        Mean = torch.empty((X.shape[0],), device=X.device, dtype=X.dtype)\n        Rstd = torch.empty((X.shape[0],), device=X.device, dtype=X.dtype)\n        Y = torch.empty_like(X)\n        \n        grid = (X.shape[0],)\n        _layer_norm_fwd_fused[grid](X, W, B, Y, Mean, Rstd, stride, N, eps, BLOCK_SIZE=1024)\n        \n        ctx.save_for_backward(X, W, B, Mean, Rstd)\n        ctx.eps = eps\n        return Y\n\n    @staticmethod\n    def backward(ctx, DY):\n        X, W, B, Mean, Rstd = ctx.saved_tensors\n        eps = ctx.eps\n        N = X.shape[-1]\n        stride = X.stride(0)\n        DX = torch.empty_like(X)\n        DW = torch.zeros_like(W)\n        DB = torch.zeros_like(B)\n        \n        grid = (X.shape[0],)\n        _layer_norm_bwd_dx_fused[grid](DY, X, W, B, Mean, Rstd, DX, DW, DB, stride, N, BLOCK_SIZE=1024)\n        \n        FINAL_DW = torch.zeros_like(W)\n        FINAL_DB = torch.zeros_like(B)\n        grid = (1,)\n        _layer_norm_bwd_dwdb[grid](DW, DB, FINAL_DW, FINAL_DB, stride, N, BLOCK_SIZE=1024)\n        \n        return DX, FINAL_DW, FINAL_DB, None\n\n# Example Usage\nif __name__ == \"__main__\":\n    # Create input tensor and parameters\n    X = torch.randn(32, 64, device='cuda')\n    W = torch.randn(64, device='cuda', requires_grad=True)\n    B = torch.randn(64, device='cuda', requires_grad=True)\n    \n    # Forward pass\n    Y = LayerNorm.apply(X, W, B)\n    \n    # Compute loss (for example, MSE loss)\n    target = torch.randn_like(Y)\n    loss = torch.nn.functional.mse_loss(Y, target)\n    \n    # Backward pass\n    loss.backward()\n    \n    # Print gradients\n    print(W.grad)\n    print(B.grad)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_fwd_fused(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    W,  # pointer to the weights\n    B,  # pointer to the biases\n    Mean,  # pointer to the mean\n    Rstd,  # pointer to the 1/std\n    stride,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n        x = tl.where(cols < N, x - mean, 0.)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n\n@triton.jit\ndef _layer_norm_bwd_dx_fused(DX,  # pointer to the input gradient\n                             DY,  # pointer to the output gradient\n                             DW,  # pointer to the partial sum of weights gradient\n                             DB,  # pointer to the partial sum of biases gradient\n                             X,  # pointer to the input\n                             W,  # pointer to the weights\n                             Mean,  # pointer to the mean\n                             Rstd,  # pointer to the 1/std\n                             Lock,  # pointer to the lock\n                             stride,  # how much to increase the pointer when moving by 1 row\n                             N,  # number of columns in X\n                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.)\n    wdy = tl.where(mask, wdy, 0.)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = (dy * xhat).to(w.dtype)\n    partial_db = (dy).to(w.dtype)\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n\n@triton.jit\ndef _layer_norm_bwd_dwdb(DW,  # pointer to the partial sum of weights gradient\n                         DB,  # pointer to the partial sum of biases gradient\n                         FINAL_DW,  # pointer to the weights gradient\n                         FINAL_DB,  # pointer to the biases gradient\n                         M,  # GROUP_SIZE_M\n                         N,  # number of columns\n                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.)\n        db += tl.load(DB + offs, mask=mask, other=0.)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n\nclass LayerNorm(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, normalized_shape, weight, bias, eps):\n        y = torch.empty_like(x)\n        x_arg = x.reshape(-1, x.shape[-1])\n        M, N = x_arg.shape\n        mean = torch.empty((M, ), dtype=torch.float32, device=x.device)\n        rstd = torch.empty((M, ), dtype=torch.float32, device=x.device)\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n        if N > BLOCK_SIZE:\n            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n        _layer_norm_fwd_fused[(M, )](  #\n            x_arg, y, weight, bias, mean, rstd,  #\n            x_arg.stride(0), N, eps,  #\n            BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_ctas=1)\n        ctx.save_for_backward(x, weight, bias, mean, rstd)\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps = num_warps\n        ctx.eps = eps\n        return y\n\n    @staticmethod\n    def backward(ctx, dy):\n        x, w, b, m, v = ctx.saved_tensors\n        N = w.shape[0]\n        GROUP_SIZE_M = 64\n        if N <= 8192: GROUP_SIZE_M = 96\n        if N <= 4096: GROUP_SIZE_M = 128\n        if N <= 1024: GROUP_SIZE_M = 256\n        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device=w.device)\n        _dw = torch.zeros((GROUP_SIZE_M, N), dtype=x.dtype, device=w.device)\n        _db = torch.zeros((GROUP_SIZE_M, N), dtype=x.dtype, device=w.device)\n        dw = torch.empty((N, ), dtype=w.dtype, device=w.device)\n        db = torch.empty((N, ), dtype=w.dtype, device=w.device)\n        dx = torch.empty_like(dy)\n        x_arg = x.reshape(-1, x.shape[-1])\n        M, N = x_arg.shape\n        _layer_norm_bwd_dx_fused[(M, )](  #\n            dx, dy, _dw, _db, x, w, m, v, locks,  #\n            x_arg.stride(0), N,  #\n            BLOCK_SIZE_N=ctx.BLOCK_SIZE,  #\n            GROUP_SIZE_M=GROUP_SIZE_M,  #\n            num_warps=ctx.num_warps)\n        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n        _layer_norm_bwd_dwdb[grid](\n            _dw, _db, dw, db, min(GROUP_SIZE_M, M), N,  #\n            BLOCK_SIZE_M=32,  #\n            BLOCK_SIZE_N=128, num_ctas=1)\n        return dx, None, dw, db, None\n\nlayer_norm = LayerNorm.apply\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton kernel 'load_reduce_kernel' calculates the maximum value along the second dimension of an input matrix and stores it in an output vector. The function 'load_reduce' serves as a test harness to validate the kernel, using random input data. Key parameters include BLOCK_M and BLOCK_N, which define the block size for processing. The kernel utilizes pointers to access data efficiently and operates with specified strides. It loads a block of data from the input matrix, computes the maximum along the second dimension, and writes the result to the output vector.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef load_reduce_kernel(\n    x_ptr,  # Pointer to the input matrix\n    y_ptr,  # Pointer to the output vector\n    M,      # Number of rows in the input matrix\n    N,      # Number of columns in the input matrix\n    stride_xm,  # Stride for rows in the input matrix\n    stride_xn,  # Stride for columns in the input matrix\n    BLOCK_M: tl.constexpr,  # Block size for rows\n    BLOCK_N: tl.constexpr   # Block size for columns\n):\n    # Compute the block ID in the M dimension\n    pid_m = tl.program_id(0)\n    # Compute the block ID in the N dimension\n    pid_n = tl.program_id(1)\n\n    # Compute the block start indices in the M and N dimensions\n    rm = pid_m * BLOCK_M\n    rn = pid_n * BLOCK_N\n\n    # Compute the block end indices in the M and N dimensions\n    rm_end = tl.minimum(rm + BLOCK_M, M)\n    rn_end = tl.minimum(rn + BLOCK_N, N)\n\n    # Load the block of data from the input matrix\n    x_block = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(rm, rm_end):\n        for j in range(rn, rn_end):\n            x_block[i - rm, j - rn] = tl.load(x_ptr + i * stride_xm + j * stride_xn)\n\n    # Compute the maximum value along the second dimension\n    max_values = tl.max(x_block, axis=1)\n\n    # Write the result to the output vector\n    for i in range(rm, rm_end):\n        tl.store(y_ptr + i, max_values[i - rm])\n\nimport numpy as np\nimport torch\n\ndef load_reduce(M, N, BLOCK_M, BLOCK_N):\n    # Generate random input data\n    x = np.random.randn(M, N).astype(np.float32)\n    x_ptr = triton.testing.to_triton(x, device='cuda')\n\n    # Allocate output vector\n    y = np.zeros(M, dtype=np.float32)\n    y_ptr = triton.testing.to_triton(y, device='cuda')\n\n    # Define grid and block dimensions\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N), 1)\n\n    # Launch the kernel\n    load_reduce_kernel[grid](\n        x_ptr, y_ptr, M, N, x.shape[1], 1, BLOCK_M, BLOCK_N\n    )\n\n    # Copy the result back to the host\n    y = y_ptr.to_numpy()\n\n    # Validate the result\n    expected = np.max(x, axis=1)\n    np.testing.assert_allclose(y, expected, rtol=1e-5, atol=1e-5)\n\n    print(\"Test passed!\")\n\n# Example usage\nM, N = 1024, 1024\nBLOCK_M, BLOCK_N = 32, 32\nload_reduce(M, N, BLOCK_M, BLOCK_N)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom torch.testing import assert_close\n\n\n@triton.jit\ndef load_reduce_kernel(\n    x_ptr,      # pointer to the input matrix\n    y_ptr,      # pointer to the output vector\n    stride_xm,  # stride of matrix x in leading dimension\n    stride_xn,  # stride of matrix x in the second dimension\n    stride_y,   # stride of output vector y\n    BLOCK_M: tl.constexpr,  # block size in leading dimension\n    BLOCK_N: tl.constexpr,  # block size in second dimension\n):\n    x_ptr = tl.make_block_ptr(\n        base=x_ptr, shape=(BLOCK_M, BLOCK_N), strides=(stride_xm, stride_xn),\n        offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_N), order=(1, 0)\n    )\n    x = tl.load(x_ptr)\n    y = tl.max(x, axis=1)\n    tl.store(y_ptr + tl.arange(0, BLOCK_M), y)\n\n# Test function for load_reduce_kernel\ndef load_reduce(BLOCK_M, BLOCK_N, dtype_str):\n    dtype_mapping = {\n        'float16': torch.float16,\n        'float32': torch.float32,\n    }\n    dtype = dtype_mapping[dtype_str]\n    x = torch.randn((BLOCK_M, BLOCK_N), device='cuda', dtype=dtype)\n    y = torch.empty((BLOCK_M,), device='cuda', dtype=dtype)\n\n    load_reduce_kernel[(1,)](x, y, x.stride(0), x.stride(1), y.stride(0), BLOCK_M, BLOCK_N)\n\n    golden = x.max(dim=1)[0]\n    torch.set_printoptions(profile='full')\n    assert_close(y, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel implements the forward pass of a layer normalization operation. The function `_layer_norm_fwd_kernel` takes an input tensor `X`, weights `W`, and outputs the normalized tensor `Y`. The normalization is computed over the last dimension of `X`, and `W` is applied as a scaling factor to the normalized values. The function `layernorm_forward` sets up necessary strides and kernel grid, and launches the Triton kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_fwd_kernel(X, W, Y, stride_x_row, stride_x_col, stride_w, stride_y_row, stride_y_col, N, eps, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    col_idx = tl.program_id(1) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n\n    # Compute the start and end indices for the row\n    start_x = row_idx * stride_x_row\n    start_y = row_idx * stride_y_row\n\n    # Load the input data for the current row\n    x = tl.load(X + start_x + col_idx * stride_x_col, mask=col_idx < N, other=0.0).to(tl.float32)\n\n    # Compute the mean\n    mean = tl.sum(x, axis=0) / N\n\n    # Compute the variance\n    x_centered = x - mean\n    var = tl.sum(x_centered * x_centered, axis=0) / N\n\n    # Compute the normalized values\n    rstd = 1.0 / tl.sqrt(var + eps)\n    x_normalized = x_centered * rstd\n\n    # Load the weights\n    w = tl.load(W + col_idx * stride_w, mask=col_idx < N, other=0.0).to(tl.float32)\n\n    # Apply the weights\n    y = x_normalized * w\n\n    # Store the output\n    tl.store(Y + start_y + col_idx * stride_y_col, y, mask=col_idx < N)\n\nimport torch\n\ndef layernorm_forward(X, W, eps=1e-5):\n    # Get the shape of the input tensor\n    B, N = X.shape\n\n    # Allocate the output tensor\n    Y = torch.empty_like(X)\n\n    # Compute the strides\n    stride_x_row = N\n    stride_x_col = 1\n    stride_w = 1\n    stride_y_row = N\n    stride_y_col = 1\n\n    # Define the grid and block sizes\n    grid = (B, (N + 1024 - 1) // 1024)\n    block = 1024\n\n    # Launch the kernel\n    _layer_norm_fwd_kernel[grid, block](\n        X, W, Y, stride_x_row, stride_x_col, stride_w, stride_y_row, stride_y_col, N, eps\n    )\n\n    return Y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# LayerNorm adapted from triton tutorial\n@triton.jit\ndef _layer_norm_fwd_kernel(\n    X,  # pointer to the input\n    W,  # pointer to the weights\n    Y,  # output pointer\n    stride_x_N,\n    stride_x_hn,\n    stride_x_hd,\n    stride_y_N,\n    stride_y_hn,\n    stride_y_hd,\n    stride_w_hn,\n    stride_w_hd,\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    Seq = tl.program_id(0)\n    H = tl.program_id(1)\n\n    X += Seq * stride_x_N + H * stride_x_hn\n    Y += Seq * stride_y_N + H * stride_y_hn\n    W += H * stride_w_hn\n\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w\n\n        tl.store(Y + cols, y.to(X.dtype.element_ty), mask=mask)\n\n\ndef layernorm_forward(\n    X,  # pointer to the input\n    W,  # pointer to the weights\n    eps,  # epsilon to avoid division by zero\n):\n    assert len(X.shape) == 3\n    assert len(W.shape) == 2\n    assert X.shape[-1] == W.shape[-1]\n    assert X.shape[-2] == W.shape[-2]\n\n    y = torch.empty_like(X)\n\n    stride_x_N = X.stride(0)\n    stride_x_hn = X.stride(1)\n    stride_x_hd = X.stride(2)\n\n    stride_y_N = y.stride(0)\n    stride_y_hn = y.stride(1)\n    stride_y_hd = y.stride(2)\n\n    stride_w_hn = W.stride(0)\n    stride_w_hd = W.stride(1)\n\n    N = X.shape[-1]\n    BLOCK_SIZE = 128\n\n    grid = (X.shape[0], X.shape[1])\n    _layer_norm_fwd_kernel[grid](\n        X,\n        W,\n        y,\n        stride_x_N,\n        stride_x_hn,\n        stride_x_hd,\n        stride_y_N,\n        stride_y_hn,\n        stride_y_hd,\n        stride_w_hn,\n        stride_w_hd,\n        N,\n        eps,\n        BLOCK_SIZE,\n    )\n\n    return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel named _fwd_kernel_aligned is designed to perform scaled dot-product attention with relative position biases. This kernel processes queries (Q), keys (K), and values (V) along with a bias matrix (B0) to compute the output (Out). It operates in blocks defined by BLOCK_M, BLOCK_N, and BLOCK_DMODEL, facilitating efficient memory access and computation. The kernel uses a grid to parallelize computations across the third dimension of Q, and the batch dimension. Input tensors are divided into blocks, loaded, and processed in loops to accumulate results. It performs scaled dot-products using tl.dot, applies biases, computes softmax using tl.math.exp2 for efficiency, and updates accumulators for the result. The final output is normalized and stored back in memory.\n\n        The wrapper function _attention_rel_h_rel_w_kernel_aligned_device configures the kernel for specific input sizes and launches it on a grid of (q.shape[2] / BLOCK_M, q.shape[0] * q.shape[1]). It validates input shapes, ensures data types are compatible, and calculates grid dimensions. It then invokes the Triton kernel with appropriate parameters, including strides, shapes, block sizes, and other constants needed for execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(\n    Q, K, V, B0, Out,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_b0b, stride_b0h, stride_b0m, stride_b0n,\n    stride_ob, stride_oh, stride_om,\n    N_CTX, H, BLOCK_M, BLOCK_N, BLOCK_DMODEL, SOFTMAX_SCALE\n):\n    pid = tl.program_id(axis=0)\n    pid_m = tl.program_id(axis=1)\n    num_pid_m = tl.num_programs(axis=1)\n    num_pid_n = tl.num_programs(axis=0)\n    block_m = pid_m * BLOCK_M\n    block_n = pid * BLOCK_N\n\n    # Offsets for Q, K, V, B0, and Out\n    offs_qm = block_m + tl.arange(0, BLOCK_M)\n    offs_kn = block_n + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :] * stride_qh)\n    k_ptrs = K + (offs_kn[None, :] * stride_kn + offs_d[:, None] * stride_kh)\n    v_ptrs = V + (offs_kn[None, :] * stride_vn + offs_d[:, None] * stride_vh)\n    b0_ptrs = B0 + (offs_qm[:, None] * stride_b0m + offs_kn[None, :] * stride_b0n)\n\n    # Load Q, K, V, and B0\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n    b0 = tl.load(b0_ptrs)\n\n    # Compute scaled dot-product\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    qk = tl.dot(q, k, trans_b=True)\n    qk *= SOFTMAX_SCALE\n    qk += b0\n\n    # Compute softmax\n    qk = tl.exp2(qk)\n    qk /= tl.sum(qk, axis=1)[:, None]\n\n    # Compute output\n    out = tl.dot(qk, v)\n\n    # Store output\n    out_ptrs = Out + (offs_qm[:, None] * stride_om + offs_d[None, :] * stride_oh)\n    tl.store(out_ptrs, out)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(q, k, v, b0, out, BLOCK_M, BLOCK_N, BLOCK_DMODEL, SOFTMAX_SCALE):\n    # Validate input shapes\n    assert q.shape == k.shape == v.shape, \"Q, K, and V must have the same shape\"\n    assert q.shape[0] == b0.shape[0], \"Batch dimension of Q and B0 must match\"\n    assert q.shape[1] == b0.shape[1], \"Head dimension of Q and B0 must match\"\n    assert q.shape[2] == b0.shape[2], \"Sequence length of Q and B0 must match\"\n    assert q.shape[3] == b0.shape[3], \"Sequence length of K and B0 must match\"\n\n    # Calculate grid dimensions\n    grid = (q.shape[2] // BLOCK_M, q.shape[0] * q.shape[1])\n\n    # Launch the kernel\n    _fwd_kernel_aligned[grid](\n        q, k, v, b0, out,\n        q.stride(0), q.stride(1), q.stride(2), k.stride(0), k.stride(1), k.stride(2),\n        v.stride(0), v.stride(1), v.stride(2), b0.stride(0), b0.stride(1), b0.stride(2), b0.stride(3),\n        out.stride(0), out.stride(1), out.stride(2),\n        q.shape[2], q.shape[1], BLOCK_M, BLOCK_N, BLOCK_DMODEL, SOFTMAX_SCALE\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(\n    Q, K, V, B0, sm_scale,\n    Out,\n    stride_qh, stride_qm, stride_qk,\n    stride_kh, stride_kn, stride_kk,\n    stride_vh, stride_vk, stride_vn,\n    stride_oh, stride_om, stride_on,\n    stride_b0h, stride_b0m,\n    Z,\n    H,\n    N_CTX,\n    P_SEQ,\n    OUT_DTYPE: tl.constexpr,\n    BIAS_LAST_SIZE: tl.constexpr,\n    B0_NUMEL: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + kv_offset,\n        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + kv_offset,\n        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n\n    # initialize offsets\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    # scale sm_scale by log_2(e) and use\n    # 2^x instead of exp in the loop because CSE and LICM\n    # don't work as expected with `exp` in the loop\n    qk_scale = sm_scale * 1.44269504\n    # load q: it will stay in SRAM throughout\n    q = tl.load(Q_block_ptr)  # , boundary_check=(1, 0), padding_option=\"zero\")\n    q = (q * qk_scale).to(OUT_DTYPE)\n    # loop over k, v and update accumulator\n    lo = 0\n    hi = N_CTX + P_SEQ\n\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = (tl.arange(0, BLOCK_N) %\n                         BIAS_LAST_SIZE) + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                 * stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        # -- load k, v --\n        # , boundary_check=(0, 1), padding_option=\"zero\")\n        k = tl.load(K_block_ptr)\n        # , boundary_check=(1, 0), padding_option=\"zero\")\n        v = tl.load(V_block_ptr)\n        # -- compute qk ---\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k) #, out_dtype=OUT_DTYPE)\n\n        # -- compute rel_h[:, None] + rel_w[None, :] bias ---\n\n        # Bias\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                     * stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += ((b0 + b1) * 1.44269504)\n\n        # -- compute scaling constant ---\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        # -- scale and update acc --\n        acc *= alpha[:, None]\n        acc += tl.dot(p.to(OUT_DTYPE), v)\n        # -- update m_i and l_i --\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        # update pointers\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    # write back l and m\n    acc = acc / l_i[:, None]\n\n    # write back O\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    tl.store(O_block_ptr, acc.to(OUT_DTYPE))\n\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(q, k, v, rel_h_w, sm_scale, o,\n                                                 BLOCK_M,\n                                                 BLOCK_N,\n                                                 num_warps,\n                                                 num_stages):\n    _, Lk, _ = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert q.size() == k.size()\n    assert q.size() == v.size()\n    assert q.size(-2) == rel_h_w.size(-2)\n    assert (q.dtype == torch.bfloat16 or q.dtype == torch.float16)\n    assert k.dtype == q.dtype\n    assert v.dtype == k.dtype\n    assert o.dtype == v.dtype\n    assert rel_h_w.dtype == q.dtype\n    assert rel_h_w.size(-1) == 128\n    # assert rel_h_w.size(-1) == 2 * BLOCK_N\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    # print(\"q.shape[0] * q.shape[1]: \", q.shape[0] * q.shape[1])\n    P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n    assert P_SEQ == 0\n    assert rel_h_w.is_contiguous(), str(rel_h_w.stride())\n    OUT_DTYPE = tl.float16 if q.dtype == torch.float16 else tl.bfloat16\n    _fwd_kernel_aligned[grid](\n        q, k, v,\n        rel_h_w,\n        sm_scale,\n        o,\n        q.stride(1), q.stride(2), q.stride(3),\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        o.stride(1), o.stride(2), o.stride(3),\n        rel_h_w.stride(1), rel_h_w.stride(2),\n        q.shape[0],\n        q.shape[1],\n        q.shape[2],\n        P_SEQ,\n        OUT_DTYPE=OUT_DTYPE,\n        BIAS_LAST_SIZE=(rel_h_w.size(-1) // 2),\n        B0_NUMEL=rel_h_w.size(-1),\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        BLOCK_DMODEL=Lk,\n        num_warps=num_warps,\n        num_stages=num_stages)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is named `_fwd_kernel_flash_decode_stage2`. Its main function is to process input tensors and compute a weighted sum for each batch and head combination. The weighted sum is then stored in the output tensor. The key inputs are:\n            - `B_Seqlen`: Tensor containing the sequence lengths for each batch.\n            - `Mid_O`, `Mid_O_LogExpSum`: Intermediate tensors needed for calculations.\n            - `O`: Output tensor where results are stored.\n            - Various stride values: Used for addressing tensor elements properly.\n            - `BLOCK_SEQ`, `BLOCK_DMODEL`: Constants determining the size of blocks used in computation.\n            The kernel utilizes loops and vector operations to optimize computation across dimensions of `Mid_O`, updating an accumulator that is normalized and stored in `O`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    B_Seqlen,  # Tensor containing the sequence lengths for each batch\n    Mid_O,     # Intermediate tensor\n    Mid_O_LogExpSum,  # Intermediate tensor for log of sum of exponentials\n    O,         # Output tensor\n    stride_B_Seqlen_b,  # Stride for B_Seqlen\n    stride_Mid_O_b,     # Stride for Mid_O batch dimension\n    stride_Mid_O_h,     # Stride for Mid_O head dimension\n    stride_Mid_O_s,     # Stride for Mid_O sequence dimension\n    stride_Mid_O_d,     # Stride for Mid_O dmodel dimension\n    stride_Mid_O_LogExpSum_b,  # Stride for Mid_O_LogExpSum batch dimension\n    stride_Mid_O_LogExpSum_h,  # Stride for Mid_O_LogExpSum head dimension\n    stride_O_b,         # Stride for O batch dimension\n    stride_O_h,         # Stride for O head dimension\n    stride_O_d,         # Stride for O dmodel dimension\n    BLOCK_SEQ: tl.constexpr,  # Block size for sequence dimension\n    BLOCK_DMODEL: tl.constexpr  # Block size for dmodel dimension\n):\n    # Get program IDs\n    pid_b = tl.program_id(axis=0)  # Batch ID\n    pid_h = tl.program_id(axis=1)  # Head ID\n\n    # Compute the sequence length for the current batch\n    seqlen = tl.load(B_Seqlen + pid_b * stride_B_Seqlen_b)\n\n    # Compute the log of the sum of exponentials for the current batch and head\n    log_exp_sum = tl.load(Mid_O_LogExpSum + pid_b * stride_Mid_O_LogExpSum_b + pid_h * stride_Mid_O_LogExpSum_h)\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_DMODEL,), dtype=tl.float32)\n\n    # Loop over the sequence dimension\n    for s in range(0, seqlen, BLOCK_SEQ):\n        # Compute the block size for the current iteration\n        block_size = tl.minimum(BLOCK_SEQ, seqlen - s)\n\n        # Load the current block from Mid_O\n        mid_o_ptr = Mid_O + pid_b * stride_Mid_O_b + pid_h * stride_Mid_O_h + s * stride_Mid_O_s\n        mid_o = tl.load(mid_o_ptr, mask=s + tl.arange(0, BLOCK_SEQ) < seqlen, other=0.0)\n\n        # Compute the weighted sum for the current block\n        for d in range(0, BLOCK_DMODEL):\n            acc[d] += tl.sum(mid_o[:, d] * tl.exp(mid_o[:, d] - log_exp_sum))\n\n    # Normalize the accumulator and store the result in O\n    for d in range(0, BLOCK_DMODEL):\n        acc[d] /= seqlen\n\n    # Store the result in the output tensor\n    o_ptr = O + pid_b * stride_O_b + pid_h * stride_O_h\n    tl.store(o_ptr, acc, mask=tl.arange(0, BLOCK_DMODEL) < BLOCK_DMODEL)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SEQ': 128, 'BLOCK_DMODEL': 64}, num_warps=4),\n        triton.Config({'BLOCK_SEQ': 256, 'BLOCK_DMODEL': 128}, num_warps=8),\n    ],\n    key=['B', 'H', 'D']\n)\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    B_Seqlen,  # Tensor containing the sequence lengths for each batch\n    Mid_O,     # Intermediate tensor\n    Mid_O_LogExpSum,  # Intermediate tensor for log of sum of exponentials\n    O,         # Output tensor\n    stride_B_Seqlen_b,  # Stride for B_Seqlen\n    stride_Mid_O_b,     # Stride for Mid_O batch dimension\n    stride_Mid_O_h,     # Stride for Mid_O head dimension\n    stride_Mid_O_s,     # Stride for Mid_O sequence dimension\n    stride_Mid_O_d,     # Stride for Mid_O dmodel dimension\n    stride_Mid_O_LogExpSum_b,  # Stride for Mid_O_LogExpSum batch dimension\n    stride_Mid_O_LogExpSum_h,  # Stride for Mid_O_LogExpSum head dimension\n    stride_O_b,         # Stride for O batch dimension\n    stride_O_h,         # Stride for O head dimension\n    stride_O_d,         # Stride for O dmodel dimension\n    B: tl.constexpr,  # Batch size\n    H: tl.constexpr,  # Number of heads\n    D: tl.constexpr,  # Dmodel dimension\n    BLOCK_SEQ: tl.constexpr,  # Block size for sequence dimension\n    BLOCK_DMODEL: tl.constexpr  # Block size for dmodel dimension\n):\n    # Get program IDs\n    pid_b = tl.program_id(axis=0)  # Batch ID\n    pid_h = tl.program_id(axis=1)  # Head ID\n\n    # Compute the sequence length for the current batch\n    seqlen = tl.load(B_Seqlen + pid_b * stride_B_Seqlen_b)\n\n    # Compute the log of the sum of exponentials for the current batch and head\n    log_exp_sum = tl.load(Mid_O_LogExpSum + pid_b * stride_Mid_O_LogExpSum_b + pid_h * stride_Mid_O_LogExpSum_h)\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_DMODEL,), dtype=tl.float32)\n\n    # Loop over the sequence dimension\n    for s in range(0, seqlen, BLOCK_SEQ):\n        # Compute the block size for the current iteration\n        block_size = tl.minimum(BLOCK_SEQ, seqlen - s)\n\n        # Load the current block from Mid_O\n        mid_o_ptr = Mid_O + pid_b * stride_Mid_O_b + pid_h * stride_Mid_O_h + s * stride_Mid_O_s\n        mid_o = tl.load(mid_o_ptr, mask=s + tl.arange(0, BLOCK_SEQ) < seqlen, other=0.0)\n\n        # Compute the weighted sum for the current block\n        for d in range(0, BLOCK_DMODEL):\n            acc[d] += tl.sum(mid_o[:, d] * tl.exp(mid_o[:, d] - log_exp_sum))\n\n    # Normalize the accumulator and store the result in O\n    for d in range(0, BLOCK_DMODEL):\n        acc[d] /= seqlen\n\n    # Store the result in the output tensor\n    o_ptr = O + pid_b * stride_O_b + pid_h * stride_O_h\n    tl.store(o_ptr, acc, mask=tl.arange(0, BLOCK_DMODEL) < BLOCK_DMODEL)\n\ndef fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, O, BLOCK_SEQ=128, BLOCK_DMODEL=64):\n    B, H, S, D = Mid_O.shape\n    grid = (B, H)\n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen, Mid_O, Mid_O_LogExpSum, O,\n        B_Seqlen.stride(0), Mid_O.stride(0), Mid_O.stride(1), Mid_O.stride(2), Mid_O.stride(3),\n        Mid_O_LogExpSum.stride(0), Mid_O_LogExpSum.stride(1),\n        O.stride(0), O.stride(1), O.stride(2),\n        B, H, D, BLOCK_SEQ, BLOCK_DMODEL\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    B_Seqlen,\n    Mid_O,  # [batch, head, seq_block_num, head_dim]\n    Mid_O_LogExpSum,  # [batch, head, seq_block_num]\n    O,  # [batch, head, head_dim]\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es,\n    stride_obs, stride_oh, stride_od,\n    BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ\n\n    sum_exp = 0.0\n    max_logic = -float(\"inf\")\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        \n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    \n    tl.store(O + cur_batch * stride_obs + cur_head * stride_oh + offs_d, acc / sum_exp)\n    return\n\n@torch.no_grad()\ndef flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq):\n    Lk = mid_out.shape[-1]\n    assert Lk in {16, 32, 64, 128}\n    batch, head_num = mid_out.shape[0], mid_out.shape[1]\n    grid = (batch, head_num)\n    \n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen, mid_out, mid_out_logexpsum, O,\n        mid_out.stride(0), mid_out.stride(1), mid_out.stride(2), mid_out.stride(3),\n        mid_out_logexpsum.stride(0), mid_out_logexpsum.stride(1), mid_out_logexpsum.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_SEQ=block_seq,\n        BLOCK_DMODEL=Lk,\n        num_warps=4,\n        num_stages=2,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton kernel function and a wrapper in PyTorch to perform a backward pass operation of a concatenated index select operation. The main function `index_select_cat_bwd` takes in `grad_source`, `index`, and `grad_output` as inputs, which are 2D CUDA tensors. The function checks the shape and stride compatibility of these inputs, and then invokes the Triton kernel `index_select_cat_bwd_kernel`. This kernel calculates the gradient by adding selected indices from the `grad_output` to `grad_source` according to `index`. The operation is performed in a block-wise parallel manner using Triton's grid-based execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef index_select_cat_bwd_kernel(\n    grad_source_ptr,  # Pointer to the gradient of the source tensor\n    index_ptr,       # Pointer to the index tensor\n    grad_output_ptr, # Pointer to the gradient of the output tensor\n    grad_source_stride0,  # Stride of the source tensor in the 0th dimension\n    grad_source_stride1,  # Stride of the source tensor in the 1st dimension\n    index_stride0,       # Stride of the index tensor in the 0th dimension\n    grad_output_stride0,  # Stride of the output tensor in the 0th dimension\n    grad_output_stride1,  # Stride of the output tensor in the 1st dimension\n    n,                   # Number of elements in the 0th dimension\n    m,                   # Number of elements in the 1st dimension\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n\n\n    for i in range(m):\n        index = tl.load(index_ptr + offsets * index_stride0, mask=mask)\n        grad_output = tl.load(grad_output_ptr + offsets * grad_output_stride0 + i * grad_output_stride1, mask=mask)\n        tl.atomic_add(grad_source_ptr + index * grad_source_stride0 + i * grad_source_stride1, grad_output, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\nclass IndexSelectCatBwdFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_source, index, grad_output):\n        # Check shape and stride compatibility\n        assert grad_source.dim() == 2, \"grad_source must be a 2D tensor\"\n        assert index.dim() == 1, \"index must be a 1D tensor\"\n        assert grad_output.dim() == 2, \"grad_output must be a 2D tensor\"\n        assert grad_source.size(0) == index.size(0), \"grad_source and index must have the same size in the 0th dimension\"\n        assert grad_source.size(1) == grad_output.size(1), \"grad_source and grad_output must have the same size in the 1st dimension\"\n\n        n, m = grad_source.size()\n        BLOCK_SIZE = 128\n\n        # Allocate output tensor\n        grad_source = grad_source.clone()\n\n        # Define grid and block dimensions\n        grid = (triton.cdiv(n, BLOCK_SIZE),)\n\n        # Launch the kernel\n        index_select_cat_bwd_kernel[grid](\n            grad_source_ptr=grad_source,\n            index_ptr=index,\n            grad_output_ptr=grad_output,\n            grad_source_stride0=grad_source.stride(0),\n            grad_source_stride1=grad_source.stride(1),\n            index_stride0=index.stride(0),\n            grad_output_stride0=grad_output.stride(0),\n            grad_output_stride1=grad_output.stride(1),\n            n=n,\n            m=m,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n\n        return grad_source\n\ndef index_select_cat_bwd(grad_source, index, grad_output):\n    return IndexSelectCatBwdFunction.apply(grad_source, index, grad_output)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef index_select_cat_bwd_kernel(\n    grad_source_ptr,  # *Pointer* to grad_source tensor.\n    index_ptr,  # *Pointer* to index tensor.\n    grad_output_ptr,  # *Pointer* to grad_output tensor.\n    num_rows,\n    num_indices,\n    num_cols,\n    stride0,  # Stride information of input and source tensor.\n    stride1,\n    BLOCK_SIZE_INDEX: tl.constexpr,  # Number of indices each program should process.\n    BLOCK_SIZE_COL: tl.constexpr,  # Number of cols each program should process.\n):\n    pid0 = tl.program_id(axis=0)  # We use 3D launch grid\n    pid1 = tl.program_id(axis=1)\n\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n\n    # load grad_output\n    grad_output_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    grad_output_offsets = (\n        grad_output_ptr\n        + grad_output_indices[:, None] * stride0\n        + cols[None, :] * stride1\n    )\n    grad_output_mask = (grad_output_indices[:, None] < num_indices) & (\n        cols[None, :] < num_cols\n    )\n    grad_output = tl.load(grad_output_offsets, mask=grad_output_mask).to(tl.float32)\n\n    # select indices from grad_source\n    grad_source_indices = tl.load(\n        index_ptr + grad_output_indices, mask=(grad_output_indices < num_indices)\n    )\n    grad_source_offsets = (\n        grad_source_ptr\n        + grad_source_indices[:, None] * stride0\n        + cols[None, :] * stride1\n    )\n\n    # compute scaled index add and save\n    tl.store(grad_source_offsets, grad_output, mask=grad_output_mask)\n\n\ndef index_select_cat_bwd(\n    grad_source: torch.Tensor,\n    index: torch.Tensor,\n    grad_output: torch.Tensor,\n):\n    if not (grad_source.is_cuda and grad_output.is_cuda):\n        raise ValueError(\"The grad_source and grad_output tensor must be of type CUDA!\")\n\n    if not (grad_source.ndim == 2 and grad_output.ndim == 2):\n        raise ValueError(\n            f\"The grad_source and grad_output must be three-dimensional \"\n            f\"(got {grad_source.ndim} and {grad_output.ndim})!\"\n        )\n    if not grad_source.shape[1] == grad_output.shape[1]:\n        raise ValueError(\n            f\"The number of elements along dimension 1 of grad_source and grad_output must be the same \"\n            f\"(got {grad_source.shape[1]} and {grad_output.shape[1]})\"\n        )\n\n    num_rows, num_cols = grad_source.shape\n    num_indices, num_cols = grad_output.shape\n    if not num_rows >= num_indices:\n        raise ValueError(\n            f\"The number of elements along dimension 0 of grad_source must be larger than that of grad_output \"\n            f\"(got {num_rows} and {num_indices})!\"\n        )\n    if not index.shape[0] == num_indices:\n        raise ValueError(\n            f\"The number of indices and the number of elements along dimension 0 of grad_output must match \"\n            f\"(got {index.shape[0]} and {num_indices})!\"\n        )\n\n    stride0, stride1 = grad_source.stride(0), grad_source.stride(1)\n    if not (grad_output.stride(0) == stride0 and grad_output.stride(1) == stride1):\n        raise ValueError(\n            f\"The strides of the grad_source and grad_output tensors must match \"\n            f\"(got {stride0} vs. {grad_output.stride(0)}, {stride1} vs. {grad_output.stride(1)})!\"\n        )\n\n    def grid(meta):\n        return (\n            triton.cdiv(num_indices, meta[\"BLOCK_SIZE_INDEX\"]),\n            triton.cdiv(num_cols, meta[\"BLOCK_SIZE_COL\"]),\n        )\n\n    index_select_cat_bwd_kernel[grid](\n        grad_source,\n        index,\n        grad_output,\n        num_rows,\n        num_indices,\n        num_cols,\n        grad_source.stride(0),\n        grad_source.stride(1),\n        BLOCK_SIZE_INDEX=1,\n        BLOCK_SIZE_COL=512,\n    )\n\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThis Triton-based operator computes the Kullback-Leibler divergence (KL divergence) in the forward and backward passes. The main functions are `kldiv_forward_triton` and `kldiv_backward_triton`, utilizing kernels `_kldiv_kernel_forward` and `_kldiv_kernel_backward` respectively. \n\n- `kldiv_forward_triton`: Computes the forward pass of the KL divergence. It takes predictions `y_pred`, ground truth `y_true`, a boolean `log_target` indicating if the target is in log-space, and `reduction` type ('none', 'sum', 'mean', 'batchmean'). The result is a tensor with the specified reduction applied. The function uses `BLOCK_SIZE` and `num_warps` for parallel execution configuration.\n\n- `_kldiv_kernel_forward`: Implements the forward computation in Triton, iterating over prediction and target values and calculating the KL divergence for each element or batch depending on the reduction mode.\n\n- `kldiv_backward_triton`: Computes the gradient of the KL divergence with respect to predictions. The function takes `input`, `target`, `grad_output`, and a boolean `log_target`. It configures and invokes `_kldiv_kernel_backward` to compute gradients.\n\n- `_kldiv_kernel_backward`: Implements the backward computation in Triton, calculating gradients based on the target values and handling log-space conditions.\n\nCommon constants include `_REDUCTION_MODE_*` for managing reduction modes, and the function `get_num_warps` helps determine the optimal number of warps based on `BLOCK_SIZE`.\n", "predict": "import triton\nimport triton.language as tl\n\n# Constants for reduction modes\n_REDUCTION_MODE_NONE = 0\n_REDUCTION_MODE_SUM = 1\n_REDUCTION_MODE_MEAN = 2\n_REDUCTION_MODE_BATCHMEAN = 3\n\n# Helper function to determine the number of warps\ndef get_num_warps(block_size):\n    if block_size <= 128:\n        return 2\n    elif block_size <= 256:\n        return 4\n    else:\n        return 8\n\n@triton.jit\ndef _kldiv_kernel_forward(\n    y_pred_ptr, y_true_ptr, output_ptr, n_elements, log_target, reduction, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n    y_pred = tl.load(y_pred_ptr + offsets, mask=mask)\n    y_true = tl.load(y_true_ptr + offsets, mask=mask)\n\n    if log_target:\n        kl_div = y_true * (tl.exp(y_true) - y_pred)\n    else:\n        kl_div = y_true * (tl.log(y_true) - y_pred)\n\n    if reduction == _REDUCTION_MODE_NONE:\n        tl.store(output_ptr + offsets, kl_div, mask=mask)\n    elif reduction == _REDUCTION_MODE_SUM:\n        output = tl.sum(kl_div, axis=0)\n        if pid == 0:\n            tl.store(output_ptr, output)\n    elif reduction == _REDUCTION_MODE_MEAN:\n        output = tl.sum(kl_div, axis=0) / n_elements\n        if pid == 0:\n            tl.store(output_ptr, output)\n    elif reduction == _REDUCTION_MODE_BATCHMEAN:\n        output = tl.sum(kl_div, axis=0) / y_true.shape[0]\n        if pid == 0:\n            tl.store(output_ptr, output)\n\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction):\n    n_elements = y_pred.numel()\n    output = triton.empty((1,) if reduction != _REDUCTION_MODE_NONE else y_pred.shape, device=y_pred.device, dtype=y_pred.dtype)\n\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    _kldiv_kernel_forward[grid](\n        y_pred, y_true, output, n_elements, log_target, reduction, BLOCK_SIZE, num_warps=num_warps\n    )\n\n    return output\n\n@triton.jit\ndef _kldiv_kernel_backward(\n    input_ptr, target_ptr, grad_output_ptr, grad_input_ptr, n_elements, log_target, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n    input = tl.load(input_ptr + offsets, mask=mask)\n    target = tl.load(target_ptr + offsets, mask=mask)\n    grad_output = tl.load(grad_output_ptr, mask=mask)\n\n    if log_target:\n        grad_input = -target * tl.exp(target) * grad_output\n    else:\n        grad_input = -target / input * grad_output\n\n    tl.store(grad_input_ptr + offsets, grad_input, mask=mask)\n\ndef kldiv_backward_triton(input, target, grad_output, log_target):\n    n_elements = input.numel()\n    grad_input = triton.empty_like(input)\n\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    _kldiv_kernel_backward[grid](\n        input, target, grad_output, grad_input, n_elements, log_target, BLOCK_SIZE, num_warps=num_warps\n    )\n\n    return grad_input\n\nimport torch\n\n# Example tensors\ny_pred = torch.randn(100, 100, device='cuda')\ny_true = torch.randn(100, 100, device='cuda').exp()  # Ensure y_true is positive\nlog_target = False\nreduction = _REDUCTION_MODE_MEAN\n\n# Forward pass\noutput = kldiv_forward_triton(y_pred, y_true, log_target, reduction)\nprint(\"KL Divergence:\", output)\n\n# Backward pass\ngrad_output = torch.ones_like(output)  # Example gradient\ngrad_input = kldiv_backward_triton(y_pred, y_true, grad_output, log_target)\nprint(\"Gradient of KL Divergence:\", grad_input)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n\ndef get_num_warps(BLOCK_SIZE):\n    num_warps = 4\n    if BLOCK_SIZE >= 32768:\n        num_warps = 32\n    elif BLOCK_SIZE >= 8192:\n        num_warps = 16\n    elif BLOCK_SIZE >= 2048:\n        num_warps = 8\n\n    return num_warps\n\n\nMAX_FUSED_SIZE = 65536 // 4\n\n_REDUCTION_MODE_NONE = tl.constexpr(0)\n_REDUCTION_MODE_SUM = tl.constexpr(1)\n_REDUCTION_MODE_MEAN = tl.constexpr(2)\n_REDUCTION_MODE_BATCHMEAN = tl.constexpr(3)\n\n_str_to_reduction_mode = {\n    \"none\": _REDUCTION_MODE_NONE.value,\n    \"sum\": _REDUCTION_MODE_SUM.value,\n    \"mean\": _REDUCTION_MODE_MEAN.value,\n    \"batchmean\": _REDUCTION_MODE_BATCHMEAN.value,\n}\n\n@triton.jit\ndef _kldiv_kernel_forward(\n    y_ptr,  # [B, S], prediction ptr, the kernel expects the prediction in log-space\n    y_stride,  # int, prediction stride\n    gt_ptr,  # [B, S], ground truth ptr\n    gt_stride,  # int, ground truth stride\n    loss_ptr,  # [B] or [B, S] if reduction == _REDUCTION_MODE_NONE, output ptr\n    loss_stride,  # int, output stride\n    n_cols,  # int, number of columns in the input tensor\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n    reduction: tl.constexpr = _REDUCTION_MODE_BATCHMEAN,\n):\n    pid = tl.program_id(0).to(tl.int64)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n\n        # KL(y_true || y) = y_true * (log(y_true) - log(y))\n        # We compute KL(y_true || y) with y in the log-space\n        if not log_target:\n            loss = y_true * (tl.log(y_true) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n\n        if reduction == _REDUCTION_MODE_NONE:\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss = tl.sum(loss, axis=0)\n            tl.store(loss_ptr, loss)\n            loss_ptr += 1\n\n@triton.jit\ndef _kldiv_kernel_backward(\n    input_ptr,\n    input_stride,\n    target_ptr,\n    target_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n):\n    pid = tl.program_id(0).to(tl.int64)\n\n    input_ptr += pid * input_stride\n    target_ptr += pid * target_stride\n\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n\n        tl.store(input_ptr + offsets, res, mask=mask)\n\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction):  # [B, S]  # [B, S]\n    B, S = y_pred.shape\n\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(S))\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    grid = (B,)\n    reduction = _str_to_reduction_mode[reduction]\n\n    out_size = (B, S) if reduction == _REDUCTION_MODE_NONE.value else (B,)\n    output_tensor = torch.zeros(\n        out_size,\n        dtype=torch.float32,\n        device=\"cuda\",\n    )\n\n    _kldiv_kernel_forward[grid](\n        y_pred,\n        y_pred.stride(0),\n        y_true,\n        y_true.stride(0),\n        output_tensor,\n        output_tensor.stride(0),\n        S,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n        reduction=reduction,\n    )\n\n    if reduction == _REDUCTION_MODE_BATCHMEAN.value:\n        return output_tensor.sum() / B\n    elif reduction == _REDUCTION_MODE_SUM.value:\n        return output_tensor.sum(axis=0)\n    elif reduction == _REDUCTION_MODE_MEAN.value:\n        return output_tensor.mean(axis=0)\n    else:\n        return output_tensor\n\ndef kldiv_backward_triton(input, target, grad_output, log_target):\n    B, S = input.shape\n\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(S))\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    grid = (B,)\n\n    _kldiv_kernel_backward[grid](\n        input,\n        input.stride(0),\n        target,\n        target.stride(0),\n        S,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n    )\n\n    if torch.equal(\n        grad_output,\n        torch.tensor(\n                1.0,\n            dtype=grad_output.dtype,\n            device=\"cuda\",\n        ),\n    ):\n        return input\n\n    return input * grad_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code defines several matrix multiplication and dequantization kernels for handling 4-bit quantized weights. The main functions are:\n\n            1. `matmul4_kernel`: This kernel computes the matrix product of a matrix `A` (float16) and a quantized matrix `B` (int32) which is encoded in 4-bits per value. The quantized values are dequantized using `scales` and `zeros`, producing an output matrix `C` (float16). It uses block-level parallelism to efficiently perform the operations in parallel.\n\n            2. `matmul_dequantize_int4_gptq`: This function wraps around `matmul4_kernel` and sets up the Triton grid and meta-parameters required to run the kernel. It accepts matrices `x`, `qweight`, `scales`, and `qzeros` and an optional `output` tensor. It determines the grid size based on `M`, `N`, and `K` dimensions and invokes the kernel to compute the matrix product.\n\n            3. `matmul_kernel`: Similar to `matmul4_kernel`, but supports a configurable `SPLIT_K` parameter for splitting the K dimension into multiple parts. This enables processing large matrices by dividing the workload.\n\n            4. `matmul_dequantize_int4_s2`: This is a wrapper function for `matmul_kernel`. It computes the matrix product and dequantizes the result from 4-bit integers. It calculates grid dimensions based on input sizes and invokes the kernel to perform the computation.\n\n            5. `dequantize_kernel`: This kernel dequantizes a quantized weight matrix `b` into a full precision matrix `fpb_ptr`. It applies scales and zero-point correction to convert 4-bit values back into float16 values.\n\n            6. `dequantize_int4`: This function calls the `dequantize_kernel` to convert a 4-bit integer matrix into a float16 matrix. It sets up the grid and other meta-parameters for the kernel.\n\n            7. `matmul_dequantize_int4_s1`: This function dequantizes the weight matrix before performing a standard `torch.mm` matrix multiplication. It is used when the weight matrix is constant and the overhead of dequantization can be amortized over multiple operations.\n\n            Each function and kernel is carefully optimized using Triton's autotuning and is designed to efficiently handle operations on quantized matrices with parallel computation.\n\n            ", "predict": "Certainly! Below is the Triton code that defines the matrix multiplication and dequantization kernels as described. The code includes the kernels and their respective wrapper functions.\n\n```python\nimport triton\nimport triton.language as tl\nimport torch\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 16\nBLOCK_SIZE_N = 16\nBLOCK_SIZE_K = 32\nGROUP_SIZE_M = 8\n\n# Kernel for matrix multiplication with 4-bit quantized weights\n@triton.jit\ndef matmul4_kernel(\n    a_ptr, b_ptr, scales_ptr, zeros_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n,\n    stride_zeros_g, stride_zeros_n,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        a = tl.trans(a)\n        b = tl.trans(b)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    scales_ptrs = scales_ptr + (offs_bn[None, :] * stride_scales_n + (offs_k[None, :] // 8) * stride_scales_g)\n    zeros_ptrs = zeros_ptr + (offs_bn[None, :] * stride_zeros_n + (offs_k[None, :] // 8) * stride_zeros_g)\n    scales = tl.load(scales_ptrs)\n    zeros = tl.load(zeros_ptrs)\n    zeros = (zeros * -1.0) + 0.5\n    accumulator = accumulator * scales + zeros\n\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n# Wrapper function for matmul4_kernel\ndef matmul_dequantize_int4_gptq(x, qweight, scales, qzeros, output=None):\n    M, K = x.shape\n    _, N = qweight.shape\n    if output is None:\n        output = torch.empty((M, N), dtype=torch.float16, device=x.device)\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n    matmul4_kernel[grid](\n        x, qweight, scales, qzeros, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M\n    )\n    return output\n\n# Kernel for matrix multiplication with 4-bit quantized weights and SPLIT_K\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, scales_ptr, zeros_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n,\n    stride_zeros_g, stride_zeros_n,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K * SPLIT_K):\n        for s in range(SPLIT_K):\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n            a = tl.trans(a)\n            b = tl.trans(b)\n            accumulator += tl.dot(a, b)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    scales_ptrs = scales_ptr + (offs_bn[None, :] * stride_scales_n + (offs_k[None, :] // 8) * stride_scales_g)\n    zeros_ptrs = zeros_ptr + (offs_bn[None, :] * stride_zeros_n + (offs_k[None, :] // 8) * stride_zeros_g)\n    scales = tl.load(scales_ptrs)\n    zeros = tl.load(zeros_ptrs)\n    zeros = (zeros * -1.0) + 0.5\n    accumulator = accumulator * scales + zeros\n\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n# Wrapper function for matmul_kernel\ndef matmul_dequantize_int4_s2(x, qweight, scales, qzeros, SPLIT_K=1, output=None):\n    M, K = x.shape\n    _, N = qweight.shape\n    if output is None:\n        output = torch.empty((M, N), dtype=torch.float16, device=x.device)\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n    matmul_kernel[grid](\n        x, qweight, scales, qzeros, output,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, SPLIT_K\n    )\n    return output\n\n# Kernel for dequantizing 4-bit weights\n@triton.jit\ndef dequantize_kernel(\n    b_ptr, scales_ptr, zeros_ptr, fpb_ptr,\n    N, K,\n    stride_bk, stride_bn,\n    stride_scales_g, stride_scales_n,\n    stride_zeros_g, stride_zeros_n,\n    stride_fpb_n, stride_fpb_k,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % num_pid_n\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    scales_ptrs =", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        # Additional configurations are omitted for brevity...\n    ],\n    key=['M', 'N', 'K', 'NO_GROUPS'],\n)\n@triton.jit\ndef matmul4_kernel(\n    a_ptr, b_ptr, c_ptr,\n    scales_ptr, zeros_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n,\n    stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n   # (BLOCK_SIZE_N,)\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)   # (BLOCK_SIZE_N,)\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32    \n        zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)   # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated    \n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)  # (BLOCK_SIZE_N,)\n            ptr = zeros_ptrs + g_id * stride_zeros_g   # (BLOCK_SIZE_N,)\n            zeros = tl.load(ptr)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32    \n            zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n            zeros = (zeros) * scales  # (BLOCK_SIZE_N,) float16    \n        b = (b >> shifter[:, None]) & 0xF  # Extract the 4-bit values\n        b = b * scales[None, :] - zeros[None, :]  # Scale and shift\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk  \n    c = accumulator.to(tl.float16)  \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n\ndef matmul_dequantize_int4_gptq(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size, output=None) -> torch.FloatTensor:\n    \"\"\"\n    Compute the matrix multiplication C = A x B + bias.\n    Where B is quantized using GPTQ and groupsize = -1 into 4-bit values.\n\n    A is of shape (..., K) float16\n    qweight is of shape (K//8, N) int32\n    scales is of shape (G, N) float16\n    qzeros is of shape (G, N//8) int32\n    bias is of shape (1, N) float16\n\n    groupsize is the number of infeatures in each group.\n    G = K // groupsize\n\n    Returns C of shape (..., N) float16\n    \"\"\"\n    assert x.shape[-1] == (qweight.shape[0] * 8), \"A must be a multiple of 8 in the last dimension\"\n    assert x.is_contiguous(), \"A must be contiguous\"\n\n    M, K = x.shape\n    N = qweight.shape[1]\n\n    if output is None:\n        inplace = False\n        output = torch.empty((M, N), device=x.device, dtype=torch.float16)\n    else:\n        inplace = True\n\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n    matmul4_kernel[grid](\n        x, qweight, output,\n        scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        group_size, group_size == K,\n    )\n    if not inplace:\n        return output\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        # Additional configurations are omitted for brevity...\n    ],\n    key=['M', 'N', 'K'],\n    reset_to_zero=['c_ptr']\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    bs_ptr, bzp_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    group_size,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr\n    ):\n    \"\"\"\n    Matrix multiplication kernel with dequantization for 4-bit integers\n    A: [M, K], B: [K//8, N], C: [M, N]\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        bs_ptrs = bs_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bsk \\\n            + offs_bn[None, :] * stride_bsn\n        bzp_ptrs = bzp_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bzpk \\\n            + (offs_bn[None, :] // 8) * stride_bzpn\n        b_shift_bits = (offs_k[:, None] % 8) * 4 \n        bzp_shift_bits = (offs_bn[None, :] % 8) * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n\n        int_b = (b >> b_shift_bits) & 0xF\n        int_bzp = (bzp >> bzp_shift_bits) & 0xF\n        b = ((int_b - int_bzp) * bs).to(tl.float16)\n        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16))\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K * SPLIT_K * stride_bk // 8)\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_dequantize_int4_s2(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size: int = 128, output=None) -> torch.FloatTensor:\n    \"\"\"\n    Perform matrix multiplication and dequantize the result from 4-bit integers.\n    \"\"\"\n    assert x.is_contiguous(), \"A must be contiguous\"\n    assert qweight.is_contiguous(), \"B must be contiguous\"  \n    M, K = x.shape\n    N = scales.shape[1]\n    if output is None:\n        output = torch.zeros((M, N), device=x.device, dtype=torch.float16)  \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n        META['SPLIT_K'],\n    )\n    matmul_kernel[grid](\n        x, qweight, output,\n        scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        group_size,\n    )\n    return output\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n        # Additional configurations are omitted for brevity...\n    ],\n    key=['K', 'N'],\n)\n@triton.jit\ndef dequantize_kernel(\n    # Pointers to matrices\n    b_ptr, b_scale_ptr, b_zp_ptr, fpb_ptr,\n    # Matrix dimensions\n    K, N, group_size,\n    stride_bk, stride_bn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    stride_fpbk, stride_fpbn,\n    # Meta-parameters\n    BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"Dequantize tile [BLOCK_SIZE_K, BLOCK_SIZE_N] in full precision.\n    We should assert BLOCK_SIZE_N % 8 == 0.\n    weight[K // 8, N], scale[K // group_size, N], zp[K // group_size, N // group_size]\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (offs_n[None, :] // 8) * stride_bzpn\n    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[None, :] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = (offs_k[:, None] % 8) * 4\n    bzp_shift = (offs_n[None, :] % 8) * 4\n    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)\n\n\ndef dequantize_int4(b, b_scale, b_zero_point, device, dtype, group_size):\n    Kw, N = b.shape\n    K = Kw * 8\n    fp_b = torch.ones((K, N), device=device, dtype=dtype)\n    grid = lambda META: (\n        triton.cdiv(K, META['BLOCK_SIZE_K']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']), \n    )\n    dequantize_kernel[grid](\n        b, b_scale, b_zero_point, fp_b,\n        K, N, group_size,\n        b.stride(0), b.stride(1),\n        b_scale.stride(0), b_scale.stride(1),\n        b_zero_point.stride(0), b_zero_point.stride(1),\n        fp_b.stride(0), fp_b.stride(1)\n    )\n    return fp_b\n\n\ndef matmul_dequantize_int4_s1(a, b, b_scale, b_zero_point, group_size=128, out=None):\n    \"\"\"\n    Matmul dequantize int4 s1 dequantize weight to `fp_b` and do fp16 torch.mm,\n    this is for `prefill` stage, since weight size is fixed so is dequantize overhead,\n    perfill stage have more tokens to amortize dequant cost.\n    \"\"\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    Kw, N = b.shape\n    if out is None:\n        out = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    fp_b = dequantize_int4(b, b_scale, b_zero_point, a.device, a.dtype, group_size)\n    torch.mm(a, fp_b, out=out)\n    fp_b = None\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel performs row-wise quantization of a 2D input tensor. The kernel, named _quantize_rowwise, operates on each row independently. For each row, it calculates the maximum absolute value of the elements, scales each element by this max value to fit within an int8 range, and stores the results in an output tensor. The corresponding max values for each row are stored in a separate output tensor. The inputs to the kernel include pointers to the input tensor, output tensor, and max values tensor, along with the number of elements and two block size-related constants (BLOCK_SIZE and P2). The main function, quantize_rowwise, prepares the tensors and configurations required to launch the kernel, and returns the quantized tensor and the max values.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel function\n@triton.jit\ndef _quantize_rowwise(\n    input_ptr: tl.tensor, \n    output_ptr: tl.tensor, \n    max_values_ptr: tl.tensor, \n    n_rows: tl.int32, \n    n_cols: tl.int32, \n    BLOCK_SIZE: tl.constexpr, \n    P2: tl.constexpr\n):\n    row = tl.program_id(0)\n    if row < n_rows:\n        max_val = 0.0\n        for col in range(0, n_cols, BLOCK_SIZE):\n            offsets = row * n_cols + col + tl.arange(0, BLOCK_SIZE)\n            input_vals = tl.load(input_ptr + offsets, mask=offsets < row * n_cols + n_cols)\n            max_val = tl.max(max_val, tl.max(tl.abs(input_vals), axis=0))\n\n        tl.store(max_values_ptr + row, max_val)\n\n        for col in range(0, n_cols, BLOCK_SIZE):\n            offsets = row * n_cols + col + tl.arange(0, BLOCK_SIZE)\n            input_vals = tl.load(input_ptr + offsets, mask=offsets < row * n_cols + n_cols)\n            quantized_vals = (127.0 * input_vals / max_val).to(tl.int8)\n            tl.store(output_ptr + offsets, quantized_vals, mask=offsets < row * n_cols + n_cols)\n\n# Main function to prepare tensors and launch the kernel\ndef quantize_rowwise(input_tensor: torch.Tensor):\n    assert input_tensor.dim() == 2, \"Input tensor must be 2D\"\n    n_rows, n_cols = input_tensor.shape\n\n    # Allocate output tensors\n    output_tensor = torch.empty((n_rows, n_cols), dtype=torch.int8, device=input_tensor.device)\n    max_values_tensor = torch.empty((n_rows,), dtype=torch.float32, device=input_tensor.device)\n\n    # Define grid and block sizes\n    BLOCK_SIZE = 128\n    P2 = 128\n    grid = (n_rows,)\n\n    # Launch the kernel\n    _quantize_rowwise[grid](\n        input_tensor, \n        output_tensor, \n        max_values_tensor, \n        n_rows, \n        n_cols, \n        BLOCK_SIZE, \n        P2\n    )\n\n    return output_tensor, max_values_tensor\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    input_tensor = torch.randn((1024, 1024), dtype=torch.float32, device=\"cuda\")\n\n    # Perform row-wise quantization\n    output_tensor, max_values_tensor = quantize_rowwise(input_tensor)\n\n    print(\"Quantized Tensor:\", output_tensor)\n    print(\"Max Values Tensor:\", max_values_tensor)\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n# TODO: autotune this better.\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_maxs,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_maxs + pid, max_val)\n\ndef quantize_rowwise(x: torch.Tensor):\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_maxs = torch.empty(x.shape[0], device=x.device, dtype=torch.float16)\n\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n\n    assert x.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_maxs, n_elements, BLOCK_SIZE=x.shape[1], P2=P2)\n    return output, output_maxs\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is called '_fwd_kernel'. It computes a forward pass of a token softmax reduction on input matrices for attention mechanisms in deep learning models. The kernel takes as input tensors: 'Logics', 'V', and output 'Out', along with auxiliary tensors like 'B_Loc', 'B_Start_Loc', 'B_Seqlen', and scalar values for dimensions. The kernel operates across batch and head dimensions, determined by 'program_id'. It processes blocks of data using two loop variables 'start_n' and 'offs_n' for efficient computation. Inside the loop, it computes softmax normalization over input logits 'Logics' and updates output tensor 'Out'. Memory access patterns and stride operations are used to efficiently access elements in 'V', 'Logics', and 'Out'. The function 'token_softmax_reducev_fwd' sets up the parameters and executes the kernel across a 2D grid, determined by the batch and head size. ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Logics_ptr,  # Input logits tensor\n    V_ptr,       # Input value tensor\n    Out_ptr,     # Output tensor\n    B_Loc_ptr,   # Auxiliary tensor for batch location\n    B_Start_Loc_ptr,  # Auxiliary tensor for batch start location\n    B_Seqlen_ptr,  # Auxiliary tensor for sequence length\n    stride_logic_h,  # Stride for logits in head dimension\n    stride_logic_s,  # Stride for logits in sequence dimension\n    stride_v_h,      # Stride for V in head dimension\n    stride_v_s,      # Stride for V in sequence dimension\n    stride_v_d,      # Stride for V in feature dimension\n    stride_out_h,    # Stride for Out in head dimension\n    stride_out_s,    # Stride for Out in sequence dimension\n    stride_out_d,    # Stride for Out in feature dimension\n    batch_size,      # Batch size\n    head_size,       # Head size\n    seq_len,         # Sequence length\n    feature_dim,     # Feature dimension\n    BLOCK_SIZE: tl.constexpr\n):\n    # Get the program ID in the grid\n    pid = tl.program_id(axis=0)\n    bid = tl.program_id(axis=1)\n\n    # Compute the start and end indices for the batch and head\n    start_n = B_Start_Loc_ptr[bid]\n    end_n = start_n + B_Seqlen_ptr[bid]\n\n    # Compute the offset for the current batch and head\n    offs_h = pid * stride_logic_h\n    offs_s = tl.arange(0, BLOCK_SIZE) * stride_logic_s\n\n    # Initialize the output and max value\n    out = tl.zeros((BLOCK_SIZE, feature_dim), dtype=tl.float32)\n    max_val = tl.full((BLOCK_SIZE,), float('-inf'), dtype=tl.float32)\n\n    # Loop over the sequence length\n    for n in range(start_n, end_n, BLOCK_SIZE):\n        # Compute the current sequence offset\n        offs_n = n + tl.arange(0, BLOCK_SIZE) * stride_logic_s\n\n        # Load the logits and values\n        logits = tl.load(Logics_ptr + offs_h + offs_n, mask=offs_n < end_n, other=float('-inf'))\n        values = tl.load(V_ptr + offs_h + offs_n * stride_v_s, mask=offs_n < end_n, other=0.0)\n\n        # Compute the max value\n        max_val = tl.maximum(max_val, logits)\n\n        # Compute the exponentials\n        exp_logits = tl.exp(logits - max_val)\n\n        # Compute the weighted sum\n        out += exp_logits[:, None] * values\n\n    # Normalize the output\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n    out /= sum_exp_logits[:, None]\n\n    # Store the result\n    tl.store(Out_ptr + offs_h + start_n * stride_out_s, out, mask=offs_n < end_n)\n\nimport torch\n\ndef token_softmax_reducev_fwd(\n    Logics,  # Input logits tensor\n    V,       # Input value tensor\n    B_Loc,   # Auxiliary tensor for batch location\n    B_Start_Loc,  # Auxiliary tensor for batch start location\n    B_Seqlen,  # Auxiliary tensor for sequence length\n    batch_size,  # Batch size\n    head_size,   # Head size\n    seq_len,     # Sequence length\n    feature_dim  # Feature dimension\n):\n    # Create the output tensor\n    Out = torch.empty_like(V)\n\n    # Define the grid size\n    grid = (head_size, batch_size)\n\n    # Launch the kernel\n    _fwd_kernel[grid](\n        Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen,\n        Logics.stride(1), Logics.stride(2),\n        V.stride(1), V.stride(2), V.stride(3),\n        Out.stride(1), Out.stride(2), Out.stride(3),\n        batch_size, head_size, seq_len, feature_dim,\n        BLOCK_SIZE=128\n    )\n\n    return Out\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n\n@triton.jit\ndef _fwd_kernel(\n    Logics, V, Out,\n    B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs,\n    stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od,\n    stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, # Avoid reading NaN data\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    off_v = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len) * stride_b_loc_s\n\n    v_ptrs = V + off_v\n\n    e_max = float(\"-inf\")\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) * stride_b_loc_s, mask=(start_n + offs_n) < cur_batch_seq_len, other=other_kv_index)\n\n        qk = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, \n                     mask=start_n + offs_n < cur_batch_seq_len, other=float(\"-inf\"))\n    \n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_softmax_reducev_fwd(logics, v, o, b_loc, b_start_loc, b_seq_len, max_input_len, other_kv_index):\n    BLOCK = 64\n    batch, head = b_seq_len.shape[0], logics.shape[0]\n    grid = (batch, head)\n    num_warps = 1\n    _fwd_kernel[grid](\n        logics, v, o, b_loc, b_start_loc, b_seq_len, max_input_len,\n        logics.stride(0), logics.stride(1),\n        v.stride(0), v.stride(1), v.stride(2),\n        o.stride(0), o.stride(1), o.stride(2),\n        b_loc.stride(0), b_loc.stride(1),\n        other_kv_index,\n        BLOCK_DMODEL=v.shape[-1],\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=3\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided code defines a Triton kernel for matrix multiplication with quantized INT4 weights and a set of functions to perform quantization and dequantization. \n        The kernel function 'matmul_kernel' computes a block of the output matrix by iterating over the K dimension, and the function 'matmul_dequantize_int4_s2' sets up the inputs and launches this kernel for computation.\n        'quantize_int4' function quantizes a floating-point weight matrix into INT4 format while 'unpack_int4' is used to verify correctness by unpacking the INT4 weights back to floating-point. \n        These functions take advantage of Triton's ability to utilize GPUs for efficient computation.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A, B, C,  # Pointers to matrices\n    M, N, K,  # Matrix dimensions\n    stride_am, stride_ak,  # Strides for matrix A\n    stride_bk, stride_bn,  # Strides for matrix B\n    stride_cm, stride_cn,  # Strides for matrix C\n    BLOCK_M: tl.constexpr,  # Block size for M dimension\n    BLOCK_N: tl.constexpr,  # Block size for N dimension\n    BLOCK_K: tl.constexpr,  # Block size for K dimension\n    GROUP_SIZE_M: tl.constexpr  # Group size for M dimension\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) * stride_am\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) * stride_bn\n    offs_k = tl.arange(0, BLOCK_K)\n\n    A_block_ptr = tl.make_block_ptr(\n        base=A,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(pid_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0)\n    )\n    B_block_ptr = tl.make_block_ptr(\n        base=B,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, pid_n * BLOCK_N),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(0, 1)\n    )\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(A_block_ptr)\n        b = tl.load(B_block_ptr)\n        acc += tl.dot(a, b)\n        A_block_ptr = tl.advance(A_block_ptr, (0, BLOCK_K))\n        B_block_ptr = tl.advance(B_block_ptr, (BLOCK_K, 0))\n\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C_block_ptr = tl.make_block_ptr(\n        base=C,\n        shape=(M, N),\n        strides=(stride_cm, stride_cn),\n        offsets=(offs_cm, offs_cn),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0)\n    )\n    tl.store(C_block_ptr, acc)\n\n### Wrapper Function to Launch the Kernel\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul_dequantize_int4_s2(\n    A, B, C,  # Pointers to matrices\n    M, N, K,  # Matrix dimensions\n    stride_am, stride_ak,  # Strides for matrix A\n    stride_bk, stride_bn,  # Strides for matrix B\n    stride_cm, stride_cn,  # Strides for matrix C\n    BLOCK_M: tl.constexpr,  # Block size for M dimension\n    BLOCK_N: tl.constexpr,  # Block size for N dimension\n    BLOCK_K: tl.constexpr,  # Block size for K dimension\n    GROUP_SIZE_M: tl.constexpr  # Group size for M dimension\n):\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    matmul_kernel[grid](\n        A, B, C,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_M, BLOCK_N, BLOCK_K, GROUP_SIZE_M\n    )\n\n### Quantization and Dequantization Functions\n\n@triton.jit\ndef quantize_int4(A, A_quant, scale, M, K):\n    pid = tl.program_id(axis=0)\n    block_size = 8\n    num_blocks = tl.cdiv(K, block_size)\n    block_id = pid % num_blocks\n    row_id = pid // num_blocks\n    offs_m = row_id * M + tl.arange(0, M)\n    offs_k = block_id * block_size + tl.arange(0, block_size)\n    A_block = tl.load(A + offs_m[:, None] * K + offs_k[None, :])\n    A_quantized = tl.round(A_block / scale)\n    A_quantized = tl.bitcast(A_quantized, tl.int4)\n    A_quant_block = tl.zeros((M, block_size // 2), dtype=tl.int8)\n    A_quant_block = tl.bitcast(A_quantized, A_quant_block)\n    tl.store(A_quant + row_id * M * (K // 2) + block_id * (block_size // 2), A_quant_block)\n\n@triton.jit\ndef unpack_int4(A_quant, A_dequant, scale, M, K):\n    pid = tl.program_id(axis=0)\n    block_size = 8\n    num_blocks = tl.cdiv(K, block_size)\n    block_id = pid % num_blocks\n    row_id = pid // num_blocks\n    offs_m = row_id * M + tl.arange(0, M)\n    offs_k = block_id * block_size + tl.arange(0, block_size)\n    A_quant_block = tl.load(A_quant + row_id * M * (K // 2) + block_id * (block_size // 2))\n    A_quantized = tl.bitcast(A_quant_block, tl.int4)\n    A_dequant_block = A_quantized * scale\n    tl.store(A_dequant + offs_m[:, None] * K + offs_k[None, :], A_dequant_block)\n\n### Example Usage\n\nimport torch\n\n# Example matrices\nM, N, K = 1024, 1024, 1024\nA = torch.randn((M, K), device='cuda', dtype=torch.float32)\nB = torch.randn((K, N), device='cuda', dtype=torch.float32)\nC = torch.zeros((M, N), device='cuda', dtype=torch.float32)\n\n# Quantize B to INT4\nscale = 1.0  # Example scale factor\nB_quant = torch.zeros((M, K // 2), device='cuda', dtype=torch.int8)\nquantize_int4[(M * K // 8,)](A, B_quant, scale, M, K)\n\n# Dequantize B to verify correctness\nB_dequant = torch.zeros((M, K), device='cuda', dtype=torch.float32)\nunpack_int4[(M * K // 8,)](B_quant, B_dequant, scale, M, K)\n\n# Perform matrix multiplication\nmatmul_dequantize_int4_s2[(M * N // (128 * 128),)](A, B_dequant, C, M, N, K, A.stride(0), A.stride(1), B_dequant.stride(0), B_dequant.stride(1), C.stride(0), C.stride(1), 128, 128, 32, 8)\n\nprint(C)\n", "label": "import time\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n\tconfigs=[\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t    \n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\t\n ],\n\tkey=['M', 'N', 'K'],\n    reset_to_zero=['c_ptr']\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    bs_ptr, bzp_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    group_size,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr\n    ):\n    \"\"\"\n    assert K % (BLOCK_SIZE_K * SPLIT_K) == 0\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    # [BLOCK_M, BLOCK_K]\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    # [BLOCK_K, BLOCK_N] but repeated 8 times in N\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn\n    # tl.static_print(\"shape\", a_ptrs, b_ptrs, bs_ptrs, bzp_ptrs)\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        # Load the next block of A and B.\n        # [BLOCK_K, BLOCK_N] but repeated group_size times in K \n        bs_ptrs = bs_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bsk \\\n            + offs_bn[None, :] * stride_bsn\n        # [BLOCK_K, BLOCK_N] but repeated in K and N\n        bzp_ptrs = bzp_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bzpk \\\n            + (offs_bn[None, :] // 8) * stride_bzpn\n        b_shift_bits = (offs_k[:, None] % 8) * 4 # assert BLOCK_SIZE_K % 8 == 0\n        bzp_shift_bits = (offs_bn[None, :] % 8) * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n        # We accumulate along the K dimension.\n        int_b = (b >> b_shift_bits) & 0xF\n        int_bzp = (bzp >> bzp_shift_bits) & 0xF\n        b = ((int_b - int_bzp) * bs).to(a.dtype)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        # Advance the ptrs to the next K block.\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K * SPLIT_K * stride_bk // 8)  # assert BLOCK_SIZE_K % 8 == 0\n    # You can fuse arbitrary activation functions here\n    # while the accumulator is still in FP32!\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    # -----------------------------------------------------------\n    # Write back the block of the output matrix C with masks.\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_dequantize_int4_s2(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size: int = 128, output=None) -> torch.FloatTensor:\n    \"\"\"\n    \"\"\"\n    assert x.is_contiguous(), \"A must be contiguous\"\n    assert qweight.is_contiguous(), \"B must be contiguous\"  \n    M, K = x.shape\n    N = scales.shape[1]\n    if output is None:\n        output = torch.zeros((M, N), device=x.device, dtype=x.dtype)  \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n        META['SPLIT_K'],\n    )\n    matmul_kernel[grid](\n        x, qweight, output,\n        scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        group_size,\n    )\n    return output\n\ndef quantize_int4(weight, group_size=128, tp_rank=0):\n    # Weight shape: [H1 // 8, H2]\n    # Scale shape: [H1 // group_size, H2]\n    # zero_pint shape: [H1 // group_size, H2 // 8]\n\n    weight = weight.transpose(1, 0)\n    h1, h2 = weight.shape\n    assert h1 % 8 == 0 and h2 % 8 == 0, \"H1 {} H2 {}\".format(h1, h2)\n    assert h2 % group_size == 0, \"H1 {} H2 {}\".format(h1, h2)\n    weight = weight.contiguous().view(-1, group_size).cuda(tp_rank)\n    weight_max = weight.amax(-1, keepdim=True)\n    weight_max = torch.where(weight_max < 0, 0, weight_max)\n    weight_min = weight.amin(-1, keepdim=True)\n    weight_min = torch.where(weight_min > 0, 0, weight_min)\n    weight_range = weight_max - weight_min \n    scale = weight_range / (2 ** 4 - 1)\n    zero_point = (-weight_min / scale).round().clamp(0, 15).to(torch.int32)\n    weight = (weight / scale + zero_point).round().clamp(0, 15).to(torch.int32).view(h1, h2)\n    int_weight = torch.empty(h1, h2 // 8).to(torch.int32).to(weight.device)\n    int_zero_point = torch.zeros(h1 // 8, h2 // group_size).to(torch.int32).to(weight.device)\n    zero_point = zero_point.view(h1, -1)\n    scale = scale.view(h1, -1)\n    # pack 8 int4 in an int32 number.\n    # Weight pack in row.\n    for pack in range(0, h2, 8):\n        for i in range(8):\n            int_weight[:, pack // 8] += weight[:, pack + i] << (i * 4)\n    # zero point pack in col.\n    for pack in range(0, h1, 8):\n        for i in range(8):\n            int_zero_point[pack // 8, :] += zero_point[pack + i, :] << (i * 4)\n    '''\n    fp_weight = torch.zeros(h1, h2).half().to(weight.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_weight[pack * 8 + i, :] = \\\n                ((int_weight[pack, :] << (28 - i * 4) >> 28) + 16) % 16\n    print((fp_weight - weight).abs().sum())\n\n    fp_zp = torch.zeros(zero_point.shape).half().to(zero_point.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zp[pack * 8 + i, :] = \\\n                (int_zero_point[pack, :] >> (i * 4)) & 15\n\n    print((fp_zp - zero_point).abs().sum())\n    '''\n    weight = None\n    return int_weight.transpose(1, 0).contiguous(), scale.transpose(1, 0).contiguous(), int_zero_point.transpose(1, 0).contiguous(), group_size\n\n\ndef unpack_int4(weight, scale, zp):\n    \"\"\"\n    Test function to verify quantize int4 is correct.\n    Will not be used in model inference.\n    \"\"\"\n    weight = weight.transpose(1, 0)\n    scale = scale.transpose(1, 0)\n    zp = zp.transpose(1, 0)\n    h1, h2 = weight.shape\n    group_size = h2 * 8 // scale.shape[1]\n    group_num = scale.shape[1]\n    fp_weight = torch.zeros(h1, h2 * 8).half().to(weight.device)\n    fp_zero_point = torch.zeros(h1, group_num).to(weight.device)\n    for pack in range(0, h2):\n        for i in range(8):\n            fp_weight[:, pack * 8 + i] = (weight[:, pack] >> (i * 4)) & 0xF\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zero_point[pack * 8 + i, :] = (zp[pack, :] >> (i * 4)) & 0xF\n    for g in range(group_num):\n        fp_weight[:, g * group_size:(g + 1) * group_size] = (fp_weight[:, g * group_size:(g + 1) * group_size] - \\\n                                                             fp_zero_point[:, g].unsqueeze(1)) * scale[:, g].unsqueeze(1)\n    return fp_weight.transpose(1, 0)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton code implements a simple kernel function called 'puzzle1_kernel' that is designed to add a constant value to each element of an input tensor 'x'. The function 'puzzle1' serves as a wrapper to call this kernel on a given input tensor 'x', producing an output tensor. The primary goal is to perform an element-wise addition of a value (10 in this case) to the input tensor when using CUDA. It operates over a 1D grid of threads with a block size defined as a constant expression (BLOCK_SIZE). The output is stored in a separate output tensor.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the block size as a constant\nBLOCK_SIZE = 128\n\n# Triton kernel function\n@triton.jit\ndef puzzle1_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the block index\n    pid = tl.program_id(axis=0)\n    # Compute the block start index\n    block_start = pid * BLOCK_SIZE\n    # Compute the offsets for the current block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Load the input elements\n    x = tl.load(x_ptr + offsets, mask=offsets < n_elements)\n    # Perform the element-wise addition\n    y = x + 10\n    # Store the output elements\n    tl.store(output_ptr + offsets, y, mask=offsets < n_elements)\n\n# Wrapper function to call the kernel\ndef puzzle1(x):\n    # Get the number of elements in the input tensor\n    n_elements = x.shape[0]\n    # Allocate memory for the output tensor\n    output = triton.empty_like(x)\n    # Define the grid and block dimensions\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    # Launch the kernel\n    puzzle1_kernel[grid](x, output, n_elements, BLOCK_SIZE)\n    return output\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n# Triton kernel\n@triton.jit\ndef puzzle1_kernel(x_ptr, output_ptr, N, BLOCK_SIZE: tl.constexpr, value):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = x + value\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# Wrapper function to call the kernel\ndef puzzle1(x: torch.Tensor):\n    output = torch.empty_like(x)\n    assert x.is_cuda and output.is_cuda\n    N = output.numel()\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    puzzle1_kernel[grid](x, output, N, BLOCK_SIZE=1024, value=10)\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code provides two Triton kernels and corresponding Python functions for type conversion between float8 and float16. The function `f8_to_f16` converts int8 (representing float8 values) to float16, and the function `f16_to_f8` converts float16 or float32 values to int8 (representing float8 values). Both kernels work with a block-based approach, using a fixed BLOCK_SIZE of 1024. \n            The main operations involve loading data, applying optional masking for out-of-bounds accesses, and storing the result. \n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n# Kernel for converting float8 to float16\n@triton.jit\ndef f8_to_f16_kernel(\n    f8_ptr,  # *int8\n    f16_ptr,  # *float16\n    n_elements,  # size_t\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load the data\n    f8_data = tl.load(f8_ptr + offsets, mask=mask)\n\n    # Convert to float16\n    f16_data = tl.cast(f8_data, tl.float16)\n\n    # Store the result\n    tl.store(f16_ptr + offsets, f16_data, mask=mask)\n\n# Kernel for converting float16 to float8\n@triton.jit\ndef f16_to_f8_kernel(\n    f16_ptr,  # *float16\n    f8_ptr,  # *int8\n    n_elements,  # size_t\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load the data\n    f16_data = tl.load(f16_ptr + offsets, mask=mask)\n\n    # Convert to float8\n    f8_data = tl.cast(f16_data, tl.int8)\n\n    # Store the result\n    tl.store(f8_ptr + offsets, f8_data, mask=mask)\n\nimport torch\n\n# Function to convert float8 to float16\ndef f8_to_f16(f8_tensor: torch.Tensor) -> torch.Tensor:\n    assert f8_tensor.dtype == torch.int8, \"Input tensor must be of type int8 (representing float8)\"\n    n_elements = f8_tensor.numel()\n    f16_tensor = torch.empty(n_elements, dtype=torch.float16, device=f8_tensor.device)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    f8_to_f16_kernel[grid](f8_tensor, f16_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return f16_tensor\n\n# Function to convert float16 to float8\ndef f16_to_f8(f16_tensor: torch.Tensor) -> torch.Tensor:\n    assert f16_tensor.dtype == torch.float16, \"Input tensor must be of type float16\"\n    n_elements = f16_tensor.numel()\n    f8_tensor = torch.empty(n_elements, dtype=torch.int8, device=f16_tensor.device)\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    f16_to_f8_kernel[grid](f16_tensor, f8_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return f8_tensor\n\n# Example usage\nf8_tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.int8, device='cuda')\nf16_tensor = f8_to_f16(f8_tensor)\nprint(f16_tensor)\n\nf16_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], dtype=torch.float16, device='cuda')\nf8_tensor = f16_to_f8(f16_tensor)\nprint(f8_tensor)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef kernel_f8_to_f16(Y, X, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n    x = tl.load(X + offs, mask=mask)\n    tl.store(Y + offs, x, mask=mask)\n    tl.store(Y + offs, x, mask=mask)\n\ndef f8_to_f16(x, dtypes=tl.float8e5) -> torch.Tensor:\n    assert x.dtype == torch.int8, f\"torch.int8 expected but got {x.dtype}\"\n    assert \"cuda\" in str(x.device), f\"CUDA tensors only but got {x.device}\"\n    ret = torch.empty_like(x, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n    numel = ret.untyped_storage().size() // ret.element_size()\n    kernel_f8_to_f16[grid](ret, triton.reinterpret(x, dtypes), numel, BLOCK_SIZE=1024)\n    return ret\n\n@triton.jit\ndef kernel_f16_to_f8(Y, X, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n    x = tl.load(X + offs, mask=mask)\n    tl.store(Y + offs, x, mask=mask)\n\ndef f16_to_f8(x: torch.Tensor, dtypes=tl.float8e5) -> torch.Tensor:\n    assert x.dtype in [torch.float16, torch.float32]\n    assert \"cuda\" in str(x.device), f\"CUDA tensors only but got {x.device}\"\n    ret = torch.empty_like(x, dtype=torch.int8)\n    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n    numel = x.untyped_storage().size() // x.element_size()\n    kernel_f16_to_f8[grid](triton.reinterpret(ret, dtypes), x, numel, BLOCK_SIZE=1024)\n    return ret\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton kernel called `matmul_kernel`, which performs matrix multiplication. The kernel takes pointers to matrices `a`, `b`, and `c`, along with block sizes `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`. Inside the kernel, each thread computes a block of the output matrix `c` by loading sub-blocks of `a` and `b`, performing a dot product, and storing the results in `c`. The `matmul` function is a wrapper that launches the kernel for specified matrix dimensions `M`, `N`, and `K`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    # Compute the block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Compute the block boundaries\n    rm = pid_m * BLOCK_SIZE_M\n    rn = pid_n * BLOCK_SIZE_N\n    \n    # Initialize the output block\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    # Iterate over the K dimension in blocks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Compute the block boundaries for the K dimension\n        rk = k\n        a_block_ptr = a_ptr + (rm * stride_am + rk * stride_ak)\n        b_block_ptr = b_ptr + (rk * stride_bk + rn * stride_bn)\n        \n        # Load the blocks from global memory to shared memory\n        a_block = tl.load(a_block_ptr, mask=(rm + tl.arange(0, BLOCK_SIZE_M)) < M)\n        b_block = tl.load(b_block_ptr, mask=(rn + tl.arange(0, BLOCK_SIZE_N)) < N)\n        \n        # Perform the dot product\n        acc += tl.dot(a_block, b_block)\n    \n    # Store the result back to global memory\n    c_block_ptr = c_ptr + (rm * stride_cm + rn * stride_cn)\n    tl.store(c_block_ptr, acc, mask=(rm + tl.arange(0, BLOCK_SIZE_M)) < M)\n\nimport torch\n\ndef matmul(a, b, BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=16):\n    # Get the dimensions of the input matrices\n    M, K = a.shape\n    K, N = b.shape\n    \n    # Allocate the output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    \n    # Compute the grid size\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n    \n    # Launch the kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n    \n    return c\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef matmul_kernel(c_ptr, a_ptr, b_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    M, N, K = 4096, 4096, 4096\n    stride_am = 4096\n    stride_ak = 1\n    stride_bk = 4096\n    stride_bn = 1\n    stride_cm = 4096\n    stride_cn = 1\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = tl.cast(accumulator, tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c)\n\ndef matmul(c, a, b, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n    matmul_kernel[triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N)](\n        c, a, b, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n    )\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a Triton-based kernel and its autograd Function for a fused recurrent retention operation. This operation handles the computation of output tensor 'o' and an optional 'final_state' using input tensors 'q', 'k', 'v', and optionally 'initial_state'. The operation scales the query 'q', multiplies it with the key-value product 'h' which is updated iteratively across a temporal dimension 'T'. The kernel uses block dimensions BK, BV for keys and values, respectively, and can conditionally use initial and store final states. The backward kernel computes gradients for 'q', 'k', 'v' using 'do' and backpropagates through temporal steps in reverse.\n        The main function 'fused_recurrent_retention' takes input tensors, optionally an 'initial_state', and returns the computed output tensor 'o' and optionally 'final_state'.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_recurrent_retention_forward_kernel(\n    q_ptr, k_ptr, v_ptr, o_ptr, initial_state_ptr, final_state_ptr,\n    q_batch_stride, q_seq_stride, q_head_stride, q_feat_stride,\n    k_batch_stride, k_seq_stride, k_head_stride, k_feat_stride,\n    v_batch_stride, v_seq_stride, v_head_stride, v_feat_stride,\n    o_batch_stride, o_seq_stride, o_head_stride, o_feat_stride,\n    initial_state_batch_stride, initial_state_head_stride, initial_state_feat_stride,\n    final_state_batch_stride, final_state_head_stride, final_state_feat_stride,\n    B, T, H, D, BK, BV, USE_INITIAL, STORE_FINAL, BLOCK_SIZE_T: tl.constexpr\n):\n    b = tl.program_id(0)\n    h = tl.program_id(1)\n    start = tl.program_id(2) * BLOCK_SIZE_T\n\n    q_offset = b * q_batch_stride + h * q_head_stride\n    k_offset = b * k_batch_stride + h * k_head_stride\n    v_offset = b * v_seq_stride + h * v_head_stride\n    o_offset = b * o_batch_stride + h * o_head_stride\n\n    if USE_INITIAL:\n        initial_state = tl.load(initial_state_ptr + b * initial_state_batch_stride + h * initial_state_head_stride)\n\n    h = tl.zeros((BK, BV), dtype=tl.float32)\n\n    for t in range(start, start + BLOCK_SIZE_T):\n        if t < T:\n            q = tl.load(q_ptr + q_offset + t * q_seq_stride, mask=t < T, other=0.0)\n            k = tl.load(k_ptr + k_offset + t * k_seq_stride, mask=t < T, other=0.0)\n            v = tl.load(v_ptr + v_offset + t * v_seq_stride, mask=t < T, other=0.0)\n\n            q = q * 1.0 / (D ** 0.5)  # Scale the query\n\n            h = h + tl.dot(q, k)  # Update the key-value product\n            o = tl.dot(h, v)  # Compute the output\n\n            tl.store(o_ptr + o_offset + t * o_seq_stride, o, mask=t < T)\n\n            if STORE_FINAL and t == T - 1:\n                tl.store(final_state_ptr + b * final_state_batch_stride + h * final_state_head_stride, h)\n\n@triton.jit\ndef fused_recurrent_retention_backward_kernel(\n    q_ptr, k_ptr, v_ptr, do_ptr, dq_ptr, dk_ptr, dv_ptr,\n    q_batch_stride, q_seq_stride, q_head_stride, q_feat_stride,\n    k_batch_stride, k_seq_stride, k_head_stride, k_feat_stride,\n    v_batch_stride, v_seq_stride, v_head_stride, v_feat_stride,\n    do_batch_stride, do_seq_stride, do_head_stride, do_feat_stride,\n    dq_batch_stride, dq_seq_stride, dq_head_stride, dq_feat_stride,\n    dk_batch_stride, dk_seq_stride, dk_head_stride, dk_feat_stride,\n    dv_batch_stride, dv_seq_stride, dv_head_stride, dv_feat_stride,\n    B, T, H, D, BK, BV, BLOCK_SIZE_T: tl.constexpr\n):\n    b = tl.program_id(0)\n    h = tl.program_id(1)\n    start = tl.program_id(2) * BLOCK_SIZE_T\n\n    q_offset = b * q_batch_stride + h * q_head_stride\n    k_offset = b * k_batch_stride + h * k_head_stride\n    v_offset = b * v_seq_stride + h * v_head_stride\n    do_offset = b * do_batch_stride + h * do_head_stride\n    dq_offset = b * dq_batch_stride + h * dq_head_stride\n    dk_offset = b * dk_batch_stride + h * dk_head_stride\n    dv_offset = b * dv_batch_stride + h * dv_head_stride\n\n    h = tl.zeros((BK, BV), dtype=tl.float32)\n\n    for t in range(T - 1, start - 1, -1):\n        if t >= start:\n            q = tl.load(q_ptr + q_offset + t * q_seq_stride, mask=t >= start, other=0.0)\n            k = tl.load(k_ptr + k_offset + t * k_seq_stride, mask=t >= start, other=0.0)\n            v = tl.load(v_ptr + v_offset + t * v_seq_stride, mask=t >= start, other=0.0)\n            do = tl.load(do_ptr + do_offset + t * do_seq_stride, mask=t >= start, other=0.0)\n\n            q = q * 1.0 / (D ** 0.5)  # Scale the query\n\n            dv = tl.dot(h, do)\n            h = h + tl.dot(q, k)\n            dk = tl.dot(q, do)\n            dq = tl.dot(do, v) * (1.0 / (D ** 0.5))\n\n            tl.store(dq_ptr + dq_offset + t * dq_seq_stride, dq, mask=t >= start)\n            tl.store(dk_ptr + dk_offset + t * dk_seq_stride, dk, mask=t >= start)\n            tl.store(dv_ptr + dv_offset + t * dv_seq_stride, dv, mask=t >= start)\n\nimport torch\nfrom torch.autograd import Function\n\nclass FusedRecurrentRetentionFunction(Function):\n    @staticmethod\n    def forward(ctx, q, k, v, initial_state=None, store_final=False):\n        B, T, H, D = q.shape\n        BK, BV = k.shape[-1], v.shape[-1]\n\n        o = torch.empty_like(q)\n        final_state = torch.empty((B, H, BK, BV), device=q.device) if store_final else None\n\n        USE_INITIAL = initial_state is not None\n        STORE_FINAL = store_final\n\n        grid = (B, H, (T + 128 - 1) // 128)\n        fused_recurrent_retention_forward_kernel[grid](\n            q, k, v, o, initial_state, final_state,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            initial_state.stride(0) if USE_INITIAL else 0, initial_state.stride(1) if USE_INITIAL else 0, initial_state.stride(2) if USE_INITIAL else 0,\n            final_state.stride(0) if STORE_FINAL else 0, final_state.stride(1) if STORE_FINAL else 0, final_state.stride(2) if STORE_FINAL else 0,\n            B, T, H, D, BK, BV, USE_INITIAL, STORE_FINAL, 128\n        )\n\n        ctx.save_for_backward(q, k, v, o)\n        ctx.B, ctx.T, ctx.H, ctx.D, ctx.BK, ctx.BV = B, T, H, D, BK, BV\n        ctx.STORE_FINAL = STORE_FINAL\n\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, d_final_state=None):\n        q, k, v, o = ctx.saved_tensors\n        B, T, H, D = ctx.B, ctx.T, ctx.H, ctx.D\n        BK, BV = ctx.BK, ctx.BV\n\n        dq = torch.zeros_like(q)\n        dk = torch.zeros_like(k)\n        dv = torch.zeros_like(v)\n\n        grid = (B, H, (T + 128 - 1) // 128)\n        fused_recurrent_retention_backward_kernel[grid](\n            q, k, v, do, dq, dk, dv,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            do.stride(0), do.stride(1), do.stride(2), do.stride(3),\n            dq.stride(0), dq.stride(1), dq.stride(2), dq.stride(3),\n            dk.stride(0), dk.stride(1), dk.stride(2), dk.stride(3),\n            dv.stride(0), dv.stride(1), dv.stride(2), dv.stride(3),\n            B, T, H, D, BK, BV, 128\n        )\n\n        return dq, dk, dv, None, None\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.jit\ndef fused_recurrent_retention_fwd_kernel(\n    q, k, v, o, initial_state, final_state, \n    s_qk_h, s_qk_t, s_qk_d, \n    s_vo_h, s_vo_t, s_vo_d, \n    B, H, T, scale, \n    BK: tl.constexpr, BV: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, \n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr\n):\n    # Kernel logic\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = (1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < DK\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + \\\n            (i_k * BK + tl.arange(0, BK)[None, :]) * \\\n            DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        _v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        _q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n\n        h = b_b * h + _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o.to(p_o.dtype.element_ty), mask=mask_bv)\n\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + \\\n            (i_k * BK + tl.arange(0, BK)[None, :]) * \\\n            DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h.to(p_final_s.dtype.element_ty), mask=mask_kv)\n\n@triton.jit\ndef fused_recurrent_retention_bwd_kernel(\n    q, k, v, do, dq, dk, dv, initial_state, \n    s_qk_h, s_qk_t, s_qk_d, \n    s_vo_h, s_vo_t, s_vo_d, \n    B, H, T, scale, \n    BK: tl.constexpr, BV: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, \n    USE_INITIAL_STATE: tl.constexpr\n):\n    # Kernel logic\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + \\\n            (i_k * BK + tl.arange(0, BK)[:, None]) * \\\n            DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0).to(tl.float32)\n\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        _v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        _do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n\n        h = b_b * h + _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n\n    tl.debug_barrier()\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * \\\n        BK + tl.arange(0, BK) + (T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * \\\n        BV + tl.arange(0, BV) + (T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        _q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        _v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n\n        d_h *= b_b\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n\nclass FusedRecurrentRetentionFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, initial_state=None, output_final_state=False):\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n\n        scale = d_head_qk ** -0.5\n        BK, BV = min(d_head_qk, 32), min(d_head_v, 32)\n        NK, NV = triton.cdiv(d_head_qk, BK), triton.cdiv(d_head_v, BV)\n        num_stages = 1\n        num_warps = 1\n\n        o = q.new_empty(NK, batch_size, n_heads, seq_len, d_head_v)\n\n        if output_final_state:\n            final_state = q.new_empty(batch_size, n_heads, d_head_qk, d_head_v)\n        else:\n            final_state = None\n\n        grid = (NV, NK, batch_size * n_heads)\n        fused_recurrent_retention_fwd_kernel[grid](\n            q, k, v, o, initial_state, final_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            DK=d_head_qk, DV=d_head_v, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None\n        )\n\n        o = o.sum(0)\n        ctx.save_for_backward(q, k, v, initial_state)\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, d_final_state=None):\n        q, k, v, initial_state = ctx.saved_tensors\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n        scale = d_head_qk ** -0.5\n\n        BK, BV = min(d_head_qk, 32), min(d_head_v, 32)\n        NK, NV = triton.cdiv(d_head_qk, BK), triton.cdiv(d_head_v, BV)\n        num_stages = 1\n        num_warps = 1\n\n        dq = q.new_empty(NV, batch_size, n_heads, seq_len, d_head_qk)\n        dk = q.new_empty(NV, batch_size, n_heads, seq_len, d_head_qk)\n        dv = q.new_empty(NK, batch_size, n_heads, seq_len, d_head_v)\n        grid = (NV, NK, batch_size * n_heads)\n\n        fused_recurrent_retention_bwd_kernel[grid](\n            q, k, v, do, dq, dk, dv, initial_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            DK=d_head_qk, DV=d_head_v, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages,\n            USE_INITIAL_STATE=initial_state is not None\n        )\n        dq = dq.sum(0)\n        dk = dk.sum(0)\n        dv = dv.sum(0)\n        return dq, dk, dv, None, None\n\ndef fused_recurrent_retention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if initial_state is not None:\n        initial_state = initial_state.detach()\n    o, final_state = FusedRecurrentRetentionFunction.apply(q, k, v, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The function 'rmsnorm_forward' applies RMS normalization on a given input tensor. It utilizes a Triton kernel '_rms_norm_fwd_fused' to perform the operation. The main inputs are 'x', the input tensor, and 'weight', the normalization weight. The output is 'y', a tensor of the same shape as 'x', after applying RMS normalization. The Triton kernel calculates the variance of each row of the input, computes the reciprocal of the square root of the variance plus a small epsilon for numerical stability, and applies this normalization along with the weight vector on the input data.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _rms_norm_fwd_fused(\n    x_ptr,  # Pointer to the input tensor\n    y_ptr,  # Pointer to the output tensor\n    weight_ptr,  # Pointer to the weight vector\n    M,  # Number of rows in the input tensor\n    N,  # Number of columns in the input tensor\n    eps,  # Small epsilon for numerical stability\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelization\n):\n    row_id = tl.program_id(0)\n    if row_id >= M:\n        return\n\n    # Pointers to the current row in x and y\n    x_row_ptr = x_ptr + row_id * N\n    y_row_ptr = y_ptr + row_id * N\n\n    # Load the row of x into a Triton block\n    x_row = tl.load(x_row_ptr, mask=tl.arange(0, BLOCK_SIZE) < N, other=0.0)\n\n    # Compute the variance of the row\n    mean = tl.sum(x_row, axis=0) / N\n    var = tl.sum((x_row - mean) ** 2, axis=0) / N\n\n    # Compute the reciprocal of the square root of the variance plus epsilon\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    # Load the weight vector\n    weight = tl.load(weight_ptr, mask=tl.arange(0, BLOCK_SIZE) < N, other=1.0)\n\n    # Apply the normalization and weight\n    y_row = (x_row - mean) * inv_std * weight\n\n    # Store the result back to the output tensor\n    tl.store(y_row_ptr, y_row, mask=tl.arange(0, BLOCK_SIZE) < N)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# Define the block size for parallelization\nBLOCK_SIZE = 1024\n\ndef rmsnorm_forward(x, weight, eps=1e-6):\n    # Ensure x and weight are on the same device\n    assert x.device == weight.device, \"x and weight must be on the same device\"\n    assert x.dim() == 2, \"x must be a 2D tensor\"\n    assert weight.dim() == 1, \"weight must be a 1D tensor\"\n    assert x.size(1) == weight.size(0), \"The second dimension of x must match the size of weight\"\n\n    M, N = x.size()\n    y = torch.empty_like(x, device=x.device)\n\n    # Launch the Triton kernel\n    grid = (M,)\n    _rms_norm_fwd_fused[grid](\n        x, y, weight, M, N, eps, BLOCK_SIZE\n    )\n\n    return y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _rms_norm_fwd_fused(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    W,  # pointer to the weights\n    stride,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    # Compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        # Write output\n        tl.store(Y + cols, y.to(tl.float16), mask=mask)\n\ndef rmsnorm_forward(x, weight, eps):\n    # allocate output\n    y = torch.empty_like(x)\n    # reshape input data into 2D tensor\n    x_arg = x.view(-1, x.shape[-1])\n    M, N = x_arg.shape\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_SIZE:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    # heuristics for number of warps\n    num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n    BLOCK_SIZE = 128 * 2 * 2 * 2 * 2 * 2 * 2 * 2\n    num_warps = 8\n    # enqueue kernel\n    _rms_norm_fwd_fused[(M,)](x_arg, y, weight,\n                              x_arg.stride(0), N, eps,\n                              BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n    return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel and a wrapper function to copy values from a source tensor K to a destination tensor Out at specific indices provided by DestLoc. The kernel, _fwd_kernel_destindex_copy_kv, is responsible for loading data from the specified positions in K and storing it in the corresponding locations in Out. The input K is a 3D tensor with dimensions for batch, head, and depth. DestLoc provides the target indices in the batch dimension. The head and depth dimensions are divided into blocks for efficient parallel processing, controlled by BLOCK_HEAD and BLOCK_DMODEL, which are powers of two. The function destindex_copy_kv initializes the grid and block dimensions and calls the kernel for each sequence in the batch.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    K,  # source tensor\n    Out,  # destination tensor\n    DestLoc,  # target indices in the batch dimension\n    stride_k_batch,  # stride of K in the batch dimension\n    stride_k_head,  # stride of K in the head dimension\n    stride_k_depth,  # stride of K in the depth dimension\n    stride_out_batch,  # stride of Out in the batch dimension\n    stride_out_head,  # stride of Out in the head dimension\n    stride_out_depth,  # stride of Out in the depth dimension\n    stride_destloc_batch,  # stride of DestLoc in the batch dimension\n    stride_destloc_head,  # stride of DestLoc in the head dimension\n    batch_size,  # batch size\n    head_size,  # number of heads\n    depth_size,  # depth size\n    BLOCK_HEAD: tl.constexpr,  # block size for head dimension\n    BLOCK_DMODEL: tl.constexpr  # block size for depth dimension\n):\n    # Get the current block's head and depth indices\n    head_pid = tl.program_id(axis=0)\n    depth_pid = tl.program_id(axis=1)\n\n    # Compute the block's range in the head and depth dimensions\n    head_range = head_pid * BLOCK_HEAD, (head_pid + 1) * BLOCK_HEAD\n    depth_range = depth_pid * BLOCK_DMODEL, (depth_pid + 1) * BLOCK_DMODEL\n\n    # Iterate over the batch dimension\n    for batch_idx in range(batch_size):\n        dest_loc = tl.load(DestLoc + batch_idx * stride_destloc_batch + head_pid * stride_destloc_head)\n\n        # Iterate over the head and depth dimensions within the block\n        for head_idx in range(*head_range):\n            for depth_idx in range(*depth_range):\n                if head_idx < head_size and depth_idx < depth_size:\n                    k_idx = batch_idx * stride_k_batch + head_idx * stride_k_head + depth_idx * stride_k_depth\n                    out_idx = dest_loc * stride_out_batch + head_idx * stride_out_head + depth_idx * stride_out_depth\n                    out_val = tl.load(K + k_idx)\n                    tl.store(Out + out_idx, out_val)\n\nimport torch\n\ndef destindex_copy_kv(K, Out, DestLoc, BLOCK_HEAD=32, BLOCK_DMODEL=32):\n    # Get the dimensions of the tensors\n    batch_size, head_size, depth_size = K.shape\n\n    # Define the grid and block dimensions\n    grid = (head_size // BLOCK_HEAD, depth_size // BLOCK_DMODEL)\n\n    # Call the Triton kernel\n    _fwd_kernel_destindex_copy_kv[grid](\n        K,  # source tensor\n        Out,  # destination tensor\n        DestLoc,  # target indices in the batch dimension\n        K.stride(0),  # stride of K in the batch dimension\n        K.stride(1),  # stride of K in the head dimension\n        K.stride(2),  # stride of K in the depth dimension\n        Out.stride(0),  # stride of Out in the batch dimension\n        Out.stride(1),  # stride of Out in the head dimension\n        Out.stride(2),  # stride of Out in the depth dimension\n        DestLoc.stride(0),  # stride of DestLoc in the batch dimension\n        DestLoc.stride(1),  # stride of DestLoc in the head dimension\n        batch_size,  # batch size\n        head_size,  # number of heads\n        depth_size,  # depth size\n        BLOCK_HEAD,  # block size for head dimension\n        BLOCK_DMODEL  # block size for depth dimension\n    )\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    K,\n    Dest_loc,\n    Out,\n    stride_k_bs,\n    stride_k_h,\n    stride_k_d,\n    stride_o_bs,\n    stride_o_h,\n    stride_o_d,\n    head_num,\n    head_dim,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n\n    k = tl.load(k_ptrs, mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim), other=0.0)\n    tl.store(o_ptrs, k, mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim))\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_kv(K, DestLoc, Out):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        K,\n        DestLoc,\n        Out,\n        K.stride(0),\n        K.stride(1),\n        K.stride(2),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        head_num,\n        head_dim,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel named `_rotary_kernel` is designed to perform a rotary transformation on input tensors `Q` and `K` using cosine (`Cos`) and sine (`Sin`) embeddings. This transformation is often used in advanced transformer models for enhanced positional encoding. The kernel is executed on a 2D grid where the dimensions correspond to the head and sequence blocks.\n\n            Inputs:\n            - `Q`, `K`: Input tensors for query and key matrices.\n            - `Cos`, `Sin`: Tensors containing cosine and sine embeddings for rotary transformation.\n            - Several stride values for indexing in `Q`, `K`, `Cos`, and `Sin`.\n            - `max_total_len`, `HEAD_Q`, `HEAD_K`: Dimension specifications.\n            - Compile-time constants `BLOCK_HEAD`, `BLOCK_SEQ`, `BLOCK_DMODEL` to define processing block sizes.\n\n            The function loads slices from `Q` and `K`, applies the rotary transformation using `Cos` and `Sin`, and stores the results back into `Q` and `K`. The process uses triton `load` and `store` instructions with masks to handle boundary conditions and ensure safe memory access.\n\n            The wrapper function `rotary_emb_fwd` prepares inputs and launches the kernel. It determines the grid size for execution based on the number of query heads and sequence length and chooses the number of warps based on the dimension size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _rotary_kernel(\n    Q, K, Cos, Sin,\n    stride_qh, stride_qd, stride_qm,\n    stride_kh, stride_kd, stride_km,\n    stride_ch, stride_cd, stride_cm,\n    stride_sh, stride_sd, stride_sm,\n    max_total_len, HEAD_Q, HEAD_K,\n    BLOCK_HEAD: tl.constexpr, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr\n):\n    # Get the current block's coordinates in the 2D grid\n    pid_head = tl.program_id(axis=0)\n    pid_seq = tl.program_id(axis=1)\n\n    # Compute the starting indices for the current block\n    head_start = pid_head * BLOCK_HEAD\n    seq_start = pid_seq * BLOCK_SEQ\n\n    # Initialize the offsets for the current block\n    offsets_head = head_start + tl.arange(0, BLOCK_HEAD)\n    offsets_seq = seq_start + tl.arange(0, BLOCK_SEQ)\n    offsets_dmodel = tl.arange(0, BLOCK_DMODEL)\n\n    # Create masks to handle boundary conditions\n    mask_head = offsets_head < HEAD_Q\n    mask_seq = offsets_seq < max_total_len\n\n    # Load the slices from Q and K\n    Q_offsets = (offsets_head[:, None, None] * stride_qh + offsets_seq[None, :, None] * stride_qm + offsets_dmodel[None, None, :] * stride_qd)\n    K_offsets = (offsets_head[:, None, None] * stride_kh + offsets_seq[None, :, None] * stride_km + offsets_dmodel[None, None, :] * stride_kd)\n    Q_block = tl.load(Q + Q_offsets, mask=mask_head[:, None, None] & mask_seq[None, :, None], other=0.0)\n    K_block = tl.load(K + K_offsets, mask=mask_head[:, None, None] & mask_seq[None, :, None], other=0.0)\n\n    # Load the cosine and sine embeddings\n    Cos_offsets = (offsets_head[:, None, None] * stride_ch + offsets_seq[None, :, None] * stride_cm + offsets_dmodel[None, None, :] * stride_cd)\n    Sin_offsets = (offsets_head[:, None, None] * stride_sh + offsets_seq[None, :, None] * stride_sm + offsets_dmodel[None, None, :] * stride_sd)\n    Cos_block = tl.load(Cos + Cos_offsets, mask=mask_head[:, None, None] & mask_seq[None, :, None], other=0.0)\n    Sin_block = tl.load(Sin + Sin_offsets, mask=mask_head[:, None, None] & mask_seq[None, :, None], other=0.0)\n\n    # Apply the rotary transformation\n    Q_rotated = Q_block * Cos_block - tl.flip(Q_block, 2) * Sin_block\n    K_rotated = K_block * Cos_block - tl.flip(K_block, 2) * Sin_block\n\n    # Store the results back into Q and K\n    tl.store(Q + Q_offsets, Q_rotated, mask=mask_head[:, None, None] & mask_seq[None, :, None])\n    tl.store(K + K_offsets, K_rotated, mask=mask_head[:, None, None] & mask_seq[None, :, None])\n\nimport torch\n\ndef rotary_emb_fwd(Q, K, Cos, Sin, max_total_len, HEAD_Q, HEAD_K, BLOCK_HEAD, BLOCK_SEQ, BLOCK_DMODEL):\n    # Determine the grid size for execution\n    grid = (HEAD_Q // BLOCK_HEAD, max_total_len // BLOCK_SEQ)\n\n    # Determine the number of warps based on the dimension size\n    num_warps = 4 if BLOCK_DMODEL <= 256 else 8\n\n    # Launch the kernel\n    _rotary_kernel[grid](\n        Q, K, Cos, Sin,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        Cos.stride(0), Cos.stride(1), Cos.stride(2),\n        Sin.stride(0), Sin.stride(1), Sin.stride(2),\n        max_total_len, HEAD_Q, HEAD_K,\n        BLOCK_HEAD, BLOCK_SEQ, BLOCK_DMODEL,\n        num_warps=num_warps\n    )\n\n# Example usage\nQ = torch.randn((8, 128, 64), device='cuda')\nK = torch.randn((8, 128, 64), device='cuda')\nCos = torch.randn((8, 128, 64), device='cuda')\nSin = torch.randn((8, 128, 64), device='cuda')\nmax_total_len = 128\nHEAD_Q = 8\nHEAD_K = 8\nBLOCK_HEAD = 1\nBLOCK_SEQ = 32\nBLOCK_DMODEL = 64\n\nrotary_emb_fwd(Q, K, Cos, Sin, max_total_len, HEAD_Q, HEAD_K, BLOCK_HEAD, BLOCK_SEQ, BLOCK_DMODEL)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _rotary_kernel(\n    Q,\n    K,\n    Cos,\n    Sin,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_cosbs,\n    stride_cosd,\n    stride_sinbs,\n    stride_sind,\n    max_total_len,\n    HEAD_Q,\n    HEAD_K,\n    BLOCK_HEAD: tl.constexpr,\n    BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2) * 2\n    dim_range1 = tl.arange(0, BLOCK_DMODEL // 2) * 2 + 1\n\n    off_q0 = (\n        cur_seq_range[:, None, None] * stride_qbs\n        + cur_head_range[None, :, None] * stride_qh\n        + dim_range0[None, None, :] * stride_qd\n    )\n    off_q1 = (\n        cur_seq_range[:, None, None] * stride_qbs\n        + cur_head_range[None, :, None] * stride_qh\n        + dim_range1[None, None, :] * stride_qd\n    )\n\n    off_dimcos_sin0 = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[None, None, :] * stride_cosd\n    off_dimcos_sin1 = cur_seq_range[:, None, None] * stride_cosbs + dim_range1[None, None, :] * stride_cosd\n\n    q0 = tl.load(\n        Q + off_q0,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q),\n        other=0.0,\n    )\n    q1 = tl.load(\n        Q + off_q1,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q),\n        other=0.0,\n    )\n\n    cos0 = tl.load(Cos + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin0 = tl.load(Sin + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    cos1 = tl.load(Cos + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin1 = tl.load(Sin + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    out0 = q0 * cos0 - q1 * sin0\n    out1 = q0 * sin1 + q1 * cos1\n\n    tl.store(\n        Q + off_q0, out0, mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q)\n    )\n    tl.store(\n        Q + off_q1, out1, mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q)\n    )\n\n    off_k0 = (\n        cur_seq_range[:, None, None] * stride_kbs\n        + cur_head_range[None, :, None] * stride_kh\n        + dim_range0[None, None, :] * stride_kd\n    )\n    off_k1 = (\n        cur_seq_range[:, None, None] * stride_kbs\n        + cur_head_range[None, :, None] * stride_kh\n        + dim_range1[None, None, :] * stride_kd\n    )\n\n    off_dimcos_sin0 = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[None, None, :] * stride_cosd\n    off_dimcos_sin1 = cur_seq_range[:, None, None] * stride_cosbs + dim_range1[None, None, :] * stride_cosd\n\n    k0 = tl.load(\n        K + off_k0,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n        other=0.0,\n    )\n    k1 = tl.load(\n        K + off_k1,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n        other=0.0,\n    )\n\n    cos0 = tl.load(Cos + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin0 = tl.load(Sin + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    cos1 = tl.load(Cos + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin1 = tl.load(Sin + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    out_k0 = k0 * cos0 - k1 * sin0\n    out_k1 = k0 * sin1 + k1 * cos1\n\n    tl.store(\n        K + off_k0,\n        out_k0,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n    )\n    tl.store(\n        K + off_k1,\n        out_k1,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n    )\n    return\n\n@torch.no_grad()\ndef rotary_emb_fwd(q, k, cos, sin, partial_rotary_factor=1.0):\n    total_len = q.shape[0]\n    head_num_q, head_num_k = q.shape[1], k.shape[1]\n    head_dim = int(q.shape[2] * partial_rotary_factor)\n    assert q.shape[0] == cos.shape[0] and q.shape[0] == sin.shape[0], f\"q shape {q.shape} cos shape {cos.shape}\"\n    assert k.shape[0] == cos.shape[0] and k.shape[0] == sin.shape[0], f\"k shape {k.shape} cos shape {cos.shape}\"\n\n    BLOCK_SEQ = 16\n    BLOCK_HEAD = 4\n    if head_dim >= 128:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    grid = (triton.cdiv(head_num_q, BLOCK_HEAD), triton.cdiv(total_len, BLOCK_SEQ))\n    _rotary_kernel[grid](\n        q,\n        k,\n        cos,\n        sin,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        cos.stride(0),\n        cos.stride(1),\n        sin.stride(0),\n        sin.stride(1),\n        total_len,\n        head_num_q,\n        head_num_k,\n        BLOCK_HEAD=BLOCK_HEAD,\n        BLOCK_SEQ=BLOCK_SEQ,\n        BLOCK_DMODEL=head_dim,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel function `_fwd_kernel_flash_decode_stage2` is designed to perform a specific computation involving a batch of sequences and their corresponding head dimensions. The main logic is implemented in a parallel kernel that runs on each batch and head. The function loads the sequence length for the current batch, calculates a block size for sequences, and initializes several accumulators and logic variables. It processes each block of sequence data by loading values and logic sums, computing scaling factors, and accumulating results. The final output is stored by normalizing the accumulated value by the sum of exponentials. The kernel is launched by the `flash_decode_stage2` function, which determines grid size and other parameters before executing the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    Q, K, V, O,  # Input and output tensors\n    seq_lengths,  # Sequence lengths for each batch\n    batch_size,   # Batch size\n    head_dim,     # Head dimension\n    num_heads,    # Number of heads\n    BLOCK_SIZE_SEQ: tl.constexpr,  # Block size for sequences\n    BLOCK_SIZE_HEAD: tl.constexpr  # Block size for heads\n):\n    # Get the current batch and head indices\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n\n    # Check if the current batch and head indices are within bounds\n    if batch_idx >= batch_size or head_idx >= num_heads:\n        return\n\n    # Load the sequence length for the current batch\n    seq_len = seq_lengths[batch_idx]\n\n    # Initialize accumulators and logic variables\n    acc = tl.zeros((BLOCK_SIZE_HEAD,), dtype=tl.float32)\n    logic_sum = tl.zeros((BLOCK_SIZE_HEAD,), dtype=tl.float32)\n\n    # Process each block of sequence data\n    for start in range(0, seq_len, BLOCK_SIZE_SEQ):\n        # Calculate the end index for the current block\n        end = min(start + BLOCK_SIZE_SEQ, seq_len)\n\n        # Load the sequence data for the current block\n        q = tl.load(Q + batch_idx * head_dim * num_heads + head_idx * head_dim + start * BLOCK_SIZE_HEAD, mask=start + tl.arange(0, BLOCK_SIZE_SEQ) < seq_len)\n        k = tl.load(K + batch_idx * head_dim * num_heads + head_idx * head_dim + start * BLOCK_SIZE_HEAD, mask=start + tl.arange(0, BLOCK_SIZE_SEQ) < seq_len)\n        v = tl.load(V + batch_idx * head_dim * num_heads + head_idx * head_dim + start * BLOCK_SIZE_HEAD, mask=start + tl.arange(0, BLOCK_SIZE_SEQ) < seq_len)\n\n        # Compute the scaling factor\n        scale = 1.0 / tl.sqrt(tl.float32(head_dim))\n\n        # Compute the attention scores\n        attn_scores = tl.exp(q * k * scale)\n\n        # Accumulate the attention scores and the value\n        acc += attn_scores * v\n        logic_sum += attn_scores\n\n    # Normalize the accumulated value by the sum of exponentials\n    acc /= logic_sum\n\n    # Store the final output\n    tl.store(O + batch_idx * head_dim * num_heads + head_idx * head_dim, acc, mask=tl.arange(0, BLOCK_SIZE_HEAD) < head_dim)\n\nimport torch\n\ndef flash_decode_stage2(Q, K, V, O, seq_lengths, batch_size, head_dim, num_heads, BLOCK_SIZE_SEQ=128, BLOCK_SIZE_HEAD=64):\n    # Convert inputs to Triton tensors\n    Q = Q.contiguous()\n    K = K.contiguous()\n    V = V.contiguous()\n    O = O.contiguous()\n    seq_lengths = seq_lengths.contiguous()\n\n    # Determine the grid size\n    grid = (batch_size, num_heads)\n\n    # Launch the kernel\n    _fwd_kernel_flash_decode_stage2[grid](\n        Q, K, V, O,\n        seq_lengths,\n        batch_size,\n        head_dim,\n        num_heads,\n        BLOCK_SIZE_SEQ,\n        BLOCK_SIZE_HEAD\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    B_Seqlen,\n    Mid_O,  # [batch, head, seq_block_num, head_dim]\n    Mid_O_LogExpSum,  # [batch, head, seq_block_num]\n    Out,  # [batch, head, head_dim]\n    stride_mid_ob,\n    stride_mid_oh,\n    stride_mid_os,\n    stride_mid_od,\n    stride_mid_o_eb,\n    stride_mid_o_eh,\n    stride_mid_o_es,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    head_dim,\n    BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ\n\n    sum_exp = 0.0\n    max_logic = -float(\"inf\")\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os, mask=offs_d < head_dim, other=0.0)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n\n    tl.store(Out + cur_batch * stride_obs + cur_head * stride_oh + offs_d, acc / sum_exp, mask=offs_d < head_dim)\n    return\n\n@torch.no_grad()\ndef flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, Out, block_seq):\n    Lk = mid_out.shape[-1]\n    head_dim = Lk\n    batch, head_num = mid_out.shape[0], mid_out.shape[1]\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    grid = (batch, head_num)\n\n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen,\n        mid_out,\n        mid_out_logexpsum,\n        Out,\n        mid_out.stride(0),\n        mid_out.stride(1),\n        mid_out.stride(2),\n        mid_out.stride(3),\n        mid_out_logexpsum.stride(0),\n        mid_out_logexpsum.stride(1),\n        mid_out_logexpsum.stride(2),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        head_dim,\n        BLOCK_SEQ=block_seq,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        num_warps=4,\n        num_stages=2,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code defines a kernel for performing matrix multiplication using GPU with optional leaky ReLU activation. The `matmul_kernel` function performs the matrix multiplication of two input matrices, `a` and `b`, and optionally applies the leaky ReLU activation to the result. It utilizes Triton's autotuning to optimize the computation based on different configurations, specifically for CUDA and HIP backends.\n\n            The main function `matmul` is a wrapper that prepares input matrices for the kernel, checks for dimensional compatibility, and invokes the kernel using specified configurations. It returns the result matrix `c`.\n\n            Key parameters:\n            - `a_ptr`, `b_ptr`, `c_ptr`: Pointers to input matrices A, B, and output matrix C.\n            - `M`, `N`, `K`: Dimensions of the matrices involved.\n            - `stride_am`, `stride_ak`, `stride_bk`, `stride_bn`, `stride_cm`, `stride_cn`: Stride values for indexing matrices.\n            - `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, `BLOCK_SIZE_K`: Block sizes for the computation.\n            - `GROUP_SIZE_M`: Number of groups in the grid.\n            - `ACTIVATION`: Type of activation function applied.\n\n            The `leaky_relu` function is used when specified, modifying negative outputs to 0.01 times their value.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    ACTIVATION: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) * stride_am\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) * stride_bn\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] + offs_bn[None, :]) * stride_bk\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a_block = tl.load(a_ptrs)\n        b_block = tl.load(b_ptrs)\n        accumulator += tl.dot(a_block, b_block)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c_ptrs = c_ptr + (offs_am[:, None] + offs_bn[None, :]) * stride_cn\n    if ACTIVATION == 1:\n        accumulator = tl.where(accumulator > 0, accumulator, 0.01 * accumulator)\n    tl.store(c_ptrs, accumulator)\n\nimport triton\nimport triton.runtime as triton_runtime\nimport numpy as np\n\ndef matmul(a, b, activation=False):\n    M, K = a.shape\n    K, N = b.shape\n    c = np.empty((M, N), dtype=a.dtype)\n\n    a_ptr = triton_runtime.to_device(a)\n    b_ptr = triton_runtime.to_device(b)\n    c_ptr = triton_runtime.to_device(c)\n\n    BLOCK_SIZE_M = 16\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_K = 16\n    GROUP_SIZE_M = 8\n\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\n    matmul_kernel[grid](\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        a.strides[0] // a.itemsize, a.strides[1] // a.itemsize,\n        b.strides[0] // b.itemsize, b.strides[1] // b.itemsize,\n        c.strides[0] // c.itemsize, c.strides[1] // c.itemsize,\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M,\n        1 if activation else 0\n    )\n\n    return c\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Autotune configuration functions\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        # Other configurations omitted for brevity\n    ]\n\ndef get_hip_autotune_config():\n    return [\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2}, num_warps=4, num_stages=2),\n        # Other configurations omitted for brevity\n    ]\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n\n# Triton kernel for matrix multiplication with autotuning\n@triton.autotune(\n    configs=get_autotune_config(),\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,  \n        stride_bk, stride_bn,  \n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  \n        GROUP_SIZE_M: tl.constexpr,  \n        ACTIVATION: tl.constexpr  \n):\n    \"\"\"Kernel for computing the matrix multiplication C = A x B.\n    A has shape (M, K), B has shape (K, N), and C has shape (M, N).\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n@triton.jit\ndef leaky_relu(x):\n    \"\"\"Applies the Leaky ReLU activation function.\"\"\"\n    return tl.where(x >= 0, x, 0.01 * x)\n\ndef matmul(a, b, activation=\"\"):\n    \"\"\"Wrapper function for matrix multiplication using Triton.\n    a: Tensor with shape (M, K)\n    b: Tensor with shape (K, N)\n    activation: Activation function to be applied (optional)\n    Returns the result of the matrix multiplication.\n    \"\"\"\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n    matmul_kernel[grid](\n        a, b, c,  \n        M, N, K,  \n        a.stride(0), a.stride(1),  \n        b.stride(0), b.stride(1),  \n        c.stride(0), c.stride(1),  \n        ACTIVATION=activation  \n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code implements two main kernels, `prefill_cache_kernel` and `decoding_cache_kernel`, designed for efficiently managing cosine and sine caches used in sequence processing.\n\n            - `prefill_cache_kernel`: This kernel is used when `is_prompts` is `True`. It copies segments of cosine and sine cache data to output tensors based on provided cumulative sequence lengths. Inputs include `cos_cache`, `sin_cache`, `cumsum_lengths`, `cos_output`, `sin_output`, `cache_stride`, `hidden_stride`, `total_length`, with constants `HIDDEN_DIM`, `N_ELEMENTS`, and `BLOCK_SIZE`. The logic computes the original sequence index for each element, extracts relevant cache parts, and stores them in the output tensors.\n\n            - `decoding_cache_kernel`: This kernel is used when `is_prompts` is `False`. It handles the copying of cache segments for decoding purposes. It uses input lengths to compute original sequence indices for each block and retrieves corresponding cache parts. Inputs are `cos_cache`, `sin_cache`, `lengths`, `cos_output`, `sin_output`, `cache_stride`, `hidden_stride`, with constants `HIDDEN_DIM`, `NUM_SEQS`, and `BLOCK_SIZE`.\n\n            The function `get_xine_cache` coordinates these kernels. It calculates necessary parameters such as `cache_stride`, `hidden_stride`, and sets up output tensors. Depending on `is_prompts`, it either executes `prefill_cache_kernel` for prompt filling or `decoding_cache_kernel` for sequence decoding.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nHIDDEN_DIM = 1024\nN_ELEMENTS = 128\nNUM_SEQS = 64\nBLOCK_SIZE = 128\n\n@triton.jit\ndef prefill_cache_kernel(\n    cos_cache_ptr, sin_cache_ptr, cumsum_lengths_ptr, cos_output_ptr, sin_output_ptr,\n    cache_stride: tl.constexpr, hidden_stride: tl.constexpr, total_length: tl.constexpr,\n    HIDDEN_DIM: tl.constexpr, N_ELEMENTS: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < total_length\n\n    # Compute the original sequence index for each element\n    cumsum_lengths = tl.load(cumsum_lengths_ptr + offsets, mask=mask, other=0)\n    seq_indices = tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    for i in range(BLOCK_SIZE):\n        if mask[i]:\n            seq_indices[i] = tl.sum(cumsum_lengths[:i] < offsets[i])\n\n    # Compute the cache indices\n    cache_indices = offsets - cumsum_lengths[seq_indices]\n\n    # Load the cache data\n    cos_cache = tl.load(cos_cache_ptr + seq_indices * cache_stride + cache_indices * hidden_stride, mask=mask, other=0)\n    sin_cache = tl.load(sin_cache_ptr + seq_indices * cache_stride + cache_indices * hidden_stride, mask=mask, other=0)\n\n    # Store the cache data in the output tensors\n    tl.store(cos_output_ptr + offsets * hidden_stride, cos_cache, mask=mask)\n    tl.store(sin_output_ptr + offsets * hidden_stride, sin_cache, mask=mask)\n\n@triton.jit\ndef decoding_cache_kernel(\n    cos_cache_ptr, sin_cache_ptr, lengths_ptr, cos_output_ptr, sin_output_ptr,\n    cache_stride: tl.constexpr, hidden_stride: tl.constexpr,\n    HIDDEN_DIM: tl.constexpr, NUM_SEQS: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < NUM_SEQS\n\n    # Compute the original sequence index for each block\n    lengths = tl.load(lengths_ptr + offsets, mask=mask, other=0)\n    seq_indices = offsets\n\n    # Compute the cache indices\n    cache_indices = lengths - 1\n\n    # Load the cache data\n    cos_cache = tl.load(cos_cache_ptr + seq_indices * cache_stride + cache_indices * hidden_stride, mask=mask, other=0)\n    sin_cache = tl.load(sin_cache_ptr + seq_indices * cache_stride + cache_indices * hidden_stride, mask=mask, other=0)\n\n    # Store the cache data in the output tensors\n    tl.store(cos_output_ptr + offsets * hidden_stride, cos_cache, mask=mask)\n    tl.store(sin_output_ptr + offsets * hidden_stride, sin_cache, mask=mask)\n\nimport torch\n\ndef get_xine_cache(\n    cos_cache, sin_cache, cumsum_lengths, lengths, is_prompts,\n    total_length, num_seqs, hidden_dim, n_elements, num_seqs_decoding\n):\n    # Constants\n    HIDDEN_DIM = hidden_dim\n    N_ELEMENTS = n_elements\n    NUM_SEQS = num_seqs\n    BLOCK_SIZE = n_elements\n\n    # Calculate strides\n    cache_stride = cos_cache.shape[1]\n    hidden_stride = cos_cache.shape[2]\n\n    # Allocate output tensors\n    cos_output = torch.empty((total_length, hidden_dim), device=cos_cache.device, dtype=cos_cache.dtype)\n    sin_output = torch.empty((total_length, hidden_dim), device=sin_cache.device, dtype=sin_cache.dtype)\n\n    if is_prompts:\n        # Launch prefill_cache_kernel\n        grid = (total_length // BLOCK_SIZE + 1,)\n        prefill_cache_kernel[grid](\n            cos_cache, sin_cache, cumsum_lengths, cos_output, sin_output,\n            cache_stride, hidden_stride, total_length,\n            HIDDEN_DIM, N_ELEMENTS, BLOCK_SIZE\n        )\n    else:\n        # Launch decoding_cache_kernel\n        grid = (num_seqs_decoding // BLOCK_SIZE + 1,)\n        decoding_cache_kernel[grid](\n            cos_cache, sin_cache, lengths, cos_output, sin_output,\n            cache_stride, hidden_stride,\n            HIDDEN_DIM, NUM_SEQS, BLOCK_SIZE\n        )\n\n    return cos_output, sin_output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef prefill_cache_kernel(\n    cos_cache,\n    sin_cache,\n    cumsum_lengths,\n    cos_output,\n    sin_output,\n    cache_stride,\n    hidden_stride,\n    total_length,\n    HIDDEN_DIM: tl.constexpr,\n    N_ELEMENTS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    idx0 = tl.program_id(axis=0)\n    idx1 = tl.program_id(axis=1)\n    idx = idx0 * BLOCK_SIZE + idx1\n\n    # original seq_idx and pos\n    cumsum_lens = tl.load(cumsum_lengths + tl.arange(0, N_ELEMENTS))\n    ori_seq_idx = idx - tl.max(tl.where(cumsum_lens <= idx, cumsum_lens, 0))\n    cos_cache_part = tl.load(\n        cos_cache + ori_seq_idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride, mask=idx < total_length\n    )\n    sin_cache_part = tl.load(\n        sin_cache + ori_seq_idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride, mask=idx < total_length\n    )\n    tl.store(\n        cos_output + idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride,\n        cos_cache_part,\n        mask=idx < total_length,\n    )\n    tl.store(\n        sin_output + idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride,\n        sin_cache_part,\n        mask=idx < total_length,\n    )\n\n\n@triton.jit\ndef decoding_cache_kernel(\n    cos_cache,\n    sin_cache,\n    lengths,\n    cos_output,\n    sin_output,\n    cache_stride,\n    hidden_stride,\n    HIDDEN_DIM: tl.constexpr,\n    NUM_SEQS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    ori_seq_idx = tl.load(lengths + idx, mask=(idx < NUM_SEQS), other=None)  # [BLOCK_SIZE,]\n    cos_cache_part = tl.load(\n        cos_cache + ori_seq_idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n    sin_cache_part = tl.load(\n        sin_cache + ori_seq_idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n    tl.store(\n        cos_output + (idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride),\n        cos_cache_part,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n    tl.store(\n        sin_output + (idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride),\n        sin_cache_part,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n\n\ndef get_xine_cache(lengths: torch.Tensor, cos_cache: torch.Tensor, sin_cache: torch.Tensor, is_prompts: bool = False):\n    assert cos_cache.shape[1] == sin_cache.shape[1]\n    _, hidden_dim = cos_cache.shape\n    num_seqs = lengths.numel()\n\n    if hidden_dim >= 256:\n        num_warps = 16\n    elif hidden_dim >= 128:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    cache_stride = cos_cache.stride(0)\n    hidden_stride = cos_cache.stride(1)\n\n    if is_prompts:\n        BLOCK_SIZE = 16\n        total_length = lengths.sum().item()\n        cumsum_lens = torch.cumsum(lengths, dim=0)\n        cos_output = torch.empty((total_length, hidden_dim), dtype=cos_cache.dtype, device=cos_cache.device)\n        sin_output = torch.empty((total_length, hidden_dim), dtype=sin_cache.dtype, device=sin_cache.device)\n        grid = (triton.cdiv(total_length, BLOCK_SIZE), BLOCK_SIZE)\n        prefill_cache_kernel[grid](\n            cos_cache,\n            sin_cache,\n            cumsum_lens,\n            cos_output,\n            sin_output,\n            cache_stride,\n            hidden_stride,\n            total_length,\n            HIDDEN_DIM=hidden_dim,\n            N_ELEMENTS=triton.next_power_of_2(num_seqs),\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n    else:\n        BLOCK_SIZE = 4\n        nlengths = torch.as_tensor(lengths) - 1\n        cos_output = torch.empty((num_seqs, hidden_dim), dtype=cos_cache.dtype, device=cos_cache.device)\n        sin_output = torch.empty((num_seqs, hidden_dim), dtype=sin_cache.dtype, device=sin_cache.device)\n        grid = (triton.cdiv(num_seqs, BLOCK_SIZE),)\n        decoding_cache_kernel[grid](\n            cos_cache,\n            sin_cache,\n            nlengths,\n            cos_output,\n            sin_output,\n            cache_stride,\n            hidden_stride,\n            HIDDEN_DIM=hidden_dim,\n            NUM_SEQS=num_seqs,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n\n    return cos_output, sin_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This code implements parallel attention mechanisms using Triton for GPU acceleration. The main components are the `parallel_retention_fwd_kernel` and `parallel_retention_bwd_kernel` functions, which handle the forward and backward passes, respectively. The kernels utilize block pointers for efficient memory access and leverage parallelism across sequences and heads. The `forward` function computes scaled dot-product attention, where `q`, `k`, and `v` are the query, key, and value tensors. It saves these tensors for backward pass. The `backward` function calculates the gradients for `q`, `k`, and `v` using stored tensors from the forward pass. The implementation handles decay factors and strides for multi-head attention computations.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef parallel_retention_fwd_kernel(\n    Q, K, V, Out,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_ob, stride_oh, stride_om,\n    N_CTX, N_HEAD, SCALE,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(N_CTX, BLOCK_M)\n    num_pid_n = tl.cdiv(N_CTX, BLOCK_N)\n    num_pid_in_block = num_pid_m * num_pid_n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qh)\n    k_ptrs = K + (offs_n[:, None] * stride_km + offs_d[None, :] * stride_kh)\n    v_ptrs = V + (offs_n[:, None] * stride_vm + offs_d[None, :] * stride_vh)\n    o_ptrs = Out + (offs_m[:, None] * stride_om + offs_n[None, :] * stride_oh)\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for d in range(0, N_HEAD, BLOCK_D):\n        qk = tl.dot(q, k, trans_b=True)\n        acc += qk * SCALE\n    tl.store(o_ptrs, acc)\n\n@triton.jit\ndef parallel_retention_bwd_kernel(\n    Q, K, V, GradOut, GradQ, GradK, GradV,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_gob, stride_goh, stride_gom,\n    stride_gqb, stride_gqh, stride_gqm,\n    stride_gkb, stride_gkh, stride_gkm,\n    stride_gvb, stride_gvh, stride_gvm,\n    N_CTX, N_HEAD, SCALE,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(N_CTX, BLOCK_M)\n    num_pid_n = tl.cdiv(N_CTX, BLOCK_N)\n    num_pid_in_block = num_pid_m * num_pid_n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qh)\n    k_ptrs = K + (offs_n[:, None] * stride_km + offs_d[None, :] * stride_kh)\n    v_ptrs = V + (offs_n[:, None] * stride_vm + offs_d[None, :] * stride_vh)\n    go_ptrs = GradOut + (offs_m[:, None] * stride_gom + offs_n[None, :] * stride_goh)\n    gq_ptrs = GradQ + (offs_m[:, None] * stride_gqm + offs_d[None, :] * stride_gqh)\n    gk_ptrs = GradK + (offs_n[:, None] * stride_gkm + offs_d[None, :] * stride_gkh)\n    gv_ptrs = GradV + (offs_n[:, None] * stride_gvm + offs_d[None, :] * stride_gvh)\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n    go = tl.load(go_ptrs)\n    gq = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)\n    gk = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    gv = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for d in range(0, N_HEAD, BLOCK_D):\n        qk = tl.dot(q, k, trans_b=True)\n        qk *= SCALE\n        qk *= go\n        gq += tl.dot(qk, v, trans_b=True)\n        gk += tl.dot(q, qk, trans_a=True)\n        gv += tl.dot(qk, k)\n    tl.store(gq_ptrs, gq)\n    tl.store(gk_ptrs, gk)\n    tl.store(gv_ptrs, gv)\n\nimport torch\n\ndef forward(q, k, v, scale, block_m=16, block_n=16, block_d=16):\n    N_CTX, N_HEAD, _ = q.shape\n    out = torch.empty_like(q)\n    grid = (triton.cdiv(N_CTX, block_m) * triton.cdiv(N_CTX, block_n),)\n    parallel_retention_fwd_kernel[grid](\n        q, k, v, out,\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        v.stride(0), v.stride(1), v.stride(2),\n        out.stride(0), out.stride(1), out.stride(2),\n        N_CTX, N_HEAD, scale,\n        BLOCK_M=block_m, BLOCK_N=block_n, BLOCK_D=block_d\n    )\n    return out\n\ndef backward(q, k, v, grad_out, scale, block_m=16, block_n=16, block_d=16):\n    N_CTX, N_HEAD, _ = q.shape\n    grad_q = torch.empty_like(q)\n    grad_k = torch.empty_like(k)\n    grad_v = torch.empty_like(v)\n    grid = (triton.cdiv(N_CTX, block_m) * triton.cdiv(N_CTX, block_n),)\n    parallel_retention_bwd_kernel[grid](\n        q, k, v, grad_out, grad_q, grad_k, grad_v,\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        v.stride(0), v.stride(1), v.stride(2),\n        grad_out.stride(0), grad_out.stride(1), grad_out.stride(2),\n        grad_q.stride(0), grad_q.stride(1), grad_q.stride(2),\n        grad_k.stride(0), grad_k.stride(1), grad_k.stride(2),\n        grad_v.stride(0), grad_v.stride(1), grad_v.stride(2),\n        N_CTX, N_HEAD, scale,\n        BLOCK_M=block_m, BLOCK_N=block_n, BLOCK_D=block_d\n    )\n    return grad_q, grad_k, grad_v\n\nimport torch\n\n# Example tensors\nN_CTX = 128\nN_HEAD = 8\nD_HEAD = 64\nq = torch.randn(N_CTX, N_HEAD, D_HEAD, device='cuda')\nk = torch.randn(N_CTX, N_HEAD, D_HEAD, device='cuda')\nv = torch.randn(N_CTX, N_HEAD, D_HEAD, device='cuda')\nscale = 1.0 / (D_HEAD ** 0.5)\n\n# Forward pass\nout = forward(q, k, v, scale)\n\n# Backward pass\ngrad_out = torch.randn_like(out)\ngrad_q, grad_k, grad_v = backward(q, k, v, grad_out, scale)\n\nprint(out)\nprint(grad_q)\nprint(grad_k)\nprint(grad_v)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom torch.amp import custom_bwd, custom_fwd\nimport functools\n\ndef contiguous(fn):\n    @functools.wraps(fn)\n    def wrapper(ctx, *args, **kwargs):\n        return fn(ctx,\n                  *(i if not isinstance(i, torch.Tensor) else i.contiguous() for i in args),\n                  **{k: (v if not isinstance(v, torch.Tensor) else v.contiguous()) for k, v in kwargs.items()})\n    return wrapper\n\n@triton.jit\ndef parallel_retention_fwd_kernel(\n    # B: batch_size, H: n_heads, T: seq_len, D: d_head\n    q,  # query [B, H, L, D_head_K]\n    k,  # key [B, H, L, D_head_V]\n    v,  # value [B, H, L, D_head_V]\n    o,  # output [B, H, L, D_head_V]\n    s_qk_h,  # stride size: L * D_head_K\n    s_qk_t,  # stride size: D_head_K\n    s_qk_d,  # stride size: 1\n    s_vo_h,  # stride size: L * D_head_V\n    s_vo_t,  # stride size: D_head_V\n    s_vo_d,  # stride size: 1\n    B,  # batch size\n    H,  # n_heads\n    T,  # seq_len\n    scale,  # D_head_K ** -0.5\n    BTL: tl.constexpr,  # BLOCK SIZE along the sequence dimension for Q\n    BTS: tl.constexpr,  # BLOCK SIZE along the sequence dimension for K/V\n    BK: tl.constexpr,  # BLOCK SIZE along the K dimension\n    BV: tl.constexpr,  # BLOCK SIZE along the V dimension\n    DK: tl.constexpr,  # D_head_K\n    DV: tl.constexpr,  # D_head_V\n):\n    # i_c: chunk index. used for sequence parallelism\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n    i_h = i_bh % H\n    # decay rate given the head index\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    # cumulative decay from the end of the chunk\n    o_k = tl.arange(0, BTS)\n    d_h = tl.math.exp2((BTS - o_k) * b_b)\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK),\n                            (s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T),\n                            (s_qk_d, s_qk_t), (i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV),\n                            (s_vo_t, s_vo_d), (0, i_v * BV), (BTS, BV), (1, 0))\n\n    # [BQ, BD] block Q, in the shared memory throughout the whole kernel\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n\n    # Q block and K block have no overlap\n    # no need for mask, thereby saving flops\n    for _ in range(0, i_c * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_s = tl.dot(b_q, (b_k), allow_tf32=False) * d_h[None, :]\n        # [BQ, BD]\n        b_o = b_o * tl.math.exp2(b_b * BTS)\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n\n    # # rescale interchunk output\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    d_q = tl.math.exp2(tl.arange(0, BTL) * b_b)\n    b_o *= d_q[:, None]\n    # # sync threads, easy for compiler to optimize\n    # tl.debug_barrier()\n\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T),\n                            (s_qk_d, s_qk_t), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV),\n                            (s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2(\n            (o_q[:, None] - o_k[None, :]) * b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        # [BTL, BV]\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV),\n                            (s_vo_t, s_vo_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef _parallel_retention_bwd_dq(\n    i_bh, i_c, i_k, i_v, i_h,\n    k, v, do, dq, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale,\n    BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    DK: tl.constexpr,  DV: tl.constexpr,\n):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n                             (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK),\n                            (s_qk_t, s_qk_d), (0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T),\n                            (s_vo_d, s_vo_t), (i_v * BV, 0), (BV, BTS), (0, 1))\n    # decay rate given the head index\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    # overall decay rate for an entire block\n    d_b = tl.math.exp2(b_b * BTS)\n    # cumulative decay from the end of the chunk\n    d_h = tl.math.exp2((BTS - tl.arange(0, BTS)) * b_b)\n    for _ in range(0, i_c * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        # [BQ, BD]\n        b_dq *= d_b\n        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= tl.math.exp2(tl.arange(0, BTL) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK),\n                            (s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T),\n                            (s_vo_d, s_vo_t), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2(\n            (o_q[:, None] - o_k[None, :]) * b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        # [BTL, BK]\n        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK),\n                             (s_qk_t, s_qk_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n\n@triton.jit\ndef _parallel_retention_bwd_dkv(\n    i_bh, i_c, i_k, i_v, i_h,\n    q, k, v, do, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale,\n    BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    DK: tl.constexpr,  DV: tl.constexpr,\n):\n    # no overlap. no need for mask.\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    # overall decay rate for an entire block\n    d_b = tl.math.exp2(b_b * BTS)\n    # compute dk dv\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d),\n                            (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n                            (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(\n        p_v, boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(\n        [BTL, BV], dtype=tl.float32)\n    d_h = tl.math.exp2((BTL - tl.arange(0, BTL)) * b_b)\n    b_kd = (b_k * d_h[:, None]).to(b_k.dtype)\n    d_q = tl.math.exp2(tl.arange(0, BTS) * b_b)\n    for i in range((tl.cdiv(T, BTS) * BTS)-BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BK, BTS]\n        b_do = tl.load(p_do, boundary_check=(0, 1))  # [BV, BTS]\n        b_do = (b_do * d_q[None, :]).to(b_do.dtype)\n\n        b_dv *= d_b\n        b_s = tl.dot(b_kd.to(b_q.dtype), b_q, allow_tf32=False)  # [BTL, BTS]\n        b_dv += tl.dot(b_s.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n\n        b_dk *= d_b\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        b_dk += tl.dot(b_ds.to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c*BTL, (i_c+1)*BTL, BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BD, BQ]\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        # [BK, BQ]\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2(\n            (-o_k[:, None] + o_q[None, :]) * b_b.to(tl.float32)), 0) * scale\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * d_s\n        # [BK, BD]\n        b_dk += tl.dot(b_ds.to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h,\n                             (T, DK), (s_qk_t, s_qk_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h,\n                             (T, DV), (s_vo_t, s_vo_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n\n@triton.jit\ndef parallel_retention_bwd_kernel(\n    q, k, v, do, dq, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale,\n    BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    DK: tl.constexpr,  DV: tl.constexpr,\n):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n    i_h = i_bh % H\n    _parallel_retention_bwd_dq(\n        i_bh, i_c, i_k, i_v, i_h,\n        k, v, do, dq, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d, B, H, T, scale,  BTL=BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV\n    )\n    tl.debug_barrier()\n    _parallel_retention_bwd_dkv(\n        i_bh, i_c, i_k, i_v, i_h,\n        q, k, v, do, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d, B, H, T, scale, BTL, BTS, BK, BV, DK, DV\n    )\n\n\nclass ParallelRetentionFunction(torch.autograd.Function):\n    @staticmethod\n    @contiguous\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, q, k, v):\n        BTL, BTS = 128, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n        num_stages = 3 if d_head_qk <= 64 else 2\n        num_warps = 4\n        NK = triton.cdiv(d_head_qk, BK)\n        NV = triton.cdiv(d_head_v, BV)\n\n        grid = (NK * NV, triton.cdiv(seq_len, BTL), batch_size * n_heads)\n        scale = d_head_qk ** -0.5\n        o = torch.empty(NK, batch_size, n_heads, seq_len,\n                        d_head_v, dtype=q.dtype, device=q.device)\n        parallel_retention_fwd_kernel[grid](\n            q, k, v, o,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV, DK=d_head_qk, DV=d_head_v,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        ctx.save_for_backward(q, k, v)\n        return o.sum(0).to(q.dtype)\n\n    @staticmethod\n    @contiguous\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, do):\n        q, k, v = ctx.saved_tensors\n        BTL, BTS = 64, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n        num_stages = 3 if d_head_qk <= 64 else 2\n        num_warps = 4\n        NK = triton.cdiv(d_head_qk, BK)\n        NV = triton.cdiv(d_head_v, BV)\n        grid = (NK * NV, triton.cdiv(seq_len, BTL), batch_size * n_heads)\n        scale = d_head_qk ** -0.5\n\n        dq = torch.empty(NV, batch_size, n_heads, seq_len,\n                         d_head_qk, dtype=q.dtype, device=q.device)\n        dk = torch.empty(NV, batch_size, n_heads, seq_len,\n                         d_head_qk, dtype=q.dtype, device=q.device)\n        dv = torch.empty(NK, batch_size, n_heads, seq_len,\n                         d_head_v, dtype=q.dtype, device=q.device)\n\n        parallel_retention_bwd_kernel[grid](\n            q, k, v, do, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV, DK=d_head_qk, DV=d_head_v,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        return dq.sum(0).to(q.dtype), dk.sum(0).to(k.dtype), dv.sum(0).to(v.dtype)\n\n\nparallel_retention = ParallelRetentionFunction.apply\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a fast RMS Layernorm using Triton, which is a GPU programming language. It includes forward and backward kernels for computing layer normalization with improved speed on GPUs. The main function, `Fast_RMS_Layernorm`, is a PyTorch autograd function that performs the forward and backward pass for the RMS Layernorm. The forward pass calculates the normalized output and stores necessary variables for the backward pass. The `fast_rms_layernorm` function applies this layernorm operation to input data using a defined layernorm layer. The code also includes a simple layernorm module for testing purposes.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_layernorm_forward_kernel(\n    X,  # input tensor\n    Y,  # output tensor\n    W,  # weight tensor\n    B,  # bias tensor\n    Mean,  # mean tensor\n    Rstd,  # reciprocal standard deviation tensor\n    stride_xm,  # stride of input tensor in the feature dimension\n    stride_ym,  # stride of output tensor in the feature dimension\n    stride_wm,  # stride of weight tensor in the feature dimension\n    stride_bm,  # stride of bias tensor in the feature dimension\n    N,  # number of features\n    eps,  # small epsilon value for numerical stability\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    x = tl.load(X + offsets, mask=mask)\n    x_mean = tl.sum(x, axis=0) / N\n    x_var = tl.sum((x - x_mean) * (x - x_mean), axis=0) / N\n    x_rstd = 1.0 / tl.sqrt(x_var + eps)\n\n    y = (x - x_mean) * x_rstd\n    w = tl.load(W + offsets, mask=mask)\n    b = tl.load(B + offsets, mask=mask)\n    y = y * w + b\n\n    tl.store(Y + offsets, y, mask=mask)\n    tl.store(Mean + pid, x_mean)\n    tl.store(Rstd + pid, x_rstd)\n\n@triton.jit\ndef rms_layernorm_backward_kernel(\n    dY,  # gradient of output tensor\n    X,  # input tensor\n    W,  # weight tensor\n    B,  # bias tensor\n    Mean,  # mean tensor\n    Rstd,  # reciprocal standard deviation tensor\n    dX,  # gradient of input tensor\n    dW,  # gradient of weight tensor\n    dB,  # gradient of bias tensor\n    stride_xm,  # stride of input tensor in the feature dimension\n    stride_ym,  # stride of output tensor in the feature dimension\n    stride_wm,  # stride of weight tensor in the feature dimension\n    stride_bm,  # stride of bias tensor in the feature dimension\n    N,  # number of features\n    eps,  # small epsilon value for numerical stability\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    x = tl.load(X + offsets, mask=mask)\n    dy = tl.load(dY + offsets, mask=mask)\n    w = tl.load(W + offsets, mask=mask)\n    b = tl.load(B + offsets, mask=mask)\n    mean = tl.load(Mean + pid)\n    rstd = tl.load(Rstd + pid)\n\n    x_centered = x - mean\n    dy_centered = dy * w * rstd\n\n    dvar = -0.5 * rstd * rstd * rstd * tl.sum(dy_centered * x_centered, axis=0)\n    dmean = -rstd * tl.sum(dy_centered, axis=0) - 2.0 * dvar * mean / N\n\n    dx = dy_centered * rstd + dvar * 2.0 * x_centered / N + dmean / N\n    dw = tl.sum(dy * x_centered * rstd, axis=0)\n    db = tl.sum(dy, axis=0)\n\n    tl.store(dX + offsets, dx, mask=mask)\n    tl.store(dW + offsets, dw, mask=mask)\n    tl.store(dB + offsets, db, mask=mask)\n\nimport torch\nfrom torch.autograd import Function\n\nclass FastRMSLayernorm(Function):\n    @staticmethod\n    def forward(ctx, x, w, b, eps=1e-5):\n        assert x.is_cuda and w.is_cuda and b.is_cuda, \"Triton kernels only support CUDA tensors\"\n        assert x.dim() == 2, \"Input tensor must be 2D (batch_size, features)\"\n        assert w.dim() == 1 and b.dim() == 1, \"Weight and bias tensors must be 1D\"\n        assert x.size(1) == w.size(0) == b.size(0), \"Input tensor and weight/bias tensors must have the same number of features\"\n\n        batch_size, N = x.size()\n        y = torch.empty_like(x)\n        mean = torch.empty((batch_size,), device=x.device, dtype=x.dtype)\n        rstd = torch.empty((batch_size,), device=x.device, dtype=x.dtype)\n\n        grid = (batch_size,)\n        BLOCK_SIZE = 256\n\n        rms_layernorm_forward_kernel[grid](\n            x, y, w, b, mean, rstd,\n            x.stride(1), y.stride(1), w.stride(0), b.stride(0),\n            N, eps, BLOCK_SIZE\n        )\n\n        ctx.save_for_backward(x, w, b, mean, rstd)\n        ctx.eps = eps\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, w, b, mean, rstd = ctx.saved_tensors\n        eps = ctx.eps\n\n        batch_size, N = x.size()\n        grad_input = torch.empty_like(x)\n        grad_weight = torch.empty_like(w)\n        grad_bias = torch.empty_like(b)\n\n        grid = (batch_size,)\n        BLOCK_SIZE = 256\n\n        rms_layernorm_backward_kernel[grid](\n            grad_output, x, w, b, mean, rstd,\n            grad_input, grad_weight, grad_bias,\n            x.stride(1), grad_output.stride(1), w.stride(0), b.stride(0),\n            N, eps, BLOCK_SIZE\n        )\n\n        return grad_input, grad_weight, grad_bias, None\n\ndef fast_rms_layernorm(x, w, b, eps=1e-5):\n    return FastRMSLayernorm.apply(x, w, b, eps)\n\nimport torch.nn as nn\n\nclass RMSLayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5):\n        super(RMSLayerNorm, self).__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, x):\n        return fast_rms_layernorm(x, self.weight, self.bias, self.eps)\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size = 32\n    features = 128\n    x = torch.randn(batch_size, features, device='cuda')\n    layernorm = RMSLayerNorm(features).to('cuda')\n\n    y = layernorm(x)\n    print(y)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\nimport torch.nn as nn\n\n\nnext_power_of_2 = triton.next_power_of_2\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int,):\n    BLOCK_SIZE : int = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _rms_layernorm_forward(\n    Y, Y_row_stride,\n    X, X_row_stride,\n    W, W_row_stride,\n    r, r_row_stride,\n    n_cols, eps,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\n        Fast RMS Layernorm kernel\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    normed = normed.to(W_row.dtype)\n    output = normed * W_row\n    tl.store(Y + col_offsets, output, mask=mask)\n\n@triton.heuristics({\"GEMMA\": lambda args: args[\"GEMMA\"],})\n@triton.jit\ndef _rms_layernorm_backward(\n    dY, dY_row_stride,\n    X, X_row_stride,\n    W, W_row_stride,\n    r, r_row_stride,\n    dW, dW_row_stride,\n    n_cols, eps,\n    GEMMA: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n        Fast RMS Layernorm kernel for the backward pass\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dY += row_idx * dY_row_stride\n    X  += row_idx *  X_row_stride\n    r  += row_idx *  r_row_stride\n\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0).to(tl.float32)\n    X_row  = tl.load(X  + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row  = tl.load(W  + col_offsets, mask=mask, other=0).to(tl.float32)\n\n    inv_var = tl.load(r).to(tl.float32)\n    normed = X_row * inv_var\n\n    if GEMMA: dY_W = dY_row * (W_row + 1.0)\n    else:     dY_W = dY_row * W_row\n\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)\n    tl.store(dY + col_offsets, output, mask=mask)\n\n@triton.jit\ndef _gemma_rms_layernorm_forward(\n    Y, Y_row_stride,\n    X, X_row_stride,\n    W, W_row_stride,\n    r, r_row_stride,\n    n_cols, eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    output = normed * (W_row + 1.0)\n\n    tl.store(Y + col_offsets, output, mask=mask)\n\nclass Fast_RMS_Layernorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, eps, gemma=False):\n        shape = X.shape\n        dim = shape[-1]\n        X = X.view(-1, dim)\n        n_rows, n_cols = X.shape\n        BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n        Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=\"cuda:0\")\n        r = torch.empty(n_rows, dtype=torch.float32, device=\"cuda:0\")\n\n        fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward\n        fx[(n_rows,)](\n            Y, Y.stride(0),\n            X, X.stride(0),\n            W, W.stride(0),\n            r, r.stride(0),\n            n_cols, eps,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n        ctx.eps = eps\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps = num_warps\n        ctx.GEMMA = gemma\n        ctx.save_for_backward(X, W, r)\n        return Y.view(*shape)\n\n    @staticmethod\n    def backward(ctx, dY):\n        shape = dY.shape\n        dim = shape[-1]\n        dY = dY.view(-1, dim)\n        X, W, r = ctx.saved_tensors\n        n_rows, n_cols = dY.shape\n        dW = X\n\n        _rms_layernorm_backward[(n_rows,)](\n            dY, dY.stride(0),\n            X,  X.stride(0),\n            W,  W.stride(0),\n            r,  r.stride(0),\n            dW, dW.stride(0),\n            n_cols, ctx.eps,\n            GEMMA=ctx.GEMMA,\n            BLOCK_SIZE=ctx.BLOCK_SIZE,\n            num_warps=ctx.num_warps,\n        )\n        dX = dY.view(*shape)\n        return dX, None, None, None\n\ndef fast_rms_layernorm(layernorm, X, gemma=False):\n    W = layernorm.weight\n    eps = layernorm.variance_epsilon if \\\n        hasattr(layernorm, \"variance_epsilon\") \\\n        else layernorm.eps\n    out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)\n    return out\n\n\n# Define a simple layernorm module for testing\nclass SimpleLayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5):\n        super(SimpleLayerNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape).cuda())\n        self.eps = eps\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel for the forward pass of an attention mechanism, which is a common operation in transformer-based neural networks. The main function, `forward`, takes in query (`q`), key (`k`), and value (`v`) tensors along with scaling factors for query and key (`q_scale`, `k_scale`). It performs a block-wise computation over these inputs to compute the attention scores and the resulting output tensor `o`.\n\n            The `_attn_fwd` kernel is called within the `forward` function, which is responsible for the main attention computation. It uses block-wise parallelism to handle large tensor operations efficiently. The kernel computes the dot product of `q` and `k`, scales it, applies a softmax-like normalization, and finally computes the weighted sum with `v`.\n\n            The helper function `_attn_fwd_inner` is called within the `_attn_fwd` kernel to handle the computations for each block of the input tensors. It iteratively updates accumulated results (`acc`) and normalization factors (`l_i`) for attention score calculations over the context size `N_CTX`.\n\n            The function returns the computed output tensor `o`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nBLOCK_SIZE = 128\nN_CTX = 1024  # Context size\n\n@triton.jit\ndef _attn_fwd_inner(q, k, v, o, l_i, acc, q_scale, k_scale, q_offset, k_offset, v_offset, o_offset, n_heads, head_dim, BLOCK_SIZE, N_CTX):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    q_idx = q_offset + block_start + tl.arange(0, BLOCK_SIZE)\n    k_idx = k_offset + block_start + tl.arange(0, BLOCK_SIZE)\n    v_idx = v_offset + block_start + tl.arange(0, BLOCK_SIZE)\n    o_idx = o_offset + block_start + tl.arange(0, BLOCK_SIZE)\n\n    q_block = tl.load(q + q_idx)\n    k_block = tl.load(k + k_idx)\n    v_block = tl.load(v + v_idx)\n\n    q_block = q_block * q_scale\n    k_block = k_block * k_scale\n\n    for i in range(0, N_CTX, BLOCK_SIZE):\n        k_i = k_block + i\n        v_i = v_block + i\n        qk = tl.dot(q_block, k_i, allow_tf32=True)\n        qk = tl.softmax(qk)\n        o_block = tl.dot(qk, v_i, allow_tf32=True)\n        l_i += tl.sum(qk, axis=1)\n        acc += o_block\n\n    tl.store(o + o_idx, acc)\n\n@triton.jit\ndef _attn_fwd(q, k, v, o, q_scale, k_scale, n_heads, head_dim, BLOCK_SIZE, N_CTX):\n    pid = tl.program_id(0)\n    head_idx = pid // (N_CTX // BLOCK_SIZE)\n    block_idx = pid % (N_CTX // BLOCK_SIZE)\n\n    q_offset = head_idx * head_dim * N_CTX + block_idx * BLOCK_SIZE\n    k_offset = head_idx * head_dim * N_CTX + block_idx * BLOCK_SIZE\n    v_offset = head_idx * head_dim * N_CTX + block_idx * BLOCK_SIZE\n    o_offset = head_idx * head_dim * N_CTX + block_idx * BLOCK_SIZE\n\n    l_i = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_SIZE, head_dim), dtype=tl.float32)\n\n    _attn_fwd_inner(q, k, v, o, l_i, acc, q_scale, k_scale, q_offset, k_offset, v_offset, o_offset, n_heads, head_dim, BLOCK_SIZE, N_CTX)\n\ndef forward(q, k, v, q_scale, k_scale, n_heads, head_dim):\n    BLOCK_SIZE = 128\n    N_CTX = 1024\n    o = triton.testing.empty_like(q)\n\n    grid = (n_heads * (N_CTX // BLOCK_SIZE),)\n    _attn_fwd[grid](q, k, v, o, q_scale, k_scale, n_heads, head_dim, BLOCK_SIZE, N_CTX)\n\n    return o\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for the inner loop of attention forward pass\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask = k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask = (offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        \n        acc += tl.dot(p, v.to(tl.float16), out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i\n\n# Triton kernel for the attention forward pass\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr  \n              ):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N) \n    \n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX \n                                    )\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\n# Function to call the Triton kernel\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.bfloat16)\n    stage = 1\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=3)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel computes a custom operation on a batched input tensor `x` with dimensions [batch, M, K], generating an output tensor of the same shape. The operation is a form of position-dependent complex transformation using cosine and sine functions, parameterized by a constant `THETA`. The kernel is split into 2D blocks, processing blocks of size `BLOCK_SIZE_M` by `BLOCK_SIZE_K`, where `BLOCK_SIZE_M` is 2 and `BLOCK_SIZE_K` is 1024. The input tensor is divided among Triton program instances, each handling specific indices. The result is stored back in an output tensor `out` of the same shape. The main function `rbe_triton_wrapper` sets up the grid and executes the kernel, given a tensor `x` and a starting position `pos`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the constant THETA\nTHETA = 0.01\n\n# Triton kernel\n@triton.jit\ndef rbe_triton_kernel(\n    x_ptr,  # Pointer to the input tensor\n    out_ptr,  # Pointer to the output tensor\n    batch,  # Batch size\n    M,  # Dimension M\n    K,  # Dimension K\n    pos,  # Starting position\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for M\n    BLOCK_SIZE_K: tl.constexpr  # Block size for K\n):\n    # Compute the 2D block indices\n    pid_m = tl.program_id(axis=1)\n    pid_k = tl.program_id(axis=2)\n\n    # Compute the starting indices for the block\n    rm = pid_m * BLOCK_SIZE_M\n    rk = pid_k * BLOCK_SIZE_K\n\n    # Compute the range of indices for the block\n    offsets_m = tl.arange(0, BLOCK_SIZE_M) + rm\n    offsets_k = tl.arange(0, BLOCK_SIZE_K) + rk\n\n    # Compute the batch index\n    pid_b = tl.program_id(axis=0)\n    batch_idx = pid_b\n\n    # Load the input data\n    x = tl.load(x_ptr + batch_idx * M * K + offsets_m[:, None] * K + offsets_k[None, :])\n\n    # Compute the position-dependent transformation\n    position = pos + offsets_k[None, :]\n    cos_theta = tl.cos(THETA * position)\n    sin_theta = tl.sin(THETA * position)\n    out = x * cos_theta + x * sin_theta\n\n    # Store the result\n    tl.store(out_ptr + batch_idx * M * K + offsets_m[:, None] * K + offsets_k[None, :], out)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# Define the block sizes\nBLOCK_SIZE_M = 2\nBLOCK_SIZE_K = 1024\n\n# Wrapper function to set up the grid and execute the kernel\ndef rbe_triton_wrapper(x, pos):\n    # Get the dimensions of the input tensor\n    batch, M, K = x.shape\n\n    # Create the output tensor\n    out = torch.empty_like(x)\n\n    # Define the grid size\n    grid = (batch, (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M, (K + BLOCK_SIZE_K - 1) // BLOCK_SIZE_K)\n\n    # Launch the kernel\n    rbe_triton_kernel[grid](\n        x,  # Input tensor\n        out,  # Output tensor\n        batch,  # Batch size\n        M,  # Dimension M\n        K,  # Dimension K\n        pos,  # Starting position\n        BLOCK_SIZE_M,  # Block size for M\n        BLOCK_SIZE_K  # Block size for K\n    )\n\n    return out\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    batch, M, K = 2, 16, 1024\n    x = torch.randn((batch, M, K), device='cuda')\n\n    # Starting position\n    pos = 0\n\n    # Run the kernel\n    out = rbe_triton_wrapper(x, pos)\n\n    # Print the output\n    print(out)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef get_freq_multi_tokens(offs_cn, starting_idx, theta: tl.constexpr, NB_TOKENS: tl.constexpr):\n    DIM: tl.constexpr = 128  # in model, dim = self.params.dim // self.params.n_heads\n    freqs = offs_cn % DIM\n    freqs = freqs.to(tl.float32) / DIM\n    freqs = tl.extra.cuda.libdevice.pow(theta, freqs)\n    freqs = (tl.arange(0, NB_TOKENS) + starting_idx)[:, None] / freqs[None, :]\n    return tl.cos(freqs), tl.sin(freqs)\n\n\n@triton.jit\ndef rbe_triton(x_ptr, out_ptr,\n               M, K,\n               stride_x_batch, stride_x_m, stride_x_n,\n               stride_out_batch, stride_out_m, stride_out_n,\n               start_token_position,\n               THETA: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_n = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K // 2) * 2  # take only even numbers\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + stride_x_m * offs_m[:, None] + stride_x_n * offs_n[None, :])\n    x_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    real = tl.load(x_ptrs, mask=x_real_mask, other=0.0)\n    x_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    imag = tl.load(x_ptrs + 1, mask=x_imag_mask, other=0.0)\n    tl.debug_barrier()\n    start_block = start_token_position + pid_m * BLOCK_SIZE_M\n    cos, sin = get_freq_multi_tokens(offs_cn=offs_n, starting_idx=start_block, theta=THETA, NB_TOKENS=BLOCK_SIZE_M)\n\n    out_real = real * cos - imag * sin\n    out_imag = real * sin + imag * cos\n    tl.debug_barrier()\n    out_ptrs = out_ptr + (\n            pid_batch * stride_out_batch + stride_out_m * offs_m[:, None] + stride_out_n * offs_n[None, :])\n    out_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    tl.store(out_ptrs, out_real, mask=out_real_mask)\n    out_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    tl.store(out_ptrs + 1, out_imag, mask=out_imag_mask)\n\n\ndef rbe_triton_wrapper(x: torch.Tensor, pos: int) -> torch.Tensor:\n    batch, M, K = x.shape\n    out = torch.empty_like(x)\n    grid = lambda META: (\n        batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"K\"], META[\"BLOCK_SIZE_K\"]),)\n\n    rbe_triton[grid](x, out,\n                     M, K,\n                     *x.stride(),\n                     *out.stride(),\n                     start_token_position=pos, THETA=10000., BLOCK_SIZE_M=2, BLOCK_SIZE_K=1024)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code implements a matrix multiplication kernel using Triton, designed for GPUs. The main function is `iv_dependent_matmul_kernel`, which multiplies two matrices `a` and `b`, and stores the result in `c`. It supports different data loading strategies specified by the `type` parameter (like 'pre_load', 'post_load', etc.). A wrapper function `iv_dependent_matmul_wrapper` sets up the necessary parameters and invokes this kernel. The wrapper accepts matrix dimensions (`M`, `K`, `N`), block sizes, and kernel type to determine how data is loaded and processed.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef iv_dependent_matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    type: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size_m = num_pid_m\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        if type == 0:  # pre_load\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n        elif type == 1:  # post_load\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n        else:  # no_load\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n\n        accumulator += tl.dot(a, b)\n\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    tl.store(c_ptrs, accumulator)\n\nimport triton\nimport triton.language as tl\n\ndef iv_dependent_matmul_wrapper(a, b, c, M, K, N, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, type):\n    assert a.shape == (M, K) and b.shape == (K, N) and c.shape == (M, N)\n    assert a.dtype == b.dtype == c.dtype\n    assert a.is_contiguous() and b.is_contiguous() and c.is_contiguous()\n\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\n    iv_dependent_matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, type\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef iv_dependent_matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                                M, N, K,  #\n                                stride_am, stride_ak,  #\n                                stride_bk, stride_bn,  #\n                                stride_cm, stride_cn,  #\n                                BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n                                type: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptr = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptr = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    a_ptrs = a_ptr\n    b_ptrs = b_ptr\n    if type == \"post_load_two_iters\":\n        a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n        b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n    elif type == \"post_load_three_iters\":\n        a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n        b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n        a_ptrs_next_next = a_ptr + 2 * BLOCK_SIZE_K * stride_ak\n        b_ptrs_next_next = b_ptr + 2 * BLOCK_SIZE_K * stride_bk\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        if type == \"pre_load\":\n            a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n            b_ptrs = b_ptr + k * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_pre_mixed\":\n            a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        if type == \"post_load\":\n            a_ptrs = a_ptr + (k + 1) * BLOCK_SIZE_K * stride_ak\n            b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_pre_mixed\":\n            b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_load_two_iters\":\n            a_ptrs = a_ptrs_next\n            b_ptrs = b_ptrs_next\n            a_ptrs_next = a_ptr + (k + 2) * BLOCK_SIZE_K * stride_ak\n            b_ptrs_next = b_ptr + (k + 2) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_load_three_iters\":\n            a_ptrs = a_ptrs_next\n            b_ptrs = b_ptrs_next\n            a_ptrs_next = a_ptrs_next_next\n            b_ptrs_next = b_ptrs_next_next\n            a_ptrs_next_next = a_ptr + (k + 3) * BLOCK_SIZE_K * stride_ak\n            b_ptrs_next_next = b_ptr + (k + 3) * BLOCK_SIZE_K * stride_bk\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef iv_dependent_matmul_wrapper(\n    M: int, \n    K: int, \n    N: int, \n    BLOCK_SIZE_M: int, \n    BLOCK_SIZE_N: int, \n    BLOCK_SIZE_K: int, \n    type: str = \"pre_load\",  # Kernel type for scheduling (\"pre_load\", \"post_load\", etc.)\n    device: torch.device = \"cuda\"  # Device to run the test (defaults to \"cuda\")\n):\n    # Ensure the device is correct\n    device = torch.device(device)\n\n    # Generate random input matrices a and b on the specified device\n    a = torch.rand((M, K), device=device)\n    b = torch.rand((K, N), device=device)\n\n    # Create an empty tensor to store the Triton result\n    triton_output = torch.empty((M, N), device=device)\n\n    # Define Triton grid configuration\n    def grid(META):\n        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n\n    # Set the number of stages based on the kernel type\n    num_stages = 4 if type == \"post_load_three_iters\" else 3\n\n    # Run the Triton kernel\n    iv_dependent_matmul_kernel[grid](\n        a, b, triton_output, M, N, K,  #\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1),  #\n        triton_output.stride(0), triton_output.stride(1),  #\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K, type=type,  #\n        num_stages=num_stages\n    )\n\n    # Optionally print the result for inspection\n    # print(triton_output)\n\n    return triton_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel `_swiglu_bwd_kernel` is designed to compute the backward pass of the Swish-Gated Linear Units (Swiglu) operation. It takes input tensors `X` and `Y`, the derivative `DOUT`, and optionally the output `OUT` to compute gradients `DX` and `DY`. The main logic involves iterating over columns using the program ID to determine rows and column blocks. The computations involve sigmoid and element-wise operations for gradient calculation, and the results are stored in `DX` and `DY`. The function `_swiglu_bwd` serves as a Python wrapper, reshaping inputs and invoking the Triton kernel. It handles contiguous memory alignment, splits input into two tensors, and sets up the execution grid for the Triton kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _swiglu_bwd_kernel(\n    X, Y, DOUT, DX, DY, \n    stride_x, stride_y, stride_dout, \n    stride_dx, stride_dy, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load data\n    x = tl.load(X + offsets, mask=offsets < stride_x, other=0.0)\n    y = tl.load(Y + offsets, mask=offsets < stride_y, other=0.0)\n    dout = tl.load(DOUT + offsets, mask=offsets < stride_dout, other=0.0)\n    \n    # Compute sigmoid(x)\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-x))\n    \n    # Compute the gradient for Y\n    dy = dout * sigmoid_x\n    # Compute the gradient for X\n    dx = dout * y * (sigmoid_x * (1.0 - sigmoid_x))\n    \n    # Store the results\n    tl.store(DX + offsets, dx, mask=offsets < stride_dx)\n    tl.store(DY + offsets, dy, mask=offsets < stride_dy)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _swiglu_bwd(X, Y, DOUT, OUT=None):\n    # Ensure inputs are contiguous\n    X = X.contiguous()\n    Y = Y.contiguous()\n    DOUT = DOUT.contiguous()\n    \n    # Get the shapes\n    N = X.shape[0]\n    BLOCK_SIZE = 1024  # Adjust block size as needed\n    \n    # Allocate memory for gradients\n    DX = torch.empty_like(X)\n    DY = torch.empty_like(Y)\n    \n    # Define the grid\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    \n    # Launch the kernel\n    _swiglu_bwd_kernel[grid](\n        X, Y, DOUT, DX, DY,\n        X.stride(0), Y.stride(0), DOUT.stride(0),\n        DX.stride(0), DY.stride(0),\n        BLOCK_SIZE\n    )\n    \n    return DX, DY\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Backward Triton kernel for Swish-Gated Linear Units (Swiglu)\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 32}),\n        triton.Config({'BLOCK_N': 64}),\n        triton.Config({'BLOCK_N': 128}),\n        triton.Config({'BLOCK_N': 256}),\n        triton.Config({'BLOCK_N': 512}),\n        triton.Config({'BLOCK_N': 1024}),\n    ],\n    key=['ncols'],\n)\n@triton.heuristics({\"RECOMPUTE_OUTPUT\": lambda args: args[\"OUT\"] is not None})\n@triton.jit\ndef _swiglu_bwd_kernel(\n    X, Y, DOUT, OUT, DX, DY, stride_x_row, stride_y_row, stride_dout_row,\n    stride_out_row, stride_dx_row, stride_dy_row, ncols, BLOCK_N: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)\n\n# Function to invoke the backward kernel\ndef _swiglu_bwd(xy, dout, dxy=None, recompute_output=False, out=None):\n    if xy.stride(-1) != 1:\n        xy = xy.contiguous()\n    if dout.stride(-1) != 1:\n        dout = dout.contiguous()\n    batch_shape = xy.shape[:-1]\n    xy = xy.reshape(-1, xy.shape[-1])\n    x, y = xy.chunk(2, dim=-1)\n    dout = dout.reshape(-1, dout.shape[-1])\n    assert dout.shape == x.shape\n    if dxy is None:\n        dxy = torch.empty_like(xy)\n    else:\n        dxy = dxy.reshape(-1, dxy.shape[-1])\n        assert dxy.shape == xy.shape\n    dx, dy = dxy.chunk(2, dim=-1)\n    assert dx.stride(-1) == 1\n    assert dy.stride(-1) == 1\n    if recompute_output:\n        if out is None:\n            out = torch.empty_like(x)\n        else:\n            out = out.reshape(-1, out.shape[-1])\n            assert out.shape == x.shape\n        assert out.stride(-1) == 1\n    M, N = x.shape\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_N']))\n    with torch.cuda.device(x.device.index):\n        _swiglu_bwd_kernel[grid](\n            x, y, dout, out if recompute_output else None, dx, dy, x.stride(0), y.stride(0),\n            dout.stride(0), out.stride(0) if recompute_output else 0, dx.stride(0),\n            dy.stride(0), N\n        )\n    if not recompute_output:\n        return dxy.reshape(*batch_shape, dxy.shape[-1])\n    else:\n        return dxy.reshape(*batch_shape, dxy.shape[-1]), out.reshape(*batch_shape, out.shape[-1])\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe given Triton operator consists of a kernel function and a wrapper function. The kernel function named 'kernel' transposes a matrix 'M' of size (SIZE_M, D_HEAD) to an output matrix 'Out' of size (D_HEAD, SIZE_M). Inputs are 'M' (input matrix), 'Out' (output matrix), strides of the input and output matrices, and dimensions 'SIZE_M' and 'D_HEAD'. The wrapper function initializes the input matrix with random values and the output matrix with zeros, both on the GPU. It then launches the kernel to perform the matrix transpose and returns the transposed matrix.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel(M, Out, stride_m_row, stride_m_col, stride_out_row, stride_out_col, SIZE_M, D_HEAD, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_m = SIZE_M // BLOCK_SIZE\n    num_pid_n = D_HEAD // BLOCK_SIZE\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    first_pid_n = group_id * num_pid_n\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = first_pid_n + (pid % num_pid_n)\n\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_m = tl.max_contiguous(tl.multiple_of(offs_m, BLOCK_SIZE), BLOCK_SIZE)\n    offs_n = tl.max_contiguous(tl.multiple_of(offs_n, BLOCK_SIZE), BLOCK_SIZE)\n\n    m_mask = offs_m < SIZE_M\n    n_mask = offs_n < D_HEAD\n\n    offs = offs_m[:, None] * stride_m_row + offs_n[None, :] * stride_m_col\n    In = tl.load(M + offs, mask=m_mask[:, None] & n_mask[None, :], other=0.0)\n\n    offs = offs_n[:, None] * stride_out_row + offs_m[None, :] * stride_out_col\n    tl.store(Out + offs, In, mask=m_mask[None, :] & n_mask[:, None])\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef transpose_matrix(M, D_HEAD, SIZE_M, BLOCK_SIZE=128):\n    # Initialize the input matrix with random values\n    M = torch.randn((SIZE_M, D_HEAD), device='cuda', dtype=torch.float32)\n    \n    # Initialize the output matrix with zeros\n    Out = torch.zeros((D_HEAD, SIZE_M), device='cuda', dtype=torch.float32)\n    \n    # Compute the strides\n    stride_m_row = M.stride(0)\n    stride_m_col = M.stride(1)\n    stride_out_row = Out.stride(0)\n    stride_out_col = Out.stride(1)\n    \n    # Launch the kernel\n    grid = lambda meta: (triton.cdiv(SIZE_M * D_HEAD, meta['BLOCK_SIZE'] * meta['BLOCK_SIZE']),)\n    kernel[grid](M, Out, stride_m_row, stride_m_col, stride_out_row, stride_out_col, SIZE_M, D_HEAD, BLOCK_SIZE)\n    \n    return Out\n\n# Example usage\nSIZE_M = 1024\nD_HEAD = 1024\ntransposed_matrix = transpose_matrix(None, D_HEAD, SIZE_M)\nprint(transposed_matrix)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel(\n    M,\n    Out,\n    matrix_stridex,\n    matrix_stridey,\n    out_stridex,\n    out_stridey,\n    SIZE_M: tl.constexpr,\n    D_HEAD: tl.constexpr,\n):\n    size_m_arange = tl.arange(0, SIZE_M)\n    d_head_arange = tl.arange(0, D_HEAD)\n    # transpose\n    matrix_ptr = M + d_head_arange[None, :] * matrix_stridey + size_m_arange[:, None] * matrix_stridex\n    out_ptr = Out + d_head_arange[None, :] * out_stridex + size_m_arange[:, None] * out_stridey\n    matrix = tl.load(matrix_ptr)\n    tl.store(out_ptr, matrix)\n\ndef wrapper(size_m, d_head):\n    matrix = torch.randn((size_m, d_head), dtype=torch.float16, device=\"cuda\")\n    out = torch.zeros((d_head, size_m), dtype=torch.float16, device=\"cuda\")\n\n    grid = (1,)\n    kernel[grid](\n        matrix,\n        out,\n        *matrix.stride(),\n        *out.stride(),\n        size_m,\n        d_head,\n    )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The `softmax_kernel_online_v2` function computes the softmax of input data using Triton language for optimization. It processes data in tiles (sub-arrays) to handle potentially large inputs efficiently. The function takes pointers to the input and output tensors, as well as the dimensions M (number of rows) and N (number of columns) and TILE_N (tile size, power of 2). The function calculates the maximum and the sum of exponentials for stable softmax computation, and normalizes the exponentiated inputs by this sum to produce the final softmax output stored in the output pointer.\n        The helper function `prev_multiple_of` calculates the largest multiple of `b` less than `a`, used for tile alignment.\n        The `softmax` function is a wrapper to invoke the Triton kernel on a given 2D torch tensor `x`. It initializes the output tensor `out` and determines the tile size, then calls the Triton kernel, returning the computed softmax results.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel_online_v2(\n    X,  # pointer to the input tensor\n    Y,  # pointer to the output tensor\n    M,  # number of rows\n    N,  # number of columns\n    TILE_N: tl.constexpr,  # tile size, power of 2\n):\n    # Compute the block ID in the grid\n    pid = tl.program_id(0)\n    num_pid_m = M\n    num_pid_n = tl.cdiv(N, TILE_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group // num_pid_m\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) * TILE_N\n\n    # Compute the offsets for the current block\n    offs_m = pid_m\n    offs_n = pid_n + tl.arange(0, TILE_N)\n    X_block_ptr = X + (offs_m * N + offs_n)\n    Y_block_ptr = Y + (offs_m * N + offs_n)\n\n    # Load the data into the Triton block\n    x = tl.load(X_block_ptr, mask=offs_n < N, other=-float('inf'))\n\n    # Compute the maximum value in the block\n    max_val = tl.max(x, axis=0)\n\n    # Subtract the maximum value for numerical stability\n    x = x - max_val\n\n    # Compute the exponentials\n    z = tl.exp(x)\n\n    # Compute the sum of the exponentials\n    z_sum = tl.sum(z, axis=0)\n\n    # Normalize the exponentials\n    z = z / z_sum\n\n    # Store the results back to the output tensor\n    tl.store(Y_block_ptr, z, mask=offs_n < N)\n\ndef prev_multiple_of(a, b):\n    return (a // b) * b\n\nimport torch\n\ndef softmax(x: torch.Tensor):\n    M, N = x.shape\n    out = torch.empty_like(x)\n    TILE_N = 128  # Example tile size, can be adjusted\n\n    # Ensure the tile size is a power of 2\n    assert (TILE_N & (TILE_N - 1)) == 0, \"TILE_N must be a power of 2\"\n\n    # Launch the Triton kernel\n    grid = (M * ((N + TILE_N - 1) // TILE_N),)\n    softmax_kernel_online_v2[grid](x, out, M, N, TILE_N)\n\n    return out\n", "label": "import triton\nfrom triton import language as tl\nimport torch\n\n@triton.jit\ndef prev_multiple_of(a, b):\n    # the largest x<a that x%b ==0\n    return tl.cdiv(a, b) * b - b\n\n@triton.jit\ndef softmax_kernel_online_v2(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    TILE_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    m = tl.full((TILE_N,), value=-float(\"inf\"), dtype=output_ptr.dtype.element_ty)\n    z = tl.full((TILE_N,), value=0, dtype=output_ptr.dtype.element_ty)\n    prev_multiple = prev_multiple_of(N, TILE_N)\n    for start_n in range(0, prev_multiple, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs).to(output_ptr.dtype.element_ty)\n        new_m = tl.maximum(m, inp)\n        new_z = tl.exp(m - new_m) * z + tl.exp(inp - new_m)\n        m = new_m\n        z = new_z\n    for start_n in range(prev_multiple, N, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(output_ptr.dtype.element_ty)\n        new_m = tl.maximum(m, inp)\n        new_z = tl.exp(m - new_m) * z + tl.exp(inp - new_m)\n        m = new_m\n        z = new_z\n    final_m = tl.max(m, 0)\n    z = tl.sum(tl.exp(m - final_m) * z)\n    m = final_m\n\n    prev_multiple = prev_multiple_of(N, TILE_N)\n    for start_n in range(0, prev_multiple, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs).to(output_ptr.dtype.element_ty)\n        e = tl.exp(inp - m)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out)\n    for start_n in range(prev_multiple, N, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(output_ptr.dtype.element_ty)\n        e = tl.exp(inp - m)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n\ndef softmax(x):\n    M, N = x.shape\n    out = torch.empty_like(x)\n    TILE_N = min(4096, triton.next_power_of_2(N))\n    grid = (M, 1, 1)\n    softmax_kernel_online_v2[grid](out, x, M, N, TILE_N)\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This code defines a Triton kernel `_bgmv_expand_kernel` and a wrapper function `_bgmv_expand`. The kernel performs a batched generalized matrix-vector multiplication (GroupGEMV) with optional input addition and dtype casting. The wrapper function configures the kernel launch.\n\n    The kernel `_bgmv_expand_kernel` takes as inputs the pointers to the input matrix, LoRA weights, output matrix, and several parameters describing the problem size and strides. It computes the product of a portion of the input matrix and LoRA weights, storing the result in the output matrix. If `ADD_INPUTS` is set, it adds the existing output values to the computed results.\n\n    The wrapper function `_bgmv_expand` is a PyTorch inference mode function. It prepares the inputs and launches the Triton kernel with appropriate configuration. The function checks the input tensor properties, configures the execution grid, and handles type casting if necessary.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_kernel(\n    input_ptr,  # Pointer to the input matrix\n    lora_weights_ptr,  # Pointer to the LoRA weights\n    output_ptr,  # Pointer to the output matrix\n    M,  # Number of rows in the input matrix\n    N,  # Number of columns in the input matrix\n    K,  # Number of columns in the LoRA weights\n    stride_input_m,  # Stride of the input matrix in the M dimension\n    stride_input_n,  # Stride of the input matrix in the N dimension\n    stride_lora_k,  # Stride of the LoRA weights in the K dimension\n    stride_lora_n,  # Stride of the LoRA weights in the N dimension\n    stride_output_m,  # Stride of the output matrix in the M dimension\n    stride_output_k,  # Stride of the output matrix in the K dimension\n    ADD_INPUTS: tl.constexpr,  # Flag to add existing output values\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for the M dimension\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for the N dimension\n    BLOCK_SIZE_K: tl.constexpr  # Block size for the K dimension\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    first_pid_n = (pid - first_pid_m * num_pid_n) // BLOCK_SIZE_N\n    pid_m = first_pid_m + tl.arange(0, BLOCK_SIZE_M)\n    pid_n = first_pid_n + tl.arange(0, BLOCK_SIZE_N)\n    pid_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Compute the offsets for the input, LoRA weights, and output matrices\n    offsets_input = pid_m[:, None] * stride_input_m + pid_n[None, :] * stride_input_n\n    offsets_lora = pid_k[:, None] * stride_lora_k + pid_n[None, :] * stride_lora_n\n    offsets_output = pid_m[:, None] * stride_output_m + pid_k[None, :] * stride_output_k\n\n    # Load the input and LoRA weights\n    input_block = tl.load(input_ptr + offsets_input, mask=pid_m[:, None] < M, other=0.0)\n    lora_weights_block = tl.load(lora_weights_ptr + offsets_lora, mask=pid_k[:, None] < K, other=0.0)\n\n    # Compute the matrix-vector product\n    output_block = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    output_block += tl.dot(input_block, lora_weights_block)\n\n    # Add existing output values if ADD_INPUTS is set\n    if ADD_INPUTS:\n        existing_output_block = tl.load(output_ptr + offsets_output, mask=pid_m[:, None] < M, other=0.0)\n        output_block += existing_output_block\n\n    # Store the result in the output matrix\n    tl.store(output_ptr + offsets_output, output_block, mask=pid_m[:, None] < M)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _bgmv_expand(input, lora_weights, output, add_inputs=False, dtype=None):\n    # Check input tensor properties\n    assert input.dim() == 2, \"Input must be a 2D tensor\"\n    assert lora_weights.dim() == 2, \"LoRA weights must be a 2D tensor\"\n    assert output.dim() == 2, \"Output must be a 2D tensor\"\n    assert input.size(1) == lora_weights.size(1), \"Input and LoRA weights must have the same number of columns\"\n    assert input.size(0) == output.size(0), \"Input and output must have the same number of rows\"\n    assert lora_weights.size(0) == output.size(1), \"LoRA weights and output must have the same number of columns\"\n\n    # Get the problem size and strides\n    M, N = input.size()\n    K = lora_weights.size(0)\n    stride_input_m, stride_input_n = input.stride()\n    stride_lora_k, stride_lora_n = lora_weights.stride()\n    stride_output_m, stride_output_k = output.stride()\n\n    # Cast the input and output tensors if necessary\n    if dtype is not None:\n        input = input.to(dtype)\n        lora_weights = lora_weights.to(dtype)\n        output = output.to(dtype)\n\n    # Configure the grid\n    BLOCK_SIZE_M = 16\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_K = 16\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\n    # Launch the Triton kernel\n    _bgmv_expand_kernel[grid](\n        input, lora_weights, output, M, N, K, stride_input_m, stride_input_n, stride_lora_k, stride_lora_n, stride_output_m, stride_output_k, add_inputs, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n    )\n\n    return output\n\nimport torch\n\n# Create input, LoRA weights, and output tensors\ninput = torch.randn(128, 64, device='cuda')\nlora_weights = torch.randn(32, 64, device='cuda')\noutput = torch.zeros(128, 32, device='cuda')\n\n# Perform the batched generalized matrix-vector multiplication\noutput = _bgmv_expand(input, lora_weights, output, add_inputs=True, dtype=torch.float16)\n\nprint(output)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    lora_indices,\n    xm_stride,\n    xk_stride,\n    l0_stride,\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ADD_INPUTS: tl.constexpr,\n    CAST_TYPE: tl.constexpr,\n):\n    \"\"\"\n    GroupGEMV, additionally, introducing SPLIT_N can improve large hidden_size's\n    performance\n    \"\"\"\n    pid_sn = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n    offset_k = tl.arange(0, BLOCK_K)\n    offset_n = tl.arange(0, BLOCK_N)\n    if EVEN_K:\n        tiled_a = tl.load(input_ptr + cur_batch * xm_stride +\n                          offset_k * xk_stride, )  # [BLOCK_K]\n    else:\n        tiled_a = tl.load(\n            input_ptr + cur_batch * xm_stride + offset_k * xk_stride,\n            mask=offset_k < K,\n            other=0,\n        )  # [BLOCK_K]\n    # N must be divisible by SPLIT_N\n    split_n_length = tl.cdiv(N, SPLIT_N)\n    if CAST_TYPE:\n        tiled_a = tiled_a.to(lora_ptr.dtype.element_ty)\n    # sliding  to  next row-block\n    b_ptr = (lora_ptr + l0_stride * lora_index +\n             pid_sn * split_n_length * lora_k_stride)\n    c_ptr = out_ptr + cur_batch * cm_stride + pid_sn * split_n_length\n    for n in range(0, split_n_length, BLOCK_N):\n        current_n = n + offset_n\n        current_n_c = tl.max_contiguous(current_n, BLOCK_N)\n        b_ptr_mask = (current_n[:, None] < split_n_length) & (offset_k[None, :]\n                                                              < K)\n        c_mask = current_n < split_n_length\n        tiled_b = tl.load(\n            b_ptr + current_n_c[:, None] * lora_k_stride +\n            offset_k[None, :] * lora_n_stride,\n            mask=b_ptr_mask,\n            other=0.0,\n        )  # [BLOCK_N,BLOCK_K]\n        if ADD_INPUTS:\n            tiled_out = tl.load(c_ptr + current_n * cn_stride, mask=c_mask)\n            accumulator = tl.sum(tiled_a * tiled_b, 1) + tiled_out\n        else:\n            accumulator = tl.sum(tiled_a * tiled_b, 1)\n\n        tl.store(c_ptr + current_n * cn_stride, accumulator, mask=c_mask)\n\n\n@torch.inference_mode()\ndef _bgmv_expand(\n    inputs: torch.Tensor,\n    lora_b_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    add_inputs: bool = True,\n) -> None:\n    \"\"\"\n    Args:\n        inputs (torch.Tensor): input tensor\n        lora_b_weights (torch.Tensor): lora'a weight\n        output_tensor (torch.Tensor): output tensor\n        lora_indices_tensor (torch.Tensor): (batch_size,). The LoRA index\n            corresponding to each batch, An index of -1 means no lora should be\n            applied.\n        batches (int): batch size\n        add_inputs (bool, optional):  Defaults to False, adds the final lora \n            results to the output.\n    \"\"\"\n    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]\n    assert lora_b_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(1) == lora_b_weights.size(-1)\n\n    assert inputs.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    if lora_b_weights.ndim == 4:  # shape:(lora_num,1,size,rank)\n        assert lora_b_weights.size(1) == 1\n        lora_b_weights = lora_b_weights.squeeze(dim=1)\n    else:\n        assert lora_b_weights.ndim == 3  # shape:(lora_num,size,rank)\n    assert lora_b_weights.is_contiguous()\n\n    # TODO tuning this config\n    N, K = lora_b_weights.shape[-2:]  # K= rank,N=hidden_size\n    BLOCK_K = triton.next_power_of_2(K)\n    EVEN_K = K % BLOCK_K == 0\n    ADD_INPUTS = add_inputs\n    CAST_TYPE = False\n    if inputs.dtype == torch.float32 and lora_b_weights.dtype in [\n            torch.float16,\n            torch.bfloat16,\n    ]:\n        CAST_TYPE = True\n    batches = lora_indices_tensor.size(0)\n    # config = get_lora_op_configs(\"expand\", batches, N)\n    grid = lambda META: (\n        META[\"SPLIT_N\"],\n        batches,\n    )\n    _bgmv_expand_kernel[grid](\n        inputs,\n        lora_b_weights,\n        output_tensor,\n        N,\n        K,\n        lora_indices_tensor,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_b_weights.stride(0),\n        lora_b_weights.stride(1),\n        lora_b_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        BLOCK_K=BLOCK_K,\n        BLOCK_N=256,\n        SPLIT_N=lora_b_weights.shape[-2:][0],\n        EVEN_K=EVEN_K,\n        ADD_INPUTS=ADD_INPUTS,\n        CAST_TYPE=CAST_TYPE,\n        # **config,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code implements a forward pass for the attention mechanism. The core function is `_attn_fwd`, a Triton JIT-compiled kernel for efficient computation of attention outputs, with support for sliding windows in attention calculation. The kernel operates on blocks of data, indicated by `_BLOCK_M`, `_BLOCK_N`, and `BLOCK_DMODEL`, optimizing GPU resources usage. `_attn_fwd_inner` handles the blockwise computations for attention scores (QK) and updates the accumulated results. It employs a mask for sliding window calculations. The `_forward` function serves as a Python wrapper that sets up grid dimensions, manages exceptions, and calls the Triton kernel. Inputs include query (Q), key (K), and value (V) matrices, and parameters for handling batch dimensions, context lengths, and optional sliding window settings. Outputs are the attention-weighted matrix, along with normalization parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _attn_fwd(\n    Q, K, V, Out, L, M, \n    stride_qb, stride_qh, stride_qm, \n    stride_kb, stride_kh, stride_km, \n    stride_vb, stride_vh, stride_vm, \n    stride_ob, stride_oh, stride_om, \n    stride_lb, stride_lh, \n    stride_mb, stride_mh, \n    nheads, N_CTX, window_size, \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr\n):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    # initialize pointers to Q, K, V\n    q_ptr = Q + off_b * stride_qb + off_h * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * 1\n    k_ptr = K + off_b * stride_kb + off_h * stride_kh + offs_n[:, None] * stride_km + offs_d[None, :] * 1\n    v_ptr = V + off_b * stride_vb + off_h * stride_vh + offs_n[:, None] * stride_vm + offs_d[None, :] * 1\n    # initialize pointers to output\n    o_ptr = Out + off_b * stride_ob + off_h * stride_oh + offs_m[:, None] * stride_om + offs_d[None, :] * 1\n    # initialize pointers to L and M\n    l_ptr = L + off_b * stride_lb + off_h * stride_lh + offs_m * 1\n    m_ptr = M + off_b * stride_mb + off_h * stride_mh + offs_m * 1\n    # initialize accumulators\n    acc = tl.zeros((BLOCK_M, BLOCK_DMODEL), dtype=tl.float32)\n    # load Q\n    q = tl.load(q_ptr)\n    # loop over k, v\n    for start_n in range(0, N_CTX, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        k = tl.load(k_ptr + start_n * stride_km)\n        qk = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        # -- compute attention scores ----\n        if window_size > 0:\n            mask = tl.arange(0, BLOCK_N) + start_n - offs_m[:, None]\n            mask = mask < window_size\n            qk = tl.where(mask, qk, float('-inf'))\n        m_i = tl.max(qk, 1)\n        p = tl.exp(qk - m_i[:, None])\n        l_i = tl.sum(p, 1)\n        # -- update acc ----\n        v = tl.load(v_ptr + start_n * stride_vm)\n        p = p / l_i[:, None]\n        acc += tl.dot(p, v)\n        # -- update L and M ----\n        tl.atomic_add(l_ptr + start_m * BLOCK_M, l_i)\n        tl.atomic_max(m_ptr + start_m * BLOCK_M, m_i)\n    # -- write back output ----\n    tl.store(o_ptr, acc.to(tl.float16))\n\nimport torch\nfrom torch import nn\nfrom triton import cdiv\n\ndef _forward(Q, K, V, nheads, N_CTX, window_size):\n    # Check inputs\n    assert Q.dtype == torch.float16 and K.dtype == torch.float16 and V.dtype == torch.float16\n    assert Q.shape == K.shape == V.shape\n    assert Q.shape[1] == nheads\n    assert Q.shape[2] == N_CTX\n    assert Q.shape[3] == K.shape[3] == V.shape[3]\n\n    # Allocate output and intermediate tensors\n    Out = torch.empty_like(Q)\n    L = torch.empty((Q.shape[0], nheads, N_CTX), device=Q.device, dtype=torch.float32)\n    M = torch.empty((Q.shape[0], nheads, N_CTX), device=Q.device, dtype=torch.float32)\n\n    # Define grid and block dimensions\n    grid = (cdiv(N_CTX, 128), Q.shape[0] * nheads)\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_DMODEL = Q.shape[3]\n\n    # Launch the kernel\n    _attn_fwd[grid](\n        Q, K, V, Out, L, M,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        L.stride(0), L.stride(1),\n        M.stride(0), M.stride(1),\n        nheads, N_CTX, window_size,\n        BLOCK_M, BLOCK_N, BLOCK_DMODEL\n    )\n\n    return Out, L, M\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n_BLOCK_N = 64\n_BLOCK_M = 64\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q,\n                    K_block_ptr, V_block_ptr,\n                    start_m, qk_scale, N_CTX,\n                    sliding_window_offset, sliding_window_size,\n                    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, SLIDING_WINDOW: tl.constexpr,\n                    IS_EVEN_M: tl.constexpr, IS_EVEN_N: tl.constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr\n                ):\n    # range of values handled by this stage\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = (((start_m + 1) * BLOCK_M + sliding_window_offset - sliding_window_size + BLOCK_N - 1) // BLOCK_N) * BLOCK_N\n        else:\n            lo = ((start_m * BLOCK_M + sliding_window_offset - sliding_window_size + 1) // BLOCK_N) * BLOCK_N\n            hi = ((((start_m + 1) * BLOCK_M - 1) + sliding_window_offset + BLOCK_N) // BLOCK_N) * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n\n    # loop over k, v and update accumulator\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[None, :] \\\n                   + start_m * BLOCK_M - start_n + sliding_window_offset\n\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = (dist >= sliding_window_size)\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n\n            qk = tl.where(mask, qk, float(\"-inf\"))\n\n        if not IS_EVEN_N:\n            qk = tl.where(((tl.arange(0, BLOCK_N) + start_n) < N_CTX)[None, :], qk, float(\"-inf\"))\n\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n\n        if not IS_EVEN_N:\n            p = tl.where(((tl.arange(0, BLOCK_N) + start_n) < N_CTX)[None, :], p, 0)\n\n        l_ij = tl.sum(p, 1)\n        # -- update m_i and l_i\n        tmp = m_i - m_ij\n        alpha_mask = (tmp != tmp)  # check nan\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1., alpha)\n        l_i = l_i * alpha + l_ij\n        # -- update output accumulator --\n        acc = acc * alpha[:, None]\n        # update acc\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n        acc += tl.dot(p.to(v.dtype), v)\n        # update m_i and l_i\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n\n    return acc, l_i, m_i\n\n\n@triton.heuristics(\n    {\n        \"IS_EVEN_M\": lambda args: args[\"N_CTX\"] % args[\"BLOCK_M\"] == 0,\n        \"IS_EVEN_N\": lambda args: args[\"NKV_CTX\"] % args[\"BLOCK_N\"] == 0,\n    }\n)\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L,#\n              stride_qz, stride_qh, stride_qm, stride_qk,  #\n              stride_kz, stride_kh, stride_kn, stride_kk,  #\n              stride_vz, stride_vh, stride_vk, stride_vn,  #\n              stride_oz, stride_oh, stride_om, stride_on,  #\n              Z, H, H_KV, #\n              N_CTX,  #\n              ROUND_CTX,\n              NKV_CTX,\n              sliding_window_offset,\n              sliding_window_size,\n              IS_EVEN_M: tl.constexpr,\n              IS_EVEN_N: tl.constexpr,\n              BLOCK_M: tl.constexpr,  #\n              BLOCK_DMODEL: tl.constexpr,  #\n              BLOCK_N: tl.constexpr,  #\n              END: tl.constexpr,\n              INIT: tl.constexpr,\n              SLIDING_WINDOW: tl.constexpr,\n              COMPLEMENT_SLIDING_WINDOW: tl.constexpr\n            ):\n\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H//H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\n    v_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64) * stride_vh\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n\n    # block pointers\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + v_offset,\n        shape=(NKV_CTX, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, NKV_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + o_offset,\n        shape=(ROUND_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    # initialize pointer to m and l\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        # don't have to check boundary for q len\n        m_i = tl.load(m_ptrs).to(tl.float32)\n        l_i = tl.load(l_ptrs).to(tl.float32)\n        acc = tl.load(O_block_ptr).to(tl.float32)\n\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634   # 1/log(2)\n    # load q: it will stay in SRAM throughout\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, #\n                                    start_m, qk_scale, NKV_CTX, #\n                                    sliding_window_offset, sliding_window_size,\n                                    BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW, IS_EVEN_M, IS_EVEN_N,\n                                    COMPLEMENT_SLIDING_WINDOW)\n    # epilogue\n    if (END):\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))\n\ndef _forward(\n    q, k, v, sm_scale,\n    o=None, m=None, l=None, end=False,\n    sliding_window=None, init=False,\n    complement_sliding_window=False\n):\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n\n    q_round_len = math.ceil(q.shape[2] / 64) * 64\n\n    if sliding_window is not None:\n        sliding_window_offset, sliding_window_size = sliding_window\n    else:\n        sliding_window_offset, sliding_window_size = None, None\n\n    grid = lambda META: (\n        triton.cdiv(q.shape[2], META[\"BLOCK_M\"]),\n        q.shape[0] * q.shape[1],\n    )\n\n    global _BLOCK_N\n    global _BLOCK_M\n\n    try:\n        with torch.cuda.device(q.device):\n            _attn_fwd[grid](\n                q, k, v, sm_scale, m, o, l, #\n                q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n                k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n                v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n                o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n                q.shape[0], q.shape[1], k.shape[1], #\n                q.shape[2],  #\n                q_round_len,\n                k.shape[2],\n                sliding_window_offset,\n                sliding_window_size,\n                BLOCK_DMODEL=Lk,  #\n                END=end,\n                INIT=init,\n                BLOCK_M=_BLOCK_M,\n                BLOCK_N=_BLOCK_N,\n                SLIDING_WINDOW=(sliding_window is not None),\n                COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n                num_warps=4,\n                num_stages=4\n            )\n    except triton.OutOfResources as E:\n        _BLOCK_N = _BLOCK_N // 2\n        _BLOCK_M = _BLOCK_M // 2\n        from warnings import warn\n        warn(f\"Triton Attention Output Resources. {E}\\nUse smaller block size {_BLOCK_N}.\")\n        with torch.cuda.device(q.device):\n            _attn_fwd[grid](\n                q, k, v, sm_scale, m, o, l, #\n                q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n                k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n                v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n                o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n                q.shape[0], q.shape[1], k.shape[1], #\n                q.shape[2],  #\n                q_round_len,\n                k.shape[2],\n                sliding_window_offset,\n                sliding_window_size,\n                BLOCK_DMODEL=Lk,  #\n                END=end,\n                INIT=init,\n                BLOCK_M=_BLOCK_M,\n                BLOCK_N=_BLOCK_N,\n                SLIDING_WINDOW=(sliding_window is not None),\n                COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n                num_warps=4,\n                num_stages=4\n            )\n\n    if end:\n        o = o[:, :, :q.shape[2], :].contiguous().to(q.dtype)\n\n    return o, m, l\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel and its associated wrapper function, `destindex_copy_kv`, are designed to copy values from source tensors (`KV_nope` and `KV_rope`) to destination tensors (`O_nope` and `O_rope`) based on the indices provided in `DestLoc`. The `_fwd_kernel_destindex_copy_kv` kernel function is executed for each element in the sequence, determined by `DestLoc`, and uses Triton's `tl.load` and `tl.store` operations to perform the copying based on strides calculated for batch, head, and dimension. The main logic includes loading values from the source at the current index, fetching the destination index from `DestLoc`, and then storing the values to the appropriate position in the destination tensor. The wrapper function prepares the parameters for the kernel call, ensuring all necessary strides and tensor shapes are passed correctly. The dimensions are aligned to the next power of two for efficient memory access in Triton.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    KV_nope_ptr, KV_rope_ptr, O_nope_ptr, O_rope_ptr, DestLoc_ptr,\n    batch_size, seq_len, head_num, dim, stride_kv_nope_b, stride_kv_nope_h, stride_kv_nope_d,\n    stride_kv_rope_b, stride_kv_rope_h, stride_kv_rope_d,\n    stride_o_nope_b, stride_o_nope_h, stride_o_nope_d,\n    stride_o_rope_b, stride_o_rope_h, stride_o_rope_d,\n    stride_destloc_b, stride_destloc_s,\n    BLOCK_SIZE_SEQ: tl.constexpr, BLOCK_SIZE_DIM: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(seq_len, BLOCK_SIZE_SEQ)\n    num_pid_n = tl.cdiv(dim, BLOCK_SIZE_DIM)\n    num_pid_in_batch = num_pid_m * num_pid_n\n    batch_idx = pid // num_pid_in_batch\n    pid = pid % num_pid_in_batch\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_m = pid_m * BLOCK_SIZE_SEQ + tl.arange(0, BLOCK_SIZE_SEQ)\n    offs_n = pid_n * BLOCK_SIZE_DIM + tl.arange(0, BLOCK_SEDIM_DIM)\n    offs_b = batch_idx * stride_kv_nope_b\n\n    KV_nope_block_ptr = KV_nope_ptr + offs_b\n    KV_rope_block_ptr = KV_rope_ptr + offs_b\n    O_nope_block_ptr = O_nope_ptr + offs_b\n    O_rope_block_ptr = O_rope_ptr + offs_b\n    DestLoc_block_ptr = DestLoc_ptr + batch_idx * stride_destloc_b\n\n    for head_idx in range(head_num):\n        kv_nope_head_ptr = KV_nope_block_ptr + head_idx * stride_kv_nope_h\n        kv_rope_head_ptr = KV_rope_block_ptr + head_idx * stride_kv_rope_h\n        o_nope_head_ptr = O_nope_block_ptr + head_idx * stride_o_nope_h\n        o_rope_head_ptr = O_rope_block_ptr + head_idx * stride_o_rope_h\n\n        dest_loc_head_ptr = DestLoc_block_ptr + head_idx * stride_destloc_s\n\n        dest_loc = tl.load(dest_loc_head_ptr + offs_m, mask=offs_m < seq_len, other=0)\n        dest_loc = dest_loc * stride_o_nope_d\n\n        kv_nope_val = tl.load(kv_nope_head_ptr + offs_m[:, None] * stride_kv_nope_d + offs_n[None, :], mask=offs_m[:, None] < seq_len, other=0)\n        kv_rope_val = tl.load(kv_rope_head_ptr + offs_m[:, None] * stride_kv_rope_d + offs_n[None, :], mask=offs_m[:, None] < seq_len, other=0)\n\n        tl.store(o_nope_head_ptr + dest_loc[:, None] + offs_n[None, :], kv_nope_val, mask=offs_m[:, None] < seq_len)\n        tl.store(o_rope_head_ptr + dest_loc[:, None] + offs_n[None, :], kv_rope_val, mask=offs_m[:, None] < seq_len)\n\nimport torch\n\ndef destindex_copy_kv(KV_nope, KV_rope, O_nope, O_rope, DestLoc):\n    batch_size, seq_len, head_num, dim = KV_nope.shape\n    assert KV_nope.shape == KV_rope.shape == O_nope.shape == O_rope.shape\n    assert DestLoc.shape == (batch_size, seq_len, head_num)\n\n    stride_kv_nope_b = KV_nope.stride(0)\n    stride_kv_nope_h = KV_nope.stride(1)\n    stride_kv_nope_d = KV_nope.stride(2)\n\n    stride_kv_rope_b = KV_rope.stride(0)\n    stride_kv_rope_h = KV_rope.stride(1)\n    stride_kv_rope_d = KV_rope.stride(2)\n\n    stride_o_nope_b = O_nope.stride(0)\n    stride_o_nope_h = O_nope.stride(1)\n    stride_o_nope_d = O_nope.stride(2)\n\n    stride_o_rope_b = O_rope.stride(0)\n    stride_o_rope_h = O_rope.stride(1)\n    stride_o_rope_d = O_rope.stride(2)\n\n    stride_destloc_b = DestLoc.stride(0)\n    stride_destloc_s = DestLoc.stride(1)\n\n    grid = (batch_size * head_num * tl.cdiv(seq_len, 128) * tl.cdiv(dim, 128),)\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        KV_nope, KV_rope, O_nope, O_rope, DestLoc,\n        batch_size, seq_len, head_num, dim,\n        stride_kv_nope_b, stride_kv_nope_h, stride_kv_nope_d,\n        stride_kv_rope_b, stride_kv_rope_h, stride_kv_rope_d,\n        stride_o_nope_b, stride_o_nope_h, stride_o_nope_d,\n        stride_o_rope_b, stride_o_rope_h, stride_o_rope_d,\n        stride_destloc_b, stride_destloc_s,\n        BLOCK_SIZE_SEQ=128, BLOCK_SIZE_DIM=128\n    )\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    KV_nope,\n    KV_rope,\n    Dest_loc,\n    O_nope,\n    O_rope,\n    stride_kv_nope_bs,\n    stride_kv_nope_h,\n    stride_kv_nope_d,\n    stride_kv_rope_bs,\n    stride_kv_rope_h,\n    stride_kv_rope_d,\n    stride_o_nope_bs,\n    stride_o_nope_h,\n    stride_o_nope_d,\n    stride_o_rope_bs,\n    stride_o_rope_h,\n    stride_o_rope_d,\n    kv_nope_head_num,\n    kv_rope_head_num,\n    BLOCK_DMODEL_NOPE: tl.constexpr,\n    BLOCK_DMODEL_ROPE: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    offs_d_nope = tl.arange(0, BLOCK_DMODEL_NOPE)\n    offs_d_rope = tl.arange(0, BLOCK_DMODEL_ROPE)\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    kv_nope_ptrs = KV_nope + cur_index * stride_kv_nope_bs + stride_kv_nope_d * offs_d_nope[None, :]\n    kv_rope_ptrs = KV_rope + cur_index * stride_kv_rope_bs + stride_kv_rope_d * offs_d_rope[None, :]\n\n    o_nope_ptrs = O_nope + dest_index * stride_o_nope_bs + stride_o_nope_d * offs_d_nope[None, :]\n    o_rope_ptrs = O_rope + dest_index * stride_o_rope_bs + stride_o_rope_d * offs_d_rope[None, :]\n\n    kv_nope = tl.load(kv_nope_ptrs)\n    kv_rope = tl.load(kv_rope_ptrs)\n\n    tl.store(o_nope_ptrs, kv_nope)\n    tl.store(o_rope_ptrs, kv_rope)\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_kv(KV_nope, KV_rope, DestLoc, O_nope, O_rope):\n    seq_len = DestLoc.shape[0]\n    kv_nope_head_num = KV_nope.shape[1]\n    kv_rope_head_num = KV_rope.shape[1]\n\n    kv_nope_head_dim = KV_nope.shape[2]\n    kv_rope_head_dim = KV_rope.shape[2]\n\n    aligned_d_nope = triton.next_power_of_2(kv_nope_head_dim) # \u8c03\u6574\u4e3a2\u7684\u5e42\u6b21\u65b9\n    aligned_d_rope = triton.next_power_of_2(kv_rope_head_dim) # \u8c03\u6574\u4e3a2\u7684\u5e42\u6b21\u65b9\n\n    assert KV_nope.shape[1] == O_nope.shape[1]\n    assert KV_nope.shape[2] == O_nope.shape[2]\n    assert KV_rope.shape[1] == O_rope.shape[1]\n    assert KV_rope.shape[2] == O_rope.shape[2]\n    grid = (seq_len,)\n    num_warps = 2\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        KV_nope,\n        KV_rope,\n        DestLoc,\n        O_nope,\n        O_rope,\n        KV_nope.stride(0),\n        KV_nope.stride(1),\n        KV_nope.stride(2),\n        KV_rope.stride(0),\n        KV_rope.stride(1),\n        KV_rope.stride(2),\n        O_nope.stride(0),\n        O_nope.stride(1),\n        O_nope.stride(2),\n        O_rope.stride(0),\n        O_rope.stride(1),\n        O_rope.stride(2),\n        kv_nope_head_num,\n        kv_rope_head_num,\n        # BLOCK_DMODEL_NOPE=kv_nope_head_dim,\n        # BLOCK_DMODEL_ROPE=kv_rope_head_dim,\n        BLOCK_DMODEL_NOPE=aligned_d_nope,  # \u4f20\u9012\u5bf9\u9f50\u540e\u7684\u503c\n        BLOCK_DMODEL_ROPE=aligned_d_rope,  # \u4f20\u9012\u5bf9\u9f50\u540e\u7684\u503c\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The function `fused_native_layer_norm` normalizes input tensors using the Triton kernel `triton_red_fused_native_layer_norm_0`. It processes three input tensors: `primals_1`, `primals_2`, and `primals_3`, where `primals_3` is the main input tensor with shape `(S, D)`. The function computes a layer normalization which includes computing the mean and variance of the input across a specified dimension, and then normalizes the input tensor accordingly. It uses two auxiliary buffers `buf0` and `buf3` for intermediate results and `buf4` for the final normalized output tensor. The normalized result (`buf4`), the unchanged input tensor (`primals_3`), the mean (`buf0`), and an auxiliary output (`buf3`) are returned.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef triton_red_fused_native_layer_norm_0(\n    primals_3_ptr,  # pointer to the input tensor (S, D)\n    buf0_ptr,       # pointer to the mean buffer\n    buf3_ptr,       # pointer to the auxiliary buffer\n    buf4_ptr,       # pointer to the output tensor\n    S, D,           # dimensions of the input tensor\n    eps: tl.float32, # small epsilon value to avoid division by zero\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < S\n\n    # Load the input tensor\n    x = tl.load(primals_3_ptr + offsets * D, mask=mask, other=0.0)\n\n    # Compute mean\n    mean = tl.sum(x, axis=1) / D\n    tl.store(buf0_ptr + offsets, mean, mask=mask)\n\n    # Compute variance\n    x_minus_mean = x - mean[:, None]\n    var = tl.sum(x_minus_mean * x_minus_mean, axis=1) / D\n    inv_std = 1.0 / tl.sqrt(var + eps)\n\n    # Normalize the input tensor\n    normalized = x_minus_mean * inv_std[:, None]\n    tl.store(buf4_ptr + offsets * D, normalized, mask=mask)\n\n    # Store auxiliary buffer (optional, depending on the use case)\n    tl.store(buf3_ptr + offsets * D, inv_std[:, None], mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef fused_native_layer_norm(primals_1, primals_2, primals_3, eps=1e-5):\n    S, D = primals_3.shape\n\n    # Allocate buffers\n    buf0 = torch.empty((S,), dtype=torch.float32, device=primals_3.device)\n    buf3 = torch.empty((S, D), dtype=torch.float32, device=primals_3.device)\n    buf4 = torch.empty((S, D), dtype=torch.float32, device=primals_3.device)\n\n    # Define the grid and block size\n    BLOCK_SIZE = 128\n    grid = (S + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    triton_red_fused_native_layer_norm_0[grid, BLOCK_SIZE](\n        primals_3, buf0, buf3, buf4, S, D, eps\n    )\n\n    return buf4, primals_3, buf0, buf3\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\n\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch.ops.inductor._reinterpret_tensor\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 1024,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 2048,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n    ],\n    key=[\"xnumel\", \"rnumel\"],\n)\n@triton.jit\ndef triton_red_fused_native_layer_norm_0(\n    in_out_ptr0,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    out_ptr1,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(\n            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0\n        )\n        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)\n        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)\n        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)\n    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(\n        tmp3_mean, tmp3_m2, tmp3_weight, 1\n    )\n    tmp3 = tmp3_tmp[:, None]\n    tmp4 = tmp4_tmp[:, None]\n    tmp5 = tmp5_tmp[:, None]\n    tl.store(out_ptr0 + (x0), tmp3, None)\n    tmp6 = rnumel\n    tmp7 = tmp4 / tmp6\n    tmp8 = 1e-05\n    tmp9 = tmp7 + tmp8\n    tmp10 = libdevice.rsqrt(tmp9)\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp10, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp11 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp12 = tmp11.to(tl.float32)\n        tmp13 = tmp12 - tmp3\n        tmp14 = tmp13 * tmp10\n        tmp16 = tmp15.to(tl.float32)\n        tmp17 = tmp14 * tmp16\n        tmp19 = tmp18.to(tl.float32)\n        tmp20 = tmp17 + tmp19\n        tmp21 = tmp20.to(tl.float32)\n        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)\n\ndef fused_native_layer_norm(primals_1, primals_2, primals_3):\n    S, D = primals_3.shape\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((S, 1), (1, 1), torch.float32)\n        buf1 = empty_strided_cuda((S, 1), (1, S), torch.float32)\n        buf3 = reinterpret_tensor(buf1, (S, 1), (1, 1), 0)\n        del buf1  # reuse\n        buf4 = empty_strided_cuda((S, D), (D, 1), torch.bfloat16)\n        stream0 = get_raw_stream(0)\n        grid = lambda META: (triton.cdiv(S, META[\"XBLOCK\"]),)\n        triton_red_fused_native_layer_norm_0[grid](\n            buf3, primals_3, primals_1, primals_2, buf0, buf4, S, D\n        )\n    return (\n        buf4,\n        primals_3,\n        buf0,\n        buf3,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The Triton kernel named `spinning_lock_kernel` is designed to perform a parallel reduction across multiple blocks. \n    It uses atomic operations to implement a locking mechanism for synchronization between threads. The kernel accumulates \n    values from a temporary storage `P` into an accumulator `acc` and stores the result in `C`. The main loop iterates \n    for 9 cycles and performs locking on specific threads using a 'compare and swap' operation on `locks` to ensure that \n    only one thread performs the addition at a time. The function `spinning_lock` sets up the grid and launches this \n    kernel using specified block sizes.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef spinning_lock_kernel(P, C, locks, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Get the thread ID and block ID\n    pid = tl.program_id(axis=0)\n    tid = tl.program_id(axis=1)\n    \n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    \n    # Each thread processes a segment of the input\n    for i in range(tid * BLOCK_SIZE, (tid + 1) * BLOCK_SIZE):\n        if i < n_elements:\n            acc[tid] += P[i]\n    \n    # Perform the reduction within the block\n    for offset in range(BLOCK_SIZE // 2, 0, -1):\n        if tid < offset:\n            acc[tid] += acc[tid + offset]\n    \n    # Only the first thread in the block writes the result to the shared memory\n    if tid == 0:\n        tl.atomic_cas(locks + pid, 0, 1)  # Lock the block\n        for _ in range(9):  # Perform 9 cycles\n            while tl.atomic_cas(locks + pid, 1, 1) == 1:  # Spin lock\n                pass\n            C[pid] += acc[0]  # Perform the addition\n            tl.atomic_xchg(locks + pid, 0)  # Unlock the block\n\nimport triton\nimport triton.runtime\n\ndef spinning_lock(P, C, locks, block_size, grid_size):\n    # Convert inputs to Triton tensors\n    P_t = triton.runtime.Tensor(P, dtype=triton.float32)\n    C_t = triton.runtime.Tensor(C, dtype=triton.float32)\n    locks_t = triton.runtime.Tensor(locks, dtype=triton.int32)\n    \n    # Launch the kernel\n    spinning_lock_kernel[grid_size, block_size](P_t, C_t, locks_t, P_t.shape[0], BLOCK_SIZE=block_size)\n    \n    # Synchronize to ensure the kernel has finished\n    triton.runtime.synchronize()\n    \n    # Convert the output back to a NumPy array\n    return C_t.numpy()\n\n# Example usage\nimport numpy as np\n\n# Input data\nP = np.random.rand(1024).astype(np.float32)\nC = np.zeros(32).astype(np.float32)\nlocks = np.zeros(32).astype(np.int32)\n\n# Parameters\nblock_size = 32\ngrid_size = (32, 1)  # 32 blocks, 1 thread per block\n\n# Run the kernel\nC_result = spinning_lock(P, C, locks, block_size, grid_size)\n\nprint(C_result)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef spinning_lock_kernel(P, C, locks, num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    pid_m = pid // num_sms\n    pid_n = pid % num_sms\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)  # Assuming acc initialization\n\n    # Perform reduction for every kth pid\n    for iters in range(1, 10):\n        if (pid % k == 0):\n            next_pid = pid + 1\n\n            while next_pid < pid + k and next_pid < num_sms:\n                while tl.atomic_cas(locks + next_pid, 1, 1) != 1:\n                    pass\n\n                rm1 = tl.arange(0, BLOCK_SIZE_M)\n                rn1 = tl.arange(0, BLOCK_SIZE_N)\n                P_ = P + next_pid * BLOCK_SIZE_M * BLOCK_SIZE_N + rm1[:, None] * BLOCK_SIZE_N + rn1[None, :]\n                acc1 = tl.load(P_)\n                acc += acc1\n\n                next_pid += 1\n              \n        # Store results using temporary storage P for every k-1 pids\n        else:\n            rm1 = tl.arange(0, BLOCK_SIZE_M)\n            rn1 = tl.arange(0, BLOCK_SIZE_N)\n            P_ = P + pid * BLOCK_SIZE_M * BLOCK_SIZE_N + rm1[:, None] * BLOCK_SIZE_N + rn1[None, :]\n            tl.store(P_, acc)\n            tl.atomic_xchg(locks + pid, 1)\n\n        # Store final results in C\n        rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n        mask = (rm < M)[:, None] & (rn < N)[None, :]\n        tl.store(C_, acc, mask=mask)\n\n\ndef spinning_lock(P, C, locks, num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M, BLOCK_SIZE_N):\n    grid = (num_sms,)\n    spinning_lock_kernel[grid](\n        P, C, locks, num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N,)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel `rmsnorm_triton` performs RMS normalization on a 3D tensor `x`. It takes in input tensors `x` and `rms_weights`, and outputs the normalized tensor `out`. The kernel normalizes each element in the `K` dimension by dividing it by the root mean square (RMS) calculated over that dimension. The normalized values are then scaled by the corresponding elements in `rms_weights`. The `rmsnorm_wrapper` is a Python function wrapping this kernel call, setting up necessary parameters and handling the kernel launch.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_triton(\n    x_ptr,  # Pointer to the input tensor\n    rms_weights_ptr,  # Pointer to the RMS weights\n    out_ptr,  # Pointer to the output tensor\n    stride_x_B,  # Stride of x in the B dimension\n    stride_x_H,  # Stride of x in the H dimension\n    stride_x_K,  # Stride of x in the K dimension\n    stride_rms_weights_K,  # Stride of rms_weights in the K dimension\n    stride_out_B,  # Stride of out in the B dimension\n    stride_out_H,  # Stride of out in the H dimension\n    stride_out_K,  # Stride of out in the K dimension\n    B,  # Size of the B dimension\n    H,  # Size of the H dimension\n    K,  # Size of the K dimension\n    BLOCK_SIZE: tl.constexpr,  # Block size for the K dimension\n):\n    # Compute the block index in the B and H dimensions\n    pid_b = tl.program_id(axis=0)\n    pid_h = tl.program_id(axis=1)\n\n    # Compute the starting index in the B and H dimensions\n    start_b = pid_b * BLOCK_SIZE\n    start_h = pid_h * BLOCK_SIZE\n\n    # Compute the range of indices in the B and H dimensions\n    range_b = tl.arange(0, BLOCK_SIZE) + start_b\n    range_h = tl.arange(0, BLOCK_SIZE) + start_h\n\n    # Mask to filter out-of-bounds indices\n    mask_b = range_b < B\n    mask_h = range_h < H\n\n    # Compute the RMS for each element in the K dimension\n    for k in range(K):\n        # Load the elements from x\n        x = tl.load(x_ptr + range_b[:, None] * stride_x_B + range_h[None, :] * stride_x_H + k * stride_x_K, mask=mask_b[:, None] & mask_h[None, :], other=0.0)\n\n        # Compute the square of the elements\n        x_sq = x * x\n\n        # Compute the sum of squares for the current block\n        sum_sq = tl.sum(x_sq, axis=1)\n\n        # Compute the RMS for the current block\n        rms = tl.sqrt(sum_sq / K)\n\n        # Load the corresponding RMS weights\n        rms_weights = tl.load(rms_weights_ptr + k * stride_rms_weights_K)\n\n        # Normalize and scale the elements\n        out = x / rms[:, None] * rms_weights\n\n        # Store the normalized and scaled elements in the output tensor\n        tl.store(out_ptr + range_b[:, None] * stride_out_B + range_h[None, :] * stride_out_H + k * stride_out_K, out, mask=mask_b[:, None] & mask_h[None, :])\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef rmsnorm_wrapper(x, rms_weights, out):\n    # Ensure the tensors are on the same device\n    assert x.device == rms_weights.device == out.device, \"Tensors must be on the same device\"\n    assert x.dtype == rms_weights.dtype == out.dtype, \"Tensors must have the same data type\"\n    assert x.shape == out.shape, \"Input and output tensors must have the same shape\"\n\n    # Get the dimensions of the input tensor\n    B, H, K = x.shape\n\n    # Define the grid and block sizes\n    BLOCK_SIZE = 128\n    grid = (triton.cdiv(B, BLOCK_SIZE), triton.cdiv(H, BLOCK_SIZE))\n\n    # Launch the kernel\n    rmsnorm_triton[grid](\n        x,  # Pointer to the input tensor\n        rms_weights,  # Pointer to the RMS weights\n        out,  # Pointer to the output tensor\n        x.stride(0),  # Stride of x in the B dimension\n        x.stride(1),  # Stride of x in the H dimension\n        x.stride(2),  # Stride of x in the K dimension\n        rms_weights.stride(0),  # Stride of rms_weights in the K dimension\n        out.stride(0),  # Stride of out in the B dimension\n        out.stride(1),  # Stride of out in the H dimension\n        out.stride(2),  # Stride of out in the K dimension\n        B,  # Size of the B dimension\n        H,  # Size of the H dimension\n        K,  # Size of the K dimension\n        BLOCK_SIZE,  # Block size for the K dimension\n    )\n\n# Example usage\nB, H, K = 16, 32, 64\nx = torch.randn(B, H, K, device='cuda')\nrms_weights = torch.randn(K, device='cuda')\nout = torch.empty_like(x)\n\nrmsnorm_wrapper(x, rms_weights, out)\n\nprint(out)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, out_ptr,\n                   stride_x_batch, stride_x_m, stride_x_k,\n                   stride_rms_w,\n                   stride_out_batch, stride_out_m, stride_out_k,\n                   N_SIZE: tl.constexpr, eps: tl.constexpr, BLOCK_N_SIZE: tl.constexpr):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n\n    # parallel at m dimension\n    offset_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_n_size = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    # parallel between blocks\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=x_ptr_mask, other=0.)  # careful stride_x_k\n        xf = x.to(tl.float32)\n        var += xf*xf\n    var = tl.sum(var, axis=0) / N_SIZE  # reduce between wrap\n    std = tl.sqrt(var + eps)\n\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n\n        rms_w_offset = tl.load(rms_w_ptr + offset_n * stride_rms_w, mask=x_ptr_mask)\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=x_ptr_mask, other=0.)\n\n        x_new = x / std\n        out = x_new * rms_w_offset\n        out_offset = pid_batch * stride_out_batch + pid_m * stride_out_m + offset_n * stride_out_k\n        tl.store(out_ptr + out_offset, out, mask=x_ptr_mask)\n\n\ndef rmsnorm_wrapper(x, rms_weights, eps=1e-6):\n    batch, M, K = x.shape\n    out = torch.empty_like(x)\n    rmsnorm_triton[(batch, M,)](x, rms_weights, out,\n                                *x.stride(),\n                                *rms_weights.stride(),  # 1\n                                *out.stride(),\n                                N_SIZE=K, eps=eps, BLOCK_N_SIZE=4096,\n                                num_warps=16\n                                )\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel, named `chunk_simple_gla_bwd_kernel_dqkg`, is designed to compute gradients for Q, K, V, and a gating term in a transformer-like architecture. It processes input tensors `q`, `k`, `v`, `g`, `h`, and `do` along with other parameters. The kernel utilizes Triton's automatic tuning feature to choose between two configurations with different warp numbers. Outputs are stored in `dq`, `dk`, and `dg`. The kernel iteratively computes intermediate results like `b_dq`, `b_dk`, and `b_dg` using matrix multiplications and reductions. The host function `chunk_bwd_dqkg_fn` prepares the data, defines grid dimensions, and launches the Triton kernel with appropriate parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(\n    q_ptr, k_ptr, v_ptr, g_ptr, h_ptr, do_ptr,\n    dq_ptr, dk_ptr, dg_ptr,\n    M, N, H, W,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_H: tl.constexpr, BLOCK_W: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_h = tl.cdiv(H, BLOCK_H)\n    num_pid_w = tl.cdiv(W, BLOCK_W)\n    num_pid_in_w = num_pid_m * num_pid_n * num_pid_h\n    pid_in_w = pid % num_pid_in_w\n    pid_w = pid // num_pid_in_w\n    pid_m = pid_in_w // (num_pid_n * num_pid_h)\n    pid_n = (pid_in_w % (num_pid_n * num_pid_h)) // num_pid_h\n    pid_h = pid_in_w % num_pid_h\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_h = pid_h * BLOCK_H + tl.arange(0, BLOCK_H)\n    offs_w = pid_w * BLOCK_W + tl.arange(0, BLOCK_W)\n\n    q = tl.load(q_ptr + offs_m[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :])\n    k = tl.load(k_ptr + offs_n[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :])\n    v = tl.load(v_ptr + offs_n[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :])\n    g = tl.load(g_ptr + offs_m[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :])\n    h = tl.load(h_ptr + offs_n[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :])\n    do = tl.load(do_ptr + offs_m[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :])\n\n    # Compute intermediate results\n    b_dq = tl.zeros((BLOCK_M, BLOCK_H, BLOCK_W), dtype=tl.float32)\n    b_dk = tl.zeros((BLOCK_N, BLOCK_H, BLOCK_W), dtype=tl.float32)\n    b_dg = tl.zeros((BLOCK_M, BLOCK_H, BLOCK_W), dtype=tl.float32)\n\n    for i in range(BLOCK_N):\n        b_dq += tl.sum(do * v[i, :, :, :], axis=0)\n        b_dk += tl.sum(q * do[i, :, :, :], axis=0)\n        b_dg += tl.sum(do * h[i, :, :, :], axis=0)\n\n    # Store results\n    tl.store(dq_ptr + offs_m[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :], b_dq)\n    tl.store(dk_ptr + offs_n[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :], b_dk)\n    tl.store(dg_ptr + offs_m[:, None, None, None] * H * W + offs_h[None, :, None, None] * W + offs_w[None, None, :], b_dg)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef chunk_bwd_dqkg_fn(q, k, v, g, h, do, dq, dk, dg, M, N, H, W, BLOCK_M, BLOCK_N, BLOCK_H, BLOCK_W):\n    # Define grid dimensions\n    grid = lambda meta: (\n        triton.cdiv(M, meta['BLOCK_M']) * triton.cdiv(N, meta['BLOCK_N']) * triton.cdiv(H, meta['BLOCK_H']) * triton.cdiv(W, meta['BLOCK_W']),\n    )\n\n    # Launch the Triton kernel\n    chunk_simple_gla_bwd_kernel_dqkg[grid](\n        q, k, v, g, h, do,\n        dq, dk, dg,\n        M, N, H, W,\n        BLOCK_M, BLOCK_N, BLOCK_H, BLOCK_W\n    )\n\n# Example usage\nM, N, H, W = 128, 128, 64, 64\nBLOCK_M, BLOCK_N, BLOCK_H, BLOCK_W = 16, 16, 16, 16\n\nq = torch.randn((M, H, W), device='cuda', dtype=torch.float32)\nk = torch.randn((N, H, W), device='cuda', dtype=torch.float32)\nv = torch.randn((N, H, W), device='cuda', dtype=torch.float32)\ng = torch.randn((M, H, W), device='cuda', dtype=torch.float32)\nh = torch.randn((N, H, W), device='cuda', dtype=torch.float32)\ndo = torch.randn((M, H, W), device='cuda', dtype=torch.float32)\n\ndq = torch.zeros((M, H, W), device='cuda', dtype=torch.float32)\ndk = torch.zeros((N, H, W), device='cuda', dtype=torch.float32)\ndg = torch.zeros((M, H, W), device='cuda', dtype=torch.float32)\n\nchunk_bwd_dqkg_fn(q, k, v, g, h, do, dq, dk, dg, M, N, H, W, BLOCK_M, BLOCK_N, BLOCK_H, BLOCK_W)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(\n    q,\n    k,\n    v,\n    h,\n    g,\n    do,\n    dh,\n    dq,\n    dk,\n    dg,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr\n):\n    # Kernel implementation\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    last_idx = min(i_t * BT + BT, T) - 1\n    b_g_last = tl.load(g + i_bh * T + last_idx)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg_last = tl.zeros([1,], dtype=tl.float32)\n    b_dg = tl.zeros([BT,], dtype=tl.float32)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        # [BV, BK]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dg_last += (tl.sum(b_h * b_dh))\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg_last *= tl.exp(b_g_last)\n    b_dq = b_dq * tl.exp(b_g)[:, None] * scale\n    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale * tl.exp(b_g[:, None] - b_g[None, :]), 0)\n    b_ds = b_ds.to(b_k.dtype)\n    # [BT, BK]\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)\n    # (SY 09/21) revcumsum in a separate kernel due to strange triton compiler issue\n    # b_dg = tl.dot(tl.where(o_i[:, None] <= o_i[None, :], 1., 0.), b_dg, allow_tf32=False) + b_dg_last)\n    b_dg = tl.where(o_i < min(BT, T-i_t*BT) - 1, b_dg, b_dg + b_dg_last)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_k*n_bh + i_bh) * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n\ndef chunk_bwd_dqkg_fn(do, q, k, v, g, h, dh, scale):\n    # Kernel call\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    BT = 64\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NT, NK = triton.cdiv(T, BT), triton.cdiv(K, BK)\n    grid = (NK, NT, B * H)\n    dq = torch.empty_like(q)\n    dk = torch.empty_like(k)\n    dg = torch.empty(NK, B, H, T, dtype=torch.float32, device=g.device).fill_(-1e9)\n    chunk_simple_gla_bwd_kernel_dqkg[grid](\n        q, k, v, h, g, do, dh, dq, dk, dg,\n        q.stride(1), q.stride(2),\n        v.stride(1), v.stride(2),\n        dh.stride(1), dh.stride(2),\n        scale,\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT\n    )\n    return dq, dk, dg\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The code implements two Triton kernels named `fifth_order_fwd` and `fifth_order_bwd` for forward and backward computations of fifth-order spherical harmonics projections. These kernels operate on 3D input coordinate tensors to compute spherical harmonics and their gradients. The main entry point is the `FifthOrderSphericalHarmonic` class, which provides static methods for forward and backward passes. The `forward` method computes the harmonics projections, and the `backward` method computes gradients w.r.t. input coordinates. The kernels use Triton's parallelization to distribute computation across blocks, which are determined by a block size. The class saves input coordinates during the forward pass to use in the backward computation.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fifth_order_fwd(\n    x_ptr,  # Pointer to input coordinates (x, y, z)\n    y_ptr,  # Pointer to output spherical harmonics\n    n_elements,  # Number of elements in the input tensor\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelization\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets * 3 + 0, mask=mask)\n    y = tl.load(x_ptr + offsets * 3 + 1, mask=mask)\n    z = tl.load(x_ptr + offsets * 3 + 2, mask=mask)\n\n    r = tl.sqrt(x * x + y * y + z * z)\n    theta = tl.acos(z / r)\n    phi = tl.atan2(y, x)\n\n    # Compute fifth-order spherical harmonics\n    Y50 = 0.125 * tl.sqrt(11 / (2 * tl.pi)) * (35 * tl.cos(theta)**5 - 30 * tl.cos(theta)**3 + 3 * tl.cos(theta))\n    Y51 = 0.125 * tl.sqrt(21 / tl.pi) * tl.sin(theta) * (7 * tl.cos(theta)**4 - 8 * tl.cos(theta)**2 + 1) * tl.cos(phi)\n    Y52 = 0.125 * tl.sqrt(21 / (2 * tl.pi)) * tl.sin(theta)**2 * (7 * tl.cos(theta)**3 - 3 * tl.cos(theta)) * tl.cos(2 * phi)\n    Y53 = 0.125 * tl.sqrt(7 / (2 * tl.pi)) * tl.sin(theta)**3 * (7 * tl.cos(theta)**2 - 1) * tl.cos(3 * phi)\n    Y54 = 0.125 * tl.sqrt(7 / (2 * tl.pi)) * tl.sin(theta)**4 * (5 * tl.cos(theta)) * tl.cos(4 * phi)\n    Y55 = 0.125 * tl.sqrt(7 / (2 * tl.pi)) * tl.sin(theta)**5 * tl.cos(5 * phi)\n\n    # Store results\n    tl.store(y_ptr + offsets * 6 + 0, Y50, mask=mask)\n    tl.store(y_ptr + offsets * 6 + 1, Y51, mask=mask)\n    tl.store(y_ptr + offsets * 6 + 2, Y52, mask=mask)\n    tl.store(y_ptr + offsets * 6 + 3, Y53, mask=mask)\n    tl.store(y_ptr + offsets * 6 + 4, Y54, mask=mask)\n    tl.store(y_ptr + offsets * 6 + 5, Y55, mask=mask)\n", "label": "import triton\nimport torch\nfrom triton import language as tl\nimport math\n\ndef calculate_lastdim_num_blocks(input_tensor: torch.Tensor, block_size: int) -> int:\n    \"\"\"\n    Calculate the number of blocks for a tensor, assuming we\n    stride along the last dimension, and a given block size.\n\n    The corresponding pointer arithmetic looks like this:\n\n    ```python\n    block_id = tl.program_id(0)\n    striding = tl.arange(0, block_size) * stride\n    offset = (striding + (block_size * stride * block_id))\n    ```\n\n    This function is used to work out the amount of parallel\n    work that needs to be done, given as the total number of\n    elements divided by the last dimension stride, and a specified\n    block size that will then divvy up the work.\n\n    Parameters\n    ----------\n    input_tensor : torch.Tensor\n        Torch N-d tensor to operate over.\n\n    Returns\n    -------\n    int\n        Number of blocks of work, given a block size.\n    \"\"\"\n    # get the stride of the last dimension\n    stride = input_tensor.stride(-2)\n    numel = input_tensor.numel()\n    total_blocks = math.ceil(numel / stride)\n    return total_blocks\n\n\n# This kernel computes the fifth order spherical harmonics projections.\n@triton.jit\ndef fifth_order_fwd(\n    coord_ptr: tl.tensor,  # Pointer to input coordinates tensor.\n    output_ptr: tl.tensor,  # Pointer to output tensor.\n    block_size: tl.constexpr,  # Number of elements in each block.\n    coord_numel: tl.constexpr,  # Total number of elements in the coordinates tensor.\n    output_numel: tl.constexpr,  # Total number of elements in the output tensor.\n    col_offset: tl.constexpr,  # Offset for the output tensor.\n    output_stride: tl.constexpr,  # Stride of the output tensor.\n):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + (block_size * coord_stride * block_id)\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset < coord_numel)\n    y = tl.load(\n        coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 < coord_numel\n    )\n    z = tl.load(\n        coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 < coord_numel\n    )\n    # -------------------- variable and constant definitions\n    CONST000 = 1.73430461568895\n    CONST001 = 2.32681380862329\n    CONST002 = 1.60565407233314\n    CONST003 = 3.21130814466628\n    CONST004 = 3.31662479035540\n    CONST005 = 6.21867148191637\n    CONST006 = 6.21867148191637\n    CONST007 = 1.60565407233314\n    CONST009 = 11.6340690431164\n    CONST010 = 12.8452325786651\n    CONST011 = 12.4373429638327\n    CONST012 = 12.8452325786651\n    CONST013 = 13.8744369255116\n    CONST017 = 33.9852909359329\n    CONST018 = 7.35803132638072\n    CONST020 = -44.1481879582843\n    CONST021 = -41.6233107765348\n    CONST022 = -29.4321253055229\n    CONST023 = -23.2681380862329\n    CONST024 = -19.2678488679977\n    CONST025 = -19.2678488679977\n    CONST026 = -16.9926454679664\n    CONST027 = -16.9926454679664\n    CONST028 = -13.8744369255116\n    CONST029 = -16.5831239517770\n    CONST030 = 3.46860923137790\n    CONST031 = -8.49632273398321\n    CONST032 = -5.20291384706685\n    CONST033 = -3.46860923137790\n    CONST034 = -1.73430461568895\n    VAR05 = x * x * x * x * x\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR14 = y * y * y * y * y\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR23 = z * z * z * z * z\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    # -------------------- kernel implementations\n    Y00 = CONST001 * VAR05 + CONST009 * VAR24 * x + CONST023 * VAR07 * VAR26\n    Y01 = y * (CONST022 * VAR07 * z - CONST022 * VAR25 * x)\n    Y02 = (\n        CONST000 * VAR05\n        + VAR07 * (CONST028 * VAR17 + CONST033 * VAR26)\n        + x * (-CONST021 * VAR17 * VAR26 + CONST032 * VAR24)\n    )\n    Y03 = CONST027 * VAR07 * y * z + x * (CONST017 * VAR16 * z + CONST026 * VAR25 * y)\n    Y04 = (\n        CONST002 * VAR05\n        + VAR07 * (CONST003 * VAR26 + CONST025 * VAR17)\n        + x * (CONST002 * VAR24 + CONST010 * VAR15 + CONST024 * VAR17 * VAR26)\n    )\n    Y05 = (\n        CONST004 * VAR14\n        + VAR16 * (CONST029 * VAR08 + CONST029 * VAR26)\n        + y * (CONST005 * VAR06 + CONST006 * VAR24 + CONST011 * VAR08 * VAR26)\n    )\n    Y06 = (\n        CONST002 * VAR23\n        + VAR25 * (CONST003 * VAR08 + CONST024 * VAR17)\n        + z * (CONST007 * VAR06 + CONST012 * VAR15 + CONST024 * VAR08 * VAR17)\n    )\n    Y07 = VAR16 * (CONST026 * VAR08 - CONST026 * VAR26) + y * (\n        -CONST031 * VAR06 + CONST031 * VAR24\n    )\n    Y08 = (\n        CONST034 * VAR23\n        + VAR25 * (CONST013 * VAR17 + CONST030 * VAR08)\n        + z * (CONST021 * VAR08 * VAR17 - CONST032 * VAR06)\n    )\n    Y09 = y * (CONST018 * VAR06 + CONST018 * VAR24 + CONST020 * VAR08 * VAR26)\n    Y10 = CONST001 * VAR23 + CONST009 * VAR06 * z + CONST023 * VAR08 * VAR25\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (\n        output_striding + (block_size * output_stride * block_id) + col_offset\n    )\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset < output_numel)\n    tl.store(\n        output_ptr + output_row_offset + 1,\n        Y01,\n        mask=output_row_offset + 1 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 2,\n        Y02,\n        mask=output_row_offset + 2 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 3,\n        Y03,\n        mask=output_row_offset + 3 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 4,\n        Y04,\n        mask=output_row_offset + 4 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 5,\n        Y05,\n        mask=output_row_offset + 5 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 6,\n        Y06,\n        mask=output_row_offset + 6 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 7,\n        Y07,\n        mask=output_row_offset + 7 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 8,\n        Y08,\n        mask=output_row_offset + 8 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 9,\n        Y09,\n        mask=output_row_offset + 9 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 10,\n        Y10,\n        mask=output_row_offset + 10 < output_numel,\n    )\n\n\n# This kernel computes the gradients of the fifth order spherical harmonics projections.\n@triton.jit\ndef fifth_order_bwd(\n    coord_ptr: tl.tensor,  # Pointer to input coordinates tensor.\n    coord_grad_ptr: tl.tensor,  # Pointer to the gradient output tensor.\n    sph_grad_ptr: tl.tensor,  # Pointer to the gradient of the spherical harmonic.\n    block_size: tl.constexpr,  # Number of elements in each block.\n    coord_numel: tl.constexpr,  # Total number of elements in the coordinates tensor.\n    output_numel: tl.constexpr,  # Total number of elements in the output tensor.\n    col_offset: tl.constexpr,  # Offset for the output tensor.\n    output_stride: tl.constexpr,  # Stride of the output tensor.\n):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + (block_size * coord_stride * block_id)\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset < coord_numel)\n    y = tl.load(\n        coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 < coord_numel\n    )\n    z = tl.load(\n        coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 < coord_numel\n    )\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (\n        output_striding + (block_size * output_stride * block_id) + col_offset\n    )\n    g_0 = tl.load(\n        sph_grad_ptr + output_row_offset, mask=output_row_offset < output_numel\n    )\n    g_1 = tl.load(\n        sph_grad_ptr + output_row_offset + 1, mask=output_row_offset + 1 < output_numel\n    )\n    g_2 = tl.load(\n        sph_grad_ptr + output_row_offset + 2, mask=output_row_offset + 2 < output_numel\n    )\n    g_3 = tl.load(\n        sph_grad_ptr + output_row_offset + 3, mask=output_row_offset + 3 < output_numel\n    )\n    g_4 = tl.load(\n        sph_grad_ptr + output_row_offset + 4, mask=output_row_offset + 4 < output_numel\n    )\n    g_5 = tl.load(\n        sph_grad_ptr + output_row_offset + 5, mask=output_row_offset + 5 < output_numel\n    )\n    g_6 = tl.load(\n        sph_grad_ptr + output_row_offset + 6, mask=output_row_offset + 6 < output_numel\n    )\n    g_7 = tl.load(\n        sph_grad_ptr + output_row_offset + 7, mask=output_row_offset + 7 < output_numel\n    )\n    g_8 = tl.load(\n        sph_grad_ptr + output_row_offset + 8, mask=output_row_offset + 8 < output_numel\n    )\n    g_9 = tl.load(\n        sph_grad_ptr + output_row_offset + 9, mask=output_row_offset + 9 < output_numel\n    )\n    g_10 = tl.load(\n        sph_grad_ptr + output_row_offset + 10,\n        mask=output_row_offset + 10 < output_numel,\n    )\n    # -------------------- variable and constant definitions\n    CONST000 = 1.60565407233314\n    CONST001 = 3.00000000000000\n    CONST002 = 3.21130814466628\n    CONST003 = 1.60565407233314\n    CONST004 = 6.42261628933256\n    CONST005 = 6.42261628933256\n    CONST006 = 8.67152307844476\n    CONST007 = 8.02827036166571\n    CONST008 = 6.93721846275580\n    CONST009 = 11.6340690431164\n    CONST010 = 12.8452325786651\n    CONST011 = 6.21867148191637\n    CONST012 = 6.21867148191637\n    CONST014 = 12.4373429638327\n    CONST017 = 12.8452325786651\n    CONST018 = 13.8744369255116\n    CONST019 = 24.8746859276655\n    CONST020 = 24.8746859276655\n    CONST021 = 27.7488738510232\n    CONST024 = 29.4321253055229\n    CONST027 = 7.35803132638072\n    CONST029 = 46.5362761724657\n    CONST030 = 51.3809303146605\n    CONST031 = 51.3809303146605\n    CONST034 = 101.955872807799\n    CONST036 = -8.67152307844475\n    CONST037 = 3.46860923137790\n    CONST038 = -88.2963759165686\n    CONST039 = -83.2466215530696\n    CONST040 = -69.8044142586986\n    CONST041 = -50.9779364038993\n    CONST042 = -50.9779364038993\n    CONST043 = -46.5362761724657\n    CONST044 = -44.1481879582843\n    CONST045 = -41.6233107765348\n    CONST046 = -38.5356977359954\n    CONST047 = -38.5356977359954\n    CONST048 = -33.1662479035540\n    CONST049 = -33.9852909359329\n    CONST050 = 6.42261628933257\n    CONST051 = -33.9852909359329\n    CONST052 = -29.4321253055229\n    CONST053 = -27.7488738510232\n    CONST054 = -20.8116553882674\n    CONST055 = -19.2678488679977\n    CONST056 = -19.2678488679977\n    CONST057 = -16.9926454679664\n    CONST058 = -16.9926454679664\n    CONST059 = -13.8744369255116\n    CONST060 = -16.5831239517770\n    CONST061 = -8.49632273398321\n    CONST062 = -6.93721846275580\n    CONST063 = -5.20291384706685\n    CONST064 = -3.46860923137790\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    # -------------------- kernel implementations\n    g_x = tl.load(\n        coord_grad_ptr + coord_row_offset, mask=coord_row_offset < coord_numel\n    )\n    g_y = tl.load(\n        coord_grad_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 < coord_numel\n    )\n    g_z = tl.load(\n        coord_grad_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 < coord_numel\n    )\n    g_x += (\n        g_0 * (CONST009 * VAR06 + CONST009 * VAR24 + CONST040 * VAR08 * VAR26)\n        + g_1 * y * (CONST038 * VAR08 * z - CONST052 * VAR25)\n        + g_10 * (CONST029 * VAR07 * z + CONST043 * VAR25 * x)\n        + g_2\n        * (\n            CONST001 * VAR08 * (CONST059 * VAR17 + CONST064 * VAR26)\n            + CONST006 * VAR06\n            - CONST045 * VAR17 * VAR26\n            + CONST063 * VAR24\n        )\n        + g_3 * (CONST041 * VAR08 * y * z - CONST049 * VAR16 * z + CONST057 * VAR25 * y)\n        + g_4\n        * (\n            CONST000 * VAR24\n            + CONST001 * VAR08 * (CONST002 * VAR26 + CONST055 * VAR17)\n            + CONST007 * VAR06\n            + CONST010 * VAR15\n            + CONST056 * VAR17 * VAR26\n        )\n        + g_5 * (CONST048 * VAR16 * x + y * (CONST019 * VAR07 + CONST019 * VAR26 * x))\n        + g_6 * (CONST005 * VAR25 * x + z * (CONST004 * VAR07 + CONST046 * VAR17 * x))\n        + g_7 * (CONST049 * VAR16 * x - CONST051 * VAR07 * y)\n        + g_8 * (CONST008 * VAR25 * x + z * (CONST039 * VAR17 * x - CONST054 * VAR07))\n        + g_9 * y * (CONST024 * VAR07 + CONST038 * VAR26 * x)\n    )\n    g_y += (\n        g_1 * (CONST052 * VAR07 * z - CONST052 * VAR25 * x)\n        + g_2 * (-CONST039 * VAR26 * x * y + CONST053 * VAR07 * y)\n        + g_3 * (CONST058 * VAR07 * z + x * (CONST034 * VAR17 * z + CONST057 * VAR25))\n        + g_4 * (CONST047 * VAR07 * y + x * (CONST030 * VAR16 + CONST046 * VAR26 * y))\n        + g_5\n        * (\n            CONST001 * VAR17 * (CONST060 * VAR08 + CONST060 * VAR26)\n            + CONST011 * VAR06\n            + CONST012 * VAR24\n            + CONST014 * VAR08 * VAR26\n            - CONST060 * VAR15\n        )\n        + g_6 * (CONST046 * VAR25 * y + z * (CONST031 * VAR16 + CONST046 * VAR08 * y))\n        + g_7\n        * (\n            CONST001 * VAR17 * (CONST057 * VAR08 - CONST057 * VAR26)\n            - CONST061 * VAR06\n            + CONST061 * VAR24\n        )\n        + g_8 * (CONST021 * VAR25 * y + CONST039 * VAR08 * y * z)\n        + g_9 * (CONST027 * VAR06 + CONST027 * VAR24 + CONST044 * VAR08 * VAR26)\n    )\n    g_z += (\n        g_0 * (CONST029 * VAR25 * x + CONST043 * VAR07 * z)\n        + g_1 * y * (-CONST038 * VAR26 * x + CONST052 * VAR07)\n        + g_10 * (CONST009 * VAR06 + CONST009 * VAR24 + CONST040 * VAR08 * VAR26)\n        + g_2 * (CONST062 * VAR07 * z + x * (-CONST039 * VAR17 * z + CONST054 * VAR25))\n        + g_3 * (CONST058 * VAR07 * y + x * (CONST042 * VAR26 * y - CONST049 * VAR16))\n        + g_4 * (CONST005 * VAR07 * z + x * (CONST046 * VAR17 * z + CONST050 * VAR25))\n        + g_5 * (CONST048 * VAR16 * z + y * (CONST019 * VAR08 * z + CONST020 * VAR25))\n        + g_6\n        * (\n            CONST001 * VAR26 * (CONST002 * VAR08 + CONST056 * VAR17)\n            + CONST003 * VAR06\n            + CONST007 * VAR24\n            + CONST017 * VAR15\n            + CONST056 * VAR08 * VAR17\n        )\n        + g_7 * (-CONST049 * VAR16 * z + CONST051 * VAR25 * y)\n        + g_8\n        * (\n            CONST001 * VAR26 * (CONST018 * VAR17 + CONST037 * VAR08)\n            + CONST036 * VAR24\n            + CONST045 * VAR08 * VAR17\n            - CONST063 * VAR06\n        )\n        + g_9 * y * (CONST024 * VAR25 + CONST038 * VAR08 * z)\n    )\n    tl.store(\n        coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset < coord_numel\n    )\n    tl.store(\n        coord_grad_ptr + coord_row_offset + 1,\n        g_y,\n        mask=coord_row_offset + 1 < coord_numel,\n    )\n    tl.store(\n        coord_grad_ptr + coord_row_offset + 2,\n        g_z,\n        mask=coord_row_offset + 2 < coord_numel,\n    )\n\n\nclass FifthOrderSphericalHarmonic(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        coords: torch.Tensor,  # Input coordinates tensor.\n        output_tensor: torch.Tensor | None = None,  # Output tensor placeholder.\n        mask: torch.Tensor | None = None,  # Mask tensor (not used in this implementation).\n        block_size: int = 64,  # Block size for Triton kernels.\n        col_offset: int = 0,  # Column offset for the output tensor.\n    ):\n        if not isinstance(output_tensor, torch.Tensor):\n            output_tensor = torch.empty(\n                (*coords.shape[:-1], 11), dtype=coords.dtype, device=coords.device\n            )\n        coord_numel = coords.numel()\n        output_numel = output_tensor.numel()\n        num_blocks = calculate_lastdim_num_blocks(coords, block_size)\n        fifth_order_fwd[num_blocks,](\n            coords,\n            output_tensor,\n            block_size,\n            coord_numel,\n            output_numel,\n            col_offset,\n            output_tensor.stride(-2),\n        )\n        ctx.save_for_backward(coords)\n        return output_tensor\n\n    @staticmethod\n    def backward(\n        ctx,\n        sph_grad_tensor: torch.Tensor,  # Gradient of the output tensor.\n        block_size: int = 64,  # Block size for Triton kernels.\n        col_offset: int = 0,  # Column offset for the output tensor.\n    ) -> torch.Tensor:\n        (coords,) = ctx.saved_tensors\n        coord_grad_output = torch.zeros_like(coords)\n        num_blocks = calculate_lastdim_num_blocks(coords, block_size)\n        fifth_order_bwd[num_blocks,](\n            coords,\n            coord_grad_output,\n            sph_grad_tensor,\n            block_size,\n            coords.numel(),\n            sph_grad_tensor.numel(),\n            col_offset,\n            sph_grad_tensor.stride(-2),\n        )\n        return coord_grad_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel implements a fused recurrent operation for RWKV-6 model. It is designed to handle the forward pass of a recurrent neural network computation with customizable features such as initial state usage and reverse processing. The main function `fused_recurrent_rwkv6` accepts input tensors `r`, `k`, `v`, `w`, and `u`, and optionally, a scaling factor `scale`, an `initial_state`, and a flag `output_final_state`. It returns the output tensor `o` and optionally the `final_state`. This function leverages a Triton kernel to efficiently execute the computation in parallel across specified grid dimensions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_recurrent_rwkv6_kernel(\n    r_ptr, k_ptr, v_ptr, w_ptr, u_ptr, o_ptr, state_ptr, scale, initial_state_ptr, output_final_state,\n    BLOCK_SIZE: tl.constexpr, SEQ_LEN: tl.constexpr, HIDDEN_DIM: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load initial state if provided\n    if initial_state_ptr is not None:\n        state = tl.load(initial_state_ptr + offsets)\n    else:\n        state = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    for t in range(SEQ_LEN):\n        # Load input tensors for the current time step\n        r = tl.load(r_ptr + t * HIDDEN_DIM + offsets)\n        k = tl.load(k_ptr + t * HIDDEN_DIM + offsets)\n        v = tl.load(v_ptr + t * HIDDEN_DIM + offsets)\n        w = tl.load(w_ptr + t * HIDDEN_DIM + offsets)\n        u = tl.load(u_ptr + t * HIDDEN_DIM + offsets)\n\n        # Compute the recurrent state\n        state = state * w + r * k\n        o = state * u + v\n\n        # Apply scaling if provided\n        if scale is not None:\n            o = o * scale\n\n        # Store the output\n        tl.store(o_ptr + t * HIDDEN_DIM + offsets, o)\n\n    # Store the final state if required\n    if output_final_state:\n        tl.store(state_ptr + offsets, state)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef fused_recurrent_rwkv6(r, k, v, w, u, scale=None, initial_state=None, output_final_state=False):\n    # Get tensor dimensions\n    SEQ_LEN, HIDDEN_DIM = r.shape\n\n    # Allocate output tensor\n    o = torch.empty_like(r)\n\n    # Allocate state tensor if needed\n    if output_final_state:\n        final_state = torch.empty((HIDDEN_DIM,), device=r.device, dtype=r.dtype)\n    else:\n        final_state = None\n\n    # Define grid and block dimensions\n    BLOCK_SIZE = 256\n    grid = (HIDDEN_DIM // BLOCK_SIZE,)\n\n    # Launch the kernel\n    fused_recurrent_rwkv6_kernel[grid](\n        r, k, v, w, u, o, final_state, scale, initial_state, output_final_state,\n        BLOCK_SIZE=BLOCK_SIZE, SEQ_LEN=SEQ_LEN, HIDDEN_DIM=HIDDEN_DIM\n    )\n\n    # Return the output and final state if required\n    if output_final_state:\n        return o, final_state\n    else:\n        return o\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel(\n    q, k, v, w, u, o, h0, ht, s_k_h, s_v_h, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, REVERSE: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T-1) * K if REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T-1) * K if REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T-1) * V if REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + ((T-1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T-1) * K if REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < K\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    b_u = tl.load(p_u, mask=mask_bk, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0).to(tl.float32)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)\n\nclass FusedRecurrentRWKV6Function(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, r, k, v, w, u, scale=None, initial_state=None, output_final_state=False, reverse=False):\n        q = r\n        B, H, T, K, V = *q.shape, v.shape[-1]\n\n        BK, BV = min(triton.next_power_of_2(K), 32), min(triton.next_power_of_2(V), 32)\n        NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 1\n\n        final_state = q.new_empty(B, H, K, V) if output_final_state else None\n\n        o = q.new_empty(NK, B, H, T, V, dtype=torch.float32)\n        grid = (NV, NK, B * H)\n        fused_recurrent_rwkv6_fwd_kernel[grid](\n            q, k, v, w, u, o, initial_state, final_state,\n            k.stride(1),\n            v.stride(1),\n            scale,\n            B=B, H=H, T=T, K=K, V=V, BK=BK, BV=BV,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None,\n            REVERSE=reverse,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        o = o.sum(0)\n        ctx.save_for_backward(q, k, v, w, u, initial_state)\n        ctx.scale = scale\n        ctx.reverse = reverse\n        return o.to(q.dtype), final_state\n\ndef fused_recurrent_rwkv6(\n    r: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    u: torch.Tensor,\n    scale: float = -1,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if scale == -1:\n        scale = r.shape[-1] ** -0.5\n    o, final_state = FusedRecurrentRWKV6Function.apply(r, k, v, w, u, scale, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel function 'softmax_kernel' computes the softmax operation for each row of an input matrix 'x'. It stores the result in an output matrix. It uses parameters: 'output_ptr', 'input_ptr', 'input_row_stride', 'output_row_stride', 'n_cols', and 'BLOCK_SIZE'. The row index is derived from 'tl.program_id(axis=0)'. The row is loaded into SRAM using 'tl.load'. The maximum value in the row is found using 'tl.max' for stability during exponentiation. The numerator for the softmax is calculated by subtracting the max and applying 'tl.exp'. The denominator is the sum of the numerator values computed via 'tl.sum'. The normalized softmax result is obtained by dividing the numerator by the denominator and stored back using 'tl.store'. The wrapper function 'triton_softmax' sets up the computation by determining block size and launching the kernel for all rows.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr,  # Pointer to the output matrix\n    input_ptr,   # Pointer to the input matrix\n    input_row_stride,  # Row stride of the input matrix\n    output_row_stride,  # Row stride of the output matrix\n    n_cols,      # Number of columns in the matrix\n    BLOCK_SIZE: tl.constexpr  # Block size\n):\n    # Get the row index\n    row_idx = tl.program_id(axis=0)\n    \n    # Compute the starting and ending column indices for this block\n    col_start = tl.arange(0, BLOCK_SIZE)\n    col_end = tl.minimum(n_cols, col_start + BLOCK_SIZE)\n    \n    # Load the row into SRAM\n    row = tl.load(input_ptr + row_idx * input_row_stride + col_start, mask=col_start < n_cols)\n    \n    # Find the maximum value in the row for numerical stability\n    max_val = tl.max(row, axis=0)\n    \n    # Compute the numerator (exp(x - max_val))\n    numerator = tl.exp(row - max_val)\n    \n    # Compute the denominator (sum of the numerator)\n    denominator = tl.sum(numerator, axis=0)\n    \n    # Compute the softmax values\n    softmax_values = numerator / denominator\n    \n    # Store the result back to the output matrix\n    tl.store(output_ptr + row_idx * output_row_stride + col_start, softmax_values, mask=col_start < n_cols)\n\nimport triton\nimport triton.language as tl\n\ndef triton_softmax(input_ptr, output_ptr, n_rows, n_cols, BLOCK_SIZE=128):\n    # Determine the grid size\n    grid = (n_rows, )\n    \n    # Launch the kernel\n    softmax_kernel[grid](\n        output_ptr,  # Pointer to the output matrix\n        input_ptr,   # Pointer to the input matrix\n        n_cols,      # Row stride of the input matrix (same as number of columns)\n        n_cols,      # Row stride of the output matrix (same as number of columns)\n        n_cols,      # Number of columns in the matrix\n        BLOCK_SIZE   # Block size\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Get the program ID\n    row_idx = tl.program_id(axis=0)\n\n    # Compute the memory offsets for this row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    out_row_start_ptr = output_ptr + row_idx * output_row_stride\n\n    # Load the row into SRAM\n    row = tl.load(row_start_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < n_cols, other=-float('inf'))\n\n    # Compute max for numerical stability\n    row_max = tl.max(row, axis=0)\n    \n    # Subtract max from row and exponentiate\n    numerator = tl.exp(row - row_max)\n    \n    # Compute sum for normalization\n    denominator = tl.sum(numerator, axis=0)\n    \n    # Normalize\n    softmax_output = numerator / denominator\n    \n    # Store the output\n    tl.store(out_row_start_ptr + tl.arange(0, BLOCK_SIZE), softmax_output, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\ndef triton_softmax(x):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    \n    # Determine the block size\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    BLOCK_SIZE = min(BLOCK_SIZE, 1024)  \n    \n    # Launch the Triton kernel\n    grid = (n_rows,)\n    softmax_kernel[grid](\n        output, x,\n        x.stride(0), output.stride(0),\n        n_cols, BLOCK_SIZE=BLOCK_SIZE\n    )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel performs a chunked global cumulative sum (cumsum) on a 3D tensor. The main kernel function is `chunk_global_cumsum_scalar_kernel`. It is applied along the last dimension of the input tensor `s`, outputting a tensor `o` of the same shape. It computes cumsum per block of size `BT` and maintains a running total across blocks. The helper function `chunk_global_cumsum_scalar` sets up this kernel by defining the grid size and preparing an output tensor `z` to store the results.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(\n    s_ptr,  # Pointer to the input tensor\n    o_ptr,  # Pointer to the output tensor\n    n_elements,  # Total number of elements in the last dimension\n    BT,  # Block size\n    stride_s,  # Stride of the input tensor in the last dimension\n    stride_o,  # Stride of the output tensor in the last dimension\n    pid,  # Block ID\n):\n    block_start = pid * BT\n    block_end = min(block_start + BT, n_elements)\n    running_total = 0.0\n\n    for i in range(block_start, block_end):\n        s_val = tl.load(s_ptr + i * stride_s)\n        running_total += s_val\n        tl.store(o_ptr + i * stride_o, running_total)\n\n    if pid > 0:\n        prev_block_end = (pid - 1) * BT\n        prev_block_total = tl.load(o_ptr + (prev_block_end - 1) * stride_o)\n        running_total += prev_block_total\n\n        for i in range(block_start, block_end):\n            s_val = tl.load(s_ptr + i * stride_s)\n            running_total += s_val - s_val\n            tl.store(o_ptr + i * stride_o, running_total)\n\n@triton.jit\ndef chunk_global_cumsum_scalar(\n    s_ptr,  # Pointer to the input tensor\n    o_ptr,  # Pointer to the output tensor\n    n_elements,  # Total number of elements in the last dimension\n    BT,  # Block size\n    stride_s,  # Stride of the input tensor in the last dimension\n    stride_o,  # Stride of the output tensor in the last dimension\n    grid,  # Grid size\n):\n    chunk_global_cumsum_scalar_kernel[grid](\n        s_ptr, o_ptr, n_elements, BT, stride_s, stride_o, tl.program_id(0)\n    )\n\nimport torch\n\ndef chunk_global_cumsum(s, block_size):\n    assert s.dim() == 3, \"Input tensor must be 3D\"\n    n_elements = s.size(-1)\n    s_ptr = triton.core.get_tensor_ptr(s, 'cuda')\n    o = torch.empty_like(s)\n    o_ptr = triton.core.get_tensor_ptr(o, 'cuda')\n    stride_s = s.stride(-1)\n    stride_o = o.stride(-1)\n    grid = (n_elements + block_size - 1) // block_size\n\n    chunk_global_cumsum_scalar[\n        grid\n    ](\n        s_ptr, o_ptr, n_elements, block_size, stride_s, stride_o, grid\n    )\n\n    return o\n\nimport torch\n\n# Example input tensor\ns = torch.randn(2, 3, 1024, device='cuda')\n\n# Block size\nblock_size = 256\n\n# Perform chunked global cumulative sum\no = chunk_global_cumsum(s, block_size)\n\nprint(o)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=4),\n    ],\n    key=[]\n)\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(\n    s,\n    o,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_o = tl.cumsum(b_s, axis=0) + b_z[None]\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n\ndef chunk_global_cumsum_scalar(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T = s.shape\n    dtype = dtype or s.dtype\n    grid = (B * H,)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_cumsum_scalar_kernel[grid](\n        s, z,\n        T=T\n    )\n    return z\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a ReLU (Rectified Linear Unit) operation implemented using the Triton library for efficient GPU execution. The main functionality is to perform an element-wise ReLU operation on input tensors. The key components include:\n            - `heuristics_for_tile_size`: Determines the tile sizes for partitioning the computation workload based on input dimensions.\n            - `heuristics_for_num_warps`: Determines the optimal number of warps (thread groups) based on the tile size.\n            - `StridedBuffer`: A custom class for handling tensors with arbitrary strides, including negative strides for operations like tensor flipping.\n            - `relu_forward_wrapper_rank_1`: A wrapper function that sets up kernel execution for 1D tensors using `relu_forward_kernel_rank_1`.\n            - `relu_forward_kernel_rank_1`: A Triton kernel that performs the ReLU operation, handling data loading, computation, and storing results back to memory.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Heuristics for determining tile sizes\ndef heuristics_for_tile_size(input_size):\n    if input_size < 1024:\n        return 128\n    elif input_size < 4096:\n        return 256\n    else:\n        return 512\n\n# Heuristics for determining the number of warps\ndef heuristics_for_num_warps(tile_size):\n    if tile_size <= 128:\n        return 1\n    elif tile_size <= 256:\n        return 2\n    else:\n        return 4\n\n# Custom class for handling tensors with arbitrary strides\nclass StridedBuffer:\n    def __init__(self, ptr, shape, strides):\n        self.ptr = ptr\n        self.shape = shape\n        self.strides = strides\n\n    def __getitem__(self, idx):\n        offset = sum(i * s for i, s in zip(idx, self.strides))\n        return self.ptr + offset\n\n# Triton kernel for ReLU operation on 1D tensors\n@triton.jit\ndef relu_forward_kernel_rank_1(X, Y, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(X + offsets, mask=mask)\n    y = tl.where(x > 0, x, 0)\n    tl.store(Y + offsets, y, mask=mask)\n\n# Wrapper function for setting up kernel execution for 1D tensors\ndef relu_forward_wrapper_rank_1(X, Y):\n    N = X.shape[0]\n    BLOCK_SIZE = heuristics_for_tile_size(N)\n    num_warps = heuristics_for_num_warps(BLOCK_SIZE)\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    relu_forward_kernel_rank_1[grid](X, Y, N, BLOCK_SIZE, num_warps=num_warps)\n\n# Example usage\nimport torch\n\n# Create a 1D tensor with random values\nX = torch.randn(1024, device='cuda')\nY = torch.empty_like(X)\n\n# Perform the ReLU operation\nrelu_forward_wrapper_rank_1(X, Y)\n\n# Verify the result\nprint(Y)\n", "label": "import math\nfrom typing import Union\nimport torch\nimport triton\nfrom triton import language as tl\n\ndef heuristics_for_tile_size(max_tile_size, *sizes):\n    ndim = len(sizes)\n    tile_sizes = [0 for _ in range(ndim)]\n    for i in range(ndim):\n        size = sizes[ndim - 1 - i]\n        tile_size = min(max_tile_size, triton.next_power_of_2(size))\n        tile_sizes[ndim - 1 - i] = tile_size\n        max_tile_size = max(1, max_tile_size // tile_size)\n    return tuple(tile_sizes)\n\ndef heuristics_for_num_warps(tile_size):\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n    \n\nclass StridedBuffer:\n    \"\"\"A drop-in replacement of torch.Tensor that can be used in wrapper generated by\n    PointwiseDynamicFunction. It allows us to use a different shape, stride, data\n    pointer that that of the base tensor.\n\n    It is a kind of reinterpretation of the base tensor. We make this class since we\n    cannot get a Tensor view with negative strides via torch APIs, while we need this\n    to implement flip op.\n\n    Although generated code can accept torch.Tensor & StridedBuffer, but StridedBuffer\n    may not have all the methods as torch.Tensors do. We add some attributes & methods\n    with the same name as torch.Tensor, which are used in the generated code. But we\n    may not cover all the methods, add one if what you need is missing here.\n\n    And can also be used in triton kernels since it also has dtype & data_ptr().\n    \"\"\"\n\n    def __init__(\n        self, base: torch.Tensor, shape=None, strides=None, dtype=None, offset=0\n    ):\n        self._base = base\n        self.dtype = dtype or base.dtype\n        if offset == 0:\n            self._data_ptr = self._base.data_ptr()\n        else:\n            offset = self.dtype.itemsize * offset\n            self._data_ptr = self._base.data_ptr() + offset\n        self.shape = tuple(shape if shape is not None else self._base.shape)\n        self._strides = tuple(strides if strides is not None else self._base.stride())\n        self.device = self._base.device\n        self.ndim = len(self.shape)\n\n    def stride(self):\n        return self._strides\n\n    def size(self):\n        return self.shape\n\n    def element_size(self):\n        return self.dtype.itemsize\n\n    def numel(self):\n        return math.prod(self.shape)\n\n    def dim(self):\n        return self.ndim\n\n    def unwrap(self):\n        return self._base\n\n    def data_ptr(self):\n        return self._data_ptr\n\n\n\ndef relu_forward_wrapper_rank_1(in0: Union[torch.Tensor, StridedBuffer], /, *, out0: Union[torch.Tensor, StridedBuffer]): \n    \"\"\"Generated wrapper function with Pointwise: StridedBuffer, StridedBuffer(a1!) -> StridedBuffer(a1!)\"\"\"\n    assert in0.shape == out0.shape, 'operand shapes mismatch'\n    # task partitioning\n    shape = out0.shape\n    num_tasks = out0.numel()\n    tile_sizes = heuristics_for_tile_size(512, *shape)\n    tile_size = math.prod(tile_sizes)\n    num_tiles = math.prod(triton.cdiv(size, tile_size) for size, tile_size in zip(shape, tile_sizes))\n    num_ctas = min(65536, num_tiles)\n    tiles_per_cta = triton.cdiv(num_tiles, num_ctas)\n    num_warps = heuristics_for_num_warps(tile_size)\n    one_tile_per_cta = tiles_per_cta==1\n    grid = (num_ctas, 1, 1)\n    # kernel launch\n    in0_strides = in0.stride()\n    in0_stride_order = (0,)\n    out0_strides = out0.stride()\n    out0_stride_order = (0,)\n    with torch.cuda._DeviceGuard(in0.device.index):\n        relu_forward_kernel_rank_1[grid](\n            in0, out0,\n            in0_strides[0], # stride for in0\n            in0_stride_order[0], # stride order for in0\n            out0_strides[0], # stride for out0\n            out0_stride_order[0], # stride orderfor out0\n            shape[0], # task indexing space\n            num_tasks, # num tasks\n            tiles_per_cta=tiles_per_cta, # tiles_per_cta\n            tile_size0=tile_sizes[0],\n            one_tile_per_cta=one_tile_per_cta,\n            num_warps=num_warps,\n        )\n    return out0\n\n@triton.jit\ndef relu_forward(x):\n    return tl.where(x > 0, x, 0)\n\n@triton.jit\ndef relu_forward_kernel_rank_1(\n    in0_ptr: tl.tensor, # of tl.pointer_type\n    out0_ptr: tl.tensor, # of tl.pointer_type\n    in0_stride0: int, # strides for in0\n    in0_stride_order0: tl.constexpr, # stride order for in0\n    out0_stride0: int, # strides for out0\n    out0_stride_order0: tl.constexpr, # stride order for out0\n    s0: int, # task_space\n    num_tasks: int,\n    tiles_per_cta: int,\n    tile_size0: tl.constexpr,\n    one_tile_per_cta: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_tiles0 = tl.cdiv(s0, tile_size0)\n    if one_tile_per_cta: # monolitic kernel style\n        tile_id = pid\n        # pid multi index recontruction: we use c ordering, right axes changes fastest\n        tile_id0 = tile_id\n\n        # tile offsets\n        offset0 = tile_id0 * tile_size0\n        # loads\n        in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n        in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n        # compute\n        out0 = relu_forward(in0)\n\n        # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n        out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n        tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n    else: # grid-stride-loop style kernel\n        num_ctas = tl.num_programs(0)\n        for j in range(0, tiles_per_cta):\n            tile_id = pid + j * num_ctas\n            # pid multi index recontruction: we use c ordering, right axes changes fastest\n            tile_id0 = tile_id\n\n            # tile offsets\n            offset0 = tile_id0 * tile_size0\n            # loads\n            in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n            in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n            # compute\n            out0 = relu_forward(in0)\n\n            # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n            out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n            tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel and its associated Python function compute the Rotary Positional Encoding (RoPE) on input tensors, commonly used in transformer models to incorporate positional information into token embeddings. The main kernel, `_rope_embedding`, takes input tensors `Q`, `cos`, and `sin` along with their respective strides and dimensions to compute the RoPE transformation. The kernel is vectorized using Triton's parallel programming model to execute efficiently on GPUs. The process involves loading segments of the input tensor `Q`, applying rotations using precomputed `cos` and `sin` values, and storing the transformed outputs back to `Q`. The `Fast_RoPE_Embedding` class acts as an autograd-compatible wrapper, providing forward and backward methods to compute RoPE embeddings and their gradients, respectively. The `fast_rope_embedding` function serves as the API to transform both query (`Q`) and key (`K`) embeddings.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\nclass Fast_RoPE_Embedding(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, cos, sin):\n        # Save the input tensors for the backward pass\n        ctx.save_for_backward(Q, cos, sin)\n        \n        # Launch the Triton kernel\n        grid = (Q.numel() // 2, )\n        _rope_embedding[grid](Q, cos, sin, \n                              Q.stride(0), Q.stride(1), \n                              cos.stride(0), cos.stride(1), \n                              sin.stride(0), sin.stride(1), \n                              Q.size(1), Q.size(0))\n        \n        return Q\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retrieve the saved tensors\n        Q, cos, sin = ctx.saved_tensors\n        \n        # Launch the Triton kernel for the backward pass\n        grid = (Q.numel() // 2, )\n        _rope_embedding[grid](grad_output, cos, sin, \n                              grad_output.stride(0), grad_output.stride(1), \n                              cos.stride(0), cos.stride(1), \n                              sin.stride(0), sin.stride(1), \n                              Q.size(1), Q.size(0))\n        \n        return grad_output, None, None\n\ndef fast_rope_embedding(Q, K, cos, sin):\n    # Apply RoPE to both query and key embeddings\n    Q = Fast_RoPE_Embedding.apply(Q, cos, sin)\n    K = Fast_RoPE_Embedding.apply(K, cos, sin)\n    return Q, K\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\nROPE_GROUP_SIZE = 4\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int,):\n    BLOCK_SIZE : int = triton.next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _rope_embedding(\n    Q,     Q_row_stride,\n    cos, cos_row_stride,\n    sin, sin_row_stride,\n    seqlen,\n    head_dim      : tl.constexpr,\n    n_heads       : tl.constexpr,\n    BACKWARD_PASS : tl.constexpr,\n    BLOCK_SIZE    : tl.constexpr,\n):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    ROPE_GROUP_SIZE = 4\n    row_position  = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets  = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n\n    sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n    cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n\n    if BACKWARD_PASS:\n        # See our blog post for more info.\n        sin1 = -sin1\n    \n    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\n\n    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n\n        # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\n        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\n\n        tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\n        tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\n    \n\nclass Fast_RoPE_Embedding(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, cos, sin):\n        cos, sin = cos.squeeze(), sin.squeeze()\n        batch, seq_len, n_heads, head_dim = Q.shape\n        Q = Q.reshape(batch*seq_len, n_heads*head_dim)\n        n_rows, n_cols = Q.shape\n        assert(seq_len <= cos.shape[0])\n\n        # [TODO] Changing blocksize to head_dim//2 seems to have\n        # some concurrency / un-deterministic issues.\n        BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\n        \n        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\n        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\n        n_groups = div + (mod != 0)\n\n        _rope_embedding[(n_rows, n_groups, )](\n              Q,   Q.stride(0),\n            cos, cos.stride(0),\n            sin, sin.stride(0),\n            seq_len,\n            head_dim, n_heads,\n            BACKWARD_PASS = False,\n            BLOCK_SIZE = BLOCK_SIZE,\n            num_warps  = num_warps,\n        )\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps  = num_warps\n        ctx.n_groups = n_groups\n        ctx.cos = cos\n        ctx.sin = sin\n        return Q.view(batch, seq_len, n_heads, head_dim)\n    \n    @staticmethod\n    def backward(ctx, dY):\n        batch, seq_len, n_heads, head_dim = dY.shape\n        dY = dY.reshape(batch*seq_len, n_heads*head_dim)\n        # Must be reshape not view\n        n_rows, n_cols = dY.shape\n\n        cos = ctx.cos\n        sin = ctx.sin\n\n        _rope_embedding[(n_rows, ctx.n_groups, )](\n            dY,  dY .stride(0),\n            cos, cos.stride(0),\n            sin, sin.stride(0),\n            seq_len, head_dim, n_heads,\n            BACKWARD_PASS = True,\n            BLOCK_SIZE = ctx.BLOCK_SIZE,\n            num_warps  = ctx.num_warps,\n        )\n        dY = dY.view(batch, seq_len, n_heads, head_dim)\n        return dY, None, None,\n    \n\ndef fast_rope_embedding(Q, K, cos, sin):\n    Q = Fast_RoPE_Embedding.apply(Q.transpose(1, 2), cos, sin).transpose(1, 2)\n    K = Fast_RoPE_Embedding.apply(K.transpose(1, 2), cos, sin).transpose(1, 2)\n    return Q, K\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe function `chunk_global_reversed_cumsum_vector` uses a Triton kernel to compute a reversed cumulative sum across a specified dimension of a 4D input tensor `s`. The kernel iterates in reverse over the time dimension `T`, accumulating and storing results in an output tensor `z`. It operates on blocks of data with a configurable block size `BT` for temporal dimensions and `BS` for spatial dimensions. The input tensor has dimensions [B, H, T, S], where B is the batch size, H is the number of heads, T is the sequence length, and S is the number of features. The Triton kernel, `chunk_global_reversed_cumsum_vector_kernel`, performs the main computation, leveraging parallel processing. It first initializes a block-wise cumulative sum `b_z` to zero and iterates backward through time blocks, updating `b_z` using the dot product of a mask matrix `m_s` and the loaded input block `b_s`. The results are stored in the output tensor `z`. \n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(\n    s_ptr,  # pointer to the input tensor\n    z_ptr,  # pointer to the output tensor\n    m_s_ptr,  # pointer to the mask matrix\n    B, H, T, S,  # dimensions of the input tensor\n    BT, BS,  # block sizes for temporal and spatial dimensions\n    stride_s_B, stride_s_H, stride_s_T, stride_s_S,  # strides for the input tensor\n    stride_z_B, stride_z_H, stride_z_T, stride_z_S,  # strides for the output tensor\n    stride_m_T, stride_m_S,  # strides for the mask matrix\n    BLOCK_SIZE_T: tl.constexpr,  # block size for temporal dimension\n    BLOCK_SIZE_S: tl.constexpr,  # block size for spatial dimension\n):\n    # Get the program ID in the grid\n    pid_B = tl.program_id(0)\n    pid_H = tl.program_id(1)\n    pid_T = tl.program_id(2)\n    pid_S = tl.program_id(3)\n\n    # Compute the block indices\n    block_start_T = pid_T * BLOCK_SIZE_T\n    block_start_S = pid_S * BLOCK_SIZE_S\n\n    # Initialize the block-wise cumulative sum\n    b_z = tl.zeros((BLOCK_SIZE_S,), dtype=tl.float32)\n\n    # Iterate backward through time blocks\n    for t in range(T - 1, block_start_T - 1, -BLOCK_SIZE_T):\n        # Load the input block\n        b_s = tl.load(s_ptr + pid_B * stride_s_B + pid_H * stride_s_H + t * stride_s_T + block_start_S * stride_s_S, \n                      mask=block_start_S + tl.arange(0, BLOCK_SIZE_S) < S, \n                      other=0.0)\n\n        # Load the mask block\n        b_m = tl.load(m_s_ptr + t * stride_m_T + block_start_S * stride_m_S, \n                      mask=block_start_S + tl.arange(0, BLOCK_SIZE_S) < S, \n                      other=0.0)\n\n        # Compute the block-wise cumulative sum\n        b_z = b_z + tl.dot(b_m, b_s)\n\n        # Store the result in the output tensor\n        tl.store(z_ptr + pid_B * stride_z_B + pid_H * stride_z_H + t * stride_z_T + block_start_S * stride_z_S, \n                 b_z, \n                 mask=block_start_S + tl.arange(0, BLOCK_SIZE_S) < S)\n\nimport torch\n\ndef chunk_global_reversed_cumsum_vector(s, m_s, BT, BS):\n    B, H, T, S = s.shape\n    z = torch.zeros_like(s)\n\n    # Define the grid and block sizes\n    grid = (B, H, (T + BT - 1) // BT, (S + BS - 1) // BS)\n    block = (BS,)\n\n    # Launch the kernel\n    chunk_global_reversed_cumsum_vector_kernel[grid, block](\n        s, z, m_s, B, H, T, S, BT, BS,\n        s.stride(0), s.stride(1), s.stride(2), s.stride(3),\n        z.stride(0), z.stride(1), z.stride(2), z.stride(3),\n        m_s.stride(0), m_s.stride(1),\n        BT, BS\n    )\n\n    return z\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 16}, num_warps=4),\n        triton.Config({'BT': 16}, num_warps=8),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=4),\n        triton.Config({'BT': 64}, num_warps=8),\n    ],\n    key=['S']\n)\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(\n    s,\n    z,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr\n):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1., 0.)\n\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        # [BT, BS]\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n\ndef chunk_global_reversed_cumsum_vector(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T, S = s.shape\n    BS = 32\n    dtype = dtype or s.dtype\n    grid = (triton.cdiv(S, BS), B * H)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_reversed_cumsum_vector_kernel[grid](\n        s, z,\n        s.stride(1), s.stride(2), s.stride(3),\n        T=T, S=S, BS=BS\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator computes the log-sum-exp over the last dimension of a given input tensor. The function `logsumexp_fwd` wraps around the Triton kernel `logsumexp_fwd_kernel`, which efficiently computes this operation in parallel.\n\n        The kernel `logsumexp_fwd_kernel` reads a segment of the input tensor, optionally scales it, computes the maximum value, and then the log-sum-exp using Triton's specialized operations. It writes the result into an output tensor. The function accepts parameters including the input tensor `x`, an optional scale factor `scale`, and constants `D` and `B` representing the dimension size and block size, respectively. The kernel has configurations for various warp sizes to optimize performance.\n\n        The main function `logsumexp_fwd` reshapes the input tensor, determines the appropriate block size `B`, creates an empty output tensor `z`, and launches the kernel. It then reshapes the result back to the original tensor shape minus the last dimension and optionally casts it to a specified data type.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef logsumexp_fwd_kernel(\n    x_ptr,  # Pointer to the input tensor\n    z_ptr,  # Pointer to the output tensor\n    scale,  # Optional scale factor\n    D,      # Dimension size of the last dimension\n    B,      # Block size\n    stride, # Stride of the input tensor\n    **meta  # Metadata for the kernel\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * B\n    offsets = block_start + tl.arange(0, B)\n    mask = offsets < D\n    x = tl.load(x_ptr + offsets * stride, mask=mask, other=-float('inf'))\n    \n    if scale != 0.0:\n        x = x * scale\n    \n    max_val = tl.max(x, axis=0)\n    x = x - max_val\n    exp_x = tl.exp(x)\n    sum_exp_x = tl.sum(exp_x, axis=0)\n    log_sum_exp = tl.log(sum_exp_x) + max_val\n    \n    tl.store(z_ptr + pid, log_sum_exp)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef logsumexp_fwd(x, scale=1.0, dtype=None):\n    # Reshape the input tensor to a 2D tensor where the last dimension is D\n    B = 1024  # Block size\n    D = x.shape[-1]\n    x_reshaped = x.view(-1, D)\n    N = x_reshaped.shape[0]\n    \n    # Create an empty output tensor\n    z = torch.empty((N,), dtype=x.dtype, device=x.device)\n    \n    # Determine the appropriate block size\n    grid = (N,)\n    \n    # Launch the kernel\n    logsumexp_fwd_kernel[grid](\n        x_reshaped,  # Input tensor\n        z,           # Output tensor\n        scale,       # Scale factor\n        D,           # Dimension size\n        B,           # Block size\n        x.stride(-1) # Stride of the input tensor\n    )\n    \n    # Reshape the result back to the original tensor shape minus the last dimension\n    original_shape = x.shape[:-1]\n    z = z.view(original_shape)\n    \n    # Optionally cast the result to the specified data type\n    if dtype is not None:\n        z = z.to(dtype)\n    \n    return z\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=['D']\n)\n@triton.heuristics({\n    'HAS_SCALE': lambda args: args['scale'] is not None\n})\n@triton.jit\ndef logsumexp_fwd_kernel(\n    x,\n    z,\n    scale,\n    D: tl.constexpr,\n    B: tl.constexpr,\n    HAS_SCALE: tl.constexpr\n):\n    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)\n\ndef logsumexp_fwd(\n    x,\n    scale: Optional[float] = None,\n    dtype: Optional[torch.dtype] = None\n):\n    r\"\"\"\n    Compute the logsumexp of the input tensor over the last dimension.\n\n    Args:\n        x (Tensor):\n            The input tensor of any shape.\n        scale (Optional[float]):\n            The scale applied to the input tensor. Default: `None`.\n        dtype (Optional[torch.dtype]):\n            The data type of the output tensor. Default: `None`.\n    Returns:\n        Tensor: The logsumexp of the input tensor.\n    \"\"\"\n\n    shape = x.shape\n    x = x.view(-1, shape[-1])\n    N, D = x.shape\n    B = min(triton.next_power_of_2(D), 64 * 1024)\n    ND = triton.cdiv(D, B)\n\n    z = x.new_empty(N, ND, dtype=torch.float)\n    logsumexp_fwd_kernel[(N, ND)](\n        x=x,\n        z=z,\n        scale=scale,\n        D=D,\n        B=B\n    )\n    z = z.logsumexp(-1).view(*shape[:-1])\n    if dtype is not None and dtype != torch.float:\n        z = z.to(dtype)\n    return z\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator is designed for parallel computations on tensors in a Transformer-like architecture. The main functions are kernels that compute the forward and backward passes for a parallel rebased operation. The `parallel_rebased_fwd_kernel` performs matrix multiplications between query, key, and value tensors, storing results in an output tensor 'o' and a normalizer tensor 'z'. The `parallel_rebased_bwd_kernel` calculates gradients for these operations. The `ParallelBasedFunction` class encapsulates these operations for use as an autograd function in PyTorch, enabling gradient computation. Its `forward` method computes the output by applying the forward kernel, while the `backward` method computes gradients by invoking the backward kernel. The wrapper function `parallel_rebased` provides an interface, allowing customization of scale and normalization settings.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef parallel_rebased_fwd_kernel(\n    Q, K, V, o, z, \n    stride_qb, stride_qh, stride_qm, \n    stride_kb, stride_kh, stride_km, \n    stride_vb, stride_vh, stride_vm, \n    stride_ob, stride_oh, stride_om, \n    stride_zb, stride_zh, \n    n_head, n_ctx, scale, \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (n_head * n_ctx)\n    hid = (pid % (n_head * n_ctx)) // n_ctx\n    qid = (pid % (n_head * n_ctx)) % n_ctx\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q, shape=(n_ctx, n_ctx), strides=(stride_qm, stride_qh),\n        offsets=(qid * BLOCK_M, hid * BLOCK_K), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K, shape=(n_ctx, n_ctx), strides=(stride_km, stride_kh),\n        offsets=(0, hid * BLOCK_K), block_shape=(BLOCK_N, BLOCK_K), order=(1, 0)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V, shape=(n_ctx, n_ctx), strides=(stride_vm, stride_vh),\n        offsets=(0, hid * BLOCK_K), block_shape=(BLOCK_N, BLOCK_K), order=(1, 0)\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=o, shape=(n_ctx, n_ctx), strides=(stride_om, stride_oh),\n        offsets=(qid * BLOCK_M, hid * BLOCK_K), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0)\n    )\n    Z_block_ptr = tl.make_block_ptr(\n        base=z, shape=(n_ctx, n_head), strides=(stride_zm, stride_zh),\n        offsets=(qid, hid), block_shape=(1, 1), order=(1, 0)\n    )\n\n    q = tl.load(Q_block_ptr)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n    for k in range(0, n_ctx, BLOCK_N):\n        k_block_ptr = tl.advance(K_block_ptr, (k, 0))\n        v_block_ptr = tl.advance(V_block_ptr, (k, 0))\n        k = tl.load(k_block_ptr)\n        v = tl.load(v_block_ptr)\n        acc += tl.dot(q, k, trans_b=True) * scale\n\n    z = tl.max(acc, 1)\n    z = tl.exp(z - z)\n    z = tl.sum(z, 1)\n    z = 1.0 / z\n    tl.store(Z_block_ptr, z)\n\n    o = tl.dot(acc, v)\n    o = o * z\n    tl.store(O_block_ptr, o)\n\n@triton.jit\ndef parallel_rebased_bwd_kernel(\n    Q, K, V, o, z, do, dq, dk, dv, \n    stride_qb, stride_qh, stride_qm, \n    stride_kb, stride_kh, stride_km, \n    stride_vb, stride_vh, stride_vm, \n    stride_ob, stride_oh, stride_om, \n    stride_zb, stride_zh, \n    stride_dob, stride_doh, stride_dom, \n    stride_dqb, stride_dqh, stride_dqm, \n    stride_dkb, stride_dkh, stride_dkm, \n    stride_dvb, stride_dvh, stride_dvm, \n    n_head, n_ctx, scale, \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // (n_head * n_ctx)\n    hid = (pid % (n_head * n_ctx)) // n_ctx\n    qid = (pid % (n_head * n_ctx)) % n_ctx\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q, shape=(n_ctx, n_ctx), strides=(stride_qm, stride_qh),\n        offsets=(qid * BLOCK_M, hid * BLOCK_K), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K, shape=(n_ctx, n_ctx), strides=(stride_km, stride_kh),\n        offsets=(0, hid * BLOCK_K), block_shape=(BLOCK_N, BLOCK_K), order=(1, 0)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V, shape=(n_ctx, n_ctx), strides=(stride_vm, stride_vh),\n        offsets=(0, hid * BLOCK_K), block_shape=(BLOCK_N, BLOCK_K), order=(1, 0)\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=o, shape=(n_ctx, n_ctx), strides=(stride_om, stride_oh),\n        offsets=(qid * BLOCK_M, hid * BLOCK_K), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0)\n    )\n    Z_block_ptr = tl.make_block_ptr(\n        base=z, shape=(n_ctx, n_head), strides=(stride_zm, stride_zh),\n        offsets=(qid, hid), block_shape=(1, 1), order=(1, 0)\n    )\n    DO_block_ptr = tl.make_block_ptr(\n        base=do, shape=(n_ctx, n_ctx), strides=(stride_dom, stride_doh),\n        offsets=(qid * BLOCK_M, hid * BLOCK_K), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0)\n    )\n    DQ_block_ptr = tl.make_block_ptr(\n        base=dq, shape=(n_ctx, n_ctx), strides=(stride_dqm, stride_dqh),\n        offsets=(qid * BLOCK_M, hid * BLOCK_K), block_shape=(BLOCK_M, BLOCK_K), order=(1, 0)\n    )\n    DK_block_ptr = tl.make_block_ptr(\n        base=dk, shape=(n_ctx, n_ctx), strides=(stride_dkm, stride_dkh),\n        offsets=(0, hid * BLOCK_K), block_shape=(BLOCK_N, BLOCK_K), order=(1, 0)\n    )\n    DV_block_ptr = tl.make_block_ptr(\n        base=dv, shape=(n_ctx, n_ctx), strides=(stride_dvm, stride_dvh),\n        offsets=(0, hid * BLOCK_K), block_shape=(BLOCK_N, BLOCK_K), order=(1, 0)\n    )\n\n    q = tl.load(Q_block_ptr)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n    o = tl.load(O_block_ptr)\n    z = tl.load(Z_block_ptr)\n    do = tl.load(DO_block_ptr)\n\n    dz = tl.sum(do * o, 1)\n    dz = dz * z * z\n    dv = tl.dot(do, q, trans_a=True)\n    dk = tl.dot(do, v, trans_b=True)\n    dq = tl.dot(do, k)\n\n    tl.store(DQ_block_ptr, dq)\n    tl.store(DK_block_ptr, dk)\n    tl.store(DV_block_ptr, dv)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef parallel_rebased_fwd_kernel(\n    q,  # query [B, H, L, D_head_K]\n    k,  # key [B, H, L, D_head_V]\n    v,  # value [B, H, L, D_head_V]\n    o,  # output [B, H, L, D_head_V]\n    z,  # normalizer [B, H, L]\n    s_qk_h,  # stride size: L * D_head_K\n    s_qk_t,  # stride size: D_head_K\n    s_qk_d,  # stride size: 1\n    s_vo_h,  # stride size: L * D_head_V\n    s_vo_t,  # stride size: D_head_V\n    s_vo_d,  # stride size: 1\n    scale,  # D_head_K ** -0.5\n    B,  # batch size\n    H,  # H\n    T,  # T\n    K: tl.constexpr,  # D_head_K\n    V: tl.constexpr,  # D_head_V\n    BTL: tl.constexpr,  # BLOCK SIZE along the sequence dimension for Q\n    BTS: tl.constexpr,  # BLOCK SIZE along the sequence dimension for K/V\n    BK: tl.constexpr,  # BLOCK SIZE along the K dimension\n    BV: tl.constexpr,  # BLOCK SIZE along the V dimension\n):\n    # i_c: chunk index. used for sequence parallelism\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0, i_v * BV), (BTS, BV), (1, 0))\n\n    # [BQ, BD] block Q, in the shared memory throughout the whole kernel\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n\n    # Q block and K block have no overlap\n    # no need for mask, thereby saving flops\n    for _ in range(0, i_c * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_s = tl.dot(b_q, (b_k), allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n\n        # [BQ, BD]\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n\n    # # rescale interchunk output\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    # # sync threads, easy for compiler to optimize\n    # tl.debug_barrier()\n\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        # [BTL, BV]\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_z, b_z.to(p_z.dtype.element_ty),\n             mask=((i_c * BTL + tl.arange(0, BTL)) < T))\n\n@triton.jit\ndef _parallel_rebased_bwd_dq(\n    i_bh,\n    i_c,\n    i_k,\n    i_v,\n    i_h,\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dq,\n    s_k_h,\n    s_k_t,\n    s_k_d,\n    s_v_h,\n    s_v_t,\n    s_v_d,\n    scale,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n                             (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + (i_bh) * s_k_h, (T, K),\n                            (s_k_t, s_k_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K),\n                            (s_k_t, s_k_d), (0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T),\n                            (s_v_d, s_v_t), (i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=(i_c * BTL + tl.arange(0, BTL)) < T)\n\n    for _ in range(0, i_c * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        # [BQ, BD]\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K),\n                            (s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T),\n                            (s_v_d, s_v_t), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        # [BTL, BK]\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype),\n                       b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_k_h, (T, K),\n                             (s_k_t, s_k_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n@triton.jit\ndef _parallel_rebased_bwd_dkv(\n    i_bh, i_c, i_k, i_v, i_h,\n    q, k, v, do, dz, dk, dv, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d,\n    scale,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    # compute dk dv\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n                            (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n                            (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(\n        p_v, boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(\n        [BTL, BV], dtype=tl.float32)\n\n    for i in range((tl.cdiv(T, BTS) * BTS)-BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BK, BTS]\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)  # [BV, BTS]\n        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)\n        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * \\\n            scale  # [BTL, BTS]\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype),\n                       tl.trans(b_q), allow_tf32=False)\n\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c*BTL, (i_c+1)*BTL, BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BD, BQ]\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)\n        # [BK, BQ]\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        # [BK, BD]\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype),\n                       tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_k_h,\n                             (T, K), (s_k_t, s_k_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_v_h,\n                             (T, V), (s_v_t, s_v_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n@triton.jit\ndef parallel_rebased_bwd_kernel(\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dq,\n    dk,\n    dv,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    scale,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(\n        i_bh, i_c, i_k, i_v, i_h,\n        q, k, v, do, dz, dq, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d, scale,\n        B=B, H=H, T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV\n    )\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(\n        i_bh, i_c, i_k, i_v, i_h,\n        q, k, v, do, dz, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d,\n        scale,\n        B=B, H=H, T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV\n    )\n\nclass ParallelBasedFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, scale):\n        BTL, BTS = 128, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        BK, BV = max(BK, 16), max(BV, 16)\n        B, H, T, K, V = *k.shape, v.shape[-1]\n        num_stages = 2\n        num_warps = 4\n        NK = triton.cdiv(K, BK)\n        NV = triton.cdiv(V, BV)\n        grid = (NK * NV, triton.cdiv(T, BTL), B * H)\n\n        assert NK == 1, \"will encounter some synchronization issue if not.\"\n\n        o = torch.empty(NK, B, H, T, V, device=q.device)\n        z = torch.empty(NK, B, H, T, device=q.device)\n        parallel_rebased_fwd_kernel[grid](\n            q, k, v, o, z,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        ctx.save_for_backward(q, k, v)\n        ctx.scale = scale\n        return o.sum(0).to(q.dtype), z.sum(0).to(q.dtype)\n\n    @staticmethod\n    def backward(ctx, do, dz):\n        q, k, v = ctx.saved_tensors\n        scale = ctx.scale\n        BTL, BTS = 64, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        BK, BV = max(BK, 16), max(BV, 16)\n        B, H, T, K, V = *k.shape, v.shape[-1]\n        num_stages = 2\n        num_warps = 4\n        NK = triton.cdiv(K, BK)\n        NV = triton.cdiv(V, BV)\n        grid = (NK * NV, triton.cdiv(T, BTL), B * H)\n\n        assert NK == 1, \"will encounter some synchronization issue if not\"\n\n        dq = torch.empty(NV, B, H, T, K, dtype=q.dtype, device=q.device)\n        dk = torch.empty(NV, B, H, T, K, dtype=q.dtype, device=q.device)\n        dv = torch.empty(NK, B, H, T, V, dtype=q.dtype, device=q.device)\n\n        parallel_rebased_bwd_kernel[grid](\n            q, k, v, do, dz, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        return dq.sum(0).to(q.dtype), dk.sum(0).to(k.dtype), dv.sum(0).to(v.dtype), None\n\ntriton_parallel_based = ParallelBasedFunction.apply\n\ndef parallel_rebased(q, k, v, eps=1e-5, use_scale=True, use_normalize=True, return_both=False):\n    assert q.shape[-1] <= 128, \"only support feature dim up to 128\"\n    if use_scale:\n        scale = q.shape[-1] ** -0.5\n    else:\n        scale = 1\n    o, z = triton_parallel_based(q, k, v, scale)\n    if return_both:\n        return o, z\n    if use_normalize:\n        o = o / (z[..., None] + eps)\n    else:\n        o = o\n    return o.to(q.dtype)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines two Triton kernels: `matmul4_kernel` and `dequantize_kernel`, each decorated with `triton.autotune` to optimize for different configurations.\n\n        `matmul4_kernel` performs a specialized matrix multiplication where:\n        - Matrix `A` is of shape `(M, K)` with `float16` elements.\n        - Matrix `B` is stored in a packed 4-bit integer format, shaped `(K//8, N)`.\n        - It computes the matrix multiplication `C = A x B` where `C` is `(M, N)` in `float16`.\n        - It uses scales and zeros arrays for quantization adjustments.\n        - `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K` define the tile sizes for computation.\n        - It processes blocks of the matrix in a loop, unpacking 4-bit integers from `B`.\n\n        `dequantize_kernel` converts a packed 4-bit integer matrix `B` to full precision:\n        - Input `b` is `(K//8, N)` with packed int4 format.\n        - It outputs `fpb` as a full-precision matrix `(K, N)`.\n        - The dequantization uses scales and zero-points for adjustment.\n        - The kernel processes matrix tiles of size `BLOCK_SIZE_K x BLOCK_SIZE_N`.\n\n        The functions `dequantize_int4` and `matmul_dequantize_int4_s1` wrap these kernels for higher-level operations:\n        - `dequantize_int4` dequantizes matrix `B` using `dequantize_kernel`.\n        - `matmul_dequantize_int4_s1` dequantizes `B` and then performs matrix multiplication with `A`.\n\n        The code provides a function `quantize_int4` for quantizing weights into int4 format with associated scales and zero-points. It processes 8 elements into an `int32` and handles group-wise statistics for quantization. The output is a packed version of `B`, its scales, and zero-points.\n        ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Triton kernel for dequantizing packed int4 matrix\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_N': 128}, num_warps=8),\n    ],\n    key=['K', 'N']\n)\n@triton.jit\ndef dequantize_kernel(\n    b_ptr, fpb_ptr, scales_ptr, zeros_ptr,\n    K, N,\n    BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    first_pid_n = group_id * num_pid_n\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = first_pid_n + (pid % num_pid_n)\n\n    offs_k = pid_m * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (offs_k // 8) * N + offs_n\n    fpb_offs = offs_k * N + offs_n\n\n    b = tl.load(b_ptr + b_offs)\n    scales = tl.load(scales_ptr + b_offs)\n    zeros = tl.load(zeros_ptr + b_offs)\n\n    b_unpacked = tl.where(offs_k % 8 < 4, (b >> (4 * (offs_k % 8 // 4))) & 0xF, (b >> (4 * ((offs_k % 8 // 4) - 1))) & 0xF)\n    fpb = (b_unpacked - zeros) * scales\n\n    tl.store(fpb_ptr + fpb_offs, fpb)\n\n# Triton kernel for matrix multiplication with packed int4 matrix\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul4_kernel(\n    a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    first_pid_n = group_id * num_pid_n\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = first_pid_n + (pid % num_pid_n)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + ((offs_k // 8)[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n    a_mask = (offs_m[:, None] < M) & (offs_k[None, :] < K)\n    b_mask = (offs_k[:, None] < K) & (offs_n[None, :] < N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n\n        b_unpacked = tl.where(offs_k % 8 < 4, (b >> (4 * (offs_k % 8 // 4))) & 0xF, (b >> (4 * ((offs_k % 8 // 4) - 1))) & 0xF)\n        scales = tl.load(scales_ptr + b_ptrs, mask=b_mask, other=1.0)\n        zeros = tl.load(zeros_ptr + b_ptrs, mask=b_mask, other=0.0)\n\n        b_unpacked = (b_unpacked - zeros) * scales\n\n        accumulator += tl.dot(a, b_unpacked)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // 8) * stride_bk\n\n    c = accumulator.to(tl.float16)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n# Wrapper function for dequantizing packed int4 matrix\ndef dequantize_int4(b, scales, zeros):\n    M, N = b.shape\n    K = M * 8\n    fpb = torch.empty((K, N), dtype=torch.float16, device=b.device)\n    grid = lambda META: (triton.cdiv(K, META['BLOCK_SIZE_K']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    dequantize_kernel[grid](b, fpb, scales, zeros, K, N)\n    return fpb\n\n# Wrapper function for matrix multiplication with dequantized int4 matrix\ndef matmul_dequantize_int4_s1(a, b, scales, zeros):\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), dtype=torch.float16, device=a.device)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul4_kernel[grid](a, b, c, scales, zeros, M, N, K)\n    return c\n\n# Function for quantizing weights into int4 format\ndef quantize_int4(weights, group_size=128):\n    assert weights.dtype == torch.float16, \"Input weights must be float16\"\n    K, N = weights.shape\n    assert K % group_size == 0, \"K must be divisible by group_size\"\n\n    scales = torch.empty((K // group_size, N), dtype=torch.float16, device=weights.device)\n    zeros = torch.empty((K // group_size, N), dtype=torch.float16, device=weights.device)\n    b = torch.empty((K // 8, N), dtype=torch.int32, device=weights.device)\n\n    for i in range(0, K, group_size):\n        group = weights[i:i + group_size, :]\n        min_val, _ = torch.min(group, dim=0)\n        max_val, _ = torch.max(group, dim=0)\n        scale = (max_val - min_val) / 15\n        zero = -min_val / scale\n        scales[i // group_size, :] = scale\n        zeros[i // group_size, :] = zero\n        quantized = torch.round((group / scale) + zero)\n        quantized = quantized.to(torch.int32)\n        b[i // 8, :] = (quantized[0::2] << 4) | quantized[1::2]\n\n    return b, scales, zeros\n", "label": "import time\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n\tconfigs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), \n    ],\n\tkey=['M', 'N', 'K', 'NO_GROUPS'],\n)\n@triton.jit\ndef matmul4_kernel(\n\ta_ptr, b_ptr, c_ptr,\n\tscales_ptr, zeros_ptr,\n\tM, N, K,\n\tstride_am, stride_ak,\n\tstride_bk, stride_bn,\n\tstride_cm, stride_cn,\n\tstride_scales_g, stride_scales_n,\n\tstride_zeros_g, stride_zeros_n,\n\tgroupsize, NO_GROUPS: tl.constexpr,\n\tBLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n\tGROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    # b_ptrs is set up such that it repeats elements along the K axis 8 times\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n   # (BLOCK_SIZE_N,)\n    # zeros_ptrs is set up such that it repeats elements along the N axis 8 times\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)   # (BLOCK_SIZE_N,)\n    # shifter is used to extract the 4 bits of each element in the 32-bit word from B and zeros\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    # If G == 1, scales and zeros are the same for all K, so we can load them once\n    if NO_GROUPS:\n        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n        # Unpack zeros\n        zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n        # zeros = (zeros + 1) * scales  # (BLOCK_SIZE_N,) float16\n        zeros = zeros * scales\n    # Now calculate a block of output of shape (BLOCK_SIZE_M, BLOCK_SIZE_N)\n    # M is along the batch dimension, N is along the outfeatures dimension, K is along the infeatures dimension\n    # So this loop is along the infeatures dimension (K)\n    # It's calculating BLOCK_SIZE_M batches in parallel, and for each batch, BLOCK_SIZE_N outfeatures in parallel\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)  # (BLOCK_SIZE_N,)\n            ptr = zeros_ptrs + g_id * stride_zeros_g   # (BLOCK_SIZE_N,)\n            zeros = tl.load(ptr)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n            # Unpack zeros\n            zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n            zeros = (zeros) * scales  # (BLOCK_SIZE_N,) float16\t\n        # Now we need to unpack b (which is 4-bit values) into 32-bit values\n        b = (b >> shifter[:, None]) & 0xF  # Extract the 4-bit values\n        b = b * scales[None, :] - zeros[None, :]  # Scale and shift\n        # print(\"data type\", a, b)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk  \n    c = accumulator.to(c_ptr.dtype.element_ty)  \n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    ],\n    key=['K', 'N'],\n)\n@triton.jit\ndef dequantize_kernel(\n    # Pointers to matrices\n    b_ptr, b_scale_ptr, b_zp_ptr, fpb_ptr,\n    # Matrix dimensions\n    K, N, group_size,\n    stride_bk, stride_bn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    stride_fpbk, stride_fpbn,\n    # Meta-parameters\n    BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"Dequantize tile [BLOCK_SIZE_K, BLOCK_SIZE_N] in full precision.\n    We should assert BLOCK_SIZE_N % 8 == 0.\n    weight[K // 8, N], scale[K // group_size, N], zp[K // group_size, N // group_size]\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (offs_n[None, :] // 8) * stride_bzpn\n    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[None, :] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = (offs_k[:, None] % 8) * 4\n    bzp_shift = (offs_n[None, :] % 8) * 4\n    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)\n\n\ndef dequantize_int4(b, b_scale, b_zero_point, device, dtype, group_size):\n    Kw, N = b.shape\n    K = Kw * 8\n    fp_b = torch.ones((K, N), device=device, dtype=dtype)\n    grid = lambda META: (\n        triton.cdiv(K, META['BLOCK_SIZE_K']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']), \n    )\n    dequantize_kernel[grid](\n        b, b_scale, b_zero_point, fp_b,\n        K, N, group_size,\n        b.stride(0), b.stride(1),\n        b_scale.stride(0), b_scale.stride(1),\n        b_zero_point.stride(0), b_zero_point.stride(1),\n        fp_b.stride(0), fp_b.stride(1)\n    )\n    return fp_b\n\n\ndef matmul_dequantize_int4_s1(a, b, b_scale, b_zero_point, group_size=128, out=None):\n    \"\"\"\n    Matmul dequantize int4 s1 dequantize weight to `fp_b` and do fp16 torch.mm,\n    this is for `prefill` stage, since weight size is fixed so is dequantize overhead,\n    perfill stage have more tokens to amortize dequant cost.\n    \"\"\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    # assert b.is_contiguous(), \"Matrix B must be contiguous\"\n    M, K = a.shape\n    Kw, N = b.shape\n    if out is None:\n        # Allocates output.\n        out = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    fp_b = dequantize_int4(b, b_scale, b_zero_point, a.device, a.dtype, group_size)\n    torch.mm(a, fp_b, out=out)\n    fp_b = None\n    return out\n\n\ndef quantize_int4(weight, group_size=128, tp_rank=0):\n    # Weight shape: [H1 // 8, H2]\n    # Scale shape: [H1 // group_size, H2]\n    # zero_pint shape: [H1 // group_size, H2 // 8]\n\n    weight = weight.transpose(1, 0)\n    h1, h2 = weight.shape\n    assert h1 % 8 == 0 and h2 % 8 == 0, \"H1 {} H2 {}\".format(h1, h2)\n    assert h2 % group_size == 0, \"H1 {} H2 {}\".format(h1, h2)\n    weight = weight.contiguous().view(-1, group_size).cuda(tp_rank)\n    weight_max = weight.amax(-1, keepdim=True)\n    weight_max = torch.where(weight_max < 0, 0, weight_max)\n    weight_min = weight.amin(-1, keepdim=True)\n    weight_min = torch.where(weight_min > 0, 0, weight_min)\n    weight_range = weight_max - weight_min \n    scale = weight_range / (2 ** 4 - 1)\n    zero_point = (-weight_min / scale).round().clamp(0, 15).to(torch.int32)\n    weight = (weight / scale + zero_point).round().clamp(0, 15).to(torch.int32).view(h1, h2)\n    int_weight = torch.empty(h1, h2 // 8).to(torch.int32).to(weight.device)\n    int_zero_point = torch.zeros(h1 // 8, h2 // group_size).to(torch.int32).to(weight.device)\n    zero_point = zero_point.view(h1, -1)\n    scale = scale.view(h1, -1)\n    # pack 8 int4 in an int32 number.\n    # Weight pack in row.\n    for pack in range(0, h2, 8):\n        for i in range(8):\n            int_weight[:, pack // 8] += weight[:, pack + i] << (i * 4)\n    # zero point pack in col.\n    for pack in range(0, h1, 8):\n        for i in range(8):\n            int_zero_point[pack // 8, :] += zero_point[pack + i, :] << (i * 4)\n    '''\n    fp_weight = torch.zeros(h1, h2).half().to(weight.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_weight[pack * 8 + i, :] = \\\n                ((int_weight[pack, :] << (28 - i * 4) >> 28) + 16) % 16\n    print((fp_weight - weight).abs().sum())\n\n    fp_zp = torch.zeros(zero_point.shape).half().to(zero_point.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zp[pack * 8 + i, :] = \\\n                (int_zero_point[pack, :] >> (i * 4)) & 15\n\n    print((fp_zp - zero_point).abs().sum())\n    '''\n    weight = None\n    return int_weight.transpose(1, 0).contiguous(), scale.transpose(1, 0).contiguous(), int_zero_point.transpose(1, 0).contiguous(), group_size\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel '_fwd_kernel_destindex_copy_quantize_kv' is designed to perform quantization of input data from tensor 'K' and store the results into 'Out' and 'Out_scale', based on indices provided by 'Dest_loc'. The kernel reads each element from 'K', calculates the scale for quantization, converts data to int8, and writes quantized data and scaling factors to output tensors. The function 'destindex_copy_quantize_kv' sets up and calls this kernel, configuring grid dimensions based on input shapes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K_ptr,  # Pointer to the input tensor K\n    Out_ptr,  # Pointer to the output tensor Out\n    Out_scale_ptr,  # Pointer to the output tensor Out_scale\n    Dest_loc_ptr,  # Pointer to the destination indices tensor Dest_loc\n    K_batch_stride,  # Stride for batch dimension in K\n    K_head_stride,  # Stride for head dimension in K\n    K_seq_stride,  # Stride for sequence dimension in K\n    K_hidden_stride,  # Stride for hidden dimension in K\n    Out_batch_stride,  # Stride for batch dimension in Out\n    Out_head_stride,  # Stride for head dimension in Out\n    Out_seq_stride,  # Stride for sequence dimension in Out\n    Out_hidden_stride,  # Stride for hidden dimension in Out\n    Out_scale_batch_stride,  # Stride for batch dimension in Out_scale\n    Out_scale_head_stride,  # Stride for head dimension in Out_scale\n    Out_scale_seq_stride,  # Stride for sequence dimension in Out_scale\n    Dest_loc_batch_stride,  # Stride for batch dimension in Dest_loc\n    Dest_loc_head_stride,  # Stride for head dimension in Dest_loc\n    Dest_loc_seq_stride,  # Stride for sequence dimension in Dest_loc\n    batch_size,  # Batch size\n    num_heads,  # Number of heads\n    seq_len,  # Sequence length\n    hidden_dim,  # Hidden dimension\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelism\n):\n    # Compute the batch, head, and sequence indices for the current thread block\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    seq_idx = tl.program_id(2)\n\n    # Check if the indices are within bounds\n    if batch_idx >= batch_size or head_idx >= num_heads or seq_idx >= seq_len:\n        return\n\n    # Compute the base pointers for the current batch, head, and sequence\n    K_base_ptr = K_ptr + batch_idx * K_batch_stride + head_idx * K_head_stride + seq_idx * K_seq_stride\n    Out_base_ptr = Out_ptr + batch_idx * Out_batch_stride + head_idx * Out_head_stride\n    Out_scale_base_ptr = Out_scale_ptr + batch_idx * Out_scale_batch_stride + head_idx * Out_scale_head_stride\n    Dest_loc_base_ptr = Dest_loc_ptr + batch_idx * Dest_loc_batch_stride + head_idx * Dest_loc_head_stride + seq_idx * Dest_loc_seq_stride\n\n    # Load the destination index\n    dest_idx = tl.load(Dest_loc_base_ptr)\n\n    # Compute the base pointers for the output based on the destination index\n    Out_base_ptr += dest_idx * Out_seq_stride\n    Out_scale_base_ptr += dest_idx * Out_scale_seq_stride\n\n    # Quantize and store the data\n    for i in range(0, hidden_dim, BLOCK_SIZE):\n        # Load the data from K\n        k_data = tl.load(K_base_ptr + i * K_hidden_stride, mask=i + tl.arange(0, BLOCK_SIZE) < hidden_dim, other=0.0)\n\n        # Calculate the scale\n        max_val = tl.max(k_data, axis=0)\n        min_val = tl.min(k_data, axis=0)\n        scale = (max_val - min_val) / 255.0\n\n        # Quantize the data to int8\n        quantized_data = tl.cast(tl.round(k_data / scale), tl.int8)\n\n        # Store the quantized data and scale\n        tl.store(Out_base_ptr + i * Out_hidden_stride, quantized_data, mask=i + tl.arange(0, BLOCK_SIZE) < hidden_dim)\n        tl.store(Out_scale_base_ptr + i, scale, mask=i + tl.arange(0, BLOCK_SIZE) < hidden_dim)\n\nimport triton\nimport triton.runtime\n\ndef destindex_copy_quantize_kv(K, Out, Out_scale, Dest_loc):\n    # Get the shape and strides of the input tensors\n    batch_size, num_heads, seq_len, hidden_dim = K.shape\n    K_batch_stride, K_head_stride, K_seq_stride, K_hidden_stride = K.strides\n    Out_batch_stride, Out_head_stride, Out_seq_stride, Out_hidden_stride = Out.strides\n    Out_scale_batch_stride, Out_scale_head_stride, Out_scale_seq_stride = Out_scale.strides\n    Dest_loc_batch_stride, Dest_loc_head_stride, Dest_loc_seq_stride = Dest_loc.strides\n\n    # Define the grid dimensions\n    grid = (batch_size, num_heads, seq_len)\n\n    # Launch the kernel\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K.data_ptr(),  # Pointer to the input tensor K\n        Out.data_ptr(),  # Pointer to the output tensor Out\n        Out_scale.data_ptr(),  # Pointer to the output tensor Out_scale\n        Dest_loc.data_ptr(),  # Pointer to the destination indices tensor Dest_loc\n        K_batch_stride,  # Stride for batch dimension in K\n        K_head_stride,  # Stride for head dimension in K\n        K_seq_stride,  # Stride for sequence dimension in K\n        K_hidden_stride,  # Stride for hidden dimension in K\n        Out_batch_stride,  # Stride for batch dimension in Out\n        Out_head_stride,  # Stride for head dimension in Out\n        Out_seq_stride,  # Stride for sequence dimension in Out\n        Out_hidden_stride,  # Stride for hidden dimension in Out\n        Out_scale_batch_stride,  # Stride for batch dimension in Out_scale\n        Out_scale_head_stride,  # Stride for head dimension in Out_scale\n        Out_scale_seq_stride,  # Stride for sequence dimension in Out_scale\n        Dest_loc_batch_stride,  # Stride for batch dimension in Dest_loc\n        Dest_loc_head_stride,  # Stride for head dimension in Dest_loc\n        Dest_loc_seq_stride,  # Stride for sequence dimension in Dest_loc\n        batch_size,  # Batch size\n        num_heads,  # Number of heads\n        seq_len,  # Sequence length\n        hidden_dim,  # Hidden dimension\n        BLOCK_SIZE=32,  # Block size for parallelism\n    )\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K,\n    Dest_loc,\n    Out,\n    Out_scale,\n    stride_k_bs,\n    stride_k_h,\n    stride_k_d,\n    stride_o_bs,\n    stride_o_h,\n    stride_o_d,\n    stride_os_bs,\n    stride_os_h,\n    stride_os_d,\n    head_num,\n    head_dim,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(\n        K + cur_index * stride_k_bs + offs_h[:, None] * stride_k_h + stride_k_d * offs_d[None, :],\n        mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim),\n        other=0.0,\n    )\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.element_ty)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[:, None]\n    tl.store(o_ptrs, q_src_data, mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim))\n    tl.store(os_ptrs, data_scale, mask=(offs_h[:, None] < head_num))\n\n\n@torch.no_grad()\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K,\n        DestLoc,\n        Out,\n        Out_scale,\n        K.stride(0),\n        K.stride(1),\n        K.stride(2),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        Out_scale.stride(0),\n        Out_scale.stride(1),\n        Out_scale.stride(2),\n        head_num,\n        head_dim,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel `rotary_kernel` applies rotary positional embeddings to a tensor `X`. It computes the cosine and sine rotations for the specified rotary dimension, allowing for an efficient implementation of rotary position encoding in transformer models. The function `apply_rotary` serves as a Python interface to call the Triton kernel, configuring the execution parameters and managing input/output tensors. This setup allows it to handle both fixed and variable sequence lengths.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rotary_kernel(\n    X,  # Input tensor\n    cos,  # Cosine values\n    sin,  # Sine values\n    rotary_dim,  # Dimension to apply rotary embeddings\n    seq_len,  # Sequence length\n    stride,  # Stride for the input tensor\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelism\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < seq_len\n\n    x_ptrs = X + offsets * stride\n    cos_ptrs = cos + offsets * stride\n    sin_ptrs = sin + offsets * stride\n\n    x = tl.load(x_ptrs, mask=mask, other=0.0)\n    cos_val = tl.load(cos_ptrs, mask=mask, other=1.0)\n    sin_val = tl.load(sin_ptrs, mask=mask, other=0.0)\n\n    # Compute the rotary embeddings\n    x_rot = x * cos_val + tl.flip(x, 1) * sin_val\n\n    tl.store(x_ptrs, x_rot, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef apply_rotary(X, cos, sin, rotary_dim, seq_len, block_size=128):\n    # Ensure the input tensors are on the same device\n    assert X.device == cos.device == sin.device, \"Input tensors must be on the same device\"\n    device = X.device\n\n    # Ensure the input tensors have the correct shape\n    assert X.shape[1] == cos.shape[1] == sin.shape[1], \"Input tensors must have the same sequence length\"\n    assert X.shape[2] == cos.shape[2] == sin.shape[2], \"Input tensors must have the same rotary dimension\"\n\n    # Compute the number of blocks\n    num_blocks = (seq_len + block_size - 1) // block_size\n\n    # Define the grid and block configuration\n    grid = (num_blocks,)\n\n    # Launch the Triton kernel\n    rotary_kernel[grid](\n        X, cos, sin, rotary_dim, seq_len, X.stride(0), BLOCK_SIZE=block_size\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input tensor\n    X = torch.randn(1, 128, 64, device='cuda')\n    seq_len = X.shape[1]\n    rotary_dim = X.shape[2]\n\n    # Example cosine and sine values\n    cos = torch.cos(torch.linspace(0, 1, rotary_dim, device='cuda')).unsqueeze(0).expand(1, seq_len, -1)\n    sin = torch.sin(torch.linspace(0, 1, rotary_dim, device='cuda')).unsqueeze(0).expand(1, seq_len, -1)\n\n    # Apply rotary positional embeddings\n    apply_rotary(X, cos, sin, rotary_dim, seq_len)\n\n    print(X)\n", "label": "from typing import Optional, Union\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rotary_kernel(\n    OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen, nheads, rotary_dim, seqlen_ro,\n    CACHE_KEY_SEQLEN, stride_out_batch, stride_out_seqlen, stride_out_nheads,\n    stride_out_headdim, stride_x_batch, stride_x_seqlen, stride_x_nheads,\n    stride_x_headdim, BLOCK_K: tl.constexpr, IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr, INTERLEAVED: tl.constexpr, CONJUGATE: tl.constexpr, BLOCK_M: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads\n\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + ((rk + 1) % 2) * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(tl.float32)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))\n\ndef apply_rotary(\n    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, seqlen_offsets: Union[int, torch.Tensor] = 0,\n    cu_seqlens: Optional[torch.Tensor] = None, max_seqlen: Optional[int] = None,\n    interleaved=False, inplace=False, conjugate=False\n) -> torch.Tensor:\n    is_varlen = cu_seqlens is not None\n    if not is_varlen:\n        batch, seqlen, nheads, headdim = x.shape\n    else:\n        total_seqlen, nheads, headdim = x.shape\n        batch = cu_seqlens.shape[0] - 1\n        seqlen = max_seqlen\n    seqlen_ro, rotary_dim = cos.shape\n    rotary_dim *= 2\n\n    cos, sin = cos.contiguous(), sin.contiguous()\n    if isinstance(seqlen_offsets, torch.Tensor):\n        seqlen_offsets = seqlen_offsets.contiguous()\n    else:\n        seqlen_offsets += seqlen\n\n    output = torch.empty_like(x) if not inplace else x\n    if rotary_dim < headdim and not inplace:\n        output[..., rotary_dim:].copy_(x[..., rotary_dim:])\n\n    BLOCK_K = 32 if rotary_dim <= 32 else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\n    grid = lambda META: (triton.cdiv(seqlen, META[\"BLOCK_M\"]), batch, nheads)\n    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 64 else 4)\n\n    with torch.cuda.device(x.device.index):\n        rotary_kernel[grid](\n            output, x, cos, sin, cu_seqlens, seqlen_offsets, seqlen, nheads, rotary_dim,\n            seqlen_ro, seqlen // 128, output.stride(0) if not is_varlen else 0, output.stride(-3),\n            output.stride(-2), output.stride(-1), x.stride(0) if not is_varlen else 0,\n            x.stride(-3), x.stride(-2), x.stride(-1), BLOCK_K,\n            isinstance(seqlen_offsets, torch.Tensor), is_varlen, interleaved, conjugate, BLOCK_M\n        )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel, named `_fwd_kernel_apply_penalty`, adjusts the logits for each batch based on specified penalties: presence, frequency, and repetition. The kernel takes various input tensors such as `Logits`, penalty coefficients, token IDs, token counts, and sequence lengths. The main logic involves loading batch-specific penalties, applying them to logits using arithmetic operations, and storing the results back in the logits tensor. The Python function `apply_penalty` configures and calls this kernel with appropriate parameters, including calculating the block size based on maximum sequence length in the batch.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_apply_penalty(\n    logits_ptr,  # Pointer to the logits tensor\n    presence_penalty_ptr,  # Pointer to the presence penalty tensor\n    frequency_penalty_ptr,  # Pointer to the frequency penalty tensor\n    repetition_penalty_ptr,  # Pointer to the repetition penalty tensor\n    token_ids_ptr,  # Pointer to the token IDs tensor\n    token_counts_ptr,  # Pointer to the token counts tensor\n    sequence_lengths_ptr,  # Pointer to the sequence lengths tensor\n    batch_size,  # Number of batches\n    vocab_size,  # Vocabulary size\n    BLOCK_SIZE: tl.constexpr  # Block size\n):\n    # Get the batch index\n    batch_idx = tl.program_id(0)\n    if batch_idx >= batch_size:\n        return\n\n    # Load the sequence length for this batch\n    seq_len = tl.load(sequence_lengths_ptr + batch_idx)\n\n    # Load the penalties for this batch\n    presence_penalty = tl.load(presence_penalty_ptr + batch_idx)\n    frequency_penalty = tl.load(frequency_penalty_ptr + batch_idx)\n    repetition_penalty = tl.load(repetition_penalty_ptr + batch_idx)\n\n    # Compute the range of tokens for this batch\n    token_start = batch_idx * seq_len\n    token_end = token_start + seq_len\n\n    # Iterate over the tokens in this batch\n    for token_idx in range(token_start, token_end, BLOCK_SIZE):\n        # Compute the block of tokens to process\n        block_start = token_idx\n        block_end = min(token_idx + BLOCK_SIZE, token_end)\n\n        # Load the token IDs and token counts for this block\n        token_ids = tl.load(token_ids_ptr + block_start, mask=block_start < block_end, other=0)\n        token_counts = tl.load(token_counts_ptr + block_start, mask=block_start < block_end, other=0)\n\n        # Compute the penalties for each token in the block\n        for i in range(block_end - block_start):\n            token_id = token_ids[i]\n            token_count = token_counts[i]\n\n            # Apply presence penalty\n            if token_count > 0:\n                logits_ptr[batch_idx * vocab_size + token_id] -= presence_penalty\n\n            # Apply frequency penalty\n            logits_ptr[batch_idx * vocab_size + token_id] -= frequency_penalty * token_count\n\n            # Apply repetition penalty\n            if token_count > 0:\n                logits_ptr[batch_idx * vocab_size + token_id] /= repetition_penalty\n\n    # Synchronize to ensure all penalties are applied\n    tl.device_barrier()\n\nimport torch\n\ndef apply_penalty(\n    logits: torch.Tensor,\n    presence_penalty: torch.Tensor,\n    frequency_penalty: torch.Tensor,\n    repetition_penalty: torch.Tensor,\n    token_ids: torch.Tensor,\n    token_counts: torch.Tensor,\n    sequence_lengths: torch.Tensor\n):\n    # Ensure all tensors are on the same device\n    device = logits.device\n    assert all(t.device == device for t in [presence_penalty, frequency_penalty, repetition_penalty, token_ids, token_counts, sequence_lengths])\n\n    # Get the batch size and vocabulary size\n    batch_size = logits.size(0)\n    vocab_size = logits.size(1)\n\n    # Calculate the block size based on the maximum sequence length in the batch\n    max_seq_len = sequence_lengths.max().item()\n    block_size = 128  # Adjust this based on your GPU and problem size\n\n    # Configure the grid and block dimensions\n    grid = (batch_size, 1, 1)\n    block = (block_size, 1, 1)\n\n    # Launch the kernel\n    _fwd_kernel_apply_penalty[grid, block](\n        logits.data_ptr(),\n        presence_penalty.data_ptr(),\n        frequency_penalty.data_ptr(),\n        repetition_penalty.data_ptr(),\n        token_ids.data_ptr(),\n        token_counts.data_ptr(),\n        sequence_lengths.data_ptr(),\n        batch_size,\n        vocab_size,\n        block_size\n    )\n\n    return logits\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_apply_penalty(\n    Logits, presence_penalty, freqency_penalty, repetition_penalty,\n    p_token_ids, p_token_counts, p_cumsum_seq_len, \n    stride_logit_b, stride_logit_s,\n    BLOCK_P: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_repetition = tl.load(repetition_penalty + cur_batch)\n\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=cur_batch_id_offset<cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=cur_batch_id_offset<cur_batch_end_index, other=0)\n    \n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset<cur_batch_end_index, other=0.0)\n    rep_logits = tl.where(cur_logits > 0, cur_logits / cur_repetition, cur_logits * cur_repetition)\n    freq_logits = rep_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset<cur_batch_end_index)\n\n    return\n\n@torch.no_grad()\ndef apply_penalty(Logits, presence_penalty, freqency_penalty, repetition_penalty, p_token_ids, p_token_counts, p_cumsum_seq_len, p_max_len_in_batch):\n    assert Logits.is_contiguous()\n    BLOCK = triton.next_power_of_2(p_max_len_in_batch)\n    if BLOCK <= 512:\n        BLOCK = 512\n    elif BLOCK <= 1024:\n        BLOCK = 1024\n    num_warps = 8\n    _fwd_kernel_apply_penalty[(Logits.shape[0], )](\n        Logits, presence_penalty, freqency_penalty, repetition_penalty,\n        p_token_ids, p_token_counts, p_cumsum_seq_len,\n        Logits.stride(0), Logits.stride(1),\n        num_warps=num_warps,\n        BLOCK_P=BLOCK\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided Triton code implements a custom kernel named `chunk_delta_rule_fwd_kernel_h` and a wrapper function `chunk_fwd_h_fn` for processing tensor data in a highly parallel manner. The kernel performs computations involving matrices `k`, `v`, `d` to produce an output `v_new` while optionally storing intermediate results in `initial_state` and `final_state`. The kernel is designed to handle batch processing of inputs using block pointers and takes advantage of Triton's grid and block mechanisms to operate efficiently across multiple dimensions.\n\n    Key inputs for the kernel include: \n    - `k`, `v`, `d`: Input matrices representing different dimensions and data.\n    - `v_new`: Output matrix to store the updated values after processing.\n    - `h`: Intermediate storage for cumulative sum calculations.\n    - `initial_state` and `final_state`: Optional states for storing data across iterations.\n\n    The kernel is configured for different numbers of warps using `triton.autotune` to optimize performance based on the input size. The main processing loop iterates over a time dimension (`NT`) and computes block-wise operations involving dot products and cumulative sums using Triton's block pointer mechanics.\n\n    The function `chunk_fwd_h_fn` serves as a wrapper for setting up kernel execution. It initializes output tensors, calculates grid and block sizes based on input dimensions, and invokes the Triton kernel with appropriate parameters. This function handles batching and reshaping of input data for parallel processing.\n\n    Both the kernel and the wrapper function heavily utilize Triton features such as `tl.program_id` for grid configuration and `tl.make_block_ptr` for efficient memory access, enabling high performance on GPU architectures.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(\n    k_ptr, v_ptr, d_ptr, v_new_ptr, h_ptr, initial_state_ptr, final_state_ptr,\n    K_BLOCK_SIZE: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, D_BLOCK_SIZE: tl.constexpr,\n    H_BLOCK_SIZE: tl.constexpr, NT: tl.constexpr, B: tl.constexpr, H: tl.constexpr,\n    S: tl.constexpr, C: tl.constexpr, store_initial_state: tl.constexpr, store_final_state: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_blocks = (B * H * S) // (K_BLOCK_SIZE * V_BLOCK_SIZE * D_BLOCK_SIZE * H_BLOCK_SIZE)\n    if pid >= num_blocks:\n        return\n\n    # Compute block indices\n    b_idx = pid // (H * S)\n    h_idx = (pid % (H * S)) // S\n    s_idx = (pid % (H * S)) % S\n\n    # Initialize block pointers\n    k_block_ptr = tl.make_block_ptr(\n        base=k_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * K_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(K_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n    v_block_ptr = tl.make_block_ptr(\n        base=v_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * V_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(V_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n    d_block_ptr = tl.make_block_ptr(\n        base=d_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * D_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(D_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n    v_new_block_ptr = tl.make_block_ptr(\n        base=v_new_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * V_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(V_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n    h_block_ptr = tl.make_block_ptr(\n        base=h_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * H_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n    initial_state_block_ptr = tl.make_block_ptr(\n        base=initial_state_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * H_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n    final_state_block_ptr = tl.make_block_ptr(\n        base=final_state_ptr, shape=(B, H, S, C), strides=(H * S * C, S * C, C, 1),\n        offsets=(b_idx * H_BLOCK_SIZE, h_idx * H_BLOCK_SIZE, s_idx * H_BLOCK_SIZE, 0),\n        block_shape=(H_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C),\n        order=(0, 1, 2, 3)\n    )\n\n    # Load initial state if needed\n    if store_initial_state:\n        initial_state = tl.load(initial_state_block_ptr)\n    else:\n        initial_state = tl.zeros((H_BLOCK_SIZE, H_BLOCK_SIZE, H_BLOCK_SIZE, C), dtype=tl.float32)\n\n    # Main loop over time dimension\n    for t in range(NT):\n        k = tl.load(k_block_ptr)\n        v = tl.load(v_block_ptr)\n        d = tl.load(d_block_ptr)\n\n        # Compute dot products and cumulative sums\n        h = tl.dot(k, v) + initial_state\n        v_new = h * d\n\n        # Store results\n        tl.store(h_block_ptr, h)\n        tl.store(v_new_block_ptr, v_new)\n\n        # Update initial state for next iteration\n        initial_state = h\n\n    # Store final state if needed\n    if store_final_state:\n        tl.store(final_state_block_ptr, initial_state)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef chunk_fwd_h_fn(k, v, d, v_new, h, initial_state, final_state, NT, B, H, S, C, store_initial_state, store_final_state):\n    # Initialize output tensors\n    v_new.zero_()\n    h.zero_()\n    if store_initial_state:\n        initial_state.zero_()\n    if store_final_state:\n        final_state.zero_()\n\n    # Calculate grid and block sizes\n    K_BLOCK_SIZE = 16\n    V_BLOCK_SIZE = 16\n    D_BLOCK_SIZE = 16\n    H_BLOCK_SIZE = 16\n    grid = (B * H * S) // (K_BLOCK_SIZE * V_BLOCK_SIZE * D_BLOCK_SIZE * H_BLOCK_SIZE)\n\n    # Invoke the Triton kernel\n    chunk_delta_rule_fwd_kernel_h[grid](\n        k, v, d, v_new, h, initial_state, final_state,\n        K_BLOCK_SIZE, V_BLOCK_SIZE, D_BLOCK_SIZE, H_BLOCK_SIZE, NT, B, H, S, C,\n        store_initial_state, store_final_state\n    )\n\n    return v_new, h, initial_state, final_state\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"BT\", \"BK\", \"BV\"], \n)\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(\n    k,\n    v,\n    d, \n    v_new,\n    h,\n    initial_state,\n    final_state,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_h_cumsum = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC)):\n            p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n            p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n            p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            p_v_new = tl.make_block_ptr(v_new + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))   \n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_v -= tl.dot(b_d, b_h.to(b_k.dtype), allow_tf32=False)\n            tl.store(p_v_new, b_v.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n            b_h_cumsum += tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)\n        b_h += b_h_cumsum      \n        \n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\ndef chunk_fwd_h_fn(k, w, u, BT, initial_state, final_state):\n    B, H, T, K, V = *k.shape, u.shape[-1]\n\n    BK = triton.next_power_of_2(K)\n    assert BK <= 256, \"current kernel does not support head dimension larger than 256.\"\n    BV = 16 if BK > 128 else 32        \n    BV = 64 if BK <= 64 else BV\n    BC = 16 if BK > 128 else 32 \n    BC = 64 if BK <= 64 else BC\n    BC = min(BT, BC)\n    NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n    assert NK == 1, 'NK > 1 is not supported because it involves time-consuming synchronization'\n\n    h = k.new_empty(B, H, NT * K, V)\n    grid = (NK, NV, B * H)\n    v_new = torch.empty_like(u)\n    chunk_delta_rule_fwd_kernel_h[grid](\n        k, u, w, v_new, h, initial_state, final_state,\n        k.stride(1), k.stride(2), k.stride(3),\n        u.stride(1), u.stride(2), u.stride(3),\n        h.stride(1), h.stride(2),\n        H=H, T=T, K=K, V=V, BT=BT, BC=BC, BK=BK, BV=BV, NT=NT,\n        USE_INITIAL_STATE=initial_state is not None,\n        STORE_FINAL_STATE=final_state is not None,\n        )\n    return h, v_new\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided code defines a Triton kernel for matrix multiplication. The `matmul_kernel` function performs matrix multiplication on input matrices A and B, storing the result in matrix C. It supports an optional activation function, specifically 'leaky_relu'. The kernel is designed to efficiently compute the product by iterating over blocks of A and B, using blocks of size `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`. The `matmul` function is a wrapper around `matmul_kernel`, handling input validation, setting up the grid for execution, and calling the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the matrix multiplication kernel\n@triton.jit\ndef matmul_kernel(\n    A, B, C, \n    stride_a_row, stride_a_col, \n    stride_b_row, stride_b_col, \n    stride_c_row, stride_c_col, \n    M, N, K, \n    use_leaky_relu: tl.constexpr, \n    alpha: tl.float32, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr\n):\n    # Compute the program ID in a 2D grid\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_warp = num_pid_m * num_pid_n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    # Compute the block offsets\n    rm = pid_m * BLOCK_SIZE_M\n    rn = pid_n * BLOCK_SIZE_N\n\n    # Compute the block bounds\n    rm_bound = tl.minimum(rm + BLOCK_SIZE_M, M)\n    rn_bound = tl.minimum(rn + BLOCK_SIZE_N, N)\n\n    # Initialize the accumulator\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the A and B blocks\n        a_tile = tl.load(A + rm * stride_a_row + k * stride_a_col, mask=rm < M and k < K, other=0.0)\n        b_tile = tl.load(B + k * stride_b_row + rn * stride_b_col, mask=k < K and rn < N, other=0.0)\n\n        # Perform the matrix multiplication\n        accumulator += tl.dot(a_tile, b_tile)\n\n    # Apply the leaky ReLU activation if requested\n    if use_leaky_relu:\n        accumulator = tl.where(accumulator > 0, accumulator, accumulator * alpha)\n\n    # Store the result in the output matrix C\n    tl.store(C + rm * stride_c_row + rn * stride_c_col, accumulator, mask=rm < M and rn < N)\n\n# Define the wrapper function for the kernel\ndef matmul(A, B, C, use_leaky_relu=False, alpha=0.01, BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=16):\n    # Validate the input shapes\n    M, K = A.shape\n    K, N = B.shape\n    assert C.shape == (M, N), \"Output matrix C must have shape (M, N)\"\n    assert A.dtype == B.dtype == C.dtype, \"Input matrices A, B, and C must have the same data type\"\n    assert A.dtype in [tl.float32, tl.float16], \"Input matrices must be of type float32 or float16\"\n\n    # Set up the grid for execution\n    grid = (tl.cdiv(M, BLOCK_SIZE_M) * tl.cdiv(N, BLOCK_SIZE_N),)\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        A, B, C, \n        A.stride(0), A.stride(1), \n        B.stride(0), B.stride(1), \n        C.stride(0), C.stride(1), \n        M, N, K, \n        use_leaky_relu, \n        alpha, \n        BLOCK_SIZE_M, \n        BLOCK_SIZE_N, \n        BLOCK_SIZE_K\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_M: tl.constexpr,\n        ACTIVATION: tl.constexpr\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # Create pointers for the first blocks of A and B\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Compute a block of the C matrix\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Apply activation function if specified\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    \n    c = accumulator.to(tl.float16)\n\n    # Write back the block of the output matrix C with masks\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n\ndef matmul(a, b, activation=\"\"):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32, BLOCK_SIZE_K=32,\n        GROUP_SIZE_M=4,\n        ACTIVATION=activation\n    )\n    return c\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton kernel `_fwd_kernel` performs the forward pass of an attention mechanism, similar to the scaled dot-product attention often used in transformer models. The kernel computes the output based on queries (Q), keys (K), and values (V), taking into account a scaling factor (`sm_scale`) and a potential causal mask (`IS_CAUSAL`). The kernel also supports flexible data types and stride formats. A key feature is the ability to manage different block sizes (`BLOCK_M`, `BLOCK_N`, `BLOCK_DMODEL`) and float precision using `USE_FP8`. The wrapper function `triton_fa` manages the setup of this kernel, preparing inputs and configuring execution parameters like grid size and warps.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, Out,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn,\n    stride_ob, stride_oh, stride_om,\n    nheads, N_CTX,\n    sm_scale: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    USE_FP8: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(N_CTX, BLOCK_M)\n    num_pid_n = tl.cdiv(N_CTX, BLOCK_N)\n    num_pid_in_block = num_pid_m * num_pid_n\n    block_id = pid // num_pid_in_block\n    pid_mn = pid % num_pid_in_block\n    pid_m = pid_mn // num_pid_n\n    pid_n = pid_mn % num_pid_n\n\n    offs_qm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + block_id * stride_qb + offs_qm[:, None] * stride_qm + offs_m[None, :] * stride_qh\n    k_ptrs = K + block_id * stride_kb + offs_n[:, None] * stride_kn + offs_m[None, :] * stride_kh\n    v_ptrs = V + block_id * stride_vb + offs_n[:, None] * stride_vn + offs_m[None, :] * stride_vh\n    o_ptrs = Out + block_id * stride_ob + offs_qm[:, None] * stride_om + offs_n[None, :] * stride_oh\n\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(0, BLOCK_DMODEL, 16):\n        q_i = q[:, i:i+16]\n        k_i = k[:, i:i+16]\n        acc += tl.dot(q_i, k_i, trans_b=True)\n\n    if IS_CAUSAL:\n        mask = offs_qm[:, None] >= (offs_n[None, :] + 1)\n        acc = tl.where(mask, acc, float('-inf'))\n\n    acc *= sm_scale\n    acc = tl.softmax(acc, axis=1)\n    acc = acc.to(USE_FP8)\n\n    o = tl.dot(acc, v)\n    tl.store(o_ptrs, o)\n\nimport triton\nimport triton.language as tl\n\ndef triton_fa(Q, K, V, Out, sm_scale, IS_CAUSAL, BLOCK_M, BLOCK_N, BLOCK_DMODEL, USE_FP8):\n    # Extract shapes and strides\n    B, H, N_CTX, D = Q.shape\n    stride_qb = Q.stride(0)\n    stride_qh = Q.stride(1)\n    stride_qm = Q.stride(2)\n    stride_kb = K.stride(0)\n    stride_kh = K.stride(1)\n    stride_kn = K.stride(2)\n    stride_vb = V.stride(0)\n    stride_vh = V.stride(1)\n    stride_vn = V.stride(2)\n    stride_ob = Out.stride(0)\n    stride_oh = Out.stride(1)\n    stride_om = Out.stride(2)\n\n    # Define grid and block sizes\n    grid = (B * H * (N_CTX // BLOCK_M) * (N_CTX // BLOCK_N),)\n\n    # Launch the kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out,\n        stride_qb, stride_qh, stride_qm,\n        stride_kb, stride_kh, stride_kn,\n        stride_vb, stride_vh, stride_vn,\n        stride_ob, stride_oh, stride_om,\n        H, N_CTX,\n        sm_scale, IS_CAUSAL,\n        BLOCK_M, BLOCK_N, BLOCK_DMODEL, USE_FP8\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale,\n    Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on,\n    N_HEAD, H, N_CTX,\n    start_position,  # <- ADDED\n    IS_CAUSAL: tl.constexpr,  # <- ADDED\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    USE_FP8: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n\n    head_idx = tl.program_id(1)\n    batch_id = head_idx // N_HEAD\n    off_hz = head_idx % N_HEAD\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = batch_id * stride_qz + off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk  # <- stride fixed\n    off_k = batch_id * stride_kz + off_hz * stride_kh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk  # <- stride fixed\n    off_v = batch_id * stride_vz + off_hz * stride_vh + offs_n[:, None] * stride_vk + offs_d[None, :] * stride_vn  # <- stride fixed\n    # Initialize pointers to Q, K, V\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    # initialize pointer to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    # load q: it will stay in SRAM throughout\n    q = tl.load(q_ptrs, offs_m[:, None] < H, other=0.0)\n    # loop over k, v and update accumulator\n    block_n_end = N_CTX  # <- ADDED (including the IF)\n    if IS_CAUSAL:\n        # in causal mode, we expect that BLOCK_M_SIZE == BLOCK_N_SIZE\n        # autotune will prune shapes not matching this rule\n        block_n_end = (start_m + 1) * BLOCK_N + start_position\n    for start_n in range(0, block_n_end, BLOCK_N):\n        block_n_offs = start_n + offs_n  # <- ADDED\n        # -- compute qk ----\n        k = tl.load(k_ptrs, block_n_offs[:, None] < N_CTX, 0.)\n        if USE_FP8:\n            k = k.to(tl.float8e5, bitcast=True)\n            k = k.to(tl.float16)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk = tl.where(offs_n[None, :] < N_CTX, qk, float(\"-inf\"))  # <- ADDED\n        qk *= sm_scale\n        if IS_CAUSAL:  # <- ADDED\n            qk = tl.where(offs_m[:, None] >= (block_n_offs[None, :] + start_position), qk, float(\"-inf\"))\n            \n        # compute new m\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        # correct old l\n        l_prev *= tl.exp(m_prev - m_curr)\n        # attention weights\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        # rescale operands of matmuls\n        l_rcp = 1. / l_curr\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None]\n        # update acc\n        p = p.to(Q.dtype.element_ty)\n        v = tl.load(v_ptrs, block_n_offs[:, None] < N_CTX, 0.0)\n        if USE_FP8:\n            v = v.to(tl.float8e5, bitcast=True)\n            v = v.to(tl.float16)\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n        # update pointers\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vk\n    # rematerialize offsets to save registers\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    # initialize pointers to output\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_o = batch_id * stride_oz + off_hz * stride_oh + offs_m[:, None] * stride_om + offs_d[None, :] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, offs_m[:, None] < H)\n\n\ndef triton_fa(q, k, v, sm_scale, is_causal, start_position):\n    assert q.dtype == torch.float16\n    assert k.dtype == v.dtype and k.dtype in [torch.float16, torch.int8]\n\n    BLOCK = 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n    o = torch.empty_like(q)\n    num_warps = 4 if Lk <= 64 else 8\n    batch, head_size, m_size, dhead = q.size()\n    grid = (triton.cdiv(m_size, BLOCK), head_size * batch)\n    n_size = k.size(2)\n    _fwd_kernel[grid](\n        q, k, v, sm_scale,\n        o,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n        head_size, m_size, n_size,\n        start_position=start_position,\n        IS_CAUSAL=is_causal,\n        BLOCK_M=BLOCK,\n        BLOCK_N=BLOCK,\n        BLOCK_DMODEL=Lk,\n        USE_FP8=k.dtype == torch.int8,  # USE_FP8\n        num_warps=num_warps,\n        num_stages=2,\n    )\n\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel function and a corresponding Python wrapper to perform an operation involving index-based copying of a tensor. The Triton kernel `_fwd_kernel_destindex_copy_kv` takes three main tensors: `K`, `Dest_loc`, and `Out`. `K` contains the input data, `Dest_loc` specifies destination indices, and `Out` stores the result. The kernel iterates over `seq_len`, determined by the length of `Dest_loc`. It loads data from `K` into `Out` based on indices from `Dest_loc`, using a head dimension loop governed by `head_num`. Key parameters include strides for navigating tensors and block sizes for dimensions, which are optimized for GPU processing. The wrapper function `destindex_copy_kv` sets up the kernel launch, determining grid size and checking shape consistency.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    K_ptr,  # Pointer to the input tensor K\n    Dest_loc_ptr,  # Pointer to the destination indices tensor\n    Out_ptr,  # Pointer to the output tensor\n    seq_len,  # Length of the sequence (length of Dest_loc)\n    head_num,  # Number of heads\n    K_batch_stride,  # Stride for batch dimension in K\n    K_head_stride,  # Stride for head dimension in K\n    K_seq_stride,  # Stride for sequence dimension in K\n    K_hidden_stride,  # Stride for hidden dimension in K\n    Dest_loc_batch_stride,  # Stride for batch dimension in Dest_loc\n    Dest_loc_head_stride,  # Stride for head dimension in Dest_loc\n    Dest_loc_seq_stride,  # Stride for sequence dimension in Dest_loc\n    Out_batch_stride,  # Stride for batch dimension in Out\n    Out_head_stride,  # Stride for head dimension in Out\n    Out_seq_stride,  # Stride for sequence dimension in Out\n    Out_hidden_stride,  # Stride for hidden dimension in Out\n    BLOCK_SEQ: tl.constexpr,  # Block size for sequence dimension\n    BLOCK_HEAD: tl.constexpr,  # Block size for head dimension\n):\n    # Get the current block's indices\n    pid = tl.program_id(axis=0)\n    num_blocks_seq = (seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ\n    num_blocks_head = (head_num + BLOCK_HEAD - 1) // BLOCK_HEAD\n    head_id = pid % num_blocks_head\n    seq_id = pid // num_blocks_head\n\n    # Compute the starting indices for the current block\n    head_start = head_id * BLOCK_HEAD\n    seq_start = seq_id * BLOCK_SEQ\n\n    # Load the destination indices for the current block\n    dest_indices = tl.load(Dest_loc_ptr + seq_start * Dest_loc_seq_stride + head_start * Dest_loc_head_stride + tl.arange(0, BLOCK_SEQ))\n\n    # Iterate over the head and sequence dimensions\n    for h in range(head_start, min(head_start + BLOCK_HEAD, head_num)):\n        for s in range(seq_start, min(seq_start + BLOCK_SEQ, seq_len)):\n            # Load the source data from K\n            k_val = tl.load(K_ptr + s * K_seq_stride + h * K_head_stride + tl.arange(0, K_hidden_stride))\n\n            # Compute the destination index\n            dest_index = dest_indices[s - seq_start]\n\n            # Store the data in the output tensor\n            tl.store(Out_ptr + dest_index * Out_seq_stride + h * Out_head_stride + tl.arange(0, Out_hidden_stride), k_val)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef destindex_copy_kv(K, Dest_loc, Out):\n    # Check shape consistency\n    assert K.shape == Out.shape, \"Input tensor K and output tensor Out must have the same shape\"\n    assert Dest_loc.shape[0] == K.shape[1], \"Length of Dest_loc must match the sequence length of K\"\n\n    # Get the dimensions\n    batch_size, seq_len, head_num, hidden_dim = K.shape\n\n    # Get the strides\n    K_batch_stride = K.stride(0)\n    K_head_stride = K.stride(2)\n    K_seq_stride = K.stride(1)\n    K_hidden_stride = K.stride(3)\n\n    Dest_loc_batch_stride = Dest_loc.stride(0)\n    Dest_loc_head_stride = Dest_loc.stride(1)\n    Dest_loc_seq_stride = Dest_loc.stride(2)\n\n    Out_batch_stride = Out.stride(0)\n    Out_head_stride = Out.stride(2)\n    Out_seq_stride = Out.stride(1)\n    Out_hidden_stride = Out.stride(3)\n\n    # Define block sizes\n    BLOCK_SEQ = 128\n    BLOCK_HEAD = 8\n\n    # Compute grid size\n    grid = (seq_len * head_num // (BLOCK_SEQ * BLOCK_HEAD),)\n\n    # Launch the kernel\n    _fwd_kernel_destindex_copy_kv[grid](\n        K,  # Pointer to the input tensor K\n        Dest_loc,  # Pointer to the destination indices tensor\n        Out,  # Pointer to the output tensor\n        seq_len,  # Length of the sequence (length of Dest_loc)\n        head_num,  # Number of heads\n        K_batch_stride,  # Stride for batch dimension in K\n        K_head_stride,  # Stride for head dimension in K\n        K_seq_stride,  # Stride for sequence dimension in K\n        K_hidden_stride,  # Stride for hidden dimension in K\n        Dest_loc_batch_stride,  # Stride for batch dimension in Dest_loc\n        Dest_loc_head_stride,  # Stride for head dimension in Dest_loc\n        Dest_loc_seq_stride,  # Stride for sequence dimension in Dest_loc\n        Out_batch_stride,  # Stride for batch dimension in Out\n        Out_head_stride,  # Stride for head dimension in Out\n        Out_seq_stride,  # Stride for sequence dimension in Out\n        Out_hidden_stride,  # Stride for hidden dimension in Out\n        BLOCK_SEQ,  # Block size for sequence dimension\n        BLOCK_HEAD,  # Block size for head dimension\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create example tensors\n    batch_size = 2\n    seq_len = 1024\n    head_num = 16\n    hidden_dim = 64\n\n    K = torch.randn((batch_size, seq_len, head_num, hidden_dim), device=\"cuda\")\n    Dest_loc = torch.randint(0, seq_len, (batch_size, head_num, seq_len), device=\"cuda\")\n    Out = torch.zeros((batch_size, seq_len, head_num, hidden_dim), device=\"cuda\")\n\n    # Call the wrapper function\n    destindex_copy_kv(K, Dest_loc, Out)\n\n    # Verify the result (for simplicity, just print the first element of each tensor)\n    print(\"K[0, 0, 0, :10]:\", K[0, 0, 0, :10])\n    print(\"Dest_loc[0, 0, 0]:\", Dest_loc[0, 0, 0])\n    print(\"Out[0, 0, 0, :10]:\", Out[0, 0, 0, :10])\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    K, Dest_loc,\n    Out,\n    stride_k_bs, stride_k_h, stride_k_d,\n    stride_o_bs, stride_o_h, stride_o_d,\n    head_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_kv(K, DestLoc, Out):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        K, DestLoc, Out,\n        K.stride(0), K.stride(1), K.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        head_num,\n        BLOCK_DMODEL=head_dim,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator defines a kernel `_fwd_kernel` used for attention computation. The kernel takes input tensors Q, K, V and outputs the result to Out. It uses a blocking strategy with block size BLOCK_M, BLOCK_DMODEL, and BLOCK_N to optimize memory usage and parallelism. The context_attention_fwd function prepares the arguments and launches the kernel, setting the grid size based on input dimensions. It enforces head dimension Lk constraints and uses scales computed from Q's dimension to adjust the dot product computations within the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, Out,\n    stride_qm, stride_qk,\n    stride_km, stride_kk,\n    stride_vm, stride_vk,\n    stride_om, stride_on,\n    Lk: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr\n):\n    # Program ID\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(Q.shape[0], BLOCK_M)\n    num_pid_n = tl.cdiv(V.shape[1], BLOCK_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) // num_pid_m\n\n    # Compute the offsets for Q, K, V\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    k_ptrs = K + (offs_k[:, None] * stride_kk + offs_n[None, :] * stride_km)\n    v_ptrs = V + (offs_m[:, None] * stride_vm + offs_n[None, :] * stride_vk)\n\n    # Initialize the output and scale\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    scale = 1.0 / (Lk ** 0.5)\n\n    # Compute the dot product\n    for k in range(0, Lk, BLOCK_DMODEL):\n        q = tl.load(q_ptrs)\n        k = tl.load(k_ptrs)\n        acc += tl.dot(q, k, allow_tf32=True) * scale\n        q_ptrs += BLOCK_DMODEL * stride_qk\n        k_ptrs += BLOCK_DMODEL * stride_km\n\n    # Compute the softmax\n    acc = tl.softmax(acc, axis=1)\n\n    # Compute the output\n    out_ptrs = Out + (offs_m[:, None] * stride_om + offs_n[None, :] * stride_on)\n    for k in range(0, Lk, BLOCK_DMODEL):\n        v = tl.load(v_ptrs)\n        acc = tl.dot(acc, v, allow_tf32=True)\n        v_ptrs += BLOCK_DMODEL * stride_vk\n\n    # Store the result\n    tl.store(out_ptrs, acc)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef context_attention_fwd(Q, K, V, Out, BLOCK_M=128, BLOCK_DMODEL=64, BLOCK_N=64):\n    # Ensure the head dimension Lk is a multiple of BLOCK_DMODEL\n    Lk = Q.shape[1]\n    assert Lk % BLOCK_DMODEL == 0, \"Lk must be a multiple of BLOCK_DMODEL\"\n\n    # Compute the grid size\n    grid = (triton.cdiv(Q.shape[0], BLOCK_M) * triton.cdiv(V.shape[1], BLOCK_N),)\n\n    # Launch the kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1),\n        K.stride(0), K.stride(1),\n        V.stride(0), V.stride(1),\n        Out.stride(0), Out.stride(1),\n        Lk, BLOCK_M, BLOCK_DMODEL, BLOCK_N\n    )\n\n# Example usage\nQ = torch.randn(1024, 64, device='cuda')\nK = torch.randn(1024, 64, device='cuda')\nV = torch.randn(1024, 64, device='cuda')\nOut = torch.empty_like(Q)\n\ncontext_attention_fwd(Q, K, V, Out)\n\nprint(Out)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\nif triton.__version__ >= \"2.1.0\":\n    @triton.jit\n    def _fwd_kernel(\n        Q, K, V, sm_scale, B_Start_Loc, B_Seqlen,  # B_LOC \u5185\u90e8\u8bb0\u5f55\u6bcf\u4e2abatch \u8f93\u5165\u7684\u771f\u5b9e\u4f4d\u7f6e\uff0c B_SEQ_len \u8bb0\u5f55\u5f53\u524d\u8f93\u5165\u7684\u771f\u5b9e\u957f\u5ea6\n        Out,\n        stride_qbs, stride_qh, stride_qd,\n        stride_kbs, stride_kh, stride_kd,\n        stride_vbs, stride_vh, stride_vd,\n        stride_obs, stride_oh, stride_od,\n        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n        BLOCK_N: tl.constexpr,\n    ):\n        cur_batch = tl.program_id(0)\n        cur_head = tl.program_id(1)\n        start_m = tl.program_id(2)\n\n        cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n        cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n        block_start_loc = BLOCK_M * start_m\n\n        # initialize offsets\n        offs_n = tl.arange(0, BLOCK_N)\n        offs_d = tl.arange(0, BLOCK_DMODEL)\n        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        off_q = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n        off_k = offs_n[None, :] * stride_kbs + cur_head * stride_kh + offs_d[:, None] * stride_kd\n        off_v = offs_n[:, None] * stride_vbs + cur_head * stride_vh + offs_d[None, :] * stride_vd\n\n        q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n        k_ptrs = K + off_k\n        v_ptrs = V + off_v\n\n        # initialize pointer to m and l\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n        block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n\n        for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n            start_n = tl.multiple_of(start_n, BLOCK_N)\n            # -- compute qk ----\n            k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,\n                        mask=(start_n + offs_n[None, :]) < cur_batch_seq_len, other=0.0)\n            # mask = tl.load(mask_ptrs + start_n, mask=start_n + offs_n < cur_batch_end_loc, other=0.0)\n\n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, k)\n            qk *= sm_scale\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n\n            # -- compute m_ij, p, l_ij\n            m_ij = tl.max(qk, 1)\n            p = tl.exp(qk - m_ij[:, None])\n            l_ij = tl.sum(p, 1)\n            # -- update m_i and l_i\n            m_i_new = tl.maximum(m_i, m_ij)\n            alpha = tl.exp(m_i - m_i_new)\n            beta = tl.exp(m_ij - m_i_new)\n            l_i_new = alpha * l_i + beta * l_ij\n            # -- update output accumulator --\n            # scale p\n            p_scale = beta / l_i_new\n            p = p * p_scale[:, None]\n            # scale acc\n            acc_scale = l_i / l_i_new * alpha\n            acc = acc * acc_scale[:, None]\n            # update acc\n            v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,\n                        mask=(start_n + offs_n[:, None]) < cur_batch_seq_len, other=0.0)\n\n            p = p.to(v.dtype)\n            acc += tl.dot(p, v)\n            # update m_i and l_i\n            l_i = l_i_new\n            m_i = m_i_new\n        # initialize pointers to output\n        off_o = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n        out_ptrs = Out + off_o\n        tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n        return\n\n    @torch.no_grad()\n    def context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len):\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        assert Lk in {16, 32, 64, 128}\n\n        sm_scale = 1.0 / (Lq**0.5)  # \u8ba1\u7b97scale\u7cfb\u6570\n        batch, head = b_seq_len.shape[0], q.shape[1]\n\n        grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n\n        num_warps = 4 if Lk <= 64 else 8\n        _fwd_kernel[grid](\n            q, k, v, sm_scale, b_start_loc, b_seq_len,\n            o,\n            q.stride(0), q.stride(1), q.stride(2),\n            k.stride(0), k.stride(1), k.stride(2),\n            v.stride(0), v.stride(1), v.stride(2),\n            o.stride(0), o.stride(1), o.stride(2),\n            BLOCK_M=BLOCK,\n            BLOCK_DMODEL=Lk,\n            BLOCK_N=BLOCK,\n            num_warps=num_warps,\n            num_stages=1,\n        )\n        return\n\nelse:\n    raise Exception(\"error triton version!\")\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines a Triton kernel function called `kernel_function` and a Python function `call_kernel` to apply the kernel. \n            The kernel computes the sine of elements in an input tensor `x` and stores the results in an output tensor.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel_function(X, Y, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the block index\n    pid = tl.program_id(axis=0)\n    # Compute the block start index\n    block_start = pid * BLOCK_SIZE\n    # Compute the block end index\n    block_end = min(block_start + BLOCK_SIZE, n_elements)\n    \n    # Load the input elements for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(X + offsets, mask=mask)\n    \n    # Compute the sine of the input elements\n    y = tl.sin(x)\n    \n    # Store the results back to the output tensor\n    tl.store(Y + offsets, y, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef call_kernel(x: torch.Tensor, y: torch.Tensor):\n    # Ensure the input and output tensors are on the same device\n    assert x.device == y.device\n    assert x.is_cuda and y.is_cuda\n    assert x.dtype == torch.float32 and y.dtype == torch.float32\n    assert x.shape == y.shape\n    \n    # Get the number of elements\n    n_elements = x.numel()\n    \n    # Define the grid and block sizes\n    BLOCK_SIZE = 1024\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    \n    # Launch the kernel\n    kernel_function[grid](x, y, n_elements, BLOCK_SIZE)\n\n# Create input and output tensors\nx = torch.randn(1024 * 1024, device='cuda', dtype=torch.float32)\ny = torch.empty_like(x)\n\n# Call the kernel\ncall_kernel(x, y)\n\n# Verify the results\nimport numpy as np\nnp.testing.assert_allclose(y.cpu().numpy(), np.sin(x.cpu().numpy()), rtol=1e-5, atol=1e-5)\nprint(\"Kernel executed successfully and results are correct.\")\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Kernel function using Triton\n@triton.jit\ndef kernel_function(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # x_ptr: pointer to input data\n    # output_ptr: pointer to output data\n    # n_elements: number of elements to process\n    # BLOCK_SIZE: block size for Triton kernel\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.math.sin(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# Function to call the Triton kernel\ndef call_kernel(x):\n    # x: input tensor\n    n_elements = x.numel()\n    output = torch.empty_like(x)\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    kernel_function[grid](x, output, n_elements, BLOCK_SIZE=1024)\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton-based operator computes cross-entropy loss with optional label smoothing and scaling for logits. The `cross_entropy_fwd_kernel` and `cross_entropy_bwd_kernel` are the forward and backward kernels, respectively. \n            \n            The `cross_entropy_loss` function wraps around the `CrossEntropyLoss` class, which implements these kernels. The forward method calculates the loss and logs-sum-exp values and can handle both split scenarios and parallelism. The backward method computes gradients with respect to logits. The inputs include logits, labels, optional smoothing and scaling factors, an ignored index, and a process group for distributed computation. The outputs are the computed losses and any auxiliary loss values (like z-losses for stability). \n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    logits_ptr,  # pointer to logits (N, C)\n    labels_ptr,  # pointer to labels (N)\n    loss_ptr,    # pointer to loss (N)\n    lse_ptr,     # pointer to log-sum-exp (N)\n    N,           # number of samples\n    C,           # number of classes\n    smoothing,   # label smoothing factor\n    ignored_index,  # index to ignore\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Load logits and labels\n    logits = tl.load(logits_ptr + offsets[:, None] * C + tl.arange(0, C), mask=mask[:, None], other=-float('inf'))\n    labels = tl.load(labels_ptr + offsets, mask=mask, other=0)\n\n    # Compute log-sum-exp\n    max_logits = tl.max(logits, axis=1)\n    logits = logits - max_logits[:, None]\n    exp_logits = tl.exp(logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=1)\n    lse = max_logits + tl.log(sum_exp_logits)\n\n    # Compute cross-entropy loss\n    valid_mask = (labels != ignored_index) & mask\n    valid_labels = tl.where(valid_mask, labels, 0)\n    valid_logits = tl.load(logits_ptr + offsets[:, None] * C + valid_labels[:, None], mask=valid_mask[:, None], other=0)\n    valid_lse = tl.load(lse_ptr + offsets, mask=valid_mask, other=0)\n    loss = valid_lse - valid_logits\n\n    # Apply label smoothing\n    if smoothing > 0:\n        smoothed_loss = -tl.sum(logits * (1 - smoothing) / C, axis=1)\n        loss = (1 - smoothing) * loss + smoothing * smoothed_loss\n\n    # Store results\n    tl.store(loss_ptr + offsets, loss, mask=valid_mask)\n    tl.store(lse_ptr + offsets, lse, mask=mask)\n\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    logits_ptr,  # pointer to logits (N, C)\n    labels_ptr,  # pointer to labels (N)\n    grad_output_ptr,  # pointer to gradient of loss (N)\n    grad_logits_ptr,  # pointer to gradient of logits (N, C)\n    N,           # number of samples\n    C,           # number of classes\n    smoothing,   # label smoothing factor\n    ignored_index,  # index to ignore\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Load logits, labels, and grad_output\n    logits = tl.load(logits_ptr + offsets[:, None] * C + tl.arange(0, C), mask=mask[:, None], other=-float('inf'))\n    labels = tl.load(labels_ptr + offsets, mask=mask, other=0)\n    grad_output = tl.load(grad_output_ptr + offsets, mask=mask, other=0)\n\n    # Compute softmax\n    max_logits = tl.max(logits, axis=1)\n    logits = logits - max_logits[:, None]\n    exp_logits = tl.exp(logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=1)\n    softmax = exp_logits / sum_exp_logits[:, None]\n\n    # Compute gradients\n    valid_mask = (labels != ignored_index) & mask\n    valid_labels = tl.where(valid_mask, labels, 0)\n    valid_softmax = tl.load(softmax + offsets[:, None] * C + valid_labels[:, None], mask=valid_mask[:, None], other=0)\n    grad_logits = softmax - tl.where(valid_mask[:, None], valid_softmax, 0)\n    grad_logits = grad_logits * grad_output[:, None]\n\n    # Apply label smoothing\n    if smoothing > 0:\n        grad_logits = grad_logits * (1 - smoothing) + smoothing / C\n\n    # Store results\n    tl.store(grad_logits_ptr + offsets[:, None] * C + tl.arange(0, C), grad_logits, mask=mask[:, None])\n\nimport torch\nimport triton\nimport triton.language as tl\n\nclass CrossEntropyLoss:\n    def __init__(self, smoothing=0.0, ignored_index=-100, process_group=None):\n        self.smoothing = smoothing\n        self.ignored_index = ignored_index\n        self.process_group = process_group\n\n    def forward(self, logits, labels):\n        N, C = logits.shape\n        loss = torch.empty(N, device=logits.device)\n        lse = torch.empty(N, device=logits.device)\n\n        grid = (triton.cdiv(N, 1024),)\n        cross_entropy_fwd_kernel[grid](\n            logits, labels, loss, lse, N, C, self.smoothing, self.ignored_index, BLOCK_SIZE=1024\n        )\n\n        # Reduce loss across process group if distributed\n        if self.process_group is not None:\n            torch.distributed.all_reduce(loss, op=torch.distributed.ReduceOp.SUM, group=self.process_group)\n            loss /= torch.distributed.get_world_size(self.process_group)\n\n        return loss, lse\n\n    def backward(self, logits, labels, grad_output):\n        N, C = logits.shape\n        grad_logits = torch.empty_like(logits)\n\n        grid = (triton.cdiv(N, 1024),)\n        cross_entropy_bwd_kernel[grid](\n            logits, labels, grad_output, grad_logits, N, C, self.smoothing, self.ignored_index, BLOCK_SIZE=1024\n        )\n\n        return grad_logits\n\ndef cross_entropy_loss(logits, labels, smoothing=0.0, ignored_index=-100, process_group=None):\n    criterion = CrossEntropyLoss(smoothing, ignored_index, process_group)\n    loss, lse = criterion.forward(logits, labels)\n    return loss, lse\n\nimport torch\n\n# Example inputs\nlogits = torch.randn(100, 10, device='cuda')\nlabels = torch.randint(0, 10, (100,), device='cuda')\n\n# Compute forward pass\nloss, lse = cross_entropy_loss(logits, labels, smoothing=0.1, ignored_index=-100)\n\n# Compute backward pass\nloss.backward()\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    loss_ptr,  # data ptrs\n    lse_ptr,\n    z_loss_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    n_rows,\n    logits_row_stride,  # strides\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n            else:\n                loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    dlogits_ptr,  # data ptrs\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    logits_row_stride,  # strides\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, (dloss * logit_scale) * probs, mask=col_offsets < n_cols)\n\nclass CrossEntropyLoss(torch.autograd.Function):\n\n    @staticmethod\n    def forward(\n        ctx,\n        logits,\n        labels,\n        smoothing=0.0,\n        logit_scale=1.0,\n        lse_square_scale=0.0,\n        ignored_index=-100,\n        inplace_backward=False,\n        process_group=None,\n    ):\n        n_rows, n_cols = logits.shape\n        assert labels.shape == (n_rows,)\n        world_size = 1 if process_group is None else torch.distributed.get_world_size(process_group)\n        total_classes = world_size * n_cols\n        rank = 0 if process_group is None else torch.distributed.get_rank(process_group)\n        class_start_idx = rank * n_cols\n\n        if logits.stride(-1) != 1:\n            logits = logits.contiguous()\n        MAX_BLOCK_SIZE = 64 * 1024\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), MAX_BLOCK_SIZE)\n        num_warps = (\n            4\n            if BLOCK_SIZE < 2048\n            else (8 if BLOCK_SIZE < 8192 else (16 if BLOCK_SIZE < 128 * 1024 else 32))\n        )\n        split = world_size > 1 or n_cols > MAX_BLOCK_SIZE\n        n_splits = (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE\n        loss_shape = (n_splits, n_rows) if n_splits > 1 else (n_rows,)\n        losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        lse = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        z_losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_fwd_kernel[(n_rows, n_splits)](\n                losses,  # data ptrs\n                lse,\n                z_losses,\n                logits,\n                labels,\n                smoothing,\n                logit_scale,\n                lse_square_scale,\n                ignored_index,\n                total_classes,\n                class_start_idx,\n                n_cols,  # shapes\n                n_rows,\n                logits.stride(0),  # strides\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n                SPLIT=split,\n            )\n\n        if split:\n            if n_splits > 1:\n                lse = torch.logsumexp(lse, dim=0)\n                losses = losses.sum(dim=0)\n            if world_size > 1:\n                lse_allgather = torch.empty(world_size, n_rows, dtype=lse.dtype, device=lse.device)\n                torch.distributed.all_gather_into_tensor(lse_allgather, lse, group=process_group)\n                handle_losses = torch.distributed.all_reduce(\n                    losses, op=torch.distributed.ReduceOp.SUM, group=process_group, async_op=True\n                )\n                lse = torch.logsumexp(lse_allgather, dim=0)\n                handle_losses.wait()\n            losses += lse\n            if lse_square_scale != 0.0:\n                z_losses = lse_square_scale * lse.square()\n                z_losses.masked_fill_(labels == ignored_index, 0.0)\n                losses += z_losses\n            else:\n                z_losses = torch.zeros_like(losses)\n            losses.masked_fill_(labels == ignored_index, 0.0)\n\n        ctx.save_for_backward(logits, lse, labels)\n        ctx.mark_non_differentiable(z_losses)\n        ctx.smoothing = smoothing\n        ctx.logit_scale = logit_scale\n        ctx.lse_square_scale = lse_square_scale\n        ctx.ignored_index = ignored_index\n        ctx.total_classes = total_classes\n        ctx.class_start_idx = class_start_idx\n        ctx.inplace_backward = inplace_backward\n\n        return losses, z_losses\n\n    @staticmethod\n    def backward(ctx, grad_losses, grad_z_losses):\n        del grad_z_losses  # z_losses are only for logging.\n\n        logits, lse, labels = ctx.saved_tensors\n        dlogits = logits if ctx.inplace_backward else torch.empty_like(logits)\n        n_rows, n_cols = logits.shape\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 4 * 1024)\n        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else 16)\n        grid = lambda META: (n_rows, triton.cdiv(n_cols, META[\"BLOCK_SIZE\"]))  # noqa\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_bwd_kernel[grid](\n                dlogits,  # data ptrs\n                grad_losses,\n                logits,\n                lse,\n                labels,\n                ctx.smoothing,\n                ctx.logit_scale,\n                ctx.lse_square_scale,\n                ctx.ignored_index,\n                ctx.total_classes,\n                ctx.class_start_idx,\n                n_cols,  # shapes\n                logits.stride(0),  # strides\n                dlogits.stride(0),\n                grad_losses.stride(0),\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n            )\n        return dlogits, None, None, None, None, None, None, None, None\n\ndef cross_entropy_loss(\n    logits: torch.Tensor,\n    labels: torch.Tensor,\n    label_smoothing: float = 0.0,\n    logit_scale: float = 1.0,\n    lse_square_scale: float = 0.0,\n    ignored_index=-100,\n    inplace_backward: bool = False,\n    process_group=None,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    return CrossEntropyLoss.apply(\n        logits,\n        labels,\n        label_smoothing,\n        logit_scale,\n        lse_square_scale,\n        ignored_index,\n        inplace_backward,\n        process_group,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code defines a kernel function called `add_kernel` and a wrapper function `add_wrapper`. The main function of `add_kernel` is to add two input tensors element-wise and store the result in an output tensor. The kernel handles elements in blocks of a defined size, `BLOCK_SIZE`. The wrapper function `add_wrapper` initializes the output tensor and launches the kernel with the necessary parameters. Inputs to the functions are pointers to input tensors, the output tensor, number of elements, and block size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    x_ptr,  # Pointer to the first input tensor\n    y_ptr,  # Pointer to the second input tensor\n    output_ptr,  # Pointer to the output tensor\n    n_elements,  # Number of elements in the tensors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallel processing\n):\n    # Compute the starting index for the current block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Compute the offsets for the current block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the elements from the input tensors\n    x = tl.load(x_ptr + offsets, mask=offsets < n_elements)\n    y = tl.load(y_ptr + offsets, mask=offsets < n_elements)\n\n    # Perform the element-wise addition\n    output = x + y\n\n    # Store the result in the output tensor\n    tl.store(output_ptr + offsets, output, mask=offsets < n_elements)\n\nimport torch\n\ndef add_wrapper(x, y, output, block_size):\n    # Ensure the input tensors are on the same device\n    assert x.device == y.device, \"Input tensors must be on the same device\"\n    assert x.device == output.device, \"Output tensor must be on the same device\"\n\n    # Get the number of elements\n    n_elements = x.numel()\n\n    # Define the grid and block dimensions\n    grid = (triton.cdiv(n_elements, block_size),)\n\n    # Launch the kernel\n    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=block_size)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the block size\n    block_size = 128\n\n    # Create input tensors\n    x = torch.randn(1024, device='cuda')\n    y = torch.randn(1024, device='cuda')\n\n    # Initialize the output tensor\n    output = torch.empty_like(x)\n\n    # Call the wrapper function\n    add_wrapper(x, y, output, block_size)\n\n    # Verify the result\n    expected_output = x + y\n    assert torch.allclose(output, expected_output), \"The output is incorrect\"\n    print(\"Output is correct!\")\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    in_ptr0,\n    in_ptr1,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef add_wrapper(x, y):\n    out = torch.zeros_like(x)\n    \n    BLOCK_SIZE = 4\n    n_elements = x.numel()\n\n    # Calculate the number of blocks needed\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)\n\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel, `block_sparse_attention_kernel`, computes block-sparse attention for a transformer model during the prompt phase, assuming no past sequence length. The key inputs are Q (queries), K (keys), V (values), and layout information in CSR format. The outputs are stored in 'out'. The kernel processes data in blocks defined by BLOCK_M, BLOCK_N, and BLOCK_D, iterating over query sequence length blocks. It computes query-key products, applies a softmax, and uses the results to produce weighted value sums for the output. It handles scenarios with multiple data blocks when NUM_D_BLOCKS is greater than 1.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef block_sparse_attention_kernel(\n    Q, K, V, out, row_ptr, col_idx,\n    stride_qb, stride_qm, stride_qd,\n    stride_kb, stride_kn, stride_kd,\n    stride_vb, stride_vn, stride_vd,\n    stride_ob, stride_om, stride_od,\n    nheads, nblocks_q, nblocks_k, d, softmax_scale,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr, NUM_D_BLOCKS: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = tl.program_id(axis=1)\n    head_id = tl.program_id(axis=2)\n\n    # Compute the block indices\n    block_m = pid * BLOCK_M\n    block_n = bid * BLOCK_N\n\n    # Pointers to the Q, K, V matrices\n    Q_block_ptr = Q + head_id * stride_qb + block_m * stride_qm\n    K_block_ptr = K + head_id * stride_kb + block_n * stride_kn\n    V_block_ptr = V + head_id * stride_vb + block_n * stride_vn\n    out_block_ptr = out + head_id * stride_ob + block_m * stride_om\n\n    # Load the Q, K, V blocks\n    Q_block = tl.load(Q_block_ptr, mask=block_m + tl.arange(0, BLOCK_M) < nblocks_q, other=0.0)\n    K_block = tl.load(K_block_ptr, mask=block_n + tl.arange(0, BLOCK_N) < nblocks_k, other=0.0)\n    V_block = tl.load(V_block_ptr, mask=block_n + tl.arange(0, BLOCK_N) < nblocks_k, other=0.0)\n\n    # Compute the query-key products\n    QK = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(0, d, BLOCK_D):\n        QK += tl.dot(Q_block[:, i:i+BLOCK_D], K_block[:, i:i+BLOCK_D].T)\n    QK *= softmax_scale\n\n    # Apply the softmax\n    QK = tl.softmax(QK, axis=1)\n\n    # Compute the weighted value sums\n    out_block = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)\n    for i in range(0, BLOCK_N, NUM_D_BLOCKS):\n        out_block += tl.dot(QK[:, i:i+NUM_D_BLOCKS], V_block[i:i+NUM_D_BLOCKS, :])\n\n    # Store the output\n    tl.store(out_block_ptr, out_block, mask=block_m + tl.arange(0, BLOCK_M) < nblocks_q)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 16, 'BLOCK_N': 16, 'BLOCK_D': 16, 'NUM_D_BLOCKS': 1}, num_warps=4),\n        triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_D': 32, 'NUM_D_BLOCKS': 1}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_D': 64, 'NUM_D_BLOCKS': 1}, num_warps=16),\n    ],\n    key=['nheads', 'nblocks_q', 'nblocks_k', 'd']\n)\n@triton.jit\ndef block_sparse_attention_kernel(\n    Q, K, V, out, row_ptr, col_idx,\n    stride_qb, stride_qm, stride_qd,\n    stride_kb, stride_kn, stride_kd,\n    stride_vb, stride_vn, stride_vd,\n    stride_ob, stride_om, stride_od,\n    nheads, nblocks_q, nblocks_k, d, softmax_scale,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr, NUM_D_BLOCKS: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = tl.program_id(axis=1)\n    head_id = tl.program_id(axis=2)\n\n    # Compute the block indices\n    block_m = pid * BLOCK_M\n    block_n = bid * BLOCK_N\n\n    # Pointers to the Q, K, V matrices\n    Q_block_ptr = Q + head_id * stride_qb + block_m * stride_qm\n    K_block_ptr = K + head_id * stride_kb + block_n * stride_kn\n    V_block_ptr = V + head_id * stride_vb + block_n * stride_vn\n    out_block_ptr = out + head_id * stride_ob + block_m * stride_om\n\n    # Load the Q, K, V blocks\n    Q_block = tl.load(Q_block_ptr, mask=block_m + tl.arange(0, BLOCK_M) < nblocks_q, other=0.0)\n    K_block = tl.load(K_block_ptr, mask=block_n + tl.arange(0, BLOCK_N) < nblocks_k, other=0.0)\n    V_block = tl.load(V_block_ptr, mask=block_n + tl.arange(0, BLOCK_N) < nblocks_k, other=0.0)\n\n    # Compute the query-key products\n    QK = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(0, d, BLOCK_D):\n        QK += tl.dot(Q_block[:, i:i+BLOCK_D], K_block[:, i:i+BLOCK_D].T)\n    QK *= softmax_scale\n\n    # Apply the softmax\n    QK = tl.softmax(QK, axis=1)\n\n    # Compute the weighted value sums\n    out_block = tl.zeros((BLOCK_M, BLOCK_D), dtype=tl.float32)\n    for i in range(0, BLOCK_N, NUM_D_BLOCKS):\n        out_block += tl.dot(QK[:, i:i+NUM_D_BLOCKS], V_block[i:i+NUM_D_BLOCKS, :])\n\n    # Store the output\n    tl.store(out_block_ptr, out_block, mask=block_m + tl.arange(0, BLOCK_M) < nblocks_q)\n\ndef block_sparse_attention(Q, K, V, row_ptr, col_idx, nheads, nblocks_q, nblocks_k, d, softmax_scale):\n    # Define the grid and block sizes\n    grid = (nblocks_q, nblocks_k, nheads)\n    block = (16, 16, 1)\n\n    # Allocate the output tensor\n    out = torch.empty_like(Q)\n\n    # Launch the kernel\n    block_sparse_attention_kernel[grid, block](\n        Q, K, V, out, row_ptr, col_idx,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        out.stride(0), out.stride(1), out.stride(2),\n        nheads, nblocks_q, nblocks_k, d, softmax_scale\n    )\n\n    return out\n", "label": "\nimport triton\nimport triton.language as tl\n\n# This kernel is for prompt only and assume that past sequence length is 0. It only supports right padding.\n@triton.jit\ndef block_sparse_attention_kernel(\n    out,  # output [B, H, M, D]. Note that B is batch_size, H is num_heads, M is q_seq_len, and D is head_size\n    Q,  # query [B, H, M, D]\n    K,  # key [B, H_kv, N, D]. Note that N is max_seq_len for kv cache, H_kv is num_kv_heads\n    V,  # value [B, H_kv, N, D]\n    layout_csr_row_indices,  # block mask CSR format. Shape is [L, num_rows + 1] where num_rows = max_seq_len / BLOCK_M\n    layout_csr_col_indices,  # block mask CSR format. Shape is [L, num_rows * num_cols] where num_cols = max_seq_len / BLOCK_N\n    layout_csr_row_stride_h,  # stride per head for csr_row_indices, i.e. num_rows + 1\n    layout_csr_col_stride_h,  # stride per head for csr_col_indices, i.e. num_rows * num_cols\n    num_layout,  # number of sparse layout (L)\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_ob,\n    stride_oh,\n    stride_om,\n    num_heads,\n    num_kv_heads,\n    total_seq_len,  # Total sequence length including past sequence length and query sequence length.\n    BLOCK_M: tl.constexpr,  # block size for q_seq_len\n    EVEN_M: tl.constexpr,  # whether q_seq_len % BLOCK_M == 0\n    BLOCK_N: tl.constexpr,  # block size for k_seq_len\n    EVEN_N: tl.constexpr,  # whether k_seq_len % BLOCK_N == 0\n    BLOCK_D: tl.constexpr,  # block size for D\n    NUM_D_BLOCKS: tl.constexpr,  # number of data blocks =  D / BLOCK_D\n):\n    tl.static_print(f\"{BLOCK_M=} {BLOCK_N=} {BLOCK_D=} {EVEN_M=} {EVEN_N=} {NUM_D_BLOCKS=}\")\n\n    # Past sequence length is 0 since this kernel is for prompt only.\n    q_seq_len = total_seq_len\n\n    # Grid is [CDiv(q_seq_len, BLOCK_M), batch_size * num_heads]\n    start_m = tl.program_id(0)\n    off_bh = tl.program_id(1)\n\n    off_h = off_bh % num_heads\n    off_b = off_bh // num_heads\n\n    # For group query attention, map the query head index to the corresponding one for key and value.\n    head_groups = num_heads // num_kv_heads\n    off_h_kv = off_h // head_groups\n\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h_kv * stride_kh\n    V += off_b * stride_vb + off_h_kv * stride_vh\n\n    # Initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    off_q = offs_m[:, None] * stride_qm + offs_d[None, :]  # [BLOCK_M, BLOCK_D]\n    off_k = offs_n[None, :] * stride_kn + offs_d[:, None]  # [BLOCK_D, BLOCK_N]\n    off_v = offs_n[:, None] * stride_vn + offs_d[None, :]  # [BLOCK_N, BLOCK_D]\n\n    # Initialize pointers to query, key, value\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n\n    # Initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n    if NUM_D_BLOCKS >= 2:\n        acc2 = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n\n    # Load q: it will stay in SRAM throughout\n    if EVEN_M:\n        q = tl.load(q_ptrs)\n        if NUM_D_BLOCKS >= 2:\n            q2 = tl.load(q_ptrs + BLOCK_D)\n    else:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < q_seq_len)\n        if NUM_D_BLOCKS >= 2:\n            q2 = tl.load(q_ptrs + BLOCK_D, mask=offs_m[:, None] < q_seq_len)\n\n    layout_h = off_h % num_layout\n\n    # This assumes that past sequence length is 0, otherwise need + (past_seq_len + 1) // BLOCK_M.\n    layout_ptr = layout_csr_row_indices + layout_h * layout_csr_row_stride_h + start_m\n    start_l = tl.load(layout_ptr).to(tl.int32)\n    end_l = tl.load(layout_ptr + 1).to(tl.int32)\n\n    # Loop over k, v and update accumulator\n    for col_idx_idx in range(start_l, end_l):\n        col_idx = tl.load(layout_csr_col_indices + layout_h * layout_csr_col_stride_h + col_idx_idx).to(tl.int32)\n        start_n = col_idx * BLOCK_N\n        # -- compute qk ----\n        if EVEN_N:\n            k = tl.load(k_ptrs + start_n * stride_kn)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_n[None, :] + start_n < total_seq_len)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n\n        if NUM_D_BLOCKS >= 2:\n            if EVEN_N:\n                k = tl.load(k_ptrs + start_n * stride_kn + BLOCK_D)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn + BLOCK_D, mask=offs_n[None, :] + start_n < total_seq_len)\n            qk += tl.dot(q2, k)\n\n        qk *= softmax_scale\n\n        # This assumes that past sequence length is 0, otherwise need offs_m[:, None] + past_seq_len >= ...\n        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n        # -- compute m_ij, p, l_ij\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        # -- update m_i and l_i\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        # -- update output accumulator --\n        # scale p\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        # scale acc\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        if NUM_D_BLOCKS >= 2:\n            acc2 = acc2 * acc_scale[:, None]\n        p = p.to(Q.dtype.element_ty)\n        # update acc\n        if EVEN_N:\n            v = tl.load(v_ptrs + start_n * stride_vn)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_n[:, None] + start_n < total_seq_len)\n        acc += tl.dot(p, v)\n\n        if NUM_D_BLOCKS >= 2:\n            if EVEN_N:\n                v = tl.load(v_ptrs + start_n * stride_vn + BLOCK_D)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn + BLOCK_D, mask=offs_n[:, None] + start_n < total_seq_len)\n            acc2 += tl.dot(p, v)\n\n        # update m_i and l_i\n        l_i = l_i_new\n        m_i = m_i_new\n\n    off_o = off_b * stride_ob + off_h * stride_oh + offs_m[:, None] * stride_om + offs_d[None, :]\n    out_ptrs = out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < q_seq_len)\n    if NUM_D_BLOCKS >= 2:\n        tl.store(out_ptrs + BLOCK_D, acc2, mask=offs_m[:, None] < q_seq_len)\n\n# Define the wrapper function to call the Triton kernel\ndef block_sparse_attention(\n    Q, K, V, layout_csr_row_indices, layout_csr_col_indices, layout_csr_row_stride_h, layout_csr_col_stride_h,\n    num_layout, softmax_scale, num_heads, num_kv_heads, total_seq_len, BLOCK_M, EVEN_M, BLOCK_N, EVEN_N, BLOCK_D, NUM_D_BLOCKS\n):\n    # Determine the grid size\n    q_seq_len = total_seq_len\n    grid = (triton.cdiv(q_seq_len, BLOCK_M), Q.shape[0] * num_heads)\n\n    # Allocate output tensor\n    out = torch.empty((Q.shape[0], num_heads, q_seq_len, Q.shape[-1]), device=Q.device, dtype=Q.dtype)\n\n    # Call the Triton kernel\n    block_sparse_attention_kernel[grid](\n        out, Q, K, V, layout_csr_row_indices, layout_csr_col_indices, layout_csr_row_stride_h, layout_csr_col_stride_h,\n        num_layout, softmax_scale, Q.stride(0), Q.stride(1), Q.stride(2), K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2), out.stride(0), out.stride(1), out.stride(2), num_heads, num_kv_heads,\n        total_seq_len, BLOCK_M=BLOCK_M, EVEN_M=EVEN_M, BLOCK_N=BLOCK_N, EVEN_N=EVEN_N, BLOCK_D=BLOCK_D, NUM_D_BLOCKS=NUM_D_BLOCKS\n    )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator code defines custom kernels for integer matrix multiplication and scaled integer matrix multiplication using Triton, a language for writing highly efficient GPU code. The main functions are `matmul_kernel_with_block_pointers` and `scaled_matmul_kernel_with_block_pointers`, both decorated with `@triton.jit`, which compiles them for execution on GPUs.\n\n            - `matmul_kernel_with_block_pointers` computes the matrix multiplication C = A x B. It takes pointers to matrices A, B, and C, along with their dimensions M, N, K, and their strides. The kernel splits the matrices into blocks defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_K`, and accumulates the results block by block to ensure efficient use of GPU resources. The kernel supports boundary checks when loading and storing data to handle out-of-bound accesses safely.\n\n            - `scaled_matmul_kernel_with_block_pointers` performs a scaled matrix multiplication where the result is multiplied by a scale factor stored in `scales1`. The implementation is similar to the previous kernel but includes additional operations to multiply the result by the scale matrix.\n\n            - `int_matmul_kernel` and `int_scaled_matmul_kernel` are the functions that set up and launch these kernels. They calculate the grid size based on matrix dimensions and use Triton's kernel launch syntax to execute the kernels on the GPU. The kernels are configured using an instance of the `Config` class, which holds parameters like `num_warps`, `num_stages`, and `num_ctas` to control the kernel's execution configuration.\n\n            - The `Config` class is a simple configuration holder that allows setting these execution parameters easily. It passes them to the kernel launch call through `config.kwargs`.\n\n            These kernels are specialized for integer arithmetic and are structured to maximize data locality and performance on modern GPUs using Triton's APIs and language features.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Configuration class to hold kernel execution parameters\nclass Config:\n    def __init__(self, num_warps=4, num_stages=3, num_ctas=1):\n        self.num_warps = num_warps\n        self.num_stages = num_stages\n        self.num_ctas = num_ctas\n        self.kwargs = {\n            'num_warps': num_warps,\n            'num_stages': num_stages,\n            'num_ctas': num_ctas\n        }\n\n# Kernel for integer matrix multiplication C = A x B\n@triton.jit\ndef matmul_kernel_with_block_pointers(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m = tl.arange(0, BLOCK_M) + (pid % grid_m) * BLOCK_M\n    pid_n = tl.arange(0, BLOCK_N) + (pid // grid_m) * BLOCK_N\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(A_ptr + pid_m[:, None] * stride_am + (k + tl.arange(0, BLOCK_K))[None, :] * stride_ak, mask=k + tl.arange(0, BLOCK_K) < K, other=0)\n        b = tl.load(B_ptr + (k + tl.arange(0, BLOCK_K))[:, None] * stride_bk + pid_n[None, :] * stride_bn, mask=k + tl.arange(0, BLOCK_K) < K, other=0)\n        acc += tl.dot(a, b)\n\n    tl.store(C_ptr + pid_m[:, None] * stride_cm + pid_n[None, :] * stride_cn, acc, mask=(pid_m[:, None] < M) & (pid_n[None, :] < N))\n\n# Kernel for scaled integer matrix multiplication C = A x B * scales\n@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(\n    A_ptr, B_ptr, C_ptr, scales_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_sm,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m = tl.arange(0, BLOCK_M) + (pid % grid_m) * BLOCK_M\n    pid_n = tl.arange(0, BLOCK_N) + (pid // grid_m) * BLOCK_N\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(A_ptr + pid_m[:, None] * stride_am + (k + tl.arange(0, BLOCK_K))[None, :] * stride_ak, mask=k + tl.arange(0, BLOCK_K) < K, other=0)\n        b = tl.load(B_ptr + (k + tl.arange(0, BLOCK_K))[:, None] * stride_bk + pid_n[None, :] * stride_bn, mask=k + tl.arange(0, BLOCK_K) < K, other=0)\n        acc += tl.dot(a, b)\n\n    scales = tl.load(scales_ptr + pid_m * stride_sm, mask=pid_m < M, other=1)\n    acc = acc * scales[:, None]\n\n    tl.store(C_ptr + pid_m[:, None] * stride_cm + pid_n[None, :] * stride_cn, acc, mask=(pid_m[:, None] < M) & (pid_n[None, :] < N))\n\n# Wrapper function for integer matrix multiplication\ndef int_matmul_kernel(A, B, C, M, N, K, config: Config):\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    matmul_kernel_with_block_pointers[grid](\n        A, B, C,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_M=config.kwargs['num_warps'] * 32,\n        BLOCK_N=config.kwargs['num_warps'] * 32,\n        BLOCK_K=32,\n        **config.kwargs\n    )\n\n# Wrapper function for scaled integer matrix multiplication\ndef int_scaled_matmul_kernel(A, B, C, scales, M, N, K, config: Config):\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    scaled_matmul_kernel_with_block_pointers[grid](\n        A, B, C, scales,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        scales.stride(0),\n        BLOCK_M=config.kwargs['num_warps'] * 32,\n        BLOCK_N=config.kwargs['num_warps'] * 32,\n        BLOCK_K=32,\n        **config.kwargs\n    )\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef matmul_kernel_with_block_pointers(\n    # Pointers to matrices\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n    # by to get the element one row down (A has M rows).\n    stride_am,\n    stride_ak,  #\n    stride_bk,\n    stride_bn,  #\n    stride_cm,\n    stride_cn,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    # -----------------------------------------------------------\n    # Map program ids `pid` to the block of C it should compute.\n    # This is done in a grouped ordering to promote L2 data reuse.\n    # See the matrix multiplication tutorial for details.\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % GROUP_M)\n    pid_n = (pid % num_pid_in_group) // GROUP_M\n\n    # ----------------------------------------------------------\n    # Create block pointers for the first blocks of A and B.\n    # We will advance this pointer as we move in the K direction and accumulate.\n    # See above `Make a Block Pointer` section for details.\n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(pid_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0),\n    )\n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, pid_n * BLOCK_N),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(1, 0),\n    )\n\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_M, BLOCK_N]` block.\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, K, BLOCK_K):\n        # Load with boundary checks, no need to calculate the mask manually.\n        # For better performance, you may remove some axis from the boundary\n        # check, if you can guarantee that the access is always in-bound in\n        # that axis.\n        # See above `Load/Store a Block Pointer` section for details.\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n        # We accumulate along the K dimension.\n        accumulator += tl.dot(a, b)\n        # Advance the block pointer to the next K block.\n        # See above `Advance a Block Pointer` section for details.\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    c = accumulator  # .to(tl.float16)\n\n    # ----------------------------------------------------------------\n    # Write back the block of the output matrix C with boundary checks.\n    # See above `Load/Store a Block Pointer` section for details.\n    c_block_ptr = tl.make_block_ptr(\n        base=c_ptr,\n        shape=(M, N),\n        strides=(stride_cm, stride_cn),\n        offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0),\n    )\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n\n\n@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(\n    # Pointers to matrices\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    s1_ptr,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n    # by to get the element one row down (A has M rows).\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_s1m,\n    stride_s1n,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr = tl.int32,\n):\n    # based on triton.ops.matmul\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = a_ptr + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = b_ptr + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)  # , allow_tf32=ALLOW_TF32)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n\n    # rematerialize rm and rn to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = rm[:, None]\n    idx_n = rn[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n\n    # inductor generates a suffix\n    xindex = idx_n + (N * idx_m)\n    tmp0 = tl.load(\n        s1_ptr + (tl.broadcast_to(idx_m, mask.shape)),\n        mask,\n        eviction_policy=\"evict_last\",\n    )\n    tl.store(c_ptr + (tl.broadcast_to(xindex, mask.shape)), acc * tmp0, mask)\n\n\ndef int_matmul_kernel(a, b, c, config):\n    M, K = a.shape\n    K, N = b.shape\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),\n    )\n    matmul_kernel_with_block_pointers[grid](\n        a,\n        b,\n        c,  #\n        M,\n        N,\n        K,  #\n        a.stride(0),\n        a.stride(1),  #\n        b.stride(0),\n        b.stride(1),  #\n        c.stride(0),\n        c.stride(1),\n        num_warps=config.num_warps,\n        num_stages=config.num_stages,\n        num_ctas=config.num_ctas,\n        **config.kwargs,\n    )\n    return c\n\n\ndef int_scaled_matmul_kernel(a, b, scales1, c, config):\n    M, K = a.shape\n    K, N = b.shape\n    # print(\"a.sizes(): \", a.size(), \"a.strides(): \", a.stride(), \"a.dtype: \", a.dtype)\n    # print(\"b.sizes(): \", b.size(), \"b.strides(): \", b.stride(), \"b.dtype: \", b.dtype)\n    # print(\"c.sizes(): \", c.size(), \"c.strides(): \", c.stride(), \"c.dtype: \", c.dtype)\n    # print(\"scales1.sizes(): \", scales1.size(), \"scales1.strides(): \", scales1.stride(), \"scales1.dtype\", scales1.dtype)\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),\n    )\n    scaled_matmul_kernel_with_block_pointers[grid](\n        a,\n        b,\n        c,\n        scales1,\n        M,\n        N,\n        K,  #\n        a.stride(0),\n        a.stride(1),  #\n        b.stride(0),\n        b.stride(1),  #\n        c.stride(0),\n        c.stride(1),\n        scales1.stride(0),\n        scales1.stride(1),\n        num_warps=config.num_warps,\n        num_stages=config.num_stages,\n        num_ctas=config.num_ctas,\n        EVEN_K=(K % 2 == 0),\n        **config.kwargs,\n    )\n    return c\n\n\nclass Config:\n    def __init__(self, num_warps=4, num_stages=2, num_ctas=1, **kwargs):\n        self.num_warps = num_warps\n        self.num_stages = num_stages\n        self.num_ctas = num_ctas\n        self.kwargs = kwargs\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines three Triton kernels for operations related to decay cumulative sums: `fwd_decay_cumsum`, `prepare_qg_kg`, and `bwd_decay_global_cumsum`. The kernels are used for forward and backward operations on tensors for tasks often found in neural networks.\n\n        1. **fwd_decay_cumsum**: Computes the cumulative sum with decay. It reads input tensor `g` and writes the cumulative result to `g_o`. The kernel iterates over `BT` blocks, scaling and accumulating values from `g` using a decay constant, and stores the results in `g_o`.\n\n        2. **prepare_qg_kg**: Prepares transformed versions of input tensors `q` and `k` with additional tensor `g` to produce `qg` and `kg`. It uses exponential scaling on `q` and `k` derived from `g`, applying transformations that incorporate cumulative decay effects.\n\n        3. **bwd_decay_global_cumsum**: Computes gradients with respect to the decay sum using input gradients `dq_inner`, `dq_inter`, `dk_inner`, and `dk_inter`, along with original inputs `q`, `k`, and `g`. It updates `dg` with cumulative gradients for decay.\n\n        The launch functions (`launch_fwd_decay_cumsum`, `launch_prepare_qg_kg`, `launch_bwd_decay_global_cumsum`) handle the preparation of grid dimensions and stride calculations before launching each kernel, adapting input and output shapes.\n\n        Common parameters:\n        - `B`, `H`, `T`: Batch size, head size, sequence length.\n        - `scale`: A scaling factor for transformations.\n        - `BT`, `BK`, `DK`: Block and kernel size constants for configuring the grid.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# 1. fwd_decay_cumsum\n@triton.jit\ndef fwd_decay_cumsum(g_ptr, g_o_ptr, decay_ptr, B, H, T, BT, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BT\n    decay = tl.load(decay_ptr)\n    \n    for i in range(block_start, min(block_start + BT, T)):\n        offset = tl.arange(0, BLOCK_SIZE)\n        g_idx = block_start + offset\n        g = tl.load(g_ptr + g_idx, mask=g_idx < T, other=0.0)\n        \n        if i == block_start:\n            g_o = g\n        else:\n            g_o = g + decay * g_o\n        \n        tl.store(g_o_ptr + g_idx, g_o, mask=g_idx < T)\n\n# 2. prepare_qg_kg\n@triton.jit\ndef prepare_qg_kg(q_ptr, k_ptr, g_ptr, qg_ptr, kg_ptr, scale_ptr, B, H, T, BT, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BT\n    scale = tl.load(scale_ptr)\n    \n    for i in range(block_start, min(block_start + BT, T)):\n        offset = tl.arange(0, BLOCK_SIZE)\n        q_idx = block_start + offset\n        k_idx = block_start + offset\n        g_idx = block_start + offset\n        \n        q = tl.load(q_ptr + q_idx, mask=q_idx < T, other=0.0)\n        k = tl.load(k_ptr + k_idx, mask=k_idx < T, other=0.0)\n        g = tl.load(g_ptr + g_idx, mask=g_idx < T, other=0.0)\n        \n        qg = q * tl.exp(g * scale)\n        kg = k * tl.exp(g * scale)\n        \n        tl.store(qg_ptr + q_idx, qg, mask=q_idx < T)\n        tl.store(kg_ptr + k_idx, kg, mask=k_idx < T)\n\n# 3. bwd_decay_global_cumsum\n@triton.jit\ndef bwd_decay_global_cumsum(dq_inner_ptr, dq_inter_ptr, dk_inner_ptr, dk_inter_ptr, q_ptr, k_ptr, g_ptr, dg_ptr, decay_ptr, B, H, T, BT, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BT\n    decay = tl.load(decay_ptr)\n    \n    for i in range(block_start, min(block_start + BT, T)):\n        offset = tl.arange(0, BLOCK_SIZE)\n        q_idx = block_start + offset\n        k_idx = block_start + offset\n        g_idx = block_start + offset\n        \n        dq_inner = tl.load(dq_inner_ptr + q_idx, mask=q_idx < T, other=0.0)\n        dq_inter = tl.load(dq_inter_ptr + q_idx, mask=q_idx < T, other=0.0)\n        dk_inner = tl.load(dk_inner_ptr + k_idx, mask=k_idx < T, other=0.0)\n        dk_inter = tl.load(dk_inter_ptr + k_idx, mask=k_idx < T, other=0.0)\n        q = tl.load(q_ptr + q_idx, mask=q_idx < T, other=0.0)\n        k = tl.load(k_ptr + k_idx, mask=k_idx < T, other=0.0)\n        g = tl.load(g_ptr + g_idx, mask=g_idx < T, other=0.0)\n        \n        dg = (dq_inner + dq_inter) * q * decay + (dk_inner + dk_inter) * k * decay\n        \n        tl.store(dg_ptr + g_idx, dg, mask=g_idx < T)\n\nimport torch\n\ndef launch_fwd_decay_cumsum(g, g_o, decay, B, H, T, BT, BLOCK_SIZE):\n    grid = (T // BT, B * H)\n    fwd_decay_cumsum[grid](g, g_o, decay, B, H, T, BT, BLOCK_SIZE)\n\ndef launch_prepare_qg_kg(q, k, g, qg, kg, scale, B, H, T, BT, BLOCK_SIZE):\n    grid = (T // BT, B * H)\n    prepare_qg_kg[grid](q, k, g, qg, kg, scale, B, H, T, BT, BLOCK_SIZE)\n\ndef launch_bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, decay, B, H, T, BT, BLOCK_SIZE):\n    grid = (T // BT, B * H)\n    bwd_decay_global_cumsum[grid](dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, decay, B, H, T, BT, BLOCK_SIZE)\n\n# Example usage with PyTorch tensors\nB, H, T = 2, 4, 128\nBT, BLOCK_SIZE = 32, 128\n\n# Input tensors\ng = torch.randn((B, H, T), device='cuda')\nq = torch.randn((B, H, T), device='cuda')\nk = torch.randn((B, H, T), device='cuda')\ndq_inner = torch.randn((B, H, T), device='cuda')\ndq_inter = torch.randn((B, H, T), device='cuda')\ndk_inner = torch.randn((B, H, T), device='cuda')\ndk_inter = torch.randn((B, H, T), device='cuda')\n\n# Output tensors\ng_o = torch.zeros((B, H, T), device='cuda')\nqg = torch.zeros((B, H, T), device='cuda')\nkg = torch.zeros((B, H, T), device='cuda')\ndg = torch.zeros((B, H, T), device='cuda')\n\n# Constants\ndecay = torch.tensor(0.9, device='cuda')\nscale = torch.tensor(0.5, device='cuda')\n\n# Launch kernels\nlaunch_fwd_decay_cumsum(g, g_o, decay, B, H, T, BT, BLOCK_SIZE)\nlaunch_prepare_qg_kg(q, k, g, qg, kg, scale, B, H, T, BT, BLOCK_SIZE)\nlaunch_bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, decay, B, H, T, BT, BLOCK_SIZE)\n", "label": "\nimport triton\nimport triton.language as tl\n\ninv_ln2 = tl.constexpr(1.44269504)\n\n# Forward decay cumulative sum kernel\n@triton.jit\ndef fwd_decay_cumsum(\n    g,\n    g_o, \n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    B,\n    H,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    DK: tl.constexpr\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_g = g + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_go = g_o + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    cum_decay = tl.zeros([BK], dtype=tl.float32)\n    mask = (i_k * BK + tl.arange(0, BK)) < DK\n\n    for i in range(BT):\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        cum_decay += _g * inv_ln2\n        tl.store(p_go, cum_decay.to(p_go.dtype.element_ty), mask=mask)\n        p_g += DK\n        p_go += DK\n\n# Prepare qg and kg kernel\n@triton.jit\ndef prepare_qg_kg(\n    q,\n    k,\n    g,\n    qg,\n    kg,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    B,\n    H,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    DK: tl.constexpr\n):\n\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    \n    mask = (i_k * BK + tl.arange(0, BK)) < DK\n\n    last_decay = tl.load(g + i_bh * s_qk_h + (i_c * BT + BT - 1) * DK + i_k * BK + tl.arange(0, BK))\n\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        _k = tl.load(p_k, mask=mask, other=0)\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        _q *= tl.math.exp2(_g) * scale\n        _k *= tl.math.exp2(last_decay - _g)\n        tl.store(p_kg, _k.to(p_kg.dtype.element_ty), mask=mask)\n        tl.store(p_qg, _q.to(p_qg.dtype.element_ty), mask=mask)\n        p_q += DK\n        p_g += DK\n        p_k += DK\n        p_kg += DK\n        p_qg += DK\n\n# Backward decay global cumulative sum kernel\n@triton.jit\ndef bwd_decay_global_cumsum(\n    dq_inner,\n    dq_inter,\n    dk_inner,\n    dk_inter,\n    q, k, g, dg,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    B,\n    H,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    DK: tl.constexpr\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_g = g + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dg = dg + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dq_inner = dq_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dk_inner = dk_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dq_inter = dq_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dk_inter = dk_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = (i_k * BK + tl.arange(0, BK)) < DK\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT-1, -1, -1):\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        if j == (BT-1):\n            last_g = _g\n        _dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        _dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        _dq2 *= tl.math.exp2(_g)\n        _dq = _dq1 + _dq2\n        tl.store(p_dq_inter, _dq, mask=mask)\n        _dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        _dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        _dk2 *= tl.math.exp2(last_g - _g)\n        _dk = _dk1 + _dk2\n        tl.store(p_dk_inter, _dk, mask=mask)\n        _q = tl.load(p_q, mask=mask, other=0)\n        _k = tl.load(p_k, mask=mask, other=0)\n        _dg = _dq * _q - _dk * _k\n        cum_grad_dg += _dg\n        tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)\n        p_g -= DK\n        p_k -= DK\n        p_q -= DK\n        p_dq_inner -= DK\n        p_dk_inner -= DK\n        p_dq_inter -= DK\n        p_dk_inter -= DK\n        p_dg -= DK\n\n# Define the kernel launch functions\ndef launch_fwd_decay_cumsum(g, g_o, B, H, T, scale, BT, BK, DK):\n    # Calculate strides\n    s_qk_h = H * T * DK\n    s_qk_t = T * DK\n    s_qk_d = DK\n\n    # Launch the kernel\n    grid = (DK // BK, T // BT, B * H)\n    fwd_decay_cumsum[grid](\n        g, g_o, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT=BT, BK=BK, DK=DK\n    )\n\ndef launch_prepare_qg_kg(q, k, g, qg, kg, B, H, T, scale, BT, BK, DK):\n    # Calculate strides\n    s_qk_h = H * T * DK\n    s_qk_t = T * DK\n    s_qk_d = DK\n\n    # Launch the kernel\n    grid = (DK // BK, T // BT, B * H)\n    prepare_qg_kg[grid](\n        q, k, g, qg, kg, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT=BT, BK=BK, DK=DK\n    )\n\ndef launch_bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, B, H, T, scale, BT, BK, DK):\n    # Calculate strides\n    s_qk_h = H * T * DK\n    s_qk_t = T * DK\n    s_qk_d = DK\n\n    # Launch the kernel\n    grid = (DK // BK, T // BT, B * H)\n    bwd_decay_global_cumsum[grid](\n        dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT=BT, BK=BK, DK=DK\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The `matmul_kernel_persistent` is a Triton kernel for performing matrix multiplication efficiently on CUDA-enabled GPUs. The kernel takes pointers to input matrices `a` and `b`, and outputs the result in matrix `c`. The kernel divides the matrices into blocks, each block being processed by a separate thread, and accumulates the result in a shared accumulator before storing it back to global memory. It supports different data types such as `float16` and `float8`.\n\n        The `matmul_persistent` function prepares and launches the kernel. It sets up the kernel's configuration based on the input data type and allocates memory for the output matrix. It ensures the input dimensions are compatible and defines the grid configuration for launching the kernel. The function calls the `matmul_kernel_persistent` with the correct configurations to perform the matrix multiplication.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel_persistent(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, ACC_TYPE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M))[:, None]\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N))[None, :]\n    offs_k = tl.arange(0, BLOCK_K)\n\n    a_ptrs = a_ptr + (offs_am * stride_am + offs_k * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n\n    for k in range(0, K, BLOCK_K):\n        a_mask = (offs_am < M) & (offs_k < K)\n        b_mask = (offs_k[:, None] < K) & (offs_bn < N)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n\n    c = accumulator.to(tl.float32)\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    c_mask = (offs_cm < M)[:, None] & (offs_cn < N)[None, :]\n    tl.store(c_ptrs, c, mask=c_mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef matmul_persistent(a, b, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, GROUP_SIZE_M=8):\n    # Ensure the input tensors are on the same device\n    device = a.device\n    assert a.device == b.device, \"Input tensors must be on the same device\"\n\n    # Ensure the input tensors are contiguous\n    a = a.contiguous()\n    b = b.contiguous()\n\n    # Ensure the input dimensions are compatible\n    M, K = a.shape\n    K, N = b.shape\n    assert K == b.shape[0], \"Incompatible dimensions for matrix multiplication\"\n\n    # Allocate memory for the output matrix\n    c = torch.empty((M, N), device=device, dtype=a.dtype)\n\n    # Define the grid configuration\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),\n    )\n\n    # Determine the accumulator type based on the input data type\n    if a.dtype == torch.float16:\n        ACC_TYPE = tl.float32\n    elif a.dtype == torch.float8:\n        ACC_TYPE = tl.float16\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n    # Launch the kernel\n    matmul_kernel_persistent[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_M, BLOCK_N, BLOCK_K,\n        GROUP_SIZE_M, ACC_TYPE\n    )\n\n    return c\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    ret[\"name\"] = f\"{kernel.name} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_persistent(a_ptr, b_ptr, c_ptr,  #\n                             M, N, K,  #\n                             stride_am, stride_ak,  #\n                             stride_bk, stride_bn,  #\n                             stride_cm, stride_cn,  #\n                             BLOCK_SIZE_M: tl.constexpr,  #\n                             BLOCK_SIZE_N: tl.constexpr,  #\n                             BLOCK_SIZE_K: tl.constexpr,  #\n                             GROUP_SIZE_M: tl.constexpr,  #\n                             NUM_SMS: tl.constexpr,  #\n                             ):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = tl.arange(0, BLOCK_SIZE_N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_SIZE_M\n            start_n = pid_n * BLOCK_SIZE_N\n            offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n            offs_am = tl.where(offs_am < M, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if (c_ptr.dtype.element_ty == tl.float8e4nv):\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_persistent(a, b):\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n    # Allocates output.\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_persistent[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        NUM_SMS=NUM_SMS,  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel to compute the forward and backward pass of the GEGLU (Gated Exponential Linear Unit) activation function using a tanh approximation. The `geglu_forward` function takes two input tensors `a` and `b`, reshapes them to align the columns, and applies the `_geglu_tanh_forward_kernel` to compute the output tensor `c`. Similarly, `geglu_backward` calculates the gradients of `a` and `b` using the `_geglu_tanh_backward_kernel`. `calculate_settings` determines the block size and number of warps, currently set to 128 and 4. The kernel utilizes `tanh` and approximates GELU through mathematical operations. Outputs are reshaped back to their original dimensions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _geglu_tanh_forward_kernel(\n    a_ptr, b_ptr, c_ptr,\n    a_size, b_size, c_size,\n    stride_a, stride_b, stride_c,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < a_size\n\n    a_offsets = offsets * stride_a\n    b_offsets = offsets * stride_b\n    c_offsets = offsets * stride_c\n\n    a = tl.load(a_ptr + a_offsets, mask=mask)\n    b = tl.load(b_ptr + b_offsets, mask=mask)\n\n    # Compute the GEGLU activation\n    tanh_approx = tl.tanh(0.7978845608 * (b + 0.044715 * tl.pow(b, 3)))\n    c = a * 0.5 * (1.0 + tanh_approx)\n\n    tl.store(c_ptr + c_offsets, c, mask=mask)\n\n@triton.jit\ndef _geglu_tanh_backward_kernel(\n    grad_c_ptr, a_ptr, b_ptr, grad_a_ptr, grad_b_ptr,\n    a_size, b_size, c_size,\n    stride_grad_c, stride_a, stride_b, stride_grad_a, stride_grad_b,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < a_size\n\n    a_offsets = offsets * stride_a\n    b_offsets = offsets * stride_b\n    grad_c_offsets = offsets * stride_grad_c\n    grad_a_offsets = offsets * stride_grad_a\n    grad_b_offsets = offsets * stride_grad_b\n\n    a = tl.load(a_ptr + a_offsets, mask=mask)\n    b = tl.load(b_ptr + b_offsets, mask=mask)\n    grad_c = tl.load(grad_c_ptr + grad_c_offsets, mask=mask)\n\n    # Compute the GEGLU activation\n    tanh_approx = tl.tanh(0.7978845608 * (b + 0.044715 * tl.pow(b, 3)))\n    c = a * 0.5 * (1.0 + tanh_approx)\n\n    # Compute gradients\n    grad_tanh_approx = 0.7978845608 * (1.0 - tl.pow(tanh_approx, 2)) * (1.0 + 0.134145 * tl.pow(b, 2))\n    grad_b = grad_c * a * 0.5 * grad_tanh_approx\n    grad_a = grad_c * 0.5 * (1.0 + tanh_approx)\n\n    tl.store(grad_a_ptr + grad_a_offsets, grad_a, mask=mask)\n    tl.store(grad_b_ptr + grad_b_offsets, grad_b, mask=mask)\n\n### Python Wrapper Code\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef calculate_settings(size):\n    BLOCK_SIZE = 128\n    num_warps = 4\n    return BLOCK_SIZE, num_warps\n\ndef geglu_forward(a, b):\n    a = a.contiguous()\n    b = b.contiguous()\n    assert a.shape == b.shape, \"Input tensors must have the same shape\"\n    c = torch.empty_like(a)\n\n    BLOCK_SIZE, num_warps = calculate_settings(a.numel())\n    grid = (triton.cdiv(a.numel(), BLOCK_SIZE),)\n\n    _geglu_tanh_forward_kernel[grid](\n        a, b, c,\n        a.numel(), b.numel(), c.numel(),\n        a.stride(0), b.stride(0), c.stride(0),\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps\n    )\n\n    return c\n\ndef geglu_backward(grad_c, a, b):\n    grad_c = grad_c.contiguous()\n    a = a.contiguous()\n    b = b.contiguous()\n    assert a.shape == b.shape == grad_c.shape, \"Input tensors must have the same shape\"\n    grad_a = torch.empty_like(a)\n    grad_b = torch.empty_like(b)\n\n    BLOCK_SIZE, num_warps = calculate_settings(a.numel())\n    grid = (triton.cdiv(a.numel(), BLOCK_SIZE),)\n\n    _geglu_tanh_backward_kernel[grid](\n        grad_c, a, b, grad_a, grad_b,\n        a.numel(), b.numel(), grad_c.numel(),\n        grad_c.stride(0), a.stride(0), b.stride(0), grad_a.stride(0), grad_b.stride(0),\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps\n    )\n\n    return grad_a, grad_b\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra.libdevice import tanh\n\n\ndef calculate_settings(n_cols):\n    # This function calculates the BLOCK_SIZE and num_warps based on n_cols.\n    # For simplicity, let's assume BLOCK_SIZE is 128 and num_warps is 4.\n    BLOCK_SIZE = 128\n    num_warps = 4\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _geglu_tanh_forward_kernel(\n    a, b, c, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n\n    # tanh approximation form of GELU is computed with:\n    # 0.5 * a * (1 + tanh(sqrt(2 / pi) * (a + 0.044715 * a^3)))\n    sqrt_2_over_pi = 0.7978845608028654  # sqrt(2 / pi)\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n\n@triton.jit\ndef _geglu_tanh_backward_kernel(\n    dc, a, b, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n\n    # recomputation to save memory\n    sqrt_2_over_pi = 0.7978845608028654  # sqrt(2 / pi)\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n\n    db_row = dc_row * geglu_a\n\n    # Gradient w.r.t. a can be computed with:\n    # b * (0.5 * (1 + tanh(z)) + 0.5 * a * (1 - tanh(z)^2) * (sqrt(2/pi) * (1 + 3 * 0.044715 * a^2)))\n    # where z = sqrt(2/pi) * (a + 0.044715 * a^3)\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = (\n        0.5\n        * a_row\n        * (1 - tanh_sq)\n        * (sqrt_2_over_pi * (1 + 3 * 0.044715 * a_row * a_row))\n    )\n    da_row = dc_row * b_row * (term1 + term2)\n\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n\ndef geglu_forward(a, b):\n    ori_shape = a.shape\n\n    n_cols = ori_shape[-1]\n    a = a.view(-1, n_cols)\n    b = b.view(-1, n_cols)\n    c = torch.empty_like(a)\n    n_rows = a.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _geglu_tanh_forward_kernel[(n_rows,)](\n        a,\n        b,\n        c,\n        c.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return a, b, c.view(*ori_shape)\n\ndef geglu_backward(a, b, dc):\n    ori_shape = dc.shape\n    n_cols = ori_shape[-1]\n    dc = dc.view(-1, n_cols)\n    n_rows = dc.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _geglu_tanh_backward_kernel[(n_rows,)](\n        dc,\n        a,\n        b,\n        dc.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n\n    return a.view(*ori_shape), b.view(*ori_shape)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel function `_bmm_chunk_fwd_kernel` used for performing batched matrix-matrix multiplication with customizable block sizes. The kernel supports optimizations like chunked processing, handling of causal masking, and sequence indexing.\n\n            The helper function `_bmm_chunk_fwd` wraps around the Triton kernel, setting up the necessary environment and data structures before launching the kernel. It computes matrix products for input tensors `a` and `b`, chunking them according to `chunk_size` and optionally applying sequence indexing or causal masking. The result is stored in `out`.\n\n            The kernel's logic involves loading sub-matrices into local variables, computing their dot products iteratively, and handling special conditions like causal masking using program IDs to index into blocks of data.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bmm_chunk_fwd_kernel(\n    a_ptr, b_ptr, out_ptr,\n    M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn,\n    stride_om, stride_on,\n    chunk_size,\n    causal, seq_index,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    # Compute program ID\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_block = num_pid_m * num_pid_n\n    block_id = pid // num_pid_in_block\n    pid_m = (pid % num_pid_in_block) // num_pid_n\n    pid_n = (pid % num_pid_in_block) % num_pid_n\n\n    # Compute block bounds\n    block_start_m = pid_m * BLOCK_SIZE_M\n    block_start_n = pid_n * BLOCK_SIZE_N\n    block_start_k = block_id * chunk_size\n\n    # Initialize output block\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Load and compute dot products\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Compute bounds for current chunk\n        block_start_k_chunk = block_start_k + k\n        block_end_k_chunk = min(block_start_k_chunk + BLOCK_SIZE_K, K)\n\n        # Load sub-matrices\n        a = tl.load(a_ptr + block_start_m * stride_am + block_start_k_chunk * stride_ak, \n                    mask=block_start_k_chunk + tl.arange(0, BLOCK_SIZE_K) < K, \n                    other=0.0)\n        b = tl.load(b_ptr + block_start_k_chunk * stride_bk + block_start_n * stride_bn, \n                    mask=block_start_k_chunk + tl.arange(0, BLOCK_SIZE_K) < K, \n                    other=0.0)\n\n        # Compute dot product\n        acc += tl.dot(a, b)\n\n        # Apply causal masking if needed\n        if causal:\n            causal_mask = block_start_m + tl.arange(0, BLOCK_SIZE_M) >= block_start_n + tl.arange(0, BLOCK_SIZE_N)\n            acc = tl.where(causal_mask, acc, float('-inf'))\n\n        # Apply sequence indexing if needed\n        if seq_index:\n            seq_mask = block_start_m + tl.arange(0, BLOCK_SIZE_M) == block_start_n + tl.arange(0, BLOCK_SIZE_N)\n            acc = tl.where(seq_mask, acc, 0.0)\n\n    # Store result\n    tl.store(out_ptr + block_start_m * stride_om + block_start_n * stride_on, acc)\n\nimport torch\n\ndef _bmm_chunk_fwd(a, b, out, chunk_size, causal=False, seq_index=False, block_size_m=16, block_size_n=16, block_size_k=16):\n    # Get dimensions\n    B, M, K = a.shape\n    B, K, N = b.shape\n\n    # Allocate output tensor if not provided\n    if out is None:\n        out = torch.empty((B, M, N), device=a.device, dtype=a.dtype)\n\n    # Launch kernel\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']) * triton.cdiv(K, chunk_size),\n    )\n    _bmm_chunk_fwd_kernel[grid](\n        a, b, out,\n        M, N, K,\n        a.stride(1), a.stride(2), b.stride(1), b.stride(2),\n        out.stride(1), out.stride(2),\n        chunk_size,\n        causal, seq_index,\n        BLOCK_SIZE_M=block_size_m, BLOCK_SIZE_N=block_size_n, BLOCK_SIZE_K=block_size_k\n    )\n\n    return out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2),\n    ],\n    key=['chunk_size', 'K', 'IS_CAUSAL'],\n)\n@triton.jit\ndef _bmm_chunk_fwd_kernel(\n    a_ptr, b_ptr, out_ptr, seq_idx_ptr,\n    seqlen, chunk_size, K, ngroups,\n    stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak,\n    stride_b_batch, stride_b_seqlen, stride_b_head, stride_bk,\n    stride_out_batch, stride_out_chunk, stride_out_head, stride_outm, stride_outn,\n    stride_seq_idx_batch, stride_seq_idx_seqlen,\n    IS_CAUSAL: tl.constexpr,\n    dot_dtype: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\n    b_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h * stride_b_head\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < K - k * BLOCK_SIZE_K), other=0.0).to(dot_dtype)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) & (offs_n[None, :] < chunk_size_limit), other=0.0).to(dot_dtype)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen, mask=offs_n < chunk_size_limit, other=-2)\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc.to(out_ptr.dtype.element_ty)\n\n    out_ptr += pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))\n\ndef _bmm_chunk_fwd(a, b, chunk_size, seq_idx=None, causal=False, output_dtype=None):\n    has_groups = a.dim() == 4\n    if not has_groups:\n        batch, seqlen, k = a.shape\n    else:\n        batch, seqlen, ngroups, k = a.shape\n    assert b.shape == a.shape\n    if seq_idx is not None:\n        assert seq_idx.shape == (batch, seqlen)\n    if a.stride(-1) != 1 and a.stride(1) != 1:\n        a = a.contiguous()\n    if b.stride(-1) != 1 and b.stride(1) != 1:\n        b = b.contiguous()\n    nchunks = math.ceil(seqlen / chunk_size)\n    out_dtype = a.dtype if output_dtype is None else output_dtype\n    out = torch.empty((batch, nchunks, chunk_size, chunk_size) if not has_groups else (batch, nchunks, ngroups, chunk_size, chunk_size),\n                      device=a.device, dtype=out_dtype)\n    dot_dtype = (tl.bfloat16 if a.dtype == torch.bfloat16 or b.dtype == torch.bfloat16 else\n                 (tl.float16 if a.dtype == torch.float16 or b.dtype == torch.float16 else tl.float32))\n    grid = lambda META: (triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(chunk_size, META['BLOCK_SIZE_N']),\n                    batch, nchunks if not has_groups else nchunks * ngroups)\n    with torch.cuda.device(a.device.index):\n        _bmm_chunk_fwd_kernel[grid](\n            a, b, out, seq_idx,\n            int(seqlen), int(chunk_size), int(k), int(ngroups if has_groups else 1),\n            a.stride(0), a.stride(1), 0 if not has_groups else a.stride(2), a.stride(-1),\n            b.stride(0), b.stride(1), 0 if not has_groups else b.stride(2), b.stride(-1),\n            out.stride(0), out.stride(1), 0 if not has_groups else out.stride(2), out.stride(-2), out.stride(-1),\n            *((seq_idx.stride(0), seq_idx.stride(1)) if seq_idx is not None else (0, 0)),\n            causal,\n            dot_dtype,\n            HAS_SEQ_IDX=seq_idx is not None,\n        )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator consists of a kernel function '_chunk_cumsum_fwd_kernel' and a wrapper function '_chunk_cumsum_fwd'. The kernel computes the cumulative sum for each chunk of data, applying optional transformations such as bias addition and softplus activation. It processes data in blocks determined by configurations for optimal performance. The wrapper function manages data preparation and execution on the GPU, organizing data into appropriate shapes and launching the kernel.\n        Inputs:\n        - dt: Input tensor of shape (batch, seqlen, nheads).\n        - A: Scaling factors, 1D tensor of shape (nheads,).\n        - chunk_size: Size of each chunk for processing.\n        - dt_bias: Optional bias tensor for dt.\n        - dt_softplus: Boolean to apply softplus transformation.\n        - dt_limit: Clamping limits for dt.\n        Outputs:\n        - dA_cumsum: Cumulative sum result.\n        - dt_out: Modified dt.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _chunk_cumsum_fwd_kernel(\n    dt_ptr,  # pointer to the input tensor dt\n    A_ptr,   # pointer to the scaling factors A\n    dA_cumsum_ptr,  # pointer to the output cumulative sum result\n    dt_out_ptr,  # pointer to the output modified dt\n    dt_bias_ptr,  # pointer to the optional bias tensor dt_bias\n    dt_softplus,  # boolean to apply softplus transformation\n    dt_limit,  # clamping limits for dt\n    batch,  # batch size\n    seqlen,  # sequence length\n    nheads,  # number of heads\n    chunk_size,  # size of each chunk\n    BLOCK_SIZE: tl.constexpr,  # block size for parallel processing\n):\n    pid = tl.program_id(axis=0)\n    num_chunks = (seqlen + chunk_size - 1) // chunk_size\n    chunk_id = pid % num_chunks\n    head_id = (pid // num_chunks) % nheads\n    batch_id = pid // (num_chunks * nheads)\n\n    chunk_start = chunk_id * chunk_size\n    chunk_end = min(chunk_start + chunk_size, seqlen)\n\n    dt_offset = batch_id * seqlen * nheads + head_id * seqlen + chunk_start\n    A_offset = head_id\n    dA_cumsum_offset = batch_id * num_chunks * nheads + head_id * num_chunks + chunk_id\n    dt_out_offset = batch_id * seqlen * nheads + head_id * seqlen + chunk_start\n\n    if dt_bias_ptr is not None:\n        dt_bias_offset = head_id * seqlen + chunk_start\n\n    # Load data\n    dt_chunk = tl.load(dt_ptr + dt_offset, mask=chunk_start + tl.arange(0, BLOCK_SIZE) < chunk_end, other=0.0)\n    A_val = tl.load(A_ptr + A_offset)\n\n    if dt_bias_ptr is not None:\n        dt_bias_chunk = tl.load(dt_bias_ptr + dt_bias_offset, mask=chunk_start + tl.arange(0, BLOCK_SIZE) < chunk_end, other=0.0)\n        dt_chunk += dt_bias_chunk\n\n    if dt_softplus:\n        dt_chunk = tl.log(1.0 + tl.exp(dt_chunk))\n\n    dt_chunk = tl.clamp(dt_chunk, -dt_limit, dt_limit)\n\n    # Compute cumulative sum\n    dA_cumsum_chunk = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for i in range(chunk_start, chunk_end):\n        dA_cumsum_chunk[i - chunk_start] = dt_chunk[i - chunk_start] * A_val\n        if i > chunk_start:\n            dA_cumsum_chunk[i - chunk_start] += dA_cumsum_chunk[i - chunk_start - 1]\n\n    # Store results\n    tl.store(dA_cumsum_ptr + dA_cumsum_offset, dA_cumsum_chunk, mask=chunk_start + tl.arange(0, BLOCK_SIZE) < chunk_end)\n    tl.store(dt_out_ptr + dt_out_offset, dt_chunk, mask=chunk_start + tl.arange(0, BLOCK_SIZE) < chunk_end)\n\nimport torch\n\ndef _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=None, dt_softplus=False, dt_limit=10.0):\n    batch, seqlen, nheads = dt.shape\n    num_chunks = (seqlen + chunk_size - 1) // chunk_size\n\n    dA_cumsum = torch.zeros((batch, num_chunks, nheads), device=dt.device, dtype=dt.dtype)\n    dt_out = torch.zeros_like(dt)\n\n    grid = (batch * nheads * num_chunks,)\n\n    if dt_bias is not None:\n        dt_bias_ptr = dt_bias.data_ptr()\n    else:\n        dt_bias_ptr = None\n\n    _chunk_cumsum_fwd_kernel[grid](\n        dt.data_ptr(),\n        A.data_ptr(),\n        dA_cumsum.data_ptr(),\n        dt_out.data_ptr(),\n        dt_bias_ptr,\n        dt_softplus,\n        dt_limit,\n        batch,\n        seqlen,\n        nheads,\n        chunk_size,\n        BLOCK_SIZE=chunk_size,\n    )\n\n    return dA_cumsum, dt_out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_H': 1}),\n        triton.Config({'BLOCK_SIZE_H': 2}),\n        triton.Config({'BLOCK_SIZE_H': 4}),\n        triton.Config({'BLOCK_SIZE_H': 8}),\n        triton.Config({'BLOCK_SIZE_H': 16}),\n        triton.Config({'BLOCK_SIZE_H': 32}),\n        triton.Config({'BLOCK_SIZE_H': 64}),\n    ],\n    key=['chunk_size', 'nheads'],\n)\n@triton.jit\ndef _chunk_cumsum_fwd_kernel(\n    # Pointers to matrices\n    dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr, dA_cumsum_ptr,\n    # Matrix dimension\n    batch, seqlen, nheads, chunk_size,\n    dt_min, dt_max,\n    # Strides\n    stride_dt_batch, stride_dt_seqlen, stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_dt_out_batch, stride_dt_out_chunk, stride_dt_out_head, stride_dt_out_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize,\n    # Meta-parameters\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n    # Triton kernel implementation for chunked cumulative sum forward pass\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen)\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize)\n    dA_cs_ptrs = dA_cumsum_ptr + (offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    dt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, tl.log(1 + tl.exp(dt)), dt)\n    # As of Triton 2.2.0, tl.clamp is not available yet\n    # dt = tl.clamp(dt, dt_min, dt_max)\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0)\n    tl.store(dt_out_ptrs, dt, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dA = dt * A[:, None]\n    dA_cs = tl.cumsum(dA, axis=1)\n    tl.store(dA_cs_ptrs, dA_cs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\n\ndef _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=None, dt_softplus=False, dt_limit=(0.0, float(\"inf\"))):\n    \"\"\"\n    Function to perform the forward cumulative sum operation in chunks.\n\n    Arguments:\n    - dt: (batch, seqlen, nheads), the input tensor.\n    - A: (nheads,), the scaling factors.\n    - chunk_size: The size of each chunk to process at a time.\n    - dt_bias: (nheads,), optional, biases for dt if applicable.\n    - dt_softplus: Boolean, whether to apply the softplus operation to dt.\n    - dt_limit: Tuple, (min, max) limits for clamping dt values.\n\n    Returns:\n    - dA_cumsum: Cumulative sum result.\n    - dt_out: Modified dt after processing.\n    \"\"\"\n    batch, seqlen, nheads = dt.shape\n    assert A.shape == (nheads,)\n    if dt_bias is not None:\n        assert dt_bias.shape == (nheads,)\n    nchunks = math.ceil(seqlen / chunk_size)\n    dt_out = torch.empty(batch, nheads, nchunks, chunk_size, device=dt.device, dtype=torch.float32)\n    dA_cumsum = torch.empty(batch, nheads, nchunks, chunk_size, device=dt.device, dtype=torch.float32)\n    grid_chunk_cs = lambda META: (batch, nchunks, triton.cdiv(nheads, META['BLOCK_SIZE_H']))\n    with torch.cuda.device(dt.device.index):\n        _chunk_cumsum_fwd_kernel[grid_chunk_cs](\n            dt, A, dt_bias, dt_out, dA_cumsum,\n            int(batch), int(seqlen), int(nheads), int(chunk_size),\n            dt_limit[0], dt_limit[1],\n            dt.stride(0), dt.stride(1), dt.stride(2),\n            A.stride(0),\n            dt_bias.stride(0) if dt_bias is not None else 0,\n            dt_out.stride(0), dt_out.stride(2), dt_out.stride(1), dt_out.stride(3),\n            dA_cumsum.stride(0), dA_cumsum.stride(2), dA_cumsum.stride(1), dA_cumsum.stride(3),\n            dt_softplus,\n            HAS_DT_BIAS=dt_bias is not None,\n            BLOCK_SIZE_CHUNK=triton.next_power_of_2(chunk_size),\n        )\n    return dA_cumsum, dt_out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The code defines a Triton kernel named 'dequantize_kernel' and a Python function 'matmul_dequantize_int8'. The Triton kernel dequantizes an int8 matrix by multiplying it with a scale factor and stores the result as a float matrix. 'matmul_dequantize_int8' then computes the matrix multiplication of a float input matrix 'a' with the dequantized float matrix, resulting in a final matrix 'c'. Both functions ensure dimensions are compatible and handle device-specific memory allocation.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef dequantize_kernel(\n    int8_input_ptr,  # Pointer to the int8 input matrix\n    float_output_ptr,  # Pointer to the float output matrix\n    scale_factor,  # Scale factor for dequantization\n    M,  # Number of rows in the input matrix\n    N,  # Number of columns in the input matrix\n    stride_im,  # Stride for rows in the int8 input matrix\n    stride_in,  # Stride for columns in the int8 input matrix\n    stride_fm,  # Stride for rows in the float output matrix\n    stride_fn,  # Stride for columns in the float output matrix\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for rows\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for columns\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_program = num_pid_m * num_pid_n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    block_offset_m = pid_m * BLOCK_SIZE_M\n    block_offset_n = pid_n * BLOCK_SIZE_N\n\n    offsets_m = block_offset_m + tl.arange(0, BLOCK_SIZE_M)\n    offsets_n = block_offset_n + tl.arange(0, BLOCK_SIZE_N)\n\n    mask_m = offsets_m < M\n    mask_n = offsets_n < N\n\n    for i in range(0, BLOCK_SIZE_M, 32):\n        for j in range(0, BLOCK_SIZE_N, 32):\n            offsets = offsets_m[i:i+32] * stride_im + offsets_n[j:j+32] * stride_in\n            int8_values = tl.load(int8_input_ptr + offsets, mask=mask_m[i:i+32] & mask_n[j:j+32], other=0)\n\n            float_values = int8_values * scale_factor\n\n            offsets = offsets_m[i:i+32] * stride_fm + offsets_n[j:j+32] * stride_fn\n            tl.store(float_output_ptr + offsets, float_values, mask=mask_m[i:i+32] & mask_n[j:j+32])\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef matmul_dequantize_int8(a: torch.Tensor, int8_b: torch.Tensor, scale_factor: float) -> torch.Tensor:\n    # Ensure input tensors are on the same device\n    device = a.device\n    assert int8_b.device == device, \"Input tensors must be on the same device\"\n\n    # Ensure input tensors have compatible dimensions\n    M, K = a.shape\n    K, N = int8_b.shape\n    assert K == int8_b.shape[0], \"Matrix dimensions are not compatible for multiplication\"\n\n    # Allocate memory for the dequantized matrix\n    float_b = torch.empty((K, N), dtype=torch.float32, device=device)\n\n    # Launch the dequantize kernel\n    grid = lambda META: (triton.cdiv(K, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    dequantize_kernel[grid](int8_b, float_b, scale_factor, K, N, int8_b.stride(0), int8_b.stride(1), float_b.stride(0), float_b.stride(1), BLOCK_SIZE_M=16, BLOCK_SIZE_N=16)\n\n    # Perform the matrix multiplication\n    c = torch.matmul(a, float_b)\n\n    return c\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    ],\n    key=['K', 'N'],\n)\n\n\n@triton.jit\ndef dequantize_kernel(\n    b_ptr, b_scale_ptr, fpb_ptr,\n    K, N,\n    stride_bk, stride_bn,\n    stride_fpbk, stride_fpbn,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + \\\n        (n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + \\\n        (n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n\ndef matmul_dequantize_int8(a, b, b_scale, out=None):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    if out == None:\n        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    else:\n        c = out\n    fp_b = torch.empty((K, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (\n        triton.cdiv(K, META['BLOCK_SIZE_K']), triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n    dequantize_kernel[grid](\n        b, b_scale, fp_b,\n        K, N,\n        b.stride(0), b.stride(1),\n        fp_b.stride(0), fp_b.stride(1)\n    )\n    torch.mm(a, fp_b, out=c)\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton kernel function and a PyTorch wrapper function for a fused operation involving addition, multiplication, and activation on tensors. The Triton kernel 'fused_add_mul_activation_kernel' takes three pointers to tensors and several constants as input. It computes a fused operation on each block of the input tensor. The function 'fused_add_mul_activation_torch' is a Python function that sets up the execution grid for the Triton kernel and calls it on given input tensors. It takes three PyTorch tensors as arguments and applies the specified operations using the Triton kernel, returning the modified tensor.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_add_mul_activation_kernel(\n    x_ptr,  # Pointer to input tensor x\n    y_ptr,  # Pointer to input tensor y\n    z_ptr,  # Pointer to input tensor z\n    output_ptr,  # Pointer to output tensor\n    n_elements,  # Number of elements in the tensors\n    BLOCK_SIZE: tl.constexpr,  # Block size for parallelism\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    block_start = pid * BLOCK_SIZE  # Calculate the start index for the block\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # Create offsets for the block\n    mask = offsets < n_elements  # Create a mask to handle the case where the block extends beyond the tensor\n\n    x = tl.load(x_ptr + offsets, mask=mask)  # Load the elements from x\n    y = tl.load(y_ptr + offsets, mask=mask)  # Load the elements from y\n    z = tl.load(z_ptr + offsets, mask=mask)  # Load the elements from z\n\n    # Perform the fused operation: (x + y) * z\n    result = (x + y) * z\n\n    # Apply an activation function (e.g., ReLU)\n    result = tl.where(result > 0, result, 0)\n\n    tl.store(output_ptr + offsets, result, mask=mask)  # Store the result back to the output tensor\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef fused_add_mul_activation_torch(x, y, z):\n    # Ensure the input tensors are on the same device and have the same shape\n    assert x.device == y.device == z.device, \"Tensors must be on the same device\"\n    assert x.shape == y.shape == z.shape, \"Tensors must have the same shape\"\n\n    # Get the number of elements in the tensors\n    n_elements = x.numel()\n\n    # Allocate the output tensor\n    output = torch.empty_like(x)\n\n    # Define the grid and block dimensions\n    grid = (triton.cdiv(n_elements, 1024),)\n    block = (1024,)\n\n    # Launch the Triton kernel\n    fused_add_mul_activation_kernel[grid, block](\n        x, y, z, output, n_elements, BLOCK_SIZE=1024\n    )\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef fused_add_mul_activation_kernel(x_ptr, bias_ptr, in_ptr,\n                                    num_weights: tl.constexpr,\n                                    xnumel: tl.constexpr,\n                                    multiplier: tl.constexpr,\n                                    activation: tl.constexpr,\n                                    BLOCK_SIZE: tl.constexpr):\n    xoffset = tl.program_id(0) * BLOCK_SIZE\n    index = xoffset + tl.arange(0, BLOCK_SIZE)[:]\n    mask = index < xnumel\n    bias_index = index % num_weights\n    tmp0 = tl.load(x_ptr + index, mask)\n    tmp1 = tl.load(bias_ptr + bias_index, mask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr + index, mask)\n    activ_input = multiplier * tmp3 + tmp0 + tmp1\n    if activation == \"sigmoid\":\n        ma_result = tl.sigmoid(activ_input)\n        # option 2 - calculate sigmoid using exp\n        # ma_result = 1.0 / (1.0 + tl.exp(-sigmoid_input))\n        # option 3: fast sigmoid - inaccurate but faster\n        # ma_result = 1.0 / (1.0 + tl.abs(sigmoid_input))\n    elif activation == \"relu\":\n        ma_result = tl.maximum(0, activ_input)\n\n    tl.store(x_ptr + index, ma_result, mask)\n\n\ndef fused_add_mul_activation_torch(in_out_tensor: torch.Tensor, bias: torch.Tensor,\n                                   in_tensor: torch.Tensor) -> torch.Tensor:\n    # print(\"calling fused_add_mul_relu_torch\")\n    grid = lambda meta: (triton.cdiv(in_out_tensor.numel(), meta['BLOCK_SIZE']),)\n    BLOCK_SIZE = min(2048, in_out_tensor.numel())\n    fused_add_mul_activation_kernel[grid](in_out_tensor, bias, in_tensor,\n                                          bias.numel(),\n                                          in_out_tensor.numel(),\n                                          multiplier=0.5,\n                                          activation=\"sigmoid\",\n                                          BLOCK_SIZE=BLOCK_SIZE)\n    return in_out_tensor\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel `matmul_tma_load_store` performs matrix multiplication using block pointers to load matrices A and B, then computes the product matrix C. The kernel can optionally output C in float16. Inputs include pointers to matrices A, B, and C, their dimensions (M, N, K), strides, and block sizes (BLOCK_M, BLOCK_N, BLOCK_K). The wrapper `warpper_tma_load_store` prepares random input matrices and calls the kernel with specified parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_tma_load_store(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n    OUTPUT_FLOAT16: tl.constexpr\n):\n    # Compute the block ID in the grid\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) // num_pid_m\n\n    # Compute the block offsets\n    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Load the block pointers\n    A = tl.make_block_ptr(\n        base=A_ptr,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(offs_am, offs_k),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0)\n    )\n    B = tl.make_block_ptr(\n        base=B_ptr,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(offs_k, offs_bn),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(0, 1)\n    )\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(A)\n        b = tl.load(B)\n        acc += tl.dot(a, b)\n        A = tl.advance(A, (0, BLOCK_K))\n        B = tl.advance(B, (BLOCK_K, 0))\n\n    # Store the result\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = tl.make_block_ptr(\n        base=C_ptr,\n        shape=(M, N),\n        strides=(stride_cm, stride_cn),\n        offsets=(offs_cm, offs_cn),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0)\n    )\n    if OUTPUT_FLOAT16:\n        acc = acc.to(tl.float16)\n    tl.store(C, acc)\n\nimport triton\nimport triton.language as tl\nimport torch\n\ndef wrapper_tma_load_store(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, OUTPUT_FLOAT16=False):\n    # Generate random input matrices\n    A = torch.randn((M, K), device='cuda', dtype=torch.float32)\n    B = torch.randn((K, N), device='cuda', dtype=torch.float32)\n    C = torch.empty((M, N), device='cuda', dtype=torch.float16 if OUTPUT_FLOAT16 else torch.float32)\n\n    # Compute the grid size\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n    # Launch the kernel\n    matmul_tma_load_store[grid](\n        A, B, C,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_M, BLOCK_N, BLOCK_K,\n        OUTPUT_FLOAT16\n    )\n\n    return C\n\n# Example usage\nM, N, K = 1024, 1024, 1024\nBLOCK_M, BLOCK_N, BLOCK_K = 16, 16, 16\nC = wrapper_tma_load_store(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, OUTPUT_FLOAT16=True)\nprint(C)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_tma_load_store(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n        OUTPUT_F16: tl.constexpr\n):\n    # Create block pointers for A, B, and C matrices\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn), offsets=(0, 0),\n                                    block_shape=(BLOCK_K, BLOCK_N), order=(0, 1))\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    # Load A and B blocks\n    a = tl.load(a_block_ptr)\n    b = tl.load(b_block_ptr)\n\n    # Compute matrix product\n    c = tl.dot(a, b)\n    # Optionally convert the result to float16\n    if OUTPUT_F16:\n        c = c.to(tl.float16)\n\n    # Store the result\n    tl.store(c_block_ptr, c)\n\n\ndef warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F16):\n    # Prepare input matrices\n    if (TRANS_A):\n        a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n    else:\n        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    if (TRANS_B):\n        b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n    else:\n        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    # Prepare output matrix\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n    if OUTPUT_F16:\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n\n    # Execute Triton kernel\n    matmul_tma_load_store[(1, 1)](\n        a_ptr=a, b_ptr=b, c_ptr=c,\n        M=M, N=N, K=K,\n        stride_am=a.stride(0), stride_ak=a.stride(1),\n        stride_bk=b.stride(0), stride_bn=b.stride(1),\n        stride_cm=c.stride(0), stride_cn=c.stride(1),\n        BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,\n        num_warps=NUM_WARPS, num_ctas=NUM_CTAS,\n        OUTPUT_F16=OUTPUT_F16)\n    return c\n    \n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton code contains two main functions, `triton_mul2` and `triton_mul2_inplace`, which double the values in a PyTorch tensor. The `triton_mul2` function uses a kernel `mul2_kernel` to output the result to a new tensor, while `triton_mul2_inplace` modifies the input tensor directly with `mul2_inplace_kernel`. Both kernels use a block-based approach to process data in parallel, where `BLOCK_SIZE` determines the number of elements processed per block. The kernels load data from the input tensor, multiply each element by 2, and store the result. They ensure no out-of-bounds memory access by using masks based on `n_elements`, the total number of elements in the tensor.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n# Kernel to double the values in the input tensor and store the result in a new tensor\n@triton.jit\ndef mul2_kernel(\n    x_ptr,  # Pointer to the input tensor\n    y_ptr,  # Pointer to the output tensor\n    n_elements,  # Number of elements in the tensor\n    BLOCK_SIZE: tl.constexpr,  # Block size\n):\n    # Compute the block ID\n    pid = tl.program_id(axis=0)\n    # Compute the start and end indices for this block\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to avoid out-of-bounds memory access\n    mask = offsets < n_elements\n    # Load the data from the input tensor\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Perform the multiplication\n    y = x * 2\n    # Store the result in the output tensor\n    tl.store(y_ptr + offsets, y, mask=mask)\n\n# Kernel to double the values in the input tensor in place\n@triton.jit\ndef mul2_inplace_kernel(\n    x_ptr,  # Pointer to the input tensor\n    n_elements,  # Number of elements in the tensor\n    BLOCK_SIZE: tl.constexpr,  # Block size\n):\n    # Compute the block ID\n    pid = tl.program_id(axis=0)\n    # Compute the start and end indices for this block\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # Create a mask to avoid out-of-bounds memory access\n    mask = offsets < n_elements\n    # Load the data from the input tensor\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # Perform the multiplication\n    y = x * 2\n    # Store the result back in the input tensor\n    tl.store(x_ptr + offsets, y, mask=mask)\n\n# Wrapper function to call the mul2_kernel\ndef triton_mul2(x: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is on the GPU\n    x = x.cuda()\n    # Create an output tensor with the same shape and type as the input tensor\n    y = torch.empty_like(x)\n    # Compute the number of elements in the tensor\n    n_elements = x.numel()\n    # Compute the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    # Launch the kernel\n    mul2_kernel[grid, BLOCK_SIZE](x, y, n_elements, BLOCK_SIZE)\n    # Return the output tensor\n    return y\n\n# Wrapper function to call the mul2_inplace_kernel\ndef triton_mul2_inplace(x: torch.Tensor) -> None:\n    # Ensure the input tensor is on the GPU\n    x = x.cuda()\n    # Compute the number of elements in the tensor\n    n_elements = x.numel()\n    # Compute the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    # Launch the kernel\n    mul2_inplace_kernel[grid, BLOCK_SIZE](x, n_elements, BLOCK_SIZE)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a PyTorch tensor\n    x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n    print(\"Original tensor:\", x)\n\n    # Double the values and store the result in a new tensor\n    y = triton_mul2(x)\n    print(\"Doubled tensor (new tensor):\", y)\n\n    # Double the values in place\n    triton_mul2_inplace(x)\n    print(\"Doubled tensor (in place):\", x)\n", "label": "import triton\nfrom triton import language as tl\nimport torch\n\n\n@triton.jit\ndef mul2_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = 2 * x\n    tl.store(out_ptr + offsets, output, mask=mask)\n\n@triton.jit\ndef mul2_inplace_kernel(\n    ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(ptr + offsets, mask=mask)\n    output = 2 * x\n    tl.store(ptr + offsets, output, mask=mask)\n\n\ndef triton_mul2(x, BLOCK_SIZE=16):\n    output = torch.zeros_like(x)\n    n_elements = x.numel()\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    mul2_kernel[grid](x, output, n_elements, BLOCK_SIZE)\n    return output\n\ndef triton_mul2_inplace(x, BLOCK_SIZE=16):\n    n_elements = x.numel()\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    mul2_inplace_kernel[grid](x, n_elements, BLOCK_SIZE)\n    return x\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs a parameter update operation for optimization. The kernel function `update_fn_kernel` is applied across blocks of elements defined by `BLOCK_SIZE`. It reads parameter (`p_ptr`), gradient (`grad_ptr`), and exponential moving average (`exp_avg_ptr`) from global memory, and updates them using weight decay, momentum, and learning rate. It stores the updated parameter and average back into global memory. The host function `update_fn` serves as a wrapper, ensuring the operation is performed on CUDA tensors and determines the execution grid size based on the number of elements.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef update_fn_kernel(\n    p_ptr,  # Pointer to the parameters\n    grad_ptr,  # Pointer to the gradients\n    exp_avg_ptr,  # Pointer to the exponential moving averages\n    n_elements,  # Number of elements\n    weight_decay,  # Weight decay factor\n    momentum,  # Momentum factor\n    learning_rate,  # Learning rate\n    BLOCK_SIZE: tl.constexpr  # Block size\n):\n    pid = tl.program_id(axis=0)  # Get the program ID\n    block_start = pid * BLOCK_SIZE  # Start index of the block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # Offsets within the block\n\n    # Load data from global memory\n    p = tl.load(p_ptr + offsets, mask=offsets < n_elements)\n    grad = tl.load(grad_ptr + offsets, mask=offsets < n_elements)\n    exp_avg = tl.load(exp_avg_ptr + offsets, mask=offsets < n_elements)\n\n    # Apply weight decay\n    p = p * (1 - weight_decay * learning_rate)\n\n    # Update exponential moving average\n    exp_avg = momentum * exp_avg + (1 - momentum) * grad\n\n    # Update parameters\n    p = p - learning_rate * exp_avg\n\n    # Store updated data back to global memory\n    tl.store(p_ptr + offsets, p, mask=offsets < n_elements)\n    tl.store(exp_avg_ptr + offsets, exp_avg, mask=offsets < n_elements)\n\nimport torch\n\ndef update_fn(p, grad, exp_avg, weight_decay, momentum, learning_rate):\n    # Ensure the tensors are on the same device\n    assert p.device == grad.device == exp_avg.device, \"All tensors must be on the same device\"\n    device = p.device\n\n    # Convert tensors to pointers\n    p_ptr = p.data_ptr()\n    grad_ptr = grad.data_ptr()\n    exp_avg_ptr = exp_avg.data_ptr()\n\n    # Number of elements\n    n_elements = p.numel()\n\n    # Define block size\n    BLOCK_SIZE = 256\n\n    # Determine grid size\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    update_fn_kernel[grid_size, BLOCK_SIZE](\n        p_ptr, grad_ptr, exp_avg_ptr, n_elements, weight_decay, momentum, learning_rate\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create example tensors\n    p = torch.tensor([1.0, 2.0, 3.0, 4.0], device='cuda')\n    grad = torch.tensor([0.1, 0.2, 0.3, 0.4], device='cuda')\n    exp_avg = torch.tensor([0.0, 0.0, 0.0, 0.0], device='cuda')\n\n    # Parameters\n    weight_decay = 0.01\n    momentum = 0.9\n    learning_rate = 0.01\n\n    # Perform the update\n    update_fn(p, grad, exp_avg, weight_decay, momentum, learning_rate)\n\n    # Print the updated parameters and exponential moving averages\n    print(\"Updated parameters:\", p)\n    print(\"Updated exponential moving averages:\", exp_avg)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(configs = [\n    triton.Config({'BLOCK_SIZE': 128}, num_warps = 4),\n    triton.Config({'BLOCK_SIZE': 1024}, num_warps = 8),\n], key = ['n_elements'], restore_value=['p_ptr', 'exp_avg_ptr'])\n\n# Triton CUDA kernel\n\n@triton.jit\ndef update_fn_kernel(\n    p_ptr,\n    grad_ptr,\n    exp_avg_ptr,\n    lr,\n    wd,\n    beta1,\n    beta2,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n\n    # Offsetted pointers\n    offset_p_ptr = p_ptr + offsets\n    offset_grad_ptr = grad_ptr + offsets\n    offset_exp_avg_ptr = exp_avg_ptr + offsets\n\n    # Load\n    p = tl.load(offset_p_ptr, mask=mask)\n    grad = tl.load(offset_grad_ptr, mask=mask)\n    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\n\n    # Stepweight decay\n    p = p * (1 - lr * wd)\n\n    # Diff between momentum running average and grad\n    diff = exp_avg - grad\n\n    # Weight update\n    update = diff * beta1 + grad\n\n    # torch.sign\n    can_update = update != 0\n    update_sign = tl.where(update > 0, -lr, lr)\n\n    p = p + update_sign * can_update\n\n    # Decay the momentum running average coefficient\n    exp_avg = diff * beta2 + grad\n\n    # Store new params and momentum running average coefficient\n    tl.store(offset_p_ptr, p, mask=mask)\n    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)\n\ndef update_fn(\n    p: torch.Tensor,\n    grad: torch.Tensor,\n    exp_avg: torch.Tensor,\n    lr: float,\n    wd: float,\n    beta1: float,\n    beta2: float\n):\n    assert all([t.is_cuda for t in (p, grad, exp_avg)])\n    n_elements = p.numel()\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    update_fn_kernel[grid](\n        p,\n        grad,\n        exp_avg,\n        lr,\n        wd,\n        beta1,\n        beta2,\n        n_elements\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton-based code provides an implementation of the argmax operation with three kernels. The `argmax` function determines the maximum value and its index across elements of a tensor. It is optimized for GPU execution.\n\n        There are three kernels: \n        1. `argmax_kernel_1`: Processes a block of the input tensor, finding the local maximum and its index in the block.\n        2. `argmax_kernel_2`: Aggregates the results from `argmax_kernel_1` to find the overall maximum value's index.\n        3. `argmax_kernel`: Handles multi-dimensional tensors, finding the maximum index along a specified dimension.\n\n        The `argmax` function adapts based on whether the operation is performed over the entire tensor or a specific dimension, adjusting intermediate block sizes and determining if int32 or int64 indices are required.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Kernel 1: Processes a block of the input tensor, finding the local maximum and its index in the block.\n@triton.jit\ndef argmax_kernel_1(X, X_size, max_val, max_idx, stride, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < X_size\n\n    x = tl.load(X + offsets, mask=mask, other=-float('inf'))\n    max_val_local = tl.max(x, axis=0)\n    max_idx_local = tl.argmax(x, axis=0)\n\n    tl.store(max_val + pid, max_val_local)\n    tl.store(max_idx + pid, max_idx_local)\n\n# Kernel 2: Aggregates the results from argmax_kernel_1 to find the overall maximum value's index.\n@triton.jit\ndef argmax_kernel_2(max_val, max_idx, max_val_out, max_idx_out, num_blocks: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    if pid == 0:\n        max_val_local = tl.load(max_val, mask=tl.arange(0, num_blocks) < num_blocks, other=-float('inf'))\n        max_idx_local = tl.argmax(max_val_local, axis=0)\n        max_val_global = tl.max(max_val_local, axis=0)\n\n        tl.store(max_val_out, max_val_global)\n        tl.store(max_idx_out, max_idx_local)\n\n# Kernel 3: Handles multi-dimensional tensors, finding the maximum index along a specified dimension.\n@triton.jit\ndef argmax_kernel(X, X_size, max_val_out, max_idx_out, stride, dim, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < X_size\n\n    x = tl.load(X + offsets * stride, mask=mask, other=-float('inf'))\n    max_val_local = tl.max(x, axis=0)\n    max_idx_local = tl.argmax(x, axis=0)\n\n    tl.store(max_val_out + pid, max_val_local)\n    tl.store(max_idx_out + pid, max_idx_local)\n\n# Wrapper function to handle the argmax operation\ndef argmax(x, dim=None):\n    if dim is None:\n        # Flatten the tensor and find the argmax\n        x_size = x.size\n        max_val = triton.empty((1,), device=x.device, dtype=x.dtype)\n        max_idx = triton.empty((1,), device=x.device, dtype=tl.int32)\n\n        # Step 1: Process blocks\n        num_blocks = (x_size + BLOCK_SIZE - 1) // BLOCK_SIZE\n        argmax_kernel_1[(num_blocks,)](x, x_size, max_val, max_idx, 1, BLOCK_SIZE=BLOCK_SIZE)\n\n        # Step 2: Aggregate results\n        argmax_kernel_2[(1,)](max_val, max_idx, max_val, max_idx, num_blocks)\n\n        return max_idx.item()\n    else:\n        # Handle multi-dimensional tensor\n        shape = x.shape\n        dim_size = shape[dim]\n        other_size = x.size // dim_size\n        max_val_out = triton.empty((other_size,), device=x.device, dtype=x.dtype)\n        max_idx_out = triton.empty((other_size,), device=x.device, dtype=tl.int32)\n\n        # Step 1: Process blocks along the specified dimension\n        num_blocks = (dim_size + BLOCK_SIZE - 1) // BLOCK_SIZE\n        argmax_kernel[(other_size,)](x, dim_size, max_val_out, max_idx_out, other_size, dim, BLOCK_SIZE=BLOCK_SIZE)\n\n        return max_idx_out\n\n# Example usage\nif __name__ == \"__main__\":\n    import torch\n\n    # Example tensor\n    x = torch.tensor([1.0, 3.0, 2.0, 5.0, 4.0], device='cuda')\n\n    # Find argmax of the entire tensor\n    max_idx = argmax(x)\n    print(f\"Max index of the entire tensor: {max_idx}\")\n\n    # Find argmax along a specific dimension\n    x_2d = torch.tensor([[1.0, 3.0, 2.0], [5.0, 4.0, 6.0]], device='cuda')\n    max_idx_2d = argmax(x_2d, dim=1)\n    print(f\"Max indices along dimension 1: {max_idx_2d}\")\n", "label": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n\ndef can_use_int32_index(tensor):\n    # This function checks if the tensor can use int32 indices\n    return tensor.numel() < 2**31\n\n# Kernel 1: argmax_kernel_1\n@triton.jit\ndef argmax_kernel_1(\n    inp,\n    mid_value,\n    mid_index,\n    M,\n    BLOCK_SIZE: tl.constexpr,\n    INT64_INDEX: tl.constexpr = False,\n):\n    pid = tl.program_id(0)\n    if INT64_INDEX:\n        pid = pid.to(tl.int64)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    inp_ptrs = inp + offset\n    mask = offset < M\n    inp_val = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    max_val, max_index = tl.max(inp_val, axis=0, return_indices=True)\n    max_index = max_index + pid * BLOCK_SIZE\n    mid_value_ptr = mid_value + pid\n    max_index_ptr = mid_index + pid\n    tl.store(mid_value_ptr, max_val)\n    tl.store(max_index_ptr, max_index)\n\n# Kernel 2: argmax_kernel_2\n@triton.jit\ndef argmax_kernel_2(mid_value, mid_index, out, mid_size, BLOCK_MID: tl.constexpr):\n    offset = tl.arange(0, BLOCK_MID)\n    mid_ptrs = mid_value + offset\n    mask = offset < mid_size\n    mid_val = tl.load(mid_ptrs, mask=mask, other=-float(\"inf\"))\n    index_val = tl.argmax(mid_val, axis=0)\n    mid_index_ptrs = mid_index + index_val\n    out_val = tl.load(mid_index_ptrs)\n    tl.store(out, out_val)\n\n# Kernel 3: argmax_kernel\n@triton.jit\ndef argmax_kernel(\n    inp,\n    out_index,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    INT64_INDEX: tl.constexpr = False,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    if INT64_INDEX:\n        pid_m = pid_m.to(tl.int64)\n        pid_k = pid_k.to(tl.int64)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    max_values = tl.full([BLOCK_M], dtype=tl.float32, value=float(\"-inf\"))\n    argmax_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)\n    for start_n in range(0, N, BLOCK_N):\n        n_offset = start_n + tl.arange(0, BLOCK_N)\n        offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n        mask = m_offset[:, None] < M and n_offset[None, :] < N\n        inp_ptrs = inp + offset\n        inp_vals = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n        local_max, local_argmax = tl.max(\n            inp_vals, 1, return_indices=True, return_indices_tie_break_left=True\n        )\n        update = local_max > max_values\n        max_values = tl.where(update, local_max, max_values)\n        argmax_values = tl.where(update, start_n + local_argmax, argmax_values)\n\n    offset_index = m_offset * K + pid_k\n    out_index_ptrs = out_index + offset_index\n    mask1 = m_offset < M\n    tl.store(out_index_ptrs, argmax_values, mask=mask1)\n\n# Function calling the kernels\ndef argmax(inp, dim=None, keepdim=False, *, dtype=None):\n    if dim is None:\n        M = inp.numel()\n        if dtype is None:\n            dtype = inp.dtype\n        block_size = triton.next_power_of_2(math.ceil(math.sqrt(M)))\n        mid_size = triton.cdiv(M, block_size)\n        block_mid = triton.next_power_of_2(mid_size)\n        use_int64_index = not can_use_int32_index(inp)\n\n        mid_value = torch.empty((mid_size,), dtype=dtype, device=inp.device)\n        mid_index = torch.empty((mid_size,), dtype=torch.int64, device=inp.device)\n        if keepdim:\n            shape = list(inp.shape)\n            for i in range(0, inp.dim()):\n                shape[i] = 1\n            out = torch.empty(shape, dtype=torch.int64, device=inp.device)\n        else:\n            out = torch.empty([], dtype=torch.int64, device=inp.device)\n\n        with torch.cuda.device(inp.device):\n            argmax_kernel_1[(mid_size, 1, 1)](\n                inp,\n                mid_value,\n                mid_index,\n                M,\n                block_size,\n                INT64_INDEX=use_int64_index,\n            )\n            argmax_kernel_2[(1, 1, 1)](mid_value, mid_index, out, mid_size, block_mid)\n        return out\n    else:\n        assert dim >= -inp.ndim and dim < inp.ndim, \"Invalid dim\"\n        shape = inp.shape\n        dim = dim % inp.ndim\n        N = shape[dim]\n        M = math.prod(shape[:dim])\n        K = inp.numel() // M // N\n\n        inp = inp.contiguous()\n        use_int64_index = not can_use_int32_index(inp)\n\n        shape_list = list(shape)\n        shape_list[dim] = 1\n        out_index = torch.empty(shape_list, dtype=torch.int64, device=inp.device)\n        if not keepdim:\n            out_index = torch.squeeze(out_index, dim)\n\n        BLOCK_M = 128  # Example, adjust as needed\n        BLOCK_N = 128  # Example, adjust as needed\n\n        grid = lambda meta: (\n            triton.cdiv(M, meta[\"BLOCK_M\"]),\n            K,\n        )\n        with torch.cuda.device(inp.device):\n            argmax_kernel[grid](\n                inp,\n                out_index,\n                M,\n                N,\n                K,\n                BLOCK_M=BLOCK_M,\n                BLOCK_N=BLOCK_N,\n                INT64_INDEX=use_int64_index,\n            )\n\n        return out_index\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel function, `_layer_norm_fwd_1pass_kernel`, performs layer normalization with optional dropout and bias on input tensors. It also supports element-wise operations with additional tensors like `x1`, `weight1`, and `bias1`. Inputs include `X`, `Y`, `W`, `B` and optionally `RESIDUAL`, `X1`, `W1`, `B1`, among others. The function computes normalized outputs and stores them in `Y` and optionally in `Y1`. It also computes the mean and inverse standard deviation for normalization if `IS_RMS_NORM` is False. Outputs include `y`, `y1`, `mean`, `rstd`, `residual_out`, `seeds`, `dropout_mask`, and `dropout_mask1`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, Y, W, B, \n    RESIDUAL, X1, W1, B1, \n    Y1, MEAN, RSTD, \n    RESIDUAL_OUT, SEEDS, DROPOUT_MASK, DROPOUT_MASK1, \n    N, E, EPS, DROPOUT_P, \n    IS_RMS_NORM: tl.constexpr, \n    HAS_RESIDUAL: tl.constexpr, \n    HAS_X1: tl.constexpr, \n    HAS_W1: tl.constexpr, \n    HAS_B1: tl.constexpr, \n    HAS_DROPOUT: tl.constexpr, \n    HAS_DROPOUT1: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    block_size = tl.num_programs(axis=0)\n    row = pid * N\n\n    # Load input data\n    x_ptr = X + row\n    w_ptr = W + row\n    b_ptr = B + row\n    x = tl.load(x_ptr, mask=row < N * E, other=0.0)\n    w = tl.load(w_ptr, mask=row < N * E, other=0.0)\n    b = tl.load(b_ptr, mask=row < N * E, other=0.0)\n\n    # Compute mean and variance\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / E\n        var = tl.sum((x - mean) ** 2, axis=0) / E\n        rstd = 1.0 / tl.sqrt(var + EPS)\n    else:\n        mean = 0.0\n        rstd = 1.0 / tl.sqrt(tl.sum(x ** 2, axis=0) / E + EPS)\n\n    # Normalize\n    x_norm = (x - mean) * rstd\n    y = x_norm * w + b\n\n    # Apply dropout if enabled\n    if HAS_DROPOUT:\n        seed = tl.load(SEEDS + pid)\n        dropout_mask = tl.rand(seed, row) > DROPOUT_P\n        y = tl.where(dropout_mask, y, 0.0)\n        tl.store(DROPOUT_MASK + row, dropout_mask, mask=row < N * E)\n\n    # Store the normalized output\n    tl.store(Y + row, y, mask=row < N * E)\n\n    # Compute and store mean and rstd\n    if MEAN is not None:\n        tl.store(MEAN + pid, mean)\n    if RSTD is not None:\n        tl.store(RSTD + pid, rstd)\n\n    # Handle residual and additional tensors\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + row, mask=row < N * E, other=0.0)\n        y = y + residual\n        tl.store(RESIDUAL_OUT + row, y, mask=row < N * E)\n\n    if HAS_X1:\n        x1_ptr = X1 + row\n        x1 = tl.load(x1_ptr, mask=row < N * E, other=0.0)\n        x1_norm = (x1 - mean) * rstd\n\n        if HAS_W1:\n            w1_ptr = W1 + row\n            w1 = tl.load(w1_ptr, mask=row < N * E, other=0.0)\n            x1_norm = x1_norm * w1\n\n        if HAS_B1:\n            b1_ptr = B1 + row\n            b1 = tl.load(b1_ptr, mask=row < N * E, other=0.0)\n            x1_norm = x1_norm + b1\n\n        y1 = x1_norm\n\n        if HAS_DROPOUT1:\n            seed1 = tl.load(SEEDS + pid + block_size)\n            dropout_mask1 = tl.rand(seed1, row) > DROPOUT_P\n            y1 = tl.where(dropout_mask1, y1, 0.0)\n            tl.store(DROPOUT_MASK1 + row, dropout_mask1, mask=row < N * E)\n\n        tl.store(Y1 + row, y1, mask=row < N * E)\n\nimport torch\n\ndef layer_norm_fwd_1pass(\n    X, W, B, \n    RESIDUAL=None, X1=None, W1=None, B1=None, \n    Y=None, Y1=None, MEAN=None, RSTD=None, \n    RESIDUAL_OUT=None, SEEDS=None, DROPOUT_MASK=None, DROPOUT_MASK1=None, \n    EPS=1e-5, DROPOUT_P=0.0, IS_RMS_NORM=False, \n    HAS_RESIDUAL=False, HAS_X1=False, HAS_W1=False, HAS_B1=False, HAS_DROPOUT=False, HAS_DROPOUT1=False\n):\n    N, E = X.shape\n    assert W.shape == (E,)\n    assert B.shape == (E,)\n    if RESIDUAL is not None:\n        assert RESIDUAL.shape == (N, E)\n    if X1 is not None:\n        assert X1.shape == (N, E)\n    if W1 is not None:\n        assert W1.shape == (E,)\n    if B1 is not None:\n        assert B1.shape == (E,)\n    if Y is None:\n        Y = torch.empty_like(X)\n    if Y1 is not None:\n        assert Y1.shape == (N, E)\n    if MEAN is not None:\n        assert MEAN.shape == (N,)\n    if RSTD is not None:\n        assert RSTD.shape == (N,)\n    if RESIDUAL_OUT is not None:\n        assert RESIDUAL_OUT.shape == (N, E)\n    if SEEDS is not None:\n        assert SEEDS.shape == (N,)\n    if DROPOUT_MASK is not None:\n        assert DROPOUT_MASK.shape == (N, E)\n    if DROPOUT_MASK1 is not None:\n        assert DROPOUT_MASK1.shape == (N, E)\n\n    grid = (N,)\n\n    _layer_norm_fwd_1pass_kernel[grid](\n        X, Y, W, B, \n        RESIDUAL, X1, W1, B1, \n        Y1, MEAN, RSTD, \n        RESIDUAL_OUT, SEEDS, DROPOUT_MASK, DROPOUT_MASK1, \n        N, E, EPS, DROPOUT_P, \n        IS_RMS_NORM, \n        HAS_RESIDUAL, \n        HAS_X1, \n        HAS_W1, \n        HAS_B1, \n        HAS_DROPOUT, \n        HAS_DROPOUT1\n    )\n\n    return Y, Y1, MEAN, RSTD, RESIDUAL_OUT, DROPOUT_MASK, DROPOUT_MASK1\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_RESIDUAL\", \"STORE_RESIDUAL_OUT\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n@triton.heuristics({\"HAS_X1\": lambda args: args[\"X1\"] is not None})\n@triton.heuristics({\"HAS_W1\": lambda args: args[\"W1\"] is not None})\n@triton.heuristics({\"HAS_B1\": lambda args: args[\"B1\"] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, Y, W, B, RESIDUAL, X1, W1, B1, Y1, RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd,\n    stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row, stride_y1_row,\n    M, N, eps, dropout_p, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr,\n    STORE_DROPOUT_MASK: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_X1: tl.constexpr,\n    HAS_W1: tl.constexpr, HAS_B1: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        x *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n            )\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)\n\ndef _layer_norm_fwd(\n    x, weight, bias, eps, residual=None, x1=None, weight1=None, bias1=None, dropout_p=0.0,\n    rowscale=None, out_dtype=None, residual_dtype=None, is_rms_norm=False, return_dropout_mask=False,\n):\n    if residual is not None:\n        residual_dtype = residual.dtype\n    M, N = x.shape\n    assert x.stride(-1) == 1\n    if residual is not None:\n        assert residual.stride(-1) == 1\n        assert residual.shape == (M, N)\n    assert weight.shape == (N,)\n    assert weight.stride(-1) == 1\n    if bias is not None:\n        assert bias.stride(-1) == 1\n        assert bias.shape == (N,)\n    if x1 is not None:\n        assert x1.shape == x.shape\n        assert rowscale is None\n        assert x1.stride(-1) == 1\n    if weight1 is not None:\n        assert weight1.shape == (N,)\n        assert weight1.stride(-1) == 1\n    if bias1 is not None:\n        assert bias1.shape == (N,)\n        assert bias1.stride(-1) == 1\n    if rowscale is not None:\n        assert rowscale.is_contiguous()\n        assert rowscale.shape == (M,)\n    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)\n    assert y.stride(-1) == 1\n    if weight1 is not None:\n        y1 = torch.empty_like(y)\n        assert y1.stride(-1) == 1\n    else:\n        y1 = None\n    if (\n        residual is not None\n        or (residual_dtype is not None and residual_dtype != x.dtype)\n        or dropout_p > 0.0\n        or rowscale is not None\n        or x1 is not None\n    ):\n        residual_out = torch.empty(\n            M, N, device=x.device, dtype=residual_dtype if residual_dtype is not None else x.dtype\n        )\n        assert residual_out.stride(-1) == 1\n    else:\n        residual_out = None\n    mean = torch.empty((M,), dtype=torch.float32, device=x.device) if not is_rms_norm else None\n    rstd = torch.empty((M,), dtype=torch.float32, device=x.device)\n    if dropout_p > 0.0:\n        seeds = torch.randint(\n            2**32, (M if x1 is None else 2 * M,), device=x.device, dtype=torch.int64\n        )\n    else:\n        seeds = None\n    if return_dropout_mask and dropout_p > 0.0:\n        dropout_mask = torch.empty(M if x1 is None else 2 * M, N, device=x.device, dtype=torch.bool)\n    else:\n        dropout_mask = None\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _layer_norm_fwd_1pass_kernel[(M,)](\n            x, y, weight, bias, residual, x1, weight1, bias1, y1, residual_out, rowscale, seeds,\n            dropout_mask, mean, rstd, x.stride(0), y.stride(0), residual.stride(0) if residual is not None else 0,\n            residual_out.stride(0) if residual_out is not None else 0, x1.stride(0) if x1 is not None else 0,\n            y1.stride(0) if y1 is not None else 0, M, N, eps, dropout_p, is_rms_norm, BLOCK_N,\n            residual is not None, residual_out is not None, bias is not None, dropout_p > 0.0,\n            dropout_mask is not None, rowscale is not None,\n        )\n    if dropout_mask is not None and x1 is not None:\n        dropout_mask, dropout_mask1 = dropout_mask.tensor_split(2, dim=0)\n    else:\n        dropout_mask1 = None\n    return (\n        y, y1, mean, rstd, residual_out if residual_out is not None else x, seeds, dropout_mask, dropout_mask1,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided code defines a set of Triton kernels and a PyTorch autograd function to perform chunk retention operations on tensors. The main functionality is split into several kernels to manage different parts of the computation, focusing on forward and backward passes for efficient memory usage and computation.\n\n        - `chunk_retention_fwd_kernel_h`: This kernel calculates an intermediate tensor 'h' based on inputs 'k' and 'v', and optionally uses an initial state. It updates the final state if needed.\n        - `chunk_retention_fwd_kernel_o`: Computes the output tensor 'o' using 'q', 'k', 'v', and the intermediate tensor 'h'.\n        - `chunk_retention_bwd_kernel_dh`: Part of the backward pass, this kernel computes the gradient 'dh'.\n        - `chunk_retention_bwd_kernel_dqkv`: Calculates the gradients 'dq', 'dk', and 'dv' for the backward pass.\n\n        The main Python function `chunk_retention` uses the `ChunkRetentionFunction` to perform these operations, managing both forward and backward computations with optional state inputs and outputs.\n    ", "predict": "Certainly! Below is the implementation of the Triton kernels and the PyTorch autograd function to perform chunk retention operations on tensors. The code is structured to handle both the forward and backward passes efficiently.\n\n### Triton Kernels\n\n```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_retention_fwd_kernel_h(\n    q, k, v, h, initial_state, final_state,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_hb, stride_hh, stride_hm,\n    stride_isb, stride_ish, stride_ism,\n    stride_fsb, stride_fsh, stride_fsm,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_heads = q.shape[1]\n    head_id = pid % num_heads\n    block_id = pid // num_heads\n\n    offs_m = block_id * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n\n    q_ptrs = q + (offs_m[:, None, None] * stride_qm + head_id * stride_qh + offs_d[None, None, :])\n    k_ptrs = k + (offs_n[None, :, None] * stride_km + head_id * stride_kh + offs_d[None, None, :])\n    v_ptrs = v + (offs_n[None, :, None] * stride_vm + head_id * stride_vh + offs_d[None, None, :])\n\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n\n    h_ptrs = h + (offs_m[:, None, None] * stride_hm + head_id * stride_hh + offs_d[None, None, :])\n    h = tl.zeros_like(h_ptrs)\n\n    if initial_state is not None:\n        initial_state_ptrs = initial_state + (head_id * stride_ish + offs_d[None, None, :])\n        initial_state = tl.load(initial_state_ptrs)\n        h += initial_state\n\n    for n in range(BLOCK_N):\n        h += tl.sum(q * k[n], axis=2)[:, :, None] * v[n]\n\n    tl.store(h_ptrs, h)\n\n    if final_state is not None:\n        final_state_ptrs = final_state + (head_id * stride_fsh + offs_d[None, None, :])\n        tl.store(final_state_ptrs, h[0, 0, :])\n\n@triton.jit\ndef chunk_retention_fwd_kernel_o(\n    q, k, v, h, o,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_hb, stride_hh, stride_hm,\n    stride_ob, stride_oh, stride_om,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_heads = q.shape[1]\n    head_id = pid % num_heads\n    block_id = pid // num_heads\n\n    offs_m = block_id * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n\n    q_ptrs = q + (offs_m[:, None, None] * stride_qm + head_id * stride_qh + offs_d[None, None, :])\n    h_ptrs = h + (offs_m[:, None, None] * stride_hm + head_id * stride_hh + offs_d[None, None, :])\n    o_ptrs = o + (offs_m[:, None, None] * stride_om + head_id * stride_oh + offs_d[None, None, :])\n\n    q = tl.load(q_ptrs)\n    h = tl.load(h_ptrs)\n\n    o = tl.zeros_like(o_ptrs)\n\n    for n in range(BLOCK_N):\n        o += tl.sum(q * h[n], axis=2)[:, :, None]\n\n    tl.store(o_ptrs, o)\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(\n    q, k, v, h, o, grad_o, grad_h,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_hb, stride_hh, stride_hm,\n    stride_ob, stride_oh, stride_om,\n    stride_gob, stride_goh, stride_gom,\n    stride_ghb, stride_ghh, stride_ghm,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_heads = q.shape[1]\n    head_id = pid % num_heads\n    block_id = pid // num_heads\n\n    offs_m = block_id * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n\n    q_ptrs = q + (offs_m[:, None, None] * stride_qm + head_id * stride_qh + offs_d[None, None, :])\n    h_ptrs = h + (offs_m[:, None, None] * stride_hm + head_id * stride_hh + offs_d[None, None, :])\n    grad_o_ptrs = grad_o + (offs_m[:, None, None] * stride_gom + head_id * stride_goh + offs_d[None, None, :])\n    grad_h_ptrs = grad_h + (offs_m[:, None, None] * stride_ghm + head_id * stride_ghh + offs_d[None, None, :])\n\n    q = tl.load(q_ptrs)\n    h = tl.load(h_ptrs)\n    grad_o = tl.load(grad_o_ptrs)\n\n    grad_h = tl.zeros_like(grad_h_ptrs)\n\n    for n in range(BLOCK_N):\n        grad_h += tl.sum(q * grad_o, axis=2)[:, :, None] * h[n]\n\n    tl.store(grad_h_ptrs, grad_h)\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(\n    q, k, v, h, o, grad_o, grad_q, grad_k, grad_v,\n    stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_km,\n    stride_vb, stride_vh, stride_vm,\n    stride_hb, stride_hh, stride_hm,\n    stride_ob, stride_oh, stride_om,\n    stride_gob, stride_goh, stride_gom,\n    stride_gqb, stride_gqh, stride_gqm,\n    stride_gkb, stride_gkh, stride_gkm,\n    stride_gvb, stride_gvh, stride_gvm,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_heads = q.shape[1]\n    head_id = pid % num_heads\n    block_id = pid // num_heads\n\n    offs_m = block_id * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n\n    q_ptrs = q + (offs_m[:, None, None] * stride_qm + head_id * stride_qh + offs_d[None, None, :])\n    k_ptrs = k + (offs_n[None, :, None] * stride_km + head_id * stride_kh + offs_d[None, None, :])\n    v_ptrs = v + (offs_n[None, :, None] * stride_vm + head_id * stride_vh + offs_d[None, None, :])\n    h_ptrs = h + (offs_m[:, None, None] * stride_hm + head_id * stride_hh + offs_d[None, None, :])\n    grad_o_ptrs = grad_o + (offs_m[:, None, None] * stride_gom + head_id * stride_goh + offs_d[None, None, :])\n    grad_q_ptrs = grad_q + (offs_m[:, None, None] * stride_gqm + head_id * stride_gqh + offs_d[None, None, :])\n    grad_k_ptrs = grad_k + (offs_n[None, :, None] * stride_gkm + head_id * stride_gkh + offs_d[None, None, :])\n    grad_v_ptrs = grad_v + (offs_n[None, :, None] * stride_gvm + head_id * stride_gvh + offs_d[None, None, :])\n\n    q = tl.load(q_ptrs)\n    k = tl.load(k_ptrs)\n    v = tl.load(v_ptrs)\n    h = tl.load(h_ptrs)\n    grad_o = tl.load(grad_o_ptrs)\n\n    grad_q = tl.zeros_like(grad_q_ptrs)\n    grad_k = tl.zeros_like(grad_k_ptrs)\n    grad_v = tl.zeros_like(grad_v_ptrs)\n\n    for n in range(BLOCK_N):\n        grad_q += tl.sum(grad_o * h[n], axis=2)[:, :, None] * k[n]\n        grad_k += tl.sum(grad_o * q, axis=2)", "label": "from typing import Tuple\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.amp import custom_bwd, custom_fwd\nimport functools\n\ndef contiguous(fn):\n    @functools.wraps(fn)\n    def wrapper(ctx, *args, **kwargs):\n        return fn(ctx,\n                  *(i if not isinstance(i, torch.Tensor) else i.contiguous() for i in args),\n                  **{k: (v if not isinstance(v, torch.Tensor) else v.contiguous()) for k, v in kwargs.items()})\n    return wrapper\n\n@triton.jit\ndef chunk_retention_fwd_kernel_h(\n    k,\n    v,\n    h,\n    initial_state,  # initial state of the chunk [B, H, D_head_K, D_head_V]\n    final_state,  # final state of the chunk [B, H, D_head_K, D_head_V]\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    # [BK, BV]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BK, BV]\n        if i_t == NT - 1 and (T % BT) != 0:\n            d_b = tl.math.exp2((T % BT) * b_b)\n            d_i = tl.math.exp2(((T % BT) - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_i[:, None]).to(b_k.dtype), allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_retention_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    o,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        # [BT, BK]\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BK, BV]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot((b_q * d_i[:, None]).to(b_q.dtype), b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n\n    b_s *= d_s\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(\n    q,\n    do,\n    dh,\n    v,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    # [BK, BV]\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_o = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        # [BT, BV]\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_dh += tl.dot((b_o * d_i[:, None]).to(b_o.dtype), b_v, allow_tf32=False)\n\n    b_dh *= d_b\n    p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_k * K * V, (K, V), (s_h_t, 1), (i_v * BV, i_t * BT), (BK, BV), (1, 0))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(\n    q,\n    k,\n    v,\n    h,\n    do,\n    dh,\n    dq,\n    dk,\n    dv,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    d_q = (d_q * scale).to(d_q.dtype)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        # [BV, BK]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        # [BK, BV]\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        # [BT, BT]\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        # [BT, BK]\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        # [BT, BV]\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    # [BT, BT]\n    b_ds = (b_ds * d_s).to(b_q.dtype)\n    # [BT, BK]\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n\nclass ChunkRetentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    @contiguous\n    def forward(ctx, q, k, v, initial_state, output_final_state):\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        scale = K ** -0.5\n\n        final_state = None\n        if output_final_state:\n            final_state = q.new_empty(B, H, K, V, dtype=torch.float32, requires_grad=False)\n\n        h = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_retention_fwd_kernel_h[grid](\n            k, v, h, initial_state, final_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=output_final_state,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        grid = (NV, NT, B * H)\n        o = torch.empty_like(v)\n        chunk_retention_fwd_kernel_o[grid](\n            q, k, v, h, o,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            scale,\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        ctx.save_for_backward(q, k, v, h)\n        return o.to(q.dtype), final_state\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    @contiguous\n    def backward(ctx, do, d_ht=None):\n        q, k, v, h = ctx.saved_tensors\n\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        scale = K ** -0.5\n\n        dh = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_retention_bwd_kernel_dh[grid](\n            q, do, dh, v,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        grid = (NK, NT, B * H)\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = v.new_empty(NK, *v.shape)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        chunk_retention_bwd_kernel_dqkv[grid](\n            q, k, v, h, do, dh, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        dv = dv.sum(0)\n        return dq.to(q.dtype), dk.to(k.dtype), dv.to(v.dtype), None, None\n\ndef chunk_retention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if initial_state is not None:\n        initial_state = initial_state.detach()\n    o, final_state = ChunkRetentionFunction.apply(q, k, v, initial_state, output_final_state)\n    return o, final_state\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines a Triton kernel named sin_kernel and a Python wrapper function sin_triton. The kernel computes the sine of input elements. The kernel sin_kernel takes input pointers in_ptr0, out_ptr, the number of elements n_elements, and a compile-time constant BLOCK_SIZE. The program calculates the sine of each input element from the input pointer and stores the result in the output pointer. The function sin_triton is a wrapper that launches sin_kernel with the number of elements and a specific block size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef sin_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the PID (program ID) and the range of elements this program instance will process\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Create a mask to guard memory operations\n    mask = offsets < n_elements\n    \n    # Load input elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    \n    # Compute the sine of each element\n    y = tl.sin(x)\n    \n    # Store the results\n    tl.store(out_ptr + offsets, y, mask=mask)\n\nimport triton\nimport triton.runtime\nimport torch\n\ndef sin_triton(input_tensor, output_tensor, block_size=1024):\n    # Ensure the input and output tensors are on the same device\n    assert input_tensor.device == output_tensor.device\n    device = input_tensor.device\n    \n    # Get the number of elements\n    n_elements = input_tensor.numel()\n    \n    # Launch the kernel\n    grid = (triton.cdiv(n_elements, block_size),)\n    sin_kernel[grid](input_tensor, output_tensor, n_elements, BLOCK_SIZE=block_size)\n    \n    # Synchronize to ensure the kernel has finished\n    torch.cuda.synchronize(device)\n\nimport torch\n\n# Create input and output tensors\ninput_tensor = torch.randn(1024, device='cuda')\noutput_tensor = torch.empty_like(input_tensor)\n\n# Call the wrapper function\nsin_triton(input_tensor, output_tensor)\n\n# Print the results\nprint(output_tensor)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef sin_triton(x, out):\n    n_elements = x.numel()\n    sin_kernel[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel function `decoding_fused_rotary_embedding_kernel` performs a rotary positional embedding transformation on query (`q`), key (`k`), and value (`v`) tensors, and updates key (`k_cache`) and value (`v_cache`) caches based on the transformation. The rotary embedding involves a rotation using cosine and sine values. The function handles queries and keys differently based on the head index.\n\n            The kernel is invoked from the `decoding_fused_rotary_embedding` function, which calculates the grid configuration and strides for memory accesses and calls the Triton kernel. The function also manages different layouts of the key cache.\n\n            Inputs to the Triton kernel include tensors `q`, `k`, `v`, `cos`, `sin`, `k_cache`, `v_cache`, and other parameters for indexing and transformation. The `decoding_fused_rotary_embedding` function configures the kernel launch parameters like grid size and warps based on tensor dimensions and other criteria.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef decoding_fused_rotary_embedding_kernel(\n    q_ptr, k_ptr, v_ptr, cos_ptr, sin_ptr, k_cache_ptr, v_cache_ptr,\n    q_batch_stride, q_head_stride, q_seq_stride, q_token_stride,\n    k_batch_stride, k_head_stride, k_seq_stride, k_token_stride,\n    v_batch_stride, v_head_stride, v_seq_stride, v_token_stride,\n    cos_batch_stride, cos_head_stride, cos_seq_stride, cos_token_stride,\n    sin_batch_stride, sin_head_stride, sin_seq_stride, sin_token_stride,\n    k_cache_batch_stride, k_cache_head_stride, k_cache_seq_stride, k_cache_token_stride,\n    v_cache_batch_stride, v_cache_head_stride, v_cache_seq_stride, v_cache_token_stride,\n    batch_size, num_heads, seq_len, token_dim, rotary_dim, block_m: tl.constexpr, block_n: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(seq_len, block_m)\n    num_pid_n = tl.cdiv(token_dim, block_n)\n    num_pid_in_batch = num_pid_m * num_pid_n\n    batch_id = pid // num_pid_in_batch\n    pid_in_batch = pid % num_pid_in_batch\n    nm = pid_in_batch\n    pid_m = nm // num_pid_n\n    pid_n = nm % num_pid_n\n    block_start_m = pid_m * block_m\n    block_start_n = pid_n * block_n\n\n    offsets_m = block_start_m + tl.arange(0, block_m)\n    offsets_n = block_start_n + tl.arange(0, block_n)\n    offsets_q = batch_id * q_batch_stride + tl.arange(0, num_heads) * q_head_stride + offsets_m[:, None] * q_seq_stride + offsets_n[None, :] * q_token_stride\n    offsets_k = batch_id * k_batch_stride + tl.arange(0, num_heads) * k_head_stride + offsets_m[:, None] * k_seq_stride + offsets_n[None, :] * k_token_stride\n    offsets_v = batch_id * v_batch_stride + tl.arange(0, num_heads) * v_head_stride + offsets_m[:, None] * v_seq_stride + offsets_n[None, :] * v_token_stride\n    offsets_cos = batch_id * cos_batch_stride + tl.arange(0, num_heads) * cos_head_stride + offsets_m[:, None] * cos_seq_stride + offsets_n[None, :] * cos_token_stride\n    offsets_sin = batch_id * sin_batch_stride + tl.arange(0, num_heads) * sin_head_stride + offsets_m[:, None] * sin_seq_stride + offsets_n[None, :] * sin_token_stride\n    offsets_k_cache = batch_id * k_cache_batch_stride + tl.arange(0, num_heads) * k_cache_head_stride + offsets_m[:, None] * k_cache_seq_stride + offsets_n[None, :] * k_cache_token_stride\n    offsets_v_cache = batch_id * v_cache_batch_stride + tl.arange(0, num_heads) * v_cache_head_stride + offsets_m[:, None] * v_cache_seq_stride + offsets_n[None, :] * v_cache_token_stride\n\n    q = tl.load(q_ptr + offsets_q, mask=offsets_m[:, None] < seq_len, other=0.0)\n    k = tl.load(k_ptr + offsets_k, mask=offsets_m[:, None] < seq_len, other=0.0)\n    v = tl.load(v_ptr + offsets_v, mask=offsets_m[:, None] < seq_len, other=0.0)\n    cos = tl.load(cos_ptr + offsets_cos, mask=offsets_m[:, None] < seq_len, other=1.0)\n    sin = tl.load(sin_ptr + offsets_sin, mask=offsets_m[:, None] < seq_len, other=0.0)\n\n    q_rot = q * cos - tl.flip(q, 1) * sin\n    k_rot = k * cos - tl.flip(k, 1) * sin\n\n    tl.store(q_ptr + offsets_q, q_rot, mask=offsets_m[:, None] < seq_len)\n    tl.store(k_ptr + offsets_k, k_rot, mask=offsets_m[:, None] < seq_len)\n\n    tl.store(k_cache_ptr + offsets_k_cache, k_rot, mask=offsets_m[:, None] < seq_len)\n    tl.store(v_cache_ptr + offsets_v_cache, v, mask=offsets_m[:, None] < seq_len)\n\nimport torch\n\ndef decoding_fused_rotary_embedding(\n    q, k, v, cos, sin, k_cache, v_cache,\n    batch_size, num_heads, seq_len, token_dim, rotary_dim,\n    block_m=128, block_n=64\n):\n    assert q.shape == (batch_size, num_heads, seq_len, token_dim)\n    assert k.shape == (batch_size, num_heads, seq_len, token_dim)\n    assert v.shape == (batch_size, num_heads, seq_len, token_dim)\n    assert cos.shape == (batch_size, num_heads, seq_len, token_dim)\n    assert sin.shape == (batch_size, num_heads, seq_len, token_dim)\n    assert k_cache.shape == (batch_size, num_heads, seq_len, token_dim)\n    assert v_cache.shape == (batch_size, num_heads, seq_len, token_dim)\n\n    q_batch_stride = q.stride(0)\n    q_head_stride = q.stride(1)\n    q_seq_stride = q.stride(2)\n    q_token_stride = q.stride(3)\n\n    k_batch_stride = k.stride(0)\n    k_head_stride = k.stride(1)\n    k_seq_stride = k.stride(2)\n    k_token_stride = k.stride(3)\n\n    v_batch_stride = v.stride(0)\n    v_head_stride = v.stride(1)\n    v_seq_stride = v.stride(2)\n    v_token_stride = v.stride(3)\n\n    cos_batch_stride = cos.stride(0)\n    cos_head_stride = cos.stride(1)\n    cos_seq_stride = cos.stride(2)\n    cos_token_stride = cos.stride(3)\n\n    sin_batch_stride = sin.stride(0)\n    sin_head_stride = sin.stride(1)\n    sin_seq_stride = sin.stride(2)\n    sin_token_stride = sin.stride(3)\n\n    k_cache_batch_stride = k_cache.stride(0)\n    k_cache_head_stride = k_cache.stride(1)\n    k_cache_seq_stride = k_cache.stride(2)\n    k_cache_token_stride = k_cache.stride(3)\n\n    v_cache_batch_stride = v_cache.stride(0)\n    v_cache_head_stride = v_cache.stride(1)\n    v_cache_seq_stride = v_cache.stride(2)\n    v_cache_token_stride = v_cache.stride(3)\n\n    grid = (batch_size * num_heads * tl.cdiv(seq_len, block_m) * tl.cdiv(token_dim, block_n),)\n\n    decoding_fused_rotary_embedding_kernel[grid](\n        q, k, v, cos, sin, k_cache, v_cache,\n        q_batch_stride, q_head_stride, q_seq_stride, q_token_stride,\n        k_batch_stride, k_head_stride, k_seq_stride, k_token_stride,\n        v_batch_stride, v_head_stride, v_seq_stride, v_token_stride,\n        cos_batch_stride, cos_head_stride, cos_seq_stride, cos_token_stride,\n        sin_batch_stride, sin_head_stride, sin_seq_stride, sin_token_stride,\n        k_cache_batch_stride, k_cache_head_stride, k_cache_seq_stride, k_cache_token_stride,\n        v_cache_batch_stride, v_cache_head_stride, v_cache_seq_stride, v_cache_token_stride,\n        batch_size, num_heads, seq_len, token_dim, rotary_dim, block_m, block_n\n    )\n", "label": "import warnings\nfrom typing import Optional\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef decoding_fused_rotary_embedding_kernel(\n    q,\n    k,\n    v,\n    cos,\n    sin,\n    k_cache,\n    v_cache,\n    BLOCK_TABLES,\n    context_lengths,\n    x,\n    q_token_stride,\n    q_head_stride,\n    k_token_stride,\n    k_head_stride,\n    head_dim_stride,\n    cos_token_stride,\n    cos_stride,\n    kcb_stride,\n    kch_stride,\n    kcsplit_x_stride,\n    kcs_stride,\n    kcd_stride,\n    vcb_stride,\n    vch_stride,\n    vcs_stride,\n    vcd_stride,\n    bts_stride,\n    btb_stride,\n    block_size,\n    KV_GROUP_NUM: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n):\n    cur_head_idx = tl.program_id(0)\n    cur_token_idx = tl.program_id(1)\n\n    dim_range = tl.arange(0, HEAD_DIM)\n    dim_range0 = tl.arange(0, HEAD_DIM // 2)\n    dim_range1 = tl.arange(HEAD_DIM // 2, HEAD_DIM)\n\n    off_q = cur_token_idx * q_token_stride + cur_head_idx * q_head_stride\n    off_q0 = off_q + dim_range0 * head_dim_stride\n    off_q1 = off_q + dim_range1 * head_dim_stride\n\n    loaded_q0 = tl.load(q + off_q0)\n    loaded_q1 = tl.load(q + off_q1)\n    off_cos_sin = cur_token_idx * cos_token_stride + dim_range0 * cos_stride\n    loaded_cos = tl.load(cos + off_cos_sin)\n    loaded_sin = tl.load(sin + off_cos_sin)\n\n    out_q0 = loaded_q0 * loaded_cos - loaded_q1 * loaded_sin\n    out_q1 = loaded_q0 * loaded_sin + loaded_q1 * loaded_cos\n    tl.store(q + off_q0, out_q0)\n    tl.store(q + off_q1, out_q1)\n\n    handle_kv = cur_head_idx % KV_GROUP_NUM == 0\n    if handle_kv:\n        cur_k_head_idx = cur_head_idx // KV_GROUP_NUM\n        off_kv = cur_token_idx * k_token_stride + cur_k_head_idx * k_head_stride\n        off_k0 = off_kv + dim_range0 * head_dim_stride\n        off_k1 = off_kv + dim_range1 * head_dim_stride\n        loaded_k0 = tl.load(k + off_k0)\n        loaded_k1 = tl.load(k + off_k1)\n\n        out_k0 = loaded_k0 * loaded_cos - loaded_k1 * loaded_sin\n        out_k1 = loaded_k0 * loaded_sin + loaded_k1 * loaded_cos\n\n        # NOTE The precondition here is that it's only for unpadded inputs during decoding stage,\n        # and so that we could directly use the token index as the sequence index\n        past_kv_seq_len = tl.load(context_lengths + cur_token_idx) - 1\n\n        last_block_idx = past_kv_seq_len // block_size\n        block_ids = tl.load(BLOCK_TABLES + cur_token_idx * bts_stride + last_block_idx * btb_stride)\n        offsets_in_last_block = past_kv_seq_len % block_size\n        offsets_cache_base = block_ids * kcb_stride + cur_k_head_idx * kch_stride\n        k_range0 = (\n            offsets_cache_base\n            + offsets_in_last_block * kcs_stride\n            + (dim_range0 // x) * kcsplit_x_stride\n            + (dim_range0 % x) * kcd_stride\n        )\n        k_range1 = (\n            offsets_cache_base\n            + offsets_in_last_block * kcs_stride\n            + (dim_range1 // x) * kcsplit_x_stride\n            + (dim_range1 % x) * kcd_stride\n        )\n        tl.store(k_cache + k_range0, out_k0)\n        tl.store(k_cache + k_range1, out_k1)\n\n        off_v = off_kv + dim_range * head_dim_stride\n        loaded_v = tl.load(v + off_v)\n        v_range = (\n            block_ids * vcb_stride\n            + cur_k_head_idx * vch_stride\n            + offsets_in_last_block * vcs_stride\n            + dim_range * vcd_stride\n        )\n        tl.store(v_cache + v_range, loaded_v)\n\n\ndef decoding_fused_rotary_embedding(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    k_cache: Optional[torch.Tensor] = None,\n    v_cache: Optional[torch.Tensor] = None,\n    block_tables: Optional[torch.Tensor] = None,\n    kv_lengths: Optional[torch.Tensor] = None,\n    use_new_kcache_layout: bool = False,\n):\n    \"\"\"\n    Args:\n        q: query tensor, [total_tokens, head_num, head_dim]\n        k: key tensor, [total_tokens, kv_head_num, head_dim]\n        v: value tensor, [total tokens, kv_head_num, head_dim]\n        cos: cosine for rotary embedding, [max_position_len, head_dim]\n        sin: sine for rotary embedding, [max_position_len, head_dim]\n        k_cache (torch.Tensor):  Blocked key cache. [num_blocks, kv_head_num, block_size, head_dim]\n        v_cache (torch.Tensor):  Blocked value cache. [num_blocks, kv_head_num, block_size, head_dim]\n        kv_lengths, Past key/value sequence lengths plus current sequence length for each sequence. [bsz]\n        block_tables: Block tables for each sequence. [bsz, max_blocks_per_sequence]\n    \"\"\"\n    q_total_tokens, q_head_num, head_dim = q.shape\n    assert q.size(0) == k.size(0) == v.size(0)\n\n    if head_dim >= 512:\n        num_warps = 16\n    elif head_dim >= 256:\n        num_warps = 8\n    else:\n        num_warps = 4\n    k_head_num = k.size(1)\n    kv_group_num = q_head_num // k_head_num\n\n    # For KCache and VCache with the same layout\n    x = head_dim\n    kcsplit_x_stride, kcs_stride, kcd_stride = 0, k_cache.stride(2), k_cache.stride(3)\n    # For KCache layout [num_blocks, num_kv_heads, head_dim//x, block_size, x]\n    if use_new_kcache_layout:\n        assert (\n            k_cache.dim() == 5\n            and k_cache.shape[1] == v_cache.shape[1]\n            and k_cache.shape[2] * k_cache.shape[4] == v_cache.shape[3]\n        ), f\"Invalid KCache shape {k_cache.shape} and VCache shape {v_cache.shape}\"\n        x = k_cache.size(-1)\n        kcsplit_x_stride, kcs_stride, kcd_stride = k_cache.stride()[-3:]\n\n    grid = (q_head_num, q_total_tokens)\n    decoding_fused_rotary_embedding_kernel[grid](\n        q,\n        k,\n        v,\n        cos,\n        sin,\n        k_cache,\n        v_cache,\n        block_tables,\n        kv_lengths,\n        x,\n        q.stride(0),\n        q.stride(1),\n        k.stride(0),\n        k.stride(1),\n        q.stride(2),\n        cos.stride(0),\n        cos.stride(1),\n        k_cache.stride(0),\n        k_cache.stride(1),\n        kcsplit_x_stride,\n        kcs_stride,\n        kcd_stride,\n        v_cache.stride(0),\n        v_cache.stride(1),\n        v_cache.stride(2),\n        v_cache.stride(3),\n        block_tables.stride(0),\n        block_tables.stride(1),\n        k_cache.size(-2),\n        KV_GROUP_NUM=kv_group_num,\n        HEAD_DIM=head_dim,\n        num_warps=num_warps,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is designed for linear attention operations, split into different kernels for forward and backward computations. The `chunk_linear_attn_fwd_kernel_h` computes the intermediate tensor `h` by iterating over time steps `T` and accumulating the dot product between key (`k`) and value (`v`) tensors. The `chunk_linear_attn_fwd_kernel_o` computes the output tensor `o` by using query (`q`), `k`, `v`, and `h` tensors. The backward operations are handled by `chunk_linear_attn_bwd_kernel_dh` and `chunk_linear_attn_bwd_kernel_dqkv`, which calculate gradients for intermediate and input tensors, respectively. These kernels rely on block pointers and matrix operations to efficiently compute large tensor operations with Triton's parallel execution model. The `ChunkLinearAttentionFunction` is a PyTorch `autograd.Function` that uses these kernels in its forward and backward passes, facilitating the use of Triton within PyTorch's automatic differentiation framework. ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(\n    k_ptr, v_ptr, h_ptr,\n    k_batch_stride, k_head_stride, k_time_stride, k_dim_stride,\n    v_batch_stride, v_head_stride, v_time_stride, v_dim_stride,\n    h_batch_stride, h_head_stride, h_time_stride, h_dim_stride,\n    B, H, T, D,\n    BLOCK_D: tl.constexpr,\n    BLOCK_T: tl.constexpr\n):\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    t_idx = tl.program_id(2)\n    d_idx = tl.program_id(3)\n\n    k_block_ptr = tl.make_block_ptr(\n        base=k_ptr,\n        shape=(B, H, T, D),\n        strides=(k_batch_stride, k_head_stride, k_time_stride, k_dim_stride),\n        offsets=(b_idx, h_idx, t_idx, 0),\n        block_shape=(1, 1, 1, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    v_block_ptr = tl.make_block_ptr(\n        base=v_ptr,\n        shape=(B, H, T, D),\n        strides=(v_batch_stride, v_head_stride, v_time_stride, v_dim_stride),\n        offsets=(b_idx, h_idx, 0, 0),\n        block_shape=(1, 1, BLOCK_T, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    h_block_ptr = tl.make_block_ptr(\n        base=h_ptr,\n        shape=(B, H, T, D),\n        strides=(h_batch_stride, h_head_stride, h_time_stride, h_dim_stride),\n        offsets=(b_idx, h_idx, t_idx, 0),\n        block_shape=(1, 1, 1, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    k = tl.load(k_block_ptr)\n    v = tl.load(v_block_ptr)\n\n    h = tl.zeros((BLOCK_D,), dtype=tl.float32)\n\n    for t in range(T):\n        v_t = tl.load(v_block_ptr, mask=t < T, other=0.0)\n        h += tl.dot(k, v_t)\n\n    tl.store(h_block_ptr, h)\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(\n    q_ptr, k_ptr, v_ptr, h_ptr, o_ptr,\n    q_batch_stride, q_head_stride, q_time_stride, q_dim_stride,\n    k_batch_stride, k_head_stride, k_time_stride, k_dim_stride,\n    v_batch_stride, v_head_stride, v_time_stride, v_dim_stride,\n    h_batch_stride, h_head_stride, h_time_stride, h_dim_stride,\n    o_batch_stride, o_head_stride, o_time_stride, o_dim_stride,\n    B, H, T, D,\n    BLOCK_D: tl.constexpr,\n    BLOCK_T: tl.constexpr\n):\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    t_idx = tl.program_id(2)\n    d_idx = tl.program_id(3)\n\n    q_block_ptr = tl.make_block_ptr(\n        base=q_ptr,\n        shape=(B, H, T, D),\n        strides=(q_batch_stride, q_head_stride, q_time_stride, q_dim_stride),\n        offsets=(b_idx, h_idx, t_idx, 0),\n        block_shape=(1, 1, 1, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    k_block_ptr = tl.make_block_ptr(\n        base=k_ptr,\n        shape=(B, H, T, D),\n        strides=(k_batch_stride, k_head_stride, k_time_stride, k_dim_stride),\n        offsets=(b_idx, h_idx, 0, 0),\n        block_shape=(1, 1, BLOCK_T, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    v_block_ptr = tl.make_block_ptr(\n        base=v_ptr,\n        shape=(B, H, T, D),\n        strides=(v_batch_stride, v_head_stride, v_time_stride, v_dim_stride),\n        offsets=(b_idx, h_idx, 0, 0),\n        block_shape=(1, 1, BLOCK_T, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    h_block_ptr = tl.make_block_ptr(\n        base=h_ptr,\n        shape=(B, H, T, D),\n        strides=(h_batch_stride, h_head_stride, h_time_stride, h_dim_stride),\n        offsets=(b_idx, h_idx, t_idx, 0),\n        block_shape=(1, 1, 1, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    o_block_ptr = tl.make_block_ptr(\n        base=o_ptr,\n        shape=(B, H, T, D),\n        strides=(o_batch_stride, o_head_stride, o_time_stride, o_dim_stride),\n        offsets=(b_idx, h_idx, t_idx, 0),\n        block_shape=(1, 1, 1, BLOCK_D),\n        order=(3, 0, 1, 2)\n    )\n\n    q = tl.load(q_block_ptr)\n    k = tl.load(k_block_ptr)\n    v = tl.load(v_block_ptr)\n    h = tl.load(h_block_ptr)\n\n    o = tl.zeros((BLOCK_D,), dtype=tl.float32)\n\n    for t in range(T):\n        k_t = tl.load(k_block_ptr, mask=t < T, other=0.0)\n        v_t = tl.load(v_block_ptr, mask=t < T, other=0.0)\n        o += tl.dot(q, v_t) * h\n\n    tl.store(o_block_ptr, o)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional, Tuple\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(\n    k, v, h, h0, ht,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    NT: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(\n    q, k, v, h, o,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t, scale,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(\n    q, do, dh,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t, scale,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do.to(b_q.dtype), allow_tf32=False)\n\n\n@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(\n    q, k, v, h, do, dh, dq, dk, dv,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t, scale,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k*n_bh+i_bh)*s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0).to(b_q.dtype)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n\nclass ChunkLinearAttentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, scale, initial_state, output_final_state):\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        ctx.scale = scale\n\n        final_state = None\n        if output_final_state:\n            final_state = q.new_empty(B, H, K, V, dtype=torch.float32, requires_grad=False)\n\n        h = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_linear_attn_fwd_kernel_h[grid](\n            k, v, h, initial_state, final_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=output_final_state,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        grid = (NV, NT, B * H)\n        o = torch.empty_like(v)\n        chunk_linear_attn_fwd_kernel_o[grid](\n            q, k, v, h, o,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            scale,\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        ctx.save_for_backward(q, k, v, h)\n        return o.to(q.dtype), final_state\n\n    @staticmethod\n    def backward(ctx, do, dht=None):\n        q, k, v, h = ctx.saved_tensors\n\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(32 if q.dtype == torch.float32 else 64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        scale = ctx.scale\n\n        dh = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_linear_attn_bwd_kernel_dh[grid](\n            q, do, dh,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        grid = (NK, NT, B * H)\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = v.new_empty(NK, *v.shape)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        chunk_linear_attn_bwd_kernel_dqkv[grid](\n            q, k, v, h, do, dh, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        dv = dv.sum(0)\n        return dq.to(q.dtype), dk.to(k.dtype), dv.to(v.dtype), None, None, None\n\n\ndef chunk_linear_attn(\n    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n    scale: Optional[float] = None, initial_state: torch.Tensor = None,\n    output_final_state: bool = False, normalize: bool = True\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if scale is None:\n        scale = q.shape[-1] ** -0.5\n    o, final_state = ChunkLinearAttentionFunction.apply(q, k, v, scale, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implements fused softmax and its backward operation over a 3D tensor. The softmax is applied along the last dimension of the tensor. There are two main functions: softmax and softmax_backward. \n\n            The softmax function takes 3D tensors Y (output), X (input), and an optional mask M. It applies a softmax operation along the last dimension, optionally applying a log transformation, masking based on a specified mask type, and considering causal constraints. \n\n            The softmax_backward function computes the gradient of the softmax operation. It takes the gradient output, the original softmax output, and computes the gradient input.\n\n            The @triton.jit decorator marks the kernels _softmax and _softmax_backward as JIT-compiled Triton kernels, allowing them to be highly optimized for different hardware. Autotuning with different configurations ensures the best performance by adjusting parameters like num_warps. \n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for the softmax operation\n@triton.jit\ndef _softmax(Y, X, M, L, S, stride_yb, stride_yh, stride_yl, stride_xb, stride_xh, stride_xl, stride_mb, stride_mh, stride_ml, n_ctx, mask_type, causal, log, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    bid = pid // (n_ctx // BLOCK_SIZE)\n    hid = pid % (n_ctx // BLOCK_SIZE)\n    row = bid * n_ctx + hid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    col = tl.arange(0, BLOCK_SIZE)\n    mask = row < n_ctx\n    X_row = X + (bid * stride_yb + hid * stride_yh + col * stride_yl)\n    Y_row = Y + (bid * stride_yb + hid * stride_yh + col * stride_yl)\n    M_row = M + (bid * stride_mb + hid * stride_mh + col * stride_ml) if M is not None else None\n\n    x = tl.load(X_row, mask=mask, other=-float('inf'))\n    if M_row is not None:\n        m = tl.load(M_row, mask=mask, other=0.0)\n        x = x * m\n    if causal:\n        causal_mask = col >= row\n        x = tl.where(causal_mask, x, -float('inf'))\n\n    max_val = tl.max(x, axis=0)\n    x = x - max_val\n    exp_x = tl.exp(x)\n    sum_exp = tl.sum(exp_x, axis=0)\n    if log:\n        y = x - tl.log(sum_exp)\n    else:\n        y = exp_x / sum_exp\n\n    tl.store(Y_row, y, mask=mask)\n\n# Triton kernel for the backward pass of the softmax operation\n@triton.jit\ndef _softmax_backward(dY, Y, dX, stride_dyb, stride_dyh, stride_dyl, stride_yb, stride_yh, stride_yl, stride_dxb, stride_dyh, stride_dxl, n_ctx, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    bid = pid // (n_ctx // BLOCK_SIZE)\n    hid = pid % (n_ctx // BLOCK_SIZE)\n    row = bid * n_ctx + hid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    col = tl.arange(0, BLOCK_SIZE)\n    mask = row < n_ctx\n    dY_row = dY + (bid * stride_dyb + hid * stride_dyh + col * stride_dyl)\n    Y_row = Y + (bid * stride_yb + hid * stride_yh + col * stride_yl)\n    dX_row = dX + (bid * stride_dxb + hid * stride_dyh + col * stride_dxl)\n\n    dY_val = tl.load(dY_row, mask=mask, other=0.0)\n    Y_val = tl.load(Y_row, mask=mask, other=0.0)\n    sum_dY = tl.sum(dY_val * Y_val, axis=0)\n    dX_val = dY_val * Y_val - Y_val * sum_dY\n\n    tl.store(dX_row, dX_val, mask=mask)\n\n# Wrapper function for the softmax operation\ndef softmax(Y, X, M=None, L=None, S=None, mask_type=None, causal=False, log=False, BLOCK_SIZE=128):\n    n_ctx = X.shape[-1]\n    grid = (X.shape[0] * (n_ctx // BLOCK_SIZE),)\n    _softmax[grid](Y, X, M, L, S, Y.stride(0), Y.stride(1), Y.stride(2), X.stride(0), X.stride(1), X.stride(2), M.stride(0) if M is not None else 0, M.stride(1) if M is not None else 0, M.stride(2) if M is not None else 0, n_ctx, mask_type, causal, log, BLOCK_SIZE)\n\n# Wrapper function for the backward pass of the softmax operation\ndef softmax_backward(dY, Y, dX, BLOCK_SIZE=128):\n    n_ctx = dY.shape[-1]\n    grid = (dY.shape[0] * (n_ctx // BLOCK_SIZE),)\n    _softmax_backward[grid](dY, Y, dX, dY.stride(0), dY.stride(1), dY.stride(2), Y.stride(0), Y.stride(1), Y.stride(2), dX.stride(0), dX.stride(1), dX.stride(2), n_ctx, BLOCK_SIZE)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef get_depth(K):\n    return triton.next_power_of_2(K)\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"K\"],\n)\n@triton.heuristics({'DEPTH': lambda nargs: get_depth(nargs['K'])})\n@triton.heuristics({'IS_FP16': lambda nargs: nargs['Y'].dtype == torch.float16})\n@triton.jit\ndef _softmax(\n    Y, X, M,\n    stride_ym, stride_yn,\n    stride_xm, stride_xn,\n    stride_m,\n    K,\n    LOG: tl.constexpr,\n    MASK_TYPE: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    DEPTH: tl.constexpr,\n    IS_FP16: tl.constexpr,\n):\n    \"\"\"\n    Fused softmax kernel over a 3d tensor.\n    The softmax is applied over the last dimension, equivalent to torch.softmax(tensor, dim=-1)\n    \"\"\"\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, DEPTH)\n    x_ptrs = X + m * stride_xm + n * stride_xn + k\n    io_mask = k < K\n    if CAUSAL:\n        io_mask = io_mask & (k <= n)\n    x = tl.load(x_ptrs, mask=io_mask, other=float(\"-inf\"))\n    if CAUSAL:\n        off = float(\"-inf\")\n        off = off.to(x.dtype)\n        x = tl.where(k > n, off, x)\n    if MASK_TYPE is not None:\n        if MASK_TYPE == 'qk':\n            mask_ptrs = M + n * stride_m + k\n        elif MASK_TYPE == 'bk':\n            mask_ptrs = M + m * stride_m + k\n        add_mask = tl.load(mask_ptrs, io_mask, other=float(\"-inf\"))\n        x += add_mask\n    z = x - tl.max(x, axis=0)\n    if IS_FP16:\n        z = z.to(tl.float32)\n    num = tl.exp(z)\n    denom = tl.sum(num, axis=0)\n    if LOG:\n        y = z - tl.log(denom)\n    else:\n        y = num / denom\n    y_ptrs = Y + m * stride_ym + n * stride_yn + k\n    tl.store(y_ptrs, y, mask=k < K)\n\ndef softmax(Y, X, M=None, log=False, mask_type=None, causal=False):\n    assert X.ndim == 3, \"Input tensor X must be 3D\"\n    assert Y.shape == X.shape, \"Output tensor Y must have the same shape as X\"\n    M = M if M is not None else torch.empty(0, device=X.device)\n    \n    K = X.shape[-1]\n    stride_ym, stride_yn = Y.stride()[:-1]\n    stride_xm, stride_xn = X.stride()[:-1]\n    stride_m = M.stride(-1) if M.numel() > 0 else 0\n\n    grid = (X.shape[0], X.shape[1])\n    _softmax[grid](\n        Y, X, M,\n        stride_ym, stride_yn,\n        stride_xm, stride_xn,\n        stride_m,\n        K,\n        LOG=log,\n        MASK_TYPE=mask_type,\n        CAUSAL=causal\n    )\n\ndef get_depth(K):\n    return triton.next_power_of_2(K)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"K\"],\n)\n@triton.heuristics({'DEPTH': lambda nargs: get_depth(nargs['K'])})\n@triton.heuristics({'IS_FP16': lambda nargs: nargs['GradIn'].dtype == torch.float16})\n@triton.jit\ndef _softmax_backward(\n    GradIn, GradOut, Out,\n    stride_bm, stride_bn,\n    stride_gm, stride_gn,\n    stride_om, stride_on,\n    K,\n    LOG: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    DEPTH: tl.constexpr,\n    IS_FP16: tl.constexpr,\n):\n    \"\"\"\n    Compute the softmax gradients.\n    \"\"\"\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, DEPTH)\n    grad_out_ptrs = GradOut + m * stride_gm + n * stride_gn + k\n    out_ptrs = Out + m * stride_om + n * stride_on + k\n    io_mask = k < K\n    if CAUSAL:\n        io_mask = io_mask & (k <= n)\n    g = tl.load(grad_out_ptrs, mask=io_mask, other=float(0))\n    o = tl.load(out_ptrs, mask=io_mask, other=float(0))\n    if CAUSAL:\n        zero = float(0)\n        zero = zero.to(g.dtype)\n        g = tl.where(k > n, zero, g)\n        o = tl.where(k > n, zero, o)\n    if LOG:\n        s = tl.sum(g, 0)\n        if IS_FP16:\n            o = o.to(tl.float32)\n        grad_in = g - tl.exp(o) * s\n    else:\n        s = tl.sum(g * o, 0)\n        grad_in = o * (g - s)\n    grad_in_ptrs = GradIn + m * stride_bm + n * stride_bn + k\n    tl.store(grad_in_ptrs, grad_in, mask=k < K)\n\n\ndef softmax_backward(GradIn, GradOut, Out, log=False, causal=False):\n    assert GradOut.shape == Out.shape, \"GradOut and Out must have the same shape\"\n    assert GradIn.shape == Out.shape, \"GradIn and Out must have the same shape\"\n    \n    K = Out.shape[-1]\n    stride_bm, stride_bn = GradIn.stride()[:-1]\n    stride_gm, stride_gn = GradOut.stride()[:-1]\n    stride_om, stride_on = Out.stride()[:-1]\n\n    grid = (Out.shape[0], Out.shape[1])\n    _softmax_backward[grid](\n        GradIn, GradOut, Out,\n        stride_bm, stride_bn,\n        stride_gm, stride_gn,\n        stride_om, stride_on,\n        K,\n        LOG=log,\n        CAUSAL=causal\n    )\n\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton-based kernel and a PyTorch module for performing RMS normalization (RMSNorm) in a more efficient manner. The kernel `rms_norm_fwd_fused` performs the normalization and scaling operations. It computes the variance of each row, calculates the reciprocal of the standard deviation, and applies the learned weights to the normalized data. The `TritonLlamaRMSNorm` class wraps this kernel into a PyTorch module, facilitating integration into neural networks. It accepts a tensor `x` and returns the normalized tensor `y`. The implementation limits the feature dimension to under 64KB and uses Triton's parallel processing to accelerate computation.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_norm_fwd_fused(\n    X,  # input tensor\n    Y,  # output tensor\n    W,  # learned weights\n    stride_xm,  # stride in the input tensor\n    stride_ym,  # stride in the output tensor\n    M,  # number of rows\n    N,  # number of columns (features)\n    eps: tl.float32,  # small epsilon value to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,  # block size for parallel processing\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < M\n\n    # Load the row from the input tensor\n    row_offsets = offsets * stride_xm\n    x_ptrs = X + row_offsets\n    x = tl.load(x_ptrs, mask=mask, other=0.0)\n\n    # Compute the mean of the row\n    mean = tl.sum(x, axis=0) / N\n\n    # Compute the variance of the row\n    x_centered = x - mean\n    var = tl.sum(x_centered * x_centered, axis=0) / N\n\n    # Compute the reciprocal of the standard deviation\n    rstd = 1.0 / tl.sqrt(var + eps)\n\n    # Apply the learned weights to the normalized data\n    w_ptr = W + offsets\n    w = tl.load(w_ptr, mask=mask, other=1.0)\n    y = x_centered * rstd * w\n\n    # Store the result in the output tensor\n    y_ptrs = Y + row_offsets\n    tl.store(y_ptrs, y, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\nclass TritonLlamaRMSNorm(torch.nn.Module):\n    def __init__(self, normalized_shape, eps=1e-6):\n        super(TritonLlamaRMSNorm, self).__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n        self.weight = torch.nn.Parameter(torch.ones(normalized_shape))\n\n    def forward(self, x):\n        assert x.is_cuda, \"Input tensor must be on a CUDA device\"\n        assert x.dim() == 2, \"Input tensor must be 2D (batch_size, features)\"\n        assert x.size(1) < 64 * 1024, \"Feature dimension must be less than 64KB\"\n\n        M, N = x.size()\n        BLOCK_SIZE = 128  # Adjust this block size as needed\n\n        # Allocate output tensor\n        y = torch.empty_like(x)\n\n        # Launch the Triton kernel\n        grid = (M + BLOCK_SIZE - 1) // BLOCK_SIZE\n        rms_norm_fwd_fused[grid, BLOCK_SIZE](\n            x, y, self.weight, x.stride(0), y.stride(0), M, N, self.eps, BLOCK_SIZE\n        )\n\n        return y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n# Kernel function for fused RMSNorm\n@triton.jit\ndef rms_norm_fwd_fused(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    W,  # pointer to the weights\n    stride,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    # Compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x = tl.where(cols < N, x, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        # Write output\n        tl.store(Y + cols, y, mask=mask)\n\n# TritonLlamaRMSNorm class for integrating the kernel into a model\nclass TritonLlamaRMSNorm(nn.Module):\n    def __init__(self, weight, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = weight\n        self.variance_epsilon = eps\n\n    def forward(self, x):\n        y = torch.empty_like(x)\n        # reshape input data into 2D tensor\n        x_arg = x.reshape(-1, x.shape[-1])\n        M, N = x_arg.shape\n        # Less than 64KB per feature: enqueue fused kernel\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n        if N > BLOCK_SIZE:\n            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n        # heuristics for number of warps\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n        # enqueue kernel\n        rms_norm_fwd_fused[(M,)](\n            x_arg,\n            y,\n            self.weight,\n            x_arg.stride(0),\n            N,\n            self.variance_epsilon,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n        return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `rmsnorm_triton` performs RMS normalization on a 3D tensor `x` using a weight vector `rms_w`. The function calculates the root mean square of elements across a specified dimension, normalizes the values, and then scales them by `rms_w`. It operates over batches and a matrix's M dimension with a fixed block size (`BLOCK_N_SIZE`). The wrapper function `rmsnorm_triton_wrapper` prepares the inputs and launches the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, output_ptr, M, N, B, BLOCK_N_SIZE: tl.constexpr):\n    # Compute the block ID in the M and N dimensions\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    pid_b = tl.program_id(axis=2)\n\n    # Compute the block range in the M and N dimensions\n    block_start_m = pid_m * BLOCK_N_SIZE\n    block_start_n = pid_n * BLOCK_N_SIZE\n\n    # Compute the block range in the batch dimension\n    batch_start = pid_b * M * N\n\n    # Initialize the output block\n    output_block = tl.zeros((BLOCK_N_SIZE, BLOCK_N_SIZE), dtype=tl.float32)\n\n    # Load the weight vector\n    rms_w = tl.load(rms_w_ptr + block_start_n)\n\n    # Iterate over the elements in the block\n    for i in range(BLOCK_N_SIZE):\n        # Compute the start and end indices for the current row\n        row_start = block_start_m + i * N\n        row_end = row_start + BLOCK_N_SIZE\n\n        # Load the row from the input tensor\n        row = tl.load(x_ptr + batch_start + row_start + block_start_n)\n\n        # Compute the mean of the squared values\n        mean = tl.sum(row * row, axis=0) / BLOCK_N_SIZE\n\n        # Compute the root mean square\n        rms = tl.sqrt(mean + 1e-6)\n\n        # Normalize the row\n        normalized_row = row / rms\n\n        # Scale the normalized row by the weight vector\n        scaled_row = normalized_row * rms_w\n\n        # Store the result in the output block\n        output_block[i, :] = scaled_row\n\n    # Store the output block back to the output tensor\n    tl.store(output_ptr + batch_start + block_start_m * N + block_start_n, output_block)\n\nimport torch\n\ndef rmsnorm_triton_wrapper(x, rms_w, output):\n    # Get the dimensions of the input tensor\n    B, M, N = x.shape\n\n    # Define the block size\n    BLOCK_N_SIZE = 128\n\n    # Compute the number of blocks in the M and N dimensions\n    grid_m = (M + BLOCK_N_SIZE - 1) // BLOCK_N_SIZE\n    grid_n = (N + BLOCK_N_SIZE - 1) // BLOCK_N_SIZE\n\n    # Define the grid and block dimensions\n    grid = (grid_m, grid_n, B)\n\n    # Launch the kernel\n    rmsnorm_triton[grid, (BLOCK_N_SIZE, BLOCK_N_SIZE)](\n        x, rms_w, output, M, N, B, BLOCK_N_SIZE\n    )\n\n# Example usage\nB, M, N = 2, 1024, 1024\nx = torch.randn((B, M, N), device='cuda')\nrms_w = torch.randn((N,), device='cuda')\noutput = torch.empty((B, M, N), device='cuda')\n\nrmsnorm_triton_wrapper(x, rms_w, output)\n\nprint(output)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, output_ptr,\n                   stride_x_batch, stride_x_m, stride_x_k,\n                   stride_rms_w,\n                   stride_out_batch, stride_out_m, stride_out_k,\n                   N_SIZE: tl.constexpr, eps: tl.constexpr, BLOCK_N_SIZE: tl.constexpr):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n\n    offs_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_N = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        offs_n = block_n_start_idx + block_N\n        x_ptr_mask = offs_n < N_SIZE\n        x = tl.load(x_ptr + offs_m + offs_n * stride_x_k, mask=x_ptr_mask, other=0.0)\n        var += tl.extra.cuda.libdevice.pow(x.to(tl.float32), 2)\n\n    var = tl.sum(var, axis=0) / N_SIZE\n    rstd = tl.math.rsqrt(var + eps)\n\n    # multiply by weight and add bias\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        offs_n = block_n_start_idx + block_N\n        x_ptr_mask = offs_n < N_SIZE\n        rms_w = tl.load(rms_w_ptr + offs_n * stride_rms_w, mask=x_ptr_mask)\n\n        x = tl.load(x_ptr + offs_m + offs_n * stride_x_k, mask=x_ptr_mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        out = x_hat * rms_w\n        out_off = pid_batch * stride_out_batch + pid_m * stride_out_m + offs_n * stride_out_k\n        tl.store(output_ptr + out_off, out, mask=x_ptr_mask)\n\n\ndef rmsnorm_triton_wrapper(x, rms_w, eps=1e-6):\n    batch, M, K = x.shape\n    assert rms_w.shape[-1] == K\n    out = torch.empty_like(x)\n    rmsnorm_triton[(batch, M,)](x, rms_w, out,\n                                *x.stride(),\n                                *rms_w.stride(),\n                                *out.stride(),\n                                N_SIZE=K, eps=eps, BLOCK_N_SIZE=1024,\n                                )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `nested3` Triton kernel performs repeated memory loads and stores with a nested loop pattern. It loads data from an input pointer `in_ptr`, shifts its position by `stride_n` and stores it to an output pointer `out_ptr`. The kernel uses 2x2 tile operations with pointers calculated using provided strides, and nested loops iterate through multiple 2x2 block positions. The `wrapper_nested3` function initializes an input tensor and output tensor, configures the execution grid, and launches the kernel, printing the output tensor.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef nested3(in_ptr, out_ptr, stride_n, M, N, BLOCK_SIZE: tl.constexpr):\n    # Compute the position of the block in the grid\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Compute the block boundaries\n    rm = pid_m * BLOCK_SIZE\n    rn = pid_n * BLOCK_SIZE\n    \n    # Compute the block boundaries\n    bounds_m = tl.min(rm + BLOCK_SIZE, M)\n    bounds_n = tl.min(rn + BLOCK_SIZE, N)\n    \n    # Load the input block\n    in_block = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            if rm + i < M and rn + j < N:\n                in_block[i, j] = tl.load(in_ptr + (rm + i) * stride_n + (rn + j))\n    \n    # Shift the block and store it to the output\n    out_block = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            if rm + i < M and rn + j < N:\n                out_block[i, j] = in_block[i, j]\n    \n    # Store the output block\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            if rm + i < M and rn + j < N:\n                tl.store(out_ptr + (rm + i) * stride_n + (rn + j), out_block[i, j])\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 2\n\n# Define the kernel\n@triton.jit\ndef nested3(in_ptr, out_ptr, stride_n, M, N, BLOCK_SIZE: tl.constexpr):\n    # Compute the position of the block in the grid\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Compute the block boundaries\n    rm = pid_m * BLOCK_SIZE\n    rn = pid_n * BLOCK_SIZE\n    \n    # Compute the block boundaries\n    bounds_m = tl.min(rm + BLOCK_SIZE, M)\n    bounds_n = tl.min(rn + BLOCK_SIZE, N)\n    \n    # Load the input block\n    in_block = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            if rm + i < M and rn + j < N:\n                in_block[i, j] = tl.load(in_ptr + (rm + i) * stride_n + (rn + j))\n    \n    # Shift the block and store it to the output\n    out_block = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            if rm + i < M and rn + j < N:\n                out_block[i, j] = in_block[i, j]\n    \n    # Store the output block\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            if rm + i < M and rn + j < N:\n                tl.store(out_ptr + (rm + i) * stride_n + (rn + j), out_block[i, j])\n\n# Define the wrapper function\ndef wrapper_nested3(M, N, stride_n):\n    # Initialize the input and output tensors\n    in_tensor = torch.arange(M * N, dtype=torch.float32).view(M, N).cuda()\n    out_tensor = torch.zeros_like(in_tensor)\n    \n    # Define the grid and block dimensions\n    grid = (M // BLOCK_SIZE, N // BLOCK_SIZE)\n    \n    # Launch the kernel\n    nested3[grid](in_tensor, out_tensor, stride_n, M, N, BLOCK_SIZE)\n    \n    # Print the output tensor\n    print(\"Output Tensor:\")\n    print(out_tensor.cpu().numpy())\n\n# Example usage\nM = 4\nN = 4\nstride_n = N\nwrapper_nested3(M, N, stride_n)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef nested3(in_ptr, out_ptr, stride_m, stride_n):\n    offs_am = tl.arange(0, 2)\n    offs_an = tl.arange(0, 2)\n    a_ptrs = in_ptr + (offs_am[:, None] * stride_m +\n                        offs_an[None, :] * stride_n)\n\n    offs_cm = tl.arange(0, 2)\n    offs_cn = tl.arange(0, 2)\n    c_ptrs = out_ptr + stride_m * offs_cm[:, None] + stride_n * offs_cn[\n        None, :]\n\n    for i in range(0, 2):\n        a1 = tl.load(a_ptrs)\n\n        for j in range(0, 2):\n            a_ptrs += 2 * stride_n\n            a2 = tl.load(a_ptrs)\n\n            for k in range(0, 2):\n                a_ptrs += 2 * stride_n\n                a3 = tl.load(a_ptrs)\n                tl.store(c_ptrs, a1)\n                c_ptrs += 2 * stride_n\n\n                tl.store(c_ptrs, a2)\n                c_ptrs += 2 * stride_n\n                tl.store(c_ptrs, a3)\n                c_ptrs += 2 * stride_n\n\n        a_ptrs += 2 * stride_n\n\n\ndef wrapper_nested3(n_rows, n_cols):\n    x = torch.arange(0, n_rows * n_cols, device=\"cuda\", dtype=torch.int32).reshape([n_rows, n_cols])\n    output = torch.zeros([n_rows, n_cols], device=x.device, dtype=x.dtype)\n    grid = lambda meta: (n_cols // 4,)\n    nested3[grid](x, output, x.stride(0), x.stride(1))\n    print(output)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator is designed for efficient matrix multiplication with specific attention to quantized inputs, where matrix B is stored in an int4 format using GPTQ (General-Purpose Tensor Quantization).\n        \n        The `matmul4_kernel` function performs the main computation of the matrix multiplication C = A x B. Matrix A is float16, matrix B is int32 (representing int4 quantized values), and matrix C is float16. The kernel utilizes scaling and zero-point values to correctly dequantize B during multiplication. It applies a loop across the K-dimension and uses block sizes (BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K) to handle chunks of data efficiently. The function also takes advantage of Triton's features like parallel execution through `program_id` and auto-tuning with `triton.autotune`.\n\n        The `matmul_dequantize_int4_gptq` function is a wrapper to execute this kernel. It sets up grid dimensions for the execution, computes the output matrix C, and checks for memory alignment. It returns the output matrix C after computation.\n\n        The `quantize_int4` function prepares data in int4 format by packing 8 such values into an int32 word. This includes computing scales and zero points per group of features and packing them into the final format used by the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul4_kernel(\n    A, B, C, scales, zeros,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a_block = tl.load(a_ptrs)\n        b_block = tl.load(b_ptrs)\n\n        b_block = b_block.to(tl.int4)\n        b_block = b_block.to(tl.int32)\n        b_block = (b_block - zeros) * scales\n\n        accumulator += tl.dot(a_block, b_block)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n    c_ptrs = C + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\nimport torch\n\ndef matmul_dequantize_int4_gptq(A, B, scales, zeros):\n    M, K = A.shape\n    _, N = B.shape\n\n    # Ensure A and B are in the correct format\n    A = A.to(torch.float16)\n    B = B.to(torch.int32)\n    scales = scales.to(torch.float16)\n    zeros = zeros.to(torch.float16)\n\n    # Allocate output tensor\n    C = torch.empty((M, N), dtype=torch.float16, device=A.device)\n\n    # Define grid and block sizes\n    BLOCK_SIZE_M = 16\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_K = 32\n    GROUP_SIZE_M = 8\n\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n\n    # Launch the kernel\n    matmul4_kernel[grid](\n        A, B, C, scales, zeros,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M\n    )\n\n    return C\n\ndef quantize_int4(X, group_size=128):\n    N, K = X.shape\n    scales = torch.empty((N, K // group_size), dtype=torch.float16, device=X.device)\n    zeros = torch.empty((N, K // group_size), dtype=torch.float16, device=X.device)\n    B = torch.empty((N, K // 2), dtype=torch.int32, device=X.device)\n\n    for i in range(0, K, group_size):\n        x = X[:, i:i + group_size]\n        max_val = torch.max(x.abs(), dim=1, keepdim=True).values\n        scales[:, i // group_size] = max_val / 7\n        zeros[:, i // group_size] = 0\n\n        x = x / scales[:, i // group_size]\n        x = x.round().to(torch.int8)\n\n        for j in range(0, group_size, 2):\n            b = (x[:, j].to(torch.int32) & 0x0F) | ((x[:, j + 1].to(torch.int32) & 0x0F) << 4)\n            B[:, i // 2 + j // 2] = b\n\n    return B, scales, zeros\n\nimport torch\n\n# Example input matrices\nM, K, N = 1024, 1024, 1024\nA = torch.randn((M, K), dtype=torch.float16, device='cuda')\nB, scales, zeros = quantize_int4(torch.randn((K, N), dtype=torch.float16, device='cuda'))\n\n# Perform matrix multiplication\nC = matmul_dequantize_int4_gptq(A, B, scales, zeros)\n\nprint(C)\n", "label": "import time\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n\tconfigs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), \n    ],\n\tkey=['M', 'N', 'K', 'NO_GROUPS'],\n)\n@triton.jit\ndef matmul4_kernel(\n\ta_ptr, b_ptr, c_ptr,\n\tscales_ptr, zeros_ptr,\n\tM, N, K,\n\tstride_am, stride_ak,\n\tstride_bk, stride_bn,\n\tstride_cm, stride_cn,\n\tstride_scales_g, stride_scales_n,\n\tstride_zeros_g, stride_zeros_n,\n\tgroupsize, NO_GROUPS: tl.constexpr,\n\tBLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n\tGROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    # b_ptrs is set up such that it repeats elements along the K axis 8 times\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n   # (BLOCK_SIZE_N,)\n    # zeros_ptrs is set up such that it repeats elements along the N axis 8 times\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)   # (BLOCK_SIZE_N,)\n    # shifter is used to extract the 4 bits of each element in the 32-bit word from B and zeros\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    # If G == 1, scales and zeros are the same for all K, so we can load them once\n    if NO_GROUPS:\n        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n        # Unpack zeros\n        zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n        # zeros = (zeros + 1) * scales  # (BLOCK_SIZE_N,) float16\n        zeros = zeros * scales\n    # Now calculate a block of output of shape (BLOCK_SIZE_M, BLOCK_SIZE_N)\n    # M is along the batch dimension, N is along the outfeatures dimension, K is along the infeatures dimension\n    # So this loop is along the infeatures dimension (K)\n    # It's calculating BLOCK_SIZE_M batches in parallel, and for each batch, BLOCK_SIZE_N outfeatures in parallel\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)  # (BLOCK_SIZE_N,)\n            ptr = zeros_ptrs + g_id * stride_zeros_g   # (BLOCK_SIZE_N,)\n            zeros = tl.load(ptr)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n            # Unpack zeros\n            zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n            zeros = (zeros) * scales  # (BLOCK_SIZE_N,) float16\t\n        # Now we need to unpack b (which is 4-bit values) into 32-bit values\n        b = (b >> shifter[:, None]) & 0xF  # Extract the 4-bit values\n        b = b * scales[None, :] - zeros[None, :]  # Scale and shift\n        # print(\"data type\", a, b)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk  \n    c = accumulator.to(c_ptr.dtype.element_ty)  \n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n\ndef matmul_dequantize_int4_gptq(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size, output=None) -> torch.FloatTensor:\n\t\"\"\"\n\tCompute the matrix multiplication C = A x B + bias.\n\tWhere B is quantized using GPTQ and groupsize = -1 into 4-bit values.\n\n\tA is of shape (..., K) float16\n\tqweight is of shape (K//8, N) int32\n\tscales is of shape (G, N) float16\n\tqzeros is of shape (G, N//8) int32\n\tbias is of shape (1, N) float16\n\n\tgroupsize is the number of infeatures in each group.\n\tG = K // groupsize\n\n\tReturns C of shape (..., N) float16\n\t\"\"\"\n\tassert x.shape[-1] == (qweight.shape[0] * 8), \"A must be a multiple of 8 in the last dimension\"\n\tassert x.is_contiguous(), \"A must be contiguous\"\n\n\tM, K = x.shape\n\tN = qweight.shape[1]\n\t# This is based on the possible BLOCK_SIZE_Ks\n\t# assert K % 16 == 0 and K % 32 == 0 and K % 64 == 0 and K % 128 == 0, \"K must be a multiple of 16, 32, 64, and 128\"\n\t# # This is based on the possible BLOCK_SIZE_Ns\n\t# assert N % 16 == 0 and N % 32 == 0 and N % 64 == 0 and N % 128 == 0 and N % 256 == 0, \"N must be a multiple of 16, 32, 64, 128, and 256\"\n\t# # This is based on the possible BLOCK_SIZE_Ks\n\t# assert groupsize % 32 == 0 and groupsize % 64 == 0 and groupsize % 128 == 0, \"groupsize must be a multiple of 32, 64, and 128\"\n\n\t# output = torch.empty((M, N), device='cuda', dtype=torch.float16)\n\tif output is None:\n\t\tinplace = False\n\t\toutput = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\telse:\n\t\tinplace = True\n\n\tgrid = lambda META: (\n\t\ttriton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n\t)\n\tmatmul4_kernel[grid](\n\t\tx, qweight, output,\n\t\tscales, qzeros,\n\t\tM, N, K,\n\t\tx.stride(0), x.stride(1),\n\t\tqweight.stride(0), qweight.stride(1),\n\t\toutput.stride(0), output.stride(1),\n\t\tscales.stride(0), scales.stride(1),\n\t\tqzeros.stride(0), qzeros.stride(1),\n\t\tgroup_size, group_size == K,\n    )\n\t# return output\n\tif not inplace:\n\t\treturn output\n\ndef quantize_int4(weight, group_size=128, tp_rank=0):\n    # Weight shape: [H1 // 8, H2]\n    # Scale shape: [H1 // group_size, H2]\n    # zero_pint shape: [H1 // group_size, H2 // 8]\n\n    weight = weight.transpose(1, 0)\n    h1, h2 = weight.shape\n    assert h1 % 8 == 0 and h2 % 8 == 0, \"H1 {} H2 {}\".format(h1, h2)\n    assert h2 % group_size == 0, \"H1 {} H2 {}\".format(h1, h2)\n    weight = weight.contiguous().view(-1, group_size).cuda(tp_rank)\n    weight_max = weight.amax(-1, keepdim=True)\n    weight_max = torch.where(weight_max < 0, 0, weight_max)\n    weight_min = weight.amin(-1, keepdim=True)\n    weight_min = torch.where(weight_min > 0, 0, weight_min)\n    weight_range = weight_max - weight_min \n    scale = weight_range / (2 ** 4 - 1)\n    zero_point = (-weight_min / scale).round().clamp(0, 15).to(torch.int32)\n    weight = (weight / scale + zero_point).round().clamp(0, 15).to(torch.int32).view(h1, h2)\n    int_weight = torch.empty(h1, h2 // 8).to(torch.int32).to(weight.device)\n    int_zero_point = torch.zeros(h1 // 8, h2 // group_size).to(torch.int32).to(weight.device)\n    zero_point = zero_point.view(h1, -1)\n    scale = scale.view(h1, -1)\n    # pack 8 int4 in an int32 number.\n    # Weight pack in row.\n    for pack in range(0, h2, 8):\n        for i in range(8):\n            int_weight[:, pack // 8] += weight[:, pack + i] << (i * 4)\n    # zero point pack in col.\n    for pack in range(0, h1, 8):\n        for i in range(8):\n            int_zero_point[pack // 8, :] += zero_point[pack + i, :] << (i * 4)\n    '''\n    fp_weight = torch.zeros(h1, h2).half().to(weight.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_weight[pack * 8 + i, :] = \\\n                ((int_weight[pack, :] << (28 - i * 4) >> 28) + 16) % 16\n    print((fp_weight - weight).abs().sum())\n\n    fp_zp = torch.zeros(zero_point.shape).half().to(zero_point.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zp[pack * 8 + i, :] = \\\n                (int_zero_point[pack, :] >> (i * 4)) & 15\n\n    print((fp_zp - zero_point).abs().sum())\n    '''\n    weight = None\n    return int_weight.transpose(1, 0).contiguous(), scale.transpose(1, 0).contiguous(), int_zero_point.transpose(1, 0).contiguous(), group_size\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel and wrapper function provided aim to convert packed 4-bit floating-point values to scaled bf16 (bfloat16) values. The kernel is defined as `triton_f4_to_scaled_bf16_kernel` and the wrapper function is `triton_f4_to_scaled_bf16`.\n\nFunction `triton_f4_to_scaled_bf16_kernel`: \n- Purpose: Convert packed fp4 values to bf16 and apply a scale.\n- Input: `x_ptr` (fp4 packed values), `s_ptr` (scale), `output_ptr` (output storage), `n_elements_in`, and several constant parameters like masks and biases for the conversion.\n- Output: A tensor of bf16 values scaled by a given factor.\n- Key Steps: \n  - It unpacks 4-bit numbers, aligns bits, applies exponent bias adjustments, and then scales the resulting bf16 values.\n  - Utilizes Triton's parallel computation model, dividing work into blocks based on `BLOCK_SIZE_IN`.\n  - Handles special cases for zero and denormal values with conditional logic.\n\nFunction `triton_f4_to_scaled_bf16`: \n- Purpose: Prepare and launch the kernel.\n- Input: A packed fp4 tensor `x`, a scale tensor `s_e8m0`, and block size `mx_block_size`.\n- Output: Bf16 tensor with scaling applied.\n- Steps:\n  - Allocates space for the output and checks input conditions.\n  - Calculates grid size for the Triton kernel launch.\n  - Passes necessary parameters to the kernel including constants like `SIGN_MASK_F4`, `ZERO_BITS_F32`, and others.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef triton_f4_to_scaled_bf16_kernel(\n    x_ptr,  # *uint8\n    s_ptr,  # *float32\n    output_ptr,  # *bfloat16\n    n_elements_in,  # size_t\n    BLOCK_SIZE_IN: tl.constexpr,\n    SIGN_MASK_F4: tl.constexpr,\n    EXP_MASK_F4: tl.constexpr,\n    MANTISSA_MASK_F4: tl.constexpr,\n    EXP_BIAS_F4: tl.constexpr,\n    EXP_BIAS_BF16: tl.constexpr,\n    SIGN_MASK_BF16: tl.constexpr,\n    EXP_MASK_BF16: tl.constexpr,\n    MANTISSA_MASK_BF16: tl.constexpr,\n    ZERO_BITS_F32: tl.constexpr,\n    DENORMAL_THRESHOLD_F32: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE_IN\n    offsets = block_start + tl.arange(0, BLOCK_SIZE_IN)\n    mask = offsets < n_elements_in\n\n    # Load packed 4-bit values\n    packed_x = tl.load(x_ptr + offsets // 2, mask=mask, other=0)\n    x_lo = packed_x & 0x0F\n    x_hi = (packed_x >> 4) & 0x0F\n\n    # Load scale\n    scale = tl.load(s_ptr)\n\n    # Convert to fp32\n    sign_f4 = (x_lo & SIGN_MASK_F4) << 27\n    exp_f4 = (x_lo & EXP_MASK_F4) << 23\n    mantissa_f4 = (x_lo & MANTISSA_MASK_F4) << 23\n    x_lo_f32 = tl.cvt(tl.float32, sign_f4 | exp_f4 | mantissa_f4) * (2 ** (EXP_BIAS_BF16 - EXP_BIAS_F4))\n\n    sign_f4 = (x_hi & SIGN_MASK_F4) << 27\n    exp_f4 = (x_hi & EXP_MASK_F4) << 23\n    mantissa_f4 = (x_hi & MANTISSA_MASK_F4) << 23\n    x_hi_f32 = tl.cvt(tl.float32, sign_f4 | exp_f4 | mantissa_f4) * (2 ** (EXP_BIAS_BF16 - EXP_BIAS_F4))\n\n    # Apply scale\n    x_lo_f32 = x_lo_f32 * scale\n    x_hi_f32 = x_hi_f32 * scale\n\n    # Handle special cases\n    x_lo_f32 = tl.where(x_lo_f32 == 0.0, ZERO_BITS_F32, x_lo_f32)\n    x_lo_f32 = tl.where(x_lo_f32 < DENORMAL_THRESHOLD_F32, DENORMAL_THRESHOLD_F32, x_lo_f32)\n    x_hi_f32 = tl.where(x_hi_f32 == 0.0, ZERO_BITS_F32, x_hi_f32)\n    x_hi_f32 = tl.where(x_hi_f32 < DENORMAL_THRESHOLD_F32, DENORMAL_THRESHOLD_F32, x_hi_f32)\n\n    # Convert to bf16\n    x_lo_bf16 = tl.cvt(tl.bfloat16, x_lo_f32)\n    x_hi_bf16 = tl.cvt(tl.bfloat16, x_hi_f32)\n\n    # Store results\n    tl.store(output_ptr + offsets, x_lo_bf16, mask=mask)\n    tl.store(output_ptr + offsets + BLOCK_SIZE_IN // 2, x_hi_bf16, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef triton_f4_to_scaled_bf16(x, s_e8m0, mx_block_size=1024):\n    # Constants for conversion\n    SIGN_MASK_F4 = 0x8\n    EXP_MASK_F4 = 0x7\n    MANTISSA_MASK_F4 = 0x0\n    EXP_BIAS_F4 = -7\n    EXP_BIAS_BF16 = -127\n    SIGN_MASK_BF16 = 0x8000\n    EXP_MASK_BF16 = 0x7F80\n    MANTISSA_MASK_BF16 = 0x007F\n    ZERO_BITS_F32 = 0x00000000\n    DENORMAL_THRESHOLD_F32 = 0x00000001\n\n    # Input and output tensors\n    n_elements_in = x.numel() * 2  # Each uint8 contains 2 fp4 values\n    output = torch.empty(n_elements_in, dtype=torch.bfloat16, device=x.device)\n\n    # Grid and block sizes\n    grid = (triton.cdiv(n_elements_in, mx_block_size),)\n\n    # Launch kernel\n    triton_f4_to_scaled_bf16_kernel[grid](\n        x, s_e8m0, output, n_elements_in,\n        BLOCK_SIZE_IN=mx_block_size,\n        SIGN_MASK_F4=SIGN_MASK_F4,\n        EXP_MASK_F4=EXP_MASK_F4,\n        MANTISSA_MASK_F4=MANTISSA_MASK_F4,\n        EXP_BIAS_F4=EXP_BIAS_F4,\n        EXP_BIAS_BF16=EXP_BIAS_BF16,\n        SIGN_MASK_BF16=SIGN_MASK_BF16,\n        EXP_MASK_BF16=EXP_MASK_BF16,\n        MANTISSA_MASK_BF16=MANTISSA_MASK_BF16,\n        ZERO_BITS_F32=ZERO_BITS_F32,\n        DENORMAL_THRESHOLD_F32=DENORMAL_THRESHOLD_F32\n    )\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fp4_packed_to_bf16(\n    x_packed,\n    sign_mask_f4,\n    mantissa_mask_f4,\n    mbits_f4_e2m1,\n    ebits_f4_e2m1,\n    f4_e2m1_exp_bias,\n    mbits_f32,\n    ebits_f32,\n    f32_exp_bias,\n    zero_bits_f32,\n    zero_point_five_bits_f32,\n):\n    \"\"\"\n    Input: a tensor of packed fp4 values\n    Output: a tensor of bfloat16 values\n    \"\"\"\n\n    # low-bits: original location 0:3\n    # high-bits: original location 4:7\n    x_low_bits = x_packed >> 4\n    x_high_bits = x_packed & 0xF\n    x = tl.interleave(x_low_bits, x_high_bits)\n\n    # cast logic below\n    # output = x_unpacked.to(tl.float32)\n\n    # save the sign\n    sign_f4 = x & sign_mask_f4\n\n    # set everything to positive, will add sign back at the end\n    x_pos = x ^ sign_f4\n\n    # Special case zero\n    zero_mask = x_pos == 0\n\n    # There is only one denormal value in fp4: s001, which is 0.5 in f32\n    # Special case it.\n    # TODO(later): will it be faster to repeat this for all 8 positive\n    # values instead of the bit manipulations?\n    denormal_mask = x_pos == 1\n\n    # calculate the new exponent and shift it to bits 2:9 of the result\n    exp_biased_f4 = x_pos >> mbits_f4_e2m1\n    exp_biased_f32 = exp_biased_f4 - f4_e2m1_exp_bias + f32_exp_bias\n    exp_biased_f32 = exp_biased_f32.to(tl.int32) << mbits_f32\n\n    # shift the mantissa to bits 10:32 of the result\n    mantissa_f4 = x_pos & mantissa_mask_f4\n    mantissa_f32 = mantissa_f4.to(tl.int32) << (mbits_f32 - mbits_f4_e2m1)\n    output = mantissa_f32\n\n    # combine the pieces\n    result = exp_biased_f32 | mantissa_f32\n    # result[zero_mask] = ZERO_BITS_F32\n    result = tl.where(zero_mask, zero_bits_f32, result)\n    # result[denormal_mask] = ZERO_POINT_FIVE_BITS_F32\n    result = tl.where(denormal_mask, zero_point_five_bits_f32, result)\n\n    # add sign back\n    sign_f32 = sign_f4.to(tl.int32) << (\n        mbits_f32 - mbits_f4_e2m1 + ebits_f32 - ebits_f4_e2m1\n    )\n    result = result | sign_f32\n\n    # The bit shifting above is for float32, so for now we\n    # bitcast to float32 and then regular cast to bfloat16\n    # TODO(later): it should be pretty easy to cast directly to bf16, just\n    # need to adjust the mbits/ebits/special values. Perf impact is likely\n    # to be small as we would not be chaning memory access patterns.\n    output = result.to(tl.float32, bitcast=True)\n    output = output.to(tl.bfloat16)\n    return output\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE_IN\": 128}),\n        triton.Config({\"BLOCK_SIZE_IN\": 256}),\n        triton.Config({\"BLOCK_SIZE_IN\": 512}),\n        triton.Config({\"BLOCK_SIZE_IN\": 1024}),\n        triton.Config({\"BLOCK_SIZE_IN\": 2048}),\n    ],\n    key=[\"n_elements_in\"],\n)\n@triton.jit\ndef triton_f4_to_scaled_bf16_kernel(\n    x_ptr,\n    s_ptr,\n    output_ptr,\n    n_elements_in,\n    mx_block_size: tl.constexpr,\n    sign_mask_f4: tl.constexpr,\n    mantissa_mask_f4: tl.constexpr,\n    mbits_f4_e2m1: tl.constexpr,\n    ebits_f4_e2m1: tl.constexpr,\n    f4_e2m1_exp_bias: tl.constexpr,\n    mbits_f32: tl.constexpr,\n    ebits_f32: tl.constexpr,\n    f32_exp_bias: tl.constexpr,\n    zero_bits_f32: tl.constexpr,\n    zero_point_five_bits_f32: tl.constexpr,\n    e8m0_exponent_bias: tl.constexpr,\n    e8m0_exponent_nan_val: tl.constexpr,\n    BLOCK_SIZE_IN: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    n_elements_s = n_elements_out // 32\n\n    BLOCK_SIZE_S: tl.constexpr = BLOCK_SIZE_IN // 16\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n    mask_in = offsets_in < n_elements_in\n    # packed uint8\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(\n        x_packed,\n        sign_mask_f4,\n        mantissa_mask_f4,\n        mbits_f4_e2m1,\n        ebits_f4_e2m1,\n        f4_e2m1_exp_bias,\n        mbits_f32,\n        ebits_f32,\n        f32_exp_bias,\n        zero_bits_f32,\n        zero_point_five_bits_f32,\n    )\n\n    # load scale\n    block_start_s = pid * BLOCK_SIZE_S\n    offsets_s = block_start_s + tl.arange(0, BLOCK_SIZE_S)\n    mask_s = offsets_s < n_elements_s\n    s = tl.load(s_ptr + offsets_s, mask=mask_s)\n\n    # create the scale in bf16\n    s_offset = s.to(tl.int16) - e8m0_exponent_bias\n    s_fp = tl.extra.cuda.libdevice.pow(2.0, s_offset).to(tl.bfloat16)\n    s_fp = tl.where(s != e8m0_exponent_nan_val, s_fp, float(\"nan\"))\n\n    # multiply output by scale\n    # TODO(later): see if manipulating the exponent instead of fp\n    # multiplication is going to give a significant speedup\n    output = tl.reshape(\n        output, (BLOCK_SIZE_OUT // mx_block_size, mx_block_size)\n    )  # noqa: E501\n    s_fp = tl.reshape(s_fp, (BLOCK_SIZE_S // 1, 1))\n    output = output * s_fp\n    output = tl.reshape(output, (BLOCK_SIZE_OUT,))\n\n    # set up output offsets\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)\n\n\nEBITS_F32, MBITS_F32 = 8, 23\nEBITS_F4_E2M1, MBITS_F4_E2M1 = 2, 1\nEBITS_F6_E2M3, MBITS_F6_E2M3 = 2, 3\nEBITS_F6_E3M2, MBITS_F6_E3M2 = 3, 2\n\nSIGN_MASK_F4 = 0x8  # 1000\nMANTISSA_MASK_F4 = 0x1  # 0001\n\nZERO_BITS_F32 = 0x0\nZERO_POINT_FIVE_BITS_F32 = 0x3F000000\nF4_E2M1_EXP_BIAS = 1\nF32_EXP_BIAS = 127\nE8M0_EXPONENT_BIAS = 127\nE8M0_EXPONENT_NAN_VAL = 255\n\n\ndef triton_f4_to_scaled_bf16(\n    x: torch.Tensor,\n    s_e8m0: torch.Tensor,\n    mx_block_size: int,\n):\n    \"\"\"\n    Input: a tensor of packed fp4 values, and a scale in e8m0 format. The block\n        size is currently assumed to be 32.\n    Output: a tensor of bfloat16 values, multiplied by the encoded scale\n    \"\"\"\n    new_shape = (*x.shape[:-1], x.shape[-1] * 2)\n    output = torch.empty(*new_shape, device=x.device, dtype=torch.bfloat16)\n    assert x.is_contiguous()\n    assert x.is_cuda and output.is_cuda\n    n_elements_in = x.numel()\n    grid = lambda meta: (  # noqa: E731\n        triton.cdiv(n_elements_in, meta[\"BLOCK_SIZE_IN\"]),\n    )\n    triton_f4_to_scaled_bf16_kernel[grid](\n        x,\n        s_e8m0,\n        output,\n        n_elements_in,\n        mx_block_size,\n        sign_mask_f4=SIGN_MASK_F4,\n        mantissa_mask_f4=MANTISSA_MASK_F4,\n        mbits_f4_e2m1=MBITS_F4_E2M1,\n        ebits_f4_e2m1=EBITS_F4_E2M1,\n        f4_e2m1_exp_bias=F4_E2M1_EXP_BIAS,\n        mbits_f32=MBITS_F32,\n        ebits_f32=EBITS_F32,\n        f32_exp_bias=F32_EXP_BIAS,\n        zero_bits_f32=ZERO_BITS_F32,\n        zero_point_five_bits_f32=ZERO_POINT_FIVE_BITS_F32,\n        e8m0_exponent_bias=E8M0_EXPONENT_BIAS,\n        e8m0_exponent_nan_val=E8M0_EXPONENT_NAN_VAL,\n    )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs matrix multiplication of two input matrices x and y, and stores the result in matrix z. \n            The kernel is named 'matmul_kernel', and it's called from the 'matmul' function.\n            The kernel uses a block-wise approach to multiply sections of the matrices. \n            It calculates the current block for both m and n dimensions based on the program's id. \n            Offsets for loading data from global memory are computed for each matrix.\n            The kernel iteratively loads sub-matrices of x and y, computes their product using `tl.dot`, and accumulates the results in z. \n            Finally, it stores the result in global memory. \n            The 'matmul' function initializes the result matrix z and calculates the grid size for launching the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    x_ptr,  # Pointer to input matrix x\n    y_ptr,  # Pointer to input matrix y\n    z_ptr,  # Pointer to output matrix z\n    M,      # Number of rows in matrix x\n    N,      # Number of columns in matrix y\n    K,      # Number of columns in matrix x (and rows in matrix y)\n    stride_xm,  # Stride for rows in matrix x\n    stride_xk,  # Stride for columns in matrix x\n    stride_yn,  # Stride for columns in matrix y\n    stride_yk,  # Stride for rows in matrix y\n    stride_zm,  # Stride for rows in matrix z\n    stride_zn,  # Stride for columns in matrix z\n    BLOCK_SIZE_M: tl.constexpr,  # Block size for rows in matrix x\n    BLOCK_SIZE_N: tl.constexpr,  # Block size for columns in matrix y\n    BLOCK_SIZE_K: tl.constexpr,  # Block size for columns in matrix x (and rows in matrix y)\n    GROUP_SIZE_M: tl.constexpr   # Group size for rows in matrix x\n):\n    # Compute the program id for the current block\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # Compute the offsets for the current block\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) * stride_xm\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) * stride_yn\n    offs_k = tl.arange(0, BLOCK_SIZE_K) * stride_xk\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the sub-matrices of x and y\n        a = tl.load(x_ptr + offs_am[:, None] + offs_k[None, :], mask=offs_k[None, :] < K - k, other=0.0)\n        b = tl.load(y_ptr + offs_k[:, None] + offs_bn[None, :], mask=offs_k[:, None] < K - k, other=0.0)\n\n        # Compute the product of the sub-matrices\n        acc += tl.dot(a, b)\n\n    # Compute the offsets for the output matrix z\n    offs_zm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_zn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    z_offsets = offs_zm[:, None] * stride_zm + offs_zn[None, :] * stride_zn\n\n    # Store the result in global memory\n    tl.store(z_ptr + z_offsets, acc)\n\nimport torch\n\ndef matmul(x, y):\n    # Get the dimensions of the input matrices\n    M, K = x.shape\n    K, N = y.shape\n\n    # Allocate the output matrix\n    z = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n    # Define the block sizes\n    BLOCK_SIZE_M = 16\n    BLOCK_SIZE_N = 16\n    BLOCK_SIZE_K = 16\n    GROUP_SIZE_M = 8\n\n    # Calculate the grid size\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        x, y, z,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        y.stride(1), y.stride(0),\n        z.stride(0), z.stride(1),\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M\n    )\n\n    return z\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    x_ptr,\n    y_ptr,\n    z_ptr,\n    m_size,\n    k_size,\n    n_size,\n    m_block_size: tl.constexpr,\n    k_block_size: tl.constexpr,\n    n_block_size: tl.constexpr,\n):\n    # Calculate the m and n blocks to compute using the program id\n    pid = tl.program_id(0)\n    num_n_blocks = tl.cdiv(n_size, n_block_size)\n    m_block = pid // num_n_blocks\n    n_block = pid % num_n_blocks\n\n    m_offsets = tl.arange(0, m_block_size) + m_block * m_block_size\n    n_offsets = tl.arange(0, n_block_size) + n_block * n_block_size\n    k_offsets = tl.arange(0, k_block_size)\n\n    x_ptrs = x_ptr + m_offsets[:, None] * k_size + k_offsets[None, :]\n    y_ptrs = y_ptr + k_offsets[:, None] * n_size + n_offsets[None, :]\n    z_ptrs = z_ptr + m_offsets[:, None] * n_size + n_offsets[None, :]\n\n    z = tl.zeros((m_block_size, n_block_size), dtype=tl.float32)\n\n    for _ in range(0, k_size, k_block_size):\n        x_sub = tl.load(x_ptrs)\n        y_sub = tl.load(y_ptrs)\n        z += tl.dot(x_sub, y_sub, allow_tf32=False)\n        x_ptrs += k_block_size\n        y_ptrs += k_block_size * n_size\n\n    tl.store(z_ptrs, z)\n\ndef matmul(x, y):\n    m_size, k_size = x.shape\n    _, n_size = y.shape\n    z = torch.empty(m_size, n_size, device='cuda:0')\n    def grid(meta):\n        return (triton.cdiv(m_size, meta['m_block_size']) * triton.cdiv(n_size, meta['n_block_size']),)\n    \n    matmul_kernel[grid](\n        x,\n        y,\n        z,\n        m_size,\n        k_size,\n        n_size,\n        m_size,\n        k_size,\n        n_size\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator defined is a matrix multiplication kernel with an optional activation function applied. It takes matrices A and B as inputs and computes C = A x B. The kernel function `matmul_kernel` is designed to handle matrices with dimensions M, N, and K, and the operation is optimized using blocks defined by BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K. The matrix A must be contiguous. The kernel uses a loop to iterate over the K dimension in chunks of BLOCK_SIZE_K. Optionally, a leaky ReLU activation can be applied. The result is stored in matrix C.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    A, B, C, M, N, K, \n    stride_am, stride_ak, \n    stride_bk, stride_bn, \n    stride_cm, stride_cn, \n    alpha, beta, \n    BLOCK_SIZE_M: tl.constexpr, \n    BLOCK_SIZE_N: tl.constexpr, \n    BLOCK_SIZE_K: tl.constexpr, \n    GROUP_SIZE_M: tl.constexpr, \n    APPLY_ACTIVATION: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) * stride_am\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) * stride_bn\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_am[:, None] + offs_k[None, :] * stride_ak)\n    b_ptrs = B + (offs_k[:, None] + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a_mask = (offs_am[:, None] + k * stride_ak) < (M * K)\n        b_mask = (offs_k[:, None] + k * stride_bn) < (K * N)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bn\n\n    c = accumulator.to(C.dtype.element_ty)\n    if APPLY_ACTIVATION:\n        c = tl.where(c > 0, c, c * alpha)\n\n    offs_cm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) * stride_cm\n    offs_cn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) * stride_cn\n    c_ptrs = C + offs_cm[:, None] + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] + offs_cn[None, :]) < (M * N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# Define the grid and block sizes\nBLOCK_SIZE_M = 16\nBLOCK_SIZE_N = 16\nBLOCK_SIZE_K = 16\nGROUP_SIZE_M = 8\n\n# Define the kernel launch configuration\ndef matmul(A, B, C, alpha=0.01, beta=0.01, apply_activation=False):\n    M, K = A.shape\n    K, N = B.shape\n    assert K == B.shape[0], \"Matrix dimensions must match for multiplication\"\n    assert A.is_contiguous(), \"Matrix A must be contiguous\"\n    assert B.is_contiguous(), \"Matrix B must be contiguous\"\n    assert C.is_contiguous(), \"Matrix C must be contiguous\"\n\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n\n    matmul_kernel[grid](\n        A, B, C, M, N, K, \n        A.stride(0), A.stride(1), \n        B.stride(0), B.stride(1), \n        C.stride(0), C.stride(1), \n        alpha, beta, \n        BLOCK_SIZE_M=BLOCK_SIZE_M, \n        BLOCK_SIZE_N=BLOCK_SIZE_N, \n        BLOCK_SIZE_K=BLOCK_SIZE_K, \n        GROUP_SIZE_M=GROUP_SIZE_M, \n        APPLY_ACTIVATION=apply_activation\n    )\n\n# Example usage\nM, N, K = 1024, 1024, 1024\nA = torch.randn((M, K), device='cuda', dtype=torch.float32)\nB = torch.randn((K, N), device='cuda', dtype=torch.float32)\nC = torch.zeros((M, N), device='cuda', dtype=torch.float32)\n\nmatmul(A, B, C, apply_activation=True)\n\nprint(C)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        # Additional configurations omitted for brevity\n    ],\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,  # Pointers to matrices\n    M, N, K,  # Matrix dimensions\n    stride_am, stride_ak,  # Strides for matrix A\n    stride_bk, stride_bn,  # Strides for matrix B\n    stride_cm, stride_cn,  # Strides for matrix C\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, ACTIVATION: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n\ndef matmul(a, b, activation=\"\"):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n    )\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        ACTIVATION=activation,\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code implements a custom linear layer in Triton. It defines several activation functions, including tanh, ReLU, GELU, and fast GELU. The main operation is the kernel_fma function, a Triton kernel that computes a matrix multiplication with optional bias and activation. It handles matrices A, B, and C to compute C = activation(A x B + bias). The kernel is autotuned for performance. The LinearLayer class is a PyTorch function that uses kernel_fma to compute the linear transformation, optionally applying an activation function and saving activation inputs for backward pass. The linear_layer function is a wrapper around LinearLayer to perform these operations easily.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define activation functions\n@triton.jit\ndef tanh(x):\n    return tl.math.tanh(x)\n\n@triton.jit\ndef relu(x):\n    return tl.math.max(x, 0)\n\n@triton.jit\ndef gelu(x):\n    return 0.5 * x * (1 + tl.math.tanh(0.7978845608 * (x + 0.044715 * x * x * x)))\n\n@triton.jit\ndef fast_gelu(x):\n    return 0.5 * x * (1 + tl.math.sign(x) * tl.math.sqrt(1 + 0.2888 * x * x))\n\n# Define the kernel function\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef kernel_fma(\n    A, B, C, bias, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, act_type: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if bias is not None:\n        bias_ptr = bias + offs_bn\n        bias_vals = tl.load(bias_ptr)\n        accumulator += bias_vals\n\n    c_ptrs = C + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    if act_type == 0:\n        accumulator = tanh(accumulator)\n    elif act_type == 1:\n        accumulator = relu(accumulator)\n    elif act_type == 2:\n        accumulator = gelu(accumulator)\n    elif act_type == 3:\n        accumulator = fast_gelu(accumulator)\n\n    tl.store(c_ptrs, accumulator.to(C.dtype.element_ty))\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\n\nclass LinearLayerFunction(Function):\n    @staticmethod\n    def forward(ctx, A, B, bias, act_type):\n        M, K = A.shape\n        N = B.shape[1]\n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        kernel_fma[grid](A, B, C, bias, M, N, K, A.stride(0), A.stride(1), B.stride(0), B.stride(1), C.stride(0), C.stride(1), act_type)\n        ctx.save_for_backward(A, B, bias)\n        ctx.act_type = act_type\n        return C\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        A, B, bias = ctx.saved_tensors\n        act_type = ctx.act_type\n        M, K = A.shape\n        N = B.shape[1]\n\n        # Compute gradient with respect to C\n        grad_C = grad_output.clone()\n\n        # Apply activation function gradient\n        if act_type == 0:\n            grad_C *= 1 - grad_C * grad_C\n        elif act_type == 1:\n            grad_C *= (A @ B > 0).float()\n        elif act_type == 2:\n            x = A @ B\n            grad_C *= 0.5 * (1 + 0.7978845608 * (x + 0.044715 * x * x * x) * (1 - tl.math.tanh(0.7978845608 * (x + 0.044715 * x * x * x)) ** 2))\n        elif act_type == 3:\n            x = A @ B\n            grad_C *= 0.5 * (1 + 0.7978845608 * x * (1 + 0.1444 * x * x))\n\n        # Compute gradient with respect to A and B\n        grad_A = grad_C @ B.T\n        grad_B = A.T @ grad_C\n\n        # Compute gradient with respect to bias\n        if bias is not None:\n            grad_bias = grad_C.sum(dim=0)\n        else:\n            grad_bias = None\n\n        return grad_A, grad_B, grad_bias, None\n\nclass LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, act_type=0):\n        super(LinearLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features)) if bias else None\n        self.act_type = act_type\n\n    def forward(self, input):\n        return LinearLayerFunction.apply(input, self.weight, self.bias, self.act_type)\n\n# Wrapper function\ndef linear_layer(input, weight, bias=None, act_type=0):\n    return LinearLayerFunction.apply(input, weight, bias, act_type)\n", "label": "from typing import Optional\nimport math\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.autograd.function import FunctionCtx\nfrom torch.cuda.amp import custom_fwd\nfrom triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time\n\n\nsqrt2pi = math.sqrt(2.0 / math.pi)\nsqrt2 = tl.constexpr(math.sqrt(2.0))\n\n\n@triton.jit\ndef tanh(x):\n    \"\"\"Tanh activation function\"\"\"\n    return tl.extra.cuda.libdevice.tanh(x)\n\n\n@triton.jit\ndef relu(x):\n    \"\"\"Relu activation function\"\"\"\n    return tl.maximum(0, x)\n\n\n@triton.jit\ndef fast_gelu(x):\n    \"\"\"Fast approximation of the gelu function. May slightly decrease accuracy.\"\"\"\n    return 0.5 * x * (1 + tanh(sqrt2pi * (x + 0.044715 * x * x * x)))\n\n\n@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.extra.cuda.libdevice.erf(x / sqrt2))\n\n\ndef init_to_zero(name):\n    return lambda nargs: nargs[name].zero_()\n\n\ndef get_configs_io_bound():\n    configs = []\n    for num_stages in [2, 3, 4, 5, 6]:\n        for block_m in [16, 32]:\n            for block_k in [32, 64]:\n                for block_n in [32, 64, 128, 256]:\n                    num_warps = 2 if block_n <= 64 else 4\n                    configs.append(\n                        triton.Config(\n                            {\"BLOCK_M\": block_m, \"BLOCK_N\": block_n, \"BLOCK_K\": block_k, \"SPLIT_K\": 1},\n                            num_stages=num_stages,\n                            num_warps=num_warps,\n                        )\n                    )\n                    # split_k not used\n                    # for split_k in [2, 4, 8, 16]:\n                    #     configs.append(triton.Config(\n                    #         {'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n                    #         num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n    return configs\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n        # good for int8\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n    ]\n    + get_configs_io_bound(),\n    key=[\"CACHE_KEY_M\", \"CACHE_KEY_N\", \"CACHE_KEY_K\"],\n    prune_configs_by={\"early_config_prune\": early_config_prune, \"perf_model\": estimate_matmul_time, \"top_k\": 10},\n)\n@triton.heuristics(\n    {\n        \"K_LOAD_MASK_NEEDED\": lambda args: args[\"K\"] % (args[\"BLOCK_K\"] * args[\"SPLIT_K\"]) == 0,\n    }\n)\n@triton.jit\ndef kernel_fma(\n    C,  # Pointers to matrices\n    ACT_INPUTS,\n    A,\n    B,\n    bias,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    CACHE_KEY_M,\n    CACHE_KEY_N,\n    CACHE_KEY_K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\n    # by to get the element one row down (A has M rows)\n    output_m_stride,\n    output_n_stride,\n    act_inputs_m_stride,\n    act_inputs_n_stride,\n    a_m_stride,\n    a_k_stride,\n    b_n_stride,\n    b_k_stride,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    # split k not used, not performant with activation, kept because early_config_prune is expecting it\n    SPLIT_K: tl.constexpr,\n    K_LOAD_MASK_NEEDED: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    SHOULD_SAVE_ACT_INPUTS: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n\n    This kernel will consolidate over K\n    \"\"\"\n    program_idx = tl.program_id(axis=0)\n\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_idx = program_idx // width\n    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)\n    block_m_idx = group_idx * GROUP_M + (program_idx % group_size)\n    block_n_idx = (program_idx % width) // group_size\n\n    # now compute the block that each program will go through\n    # m_offs (resp. n_offs) denotes a range of indices\n    # for rows (resp. col) of C\n    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    # trick to avoid masking on M and N axis\n    # m_offs_untagged and n_offs_untagged can contains addresses outside matrix boundaries\n    # modulo operation is used to wrap around the indices that go beyond the matrix boundaries\n    # The value loaded are not ok but at least we are not reading outside the A/B matrices\n    # Then, during storing in C a mask is used and the results related to these wrong values is discarded!\n    # Regarding max_contiguous and multiple_of, they are used to force the compiler to vectorize loads\n    # multiple_of indicates that the first element of rm / rn is a multiple of BLOCK_M / BLOCK_N\n    # max_contiguous indicates that the range is a block of BLOCK_M / BLOCK_N contiguous elements\n    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M), BLOCK_M)\n    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N), BLOCK_N)\n\n    k_range_offs = tl.arange(0, BLOCK_K)\n\n    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)\n    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    if HAS_BIAS:\n        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    for k in range(K, 0, -BLOCK_K):\n        if K_LOAD_MASK_NEEDED:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)\n            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n\n        A += BLOCK_K * a_k_stride\n        B += BLOCK_K * b_k_stride\n\n    # optional: save the activation inputs\n    if SHOULD_SAVE_ACT_INPUTS:\n        act_in_ptrs = ACT_INPUTS + m_offs[:, None] * act_inputs_m_stride + n_offs[None, :] * act_inputs_n_stride\n        tl.store(act_in_ptrs, acc)\n\n    # optional: fused activation (while the data is in shared memory)\n    if ACTIVATION == \"tanh\":\n        acc = tanh(acc)\n    if ACTIVATION == \"gelu\":\n        acc = gelu(acc)\n    if ACTIVATION == \"fast_gelu\":\n        acc = fast_gelu(acc)\n    if ACTIVATION == \"relu\":\n        acc = relu(acc)\n\n    # write back result\n    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :] * output_n_stride\n    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]\n    tl.store(C, acc, mask=c_ptr_mask)\n\n\nclass LinearLayer(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float16)\n    def forward(\n        ctx: FunctionCtx,\n        x: torch.Tensor,\n        weight: torch.Tensor,\n        bias: Optional[torch.Tensor],\n        activation: str,\n        act_inputs: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute e = activation(x @ weight + bias).\n        This wrapper kicks the `kernel_fma` Triton kernel\n        :param ctx: context for autograd\n        :param x: input tensor\n        :param weight: weight matrix\n        :param bias: an optional bias tensor\n        :param activation: Activation name. Needs to be a Triton kernel.\n        :param act_inputs: an optional tensor to save the activation inputs (for backward)\n        :return: result tensor\n        \"\"\"\n        x_ = x if x.ndim == 2 else x.flatten(0, 1)\n\n        assert x.dtype == weight.dtype, f\"Input and weight must have the same dtype, got {x.dtype} and {weight.dtype}\"\n        if bias is not None:\n            assert x.dtype == bias.dtype, f\"Input and bias must have the same dtype, got {x.dtype} and {bias.dtype}\"\n        assert x_.shape[1] == weight.shape[1], f\"Incompatible dimensions: {x_.shape} - {weight.shape}\"\n\n        assert bias is None or bias.is_contiguous()\n        assert bias is None or bias.shape[0] == weight.shape[0], \"Incompatible dimensions in between weight and bias\"\n        assert weight.is_contiguous()\n\n        M, K = x_.shape\n        N, K = weight.shape\n\n        outputs = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n        # 1D launch kernel where each block gets its own program.\n        grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),)  # noqa\n\n        kernel_fma[grid](\n            outputs,\n            act_inputs,\n            x_,\n            weight,  # data ptrs\n            bias if bias is not None else x,  # auto skip bias if not present\n            M,  # shapes\n            N,\n            K,\n            M // 32,  # key for triton cache (limit number of compilations)\n            N // 32,\n            K // 32,\n            output_m_stride=outputs.stride(0),  # strides\n            output_n_stride=outputs.stride(1),\n            act_inputs_m_stride=act_inputs.stride(0) if act_inputs is not None else 0,\n            act_inputs_n_stride=act_inputs.stride(1) if act_inputs is not None else 0,\n            a_m_stride=x_.stride(0),\n            a_k_stride=x_.stride(1),\n            b_n_stride=weight.stride(0),\n            b_k_stride=weight.stride(1),\n            HAS_BIAS=bias is not None,  # optional fused bias\n            SHOULD_SAVE_ACT_INPUTS=act_inputs is not None,  # optional save activation inputs\n            ACTIVATION=activation if not None else x,  # optional fused activation\n            GROUP_M=8,  # speed optimization: group the programs\n        )\n\n        outputs = outputs if x.ndim == 2 else outputs.reshape(x.shape[0], -1, N)\n        ctx.save_for_backward(weight, bias, x)\n        return outputs\n\n\ndef linear_layer(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    bias: Optional[torch.Tensor],\n    activation=\"\",\n    act_inputs: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    return LinearLayer.apply(x, weight, bias, activation, act_inputs)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code implements a softmax function optimized for GPU execution. The main components are a kernel function named `softmax_kernel` and a wrapper function `softmax`. \n            - `softmax_kernel`: This function is a Triton JIT-compiled kernel that computes the softmax across rows of an input tensor. It takes pointers to the output and input tensors, along with other parameters such as row stride and block size. The function calculates the exponentials of the input after subtracting the maximum value for numerical stability, applies an optional mask, and normalizes the result to produce the softmax output.\n            - `softmax`: This Python function sets up and launches the Triton kernel. It handles reshaping and validation of the input tensor and optional mask tensor. Depending on the input size, it configures the grid and launches the kernel with appropriate parameters. The function returns the output tensor with computed softmax values.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr,  # Pointer to the output tensor\n    input_ptr,   # Pointer to the input tensor\n    mask_ptr,    # Pointer to the mask tensor (optional)\n    n_cols,      # Number of columns in the input tensor\n    stride,      # Stride between rows in the input tensor\n    block_size: tl.constexpr,  # Block size for parallelization\n):\n    # Compute the row index for the current program instance\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, block_size)\n    input_offsets = row_idx * stride + col_offsets\n\n    # Load the input row\n    input_row = tl.load(input_ptr + input_offsets, mask=col_offsets < n_cols, other=-float('inf'))\n\n    # Compute the maximum value in the row for numerical stability\n    max_val = tl.max(input_row, axis=0)\n\n    # Subtract the maximum value from the input row\n    input_row = input_row - max_val\n\n    # Apply the mask if provided\n    if mask_ptr is not None:\n        mask_row = tl.load(mask_ptr + input_offsets, mask=col_offsets < n_cols, other=0)\n        input_row = input_row * mask_row\n\n    # Compute the exponentials\n    exp_row = tl.exp(input_row)\n\n    # Compute the sum of the exponentials\n    sum_exp = tl.sum(exp_row, axis=0)\n\n    # Normalize the exponentials to get the softmax values\n    softmax_row = exp_row / sum_exp\n\n    # Store the result in the output tensor\n    tl.store(output_ptr + input_offsets, softmax_row, mask=col_offsets < n_cols)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef softmax(input_tensor, mask_tensor=None):\n    # Validate input tensor\n    if input_tensor.dim() != 2:\n        raise ValueError(\"Input tensor must be 2D (rows x cols)\")\n    \n    # Validate mask tensor if provided\n    if mask_tensor is not None:\n        if mask_tensor.dim() != 2 or mask_tensor.shape != input_tensor.shape:\n            raise ValueError(\"Mask tensor must be 2D and have the same shape as the input tensor\")\n    \n    # Get the shape of the input tensor\n    n_rows, n_cols = input_tensor.shape\n\n    # Allocate the output tensor\n    output_tensor = torch.empty_like(input_tensor, device=input_tensor.device)\n\n    # Define the block size\n    block_size = 128\n\n    # Configure the grid\n    grid = (n_rows,)\n\n    # Launch the kernel\n    softmax_kernel[grid](\n        output_tensor,  # Pointer to the output tensor\n        input_tensor,   # Pointer to the input tensor\n        mask_tensor,    # Pointer to the mask tensor (optional)\n        n_cols,         # Number of columns in the input tensor\n        n_cols,         # Stride between rows in the input tensor\n        block_size,     # Block size for parallelization\n    )\n\n    return output_tensor\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, row_stride, n_cols, mask_ptr, BLOCK_SIZE: tl.constexpr):\n    r\"\"\"the kernel function for implementing softmax operator\n    Args:\n        output_ptr: the output after finishing softmax operation, (N, hidden_dim)\n        input_ptr: the tensor of input, shape should be (N, hidden_dim)\n        n_cols(tl.constexpr): the number of cols of input\n        BLOCK_SIZE(tl.constexpr): the block_size of your hidden_dim dimension, typically BLOCK_SIZE >= hidden_dim\n    \"\"\"\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float(\"inf\")).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n\n    if mask_ptr is not None:\n        # load mask into SRAM\n        mask_ptrs = (mask_ptr + (row_idx * row_stride)) + col_offsets\n        mask = tl.load(mask_ptrs, mask=col_offsets < n_cols, other=0).to(tl.float32)\n\n        # update\n        row_minus_max = row_minus_max + mask\n\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    # Write back output to DRAM\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\ndef softmax(input: torch.Tensor, mask: torch.Tensor = None, dim=-1) -> torch.Tensor:\n    if mask is not None:\n        # assert input[-1] == mask[-1], \"the last dimentions should be the same for input and mask\"\n        assert input.shape[-1] == mask.shape[-1], \"the last dimensions should be the same for input and mask\"\n    assert dim == -1 or dim == len(input.shape) - 1, \"currently softmax layer only support last dimention\"\n\n    hidden_dim = input.shape[-1]\n    output = torch.empty_like(input)\n    input = input.view(-1, hidden_dim)\n    if mask is not None:\n        mask = mask.view(-1, hidden_dim)\n        assert input.shape[0] == mask.shape[0], \"the fist dimention of mask and input should be the same\"\n\n    num_rows, num_cols = input.shape\n    block_size = max(triton.next_power_of_2(num_cols), 2)\n    num_warps = 16\n    if block_size >= 4096:\n        num_warps = 16\n    elif block_size >= 2048:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    if num_rows <= 350000:\n        grid = (num_rows,)\n        softmax_kernel[grid](\n            output, input, input.stride(0), num_cols, mask, BLOCK_SIZE=block_size, num_warps=num_warps\n        )\n    else:\n        grid = lambda meta: (triton.cdiv(num_rows, meta[\"BLOCK_M\"]),)\n\n        softmax_kernel[grid](\n            output_ptr=output,\n            input_ptr=input,\n            row_stride=input.stride(0),\n            n_rows=num_rows,\n            n_cols=num_cols,\n            mask_ptr=mask,\n            # currently manually setting up size\n            BLOCK_M=32,\n            BLOCK_SIZE=block_size,\n        )\n\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator is designed to implement an attention mechanism commonly used in transformer architectures. The primary function, `attention_fwd_kernel`, takes in query (q), key (k), and value (v) tensors and performs scaled dot-product attention. The input tensors are batched and represent different heads of attention. The kernel computes attention scores using the formula: scores = softmax(q * k^T), and then it applies these scores to the value tensor to produce the output.\n\n            The function `attention_fwd_kernel` is decorated with `@triton.jit` to enable just-in-time compilation. It processes data in blocks, leveraging Triton's capabilities to efficiently handle tensor computations. The kernel utilizes the variables BT (block size for the sequence dimension), BD (block size for the head dimension), and NT (number of blocks), ensuring that tensor operations are performed in parallel.\n\n            The kernel has several key operations: loading the blocks of the q, k, and v tensors, computing the dot products to get attention scores, applying these scores to get the output values, and optionally updating an intermediate tensor `h` that can be stored or used conditionally.\n\n            The `AttentionFunction` class acts as a wrapper around the kernel and provides the `forward` method for executing the kernel on input tensors. This class also sets up necessary parameters and configurations for the kernel, like the grid dimensions for parallel execution and scaling factors for the attention mechanism.\n\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef attention_fwd_kernel(\n    q_ptr, k_ptr, v_ptr, out_ptr, h_ptr,  # Pointers to input and output tensors\n    q_stride_b, q_stride_h, q_stride_s, q_stride_d,  # Strides for query tensor\n    k_stride_b, k_stride_h, k_stride_s, k_stride_d,  # Strides for key tensor\n    v_stride_b, v_stride_h, v_stride_s, v_stride_d,  # Strides for value tensor\n    out_stride_b, out_stride_h, out_stride_s, out_stride_d,  # Strides for output tensor\n    h_stride_b, h_stride_h, h_stride_s, h_stride_d,  # Strides for intermediate tensor h\n    B, H, S, D,  # Batch size, number of heads, sequence length, head dimension\n    BT, BD, NT,  # Block sizes for sequence, head, and number of blocks\n    scale,  # Scaling factor for attention scores\n    BLOCK_SIZE: tl.constexpr\n):\n    # Get the block indices\n    pid = tl.program_id(axis=0)\n    bid = pid // (NT * BD)\n    hid = (pid % (NT * BD)) // NT\n    tid = (pid % (NT * BD)) % NT\n\n    # Compute the base indices for the current block\n    q_base = bid * q_stride_b + hid * q_stride_h + tid * q_stride_s\n    k_base = bid * k_stride_b + hid * k_stride_h + tid * k_stride_s\n    v_base = bid * v_stride_b + hid * v_stride_h + tid * v_stride_s\n    out_base = bid * out_stride_b + hid * out_stride_h + tid * out_stride_s\n    h_base = bid * h_stride_b + hid * h_stride_h + tid * h_stride_s\n\n    # Initialize the attention scores and output values\n    scores = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    out = tl.zeros((BLOCK_SIZE, D), dtype=tl.float32)\n\n    # Load the query, key, and value blocks\n    q = tl.load(q_ptr + q_base * q_stride_d + tl.arange(0, D))\n    k = tl.load(k_ptr + k_base * k_stride_d + tl.arange(0, D))\n    v = tl.load(v_ptr + v_base * v_stride_d + tl.arange(0, D))\n\n    # Compute the attention scores\n    for i in range(BLOCK_SIZE):\n        for j in range(BLOCK_SIZE):\n            scores[i, j] = tl.sum(q[i] * k[j], axis=0) * scale\n\n    # Apply the softmax function to the attention scores\n    scores_max = tl.max(scores, axis=1)\n    scores_exp = tl.exp(scores - scores_max[:, None])\n    scores_sum = tl.sum(scores_exp, axis=1)\n    scores_softmax = scores_exp / scores_sum[:, None]\n\n    # Compute the output values\n    for i in range(BLOCK_SIZE):\n        out[i] = tl.sum(scores_softmax[i, :, None] * v, axis=0)\n\n    # Store the output values\n    tl.store(out_ptr + out_base * out_stride_d + tl.arange(0, D), out)\n\n    # Optionally store the intermediate tensor h\n    if h_ptr is not None:\n        tl.store(h_ptr + h_base * h_stride_d + tl.arange(0, D), scores_softmax)\n\nimport torch\nfrom torch.autograd import Function\n\nclass AttentionFunction(Function):\n    @staticmethod\n    def forward(ctx, q, k, v, h=None, scale=1.0, BT=16, BD=16, NT=16):\n        B, H, S, D = q.shape\n\n        # Allocate output tensor\n        out = torch.empty_like(q)\n\n        # Allocate intermediate tensor h if provided\n        if h is not None:\n            h = torch.empty((B, H, S, S), device=q.device, dtype=torch.float32)\n\n        # Define the grid and block dimensions\n        grid = (B * H * S // (BT * BD * NT),)\n\n        # Launch the kernel\n        attention_fwd_kernel[grid](\n            q, k, v, out, h,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            out.stride(0), out.stride(1), out.stride(2), out.stride(3),\n            h.stride(0) if h is not None else 0, h.stride(1) if h is not None else 0, h.stride(2) if h is not None else 0, h.stride(3) if h is not None else 0,\n            B, H, S, D,\n            BT, BD, NT,\n            scale,\n            BT\n        )\n\n        # Save the context for backward pass if needed\n        ctx.save_for_backward(q, k, v, out, h)\n        ctx.B, ctx.H, ctx.S, ctx.D = B, H, S, D\n        ctx.BT, ctx.BD, ctx.NT = BT, BD, NT\n        ctx.scale = scale\n\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # Implement the backward pass if needed\n        q, k, v, out, h = ctx.saved_tensors\n        B, H, S, D = ctx.B, ctx.H, ctx.S, ctx.D\n        BT, BD, NT = ctx.BT, ctx.BD, ctx.NT\n        scale = ctx.scale\n\n        # Allocate gradients\n        grad_q = torch.zeros_like(q)\n        grad_k = torch.zeros_like(k)\n        grad_v = torch.zeros_like(v)\n\n        # Define the grid and block dimensions\n        grid = (B * H * S // (BT * BD * NT),)\n\n        # Launch the backward kernel (not implemented here)\n        # attention_bwd_kernel[grid](\n        #     grad_out, q, k, v, out, h, grad_q, grad_k, grad_v,\n        #     q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        #     k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        #     v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        #     out.stride(0), out.stride(1), out.stride(2), out.stride(3),\n        #     h.stride(0) if h is not None else 0, h.stride(1) if h is not None else 0, h.stride(2) if h is not None else 0, h.stride(3) if h is not None else 0,\n        #     grad_q.stride(0), grad_q.stride(1), grad_q.stride(2), grad_q.stride(3),\n        #     grad_k.stride(0), grad_k.stride(1), grad_k.stride(2), grad_k.stride(3),\n        #     grad_v.stride(0), grad_v.stride(1), grad_v.stride(2), grad_v.stride(3),\n        #     B, H, S, D,\n        #     BT, BD, NT,\n        #     scale,\n        #     BT\n        # )\n\n        return grad_q, grad_k, grad_v, None, None, None, None\n\nimport torch\n\n# Example input tensors\nB, H, S, D = 2, 8, 128, 64\nq = torch.randn(B, H, S, D, device='cuda')\nk = torch.randn(B, H, S, D, device='cuda')\nv = torch.randn(B, H, S, D, device='cuda')\n\n# Optional intermediate tensor h\nh = torch.empty((B, H, S, S), device='cuda', dtype=torch.float32)\n\n# Forward pass\nout = AttentionFunction.apply(q, k, v, h, scale=1.0 / (D ** 0.5))\n\n# Print the output\nprint(out)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef attention_fwd_kernel(\n    q,\n    k,\n    v,\n    h,\n    o,\n    s_qh,\n    s_qt,\n    s_qd,\n    s_hh,\n    s_ht,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NT: tl.constexpr,\n    STORE: tl.constexpr,\n    IFCOND: tl.constexpr\n):\n    i_bh = tl.program_id(0)\n\n    # [BD, BD]\n    b_h = tl.zeros([BD, BD], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i * BT, 0), (BT, BD), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qh, (BD, T), (s_qd, s_qt), (0, i * BT), (BD, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i * BT, 0), (BT, BD), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (NT * BD, BD), (s_ht, s_qd), (i * BD, 0), (BD, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i * BT, 0), (BT, BD), (1, 0))\n\n        if STORE:\n            tl.store(p_h, b_h.to(p_h.dtype.element_ty))\n        # [BT, BD]\n        b_q = tl.load(p_q)\n        b_q = (b_q * scale).to(b_q.dtype)\n        # [BD, BT]\n        b_k = tl.load(p_k)\n        # [BT, BD]\n        b_v = tl.load(p_v)\n\n        # [BT, BT]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        # [BT, BD]\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if IFCOND:\n            if i == 0:\n                b_h = tl.dot(b_k, b_v, allow_tf32=False)\n            else:\n                b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n                b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty))\n\n\nclass AttentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, store=False, ifcond=False):\n        batch_size, n_heads, seq_len, d_head = q.shape\n        scale = d_head ** -0.5\n        BD = q.shape[-1]\n        BT = 32\n        NT = triton.cdiv(seq_len, BT)\n        num_stages = 3 if d_head <= 64 else 2\n        num_warps = 4\n\n        h = q.new_empty(batch_size, n_heads, NT * BD, BD)\n        o = torch.empty_like(q)\n        grid = (batch_size * n_heads,)\n        attention_fwd_kernel[grid](\n            q, k, v, h, o,\n            q.stride(1), q.stride(2), q.stride(3), h.stride(1), h.stride(2),\n            seq_len, scale,\n            BT=BT, BD=BD, NT=NT, STORE=store, IFCOND=ifcond,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        return o\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implements a forward attention mechanism. The main function, `_attn_fwd`, takes query (Q), key (K), value (V) matrices and scaling factors as inputs, and computes attention scores stored in the `Out` tensor. The function uses a block-wise approach to process input tensors in chunks of size `BLOCK_M` and `BLOCK_N`. The kernel uses the Triton JIT decorator for compiling and executing on GPUs.\n\n            The `_attn_fwd_inner` function is a helper that performs the main attention computation in the loop. It calculates the scaled dot-product of the query and key matrices, applies the exponential function, and updates the accumulated attention scores `acc` and normalization factor `l_i`. The loop iterates over the context size `N_CTX` in increments of `BLOCK_N`.\n\n            Key inputs:\n            - `Q`, `K`, `V`: Query, key, and value matrices.\n            - `Q_scale`, `K_scale`: Scaling factors for Q and K.\n            - `Out`: The output tensor for storing attention scores.\n\n            Outputs:\n            - Updated `Out` tensor containing attention scores.\n\n            Key function:\n            - `_attn_fwd_inner`: Handles the block-wise attention score computation.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for forward attention mechanism\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out, stride_qm, stride_ql, stride_km, stride_kl, stride_vm, stride_vl, stride_om, stride_ol, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Program ID\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(N_CTX, BLOCK_M)\n    num_pid_n = tl.cdiv(N_CTX, BLOCK_N)\n    num_pid_in_block = num_pid_m * num_pid_n\n    block_id = pid // num_pid_in_block\n    pid_m = (pid % num_pid_in_block) // num_pid_n\n    pid_n = (pid % num_pid_in_block) % num_pid_n\n\n    # Block bounds\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    # Offsets for Q, K, V, and Out\n    q_ptrs = Q + (block_id * stride_qm + offs_m[:, None]) * stride_ql\n    k_ptrs = K + (block_id * stride_km + offs_n[None, :]) * stride_kl\n    v_ptrs = V + (block_id * stride_vm + offs_n[None, :]) * stride_vl\n    out_ptrs = Out + (block_id * stride_om + offs_m[:, None]) * stride_ol\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    l_i = tl.zeros((BLOCK_M, 1), dtype=tl.float32)\n\n    # Loop over the context size\n    for start_n in range(0, N_CTX, BLOCK_N):\n        # Load Q, K, V blocks\n        q = tl.load(q_ptrs)\n        k = tl.load(k_ptrs + start_n * stride_kl)\n        v = tl.load(v_ptrs + start_n * stride_vl)\n\n        # Compute attention scores\n        qk = tl.dot(q, k, allow_tf32=True) * Q_scale * K_scale\n        qk = tl.exp(qk - l_i)\n\n        # Update accumulators\n        acc += qk\n        l_i += tl.sum(qk, axis=1, keepdim=True)\n\n    # Normalize and store the result\n    out = acc / l_i\n    tl.store(out_ptrs, out.to(Out.dtype.element_ty))\n\n# Helper function to call the kernel\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out, BLOCK_M, BLOCK_N):\n    # Get tensor shapes\n    M, N_CTX = Q.shape\n    _, N_CTX = K.shape\n    _, N_CTX = V.shape\n    _, N_CTX = Out.shape\n\n    # Get strides\n    stride_qm, stride_ql = Q.stride(0), Q.stride(1)\n    stride_km, stride_kl = K.stride(0), K.stride(1)\n    stride_vm, stride_vl = V.stride(0), V.stride(1)\n    stride_om, stride_ol = Out.stride(0), Out.stride(1)\n\n    # Launch the kernel\n    grid = (triton.cdiv(N_CTX, BLOCK_M) * triton.cdiv(N_CTX, BLOCK_N) * M, 1, 1)\n    _attn_fwd[grid](Q, K, V, Q_scale, K_scale, Out, stride_qm, stride_ql, stride_km, stride_kl, stride_vm, stride_vl, stride_om, stride_ol, N_CTX, BLOCK_M, BLOCK_N)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask = k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask = (offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        \n        acc += tl.dot(p, v, out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr  \n              ):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N) \n    \n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX \n                                    )\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.float16)\n    stage = 1\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=4)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a matrix multiplication kernel using Triton, a Python library for writing custom GPU kernels. \n        The kernel `matmul_kernel` is decorated with `@triton.autotune`, which enables auto-tuning of configurations for optimal performance based on input sizes M, N, and K.\n        The kernel function computes the product of matrices A and B and stores the result in matrix C. \n        It uses a block-wise approach where each block computes a partial result of C. \n        Key variables: \n        - BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K: Dimensions of the blocks used in matrix multiplication.\n        - GROUP_SIZE_M: Number of blocks processed together for load balancing. \n        A wrapper function `triton_matmul` is defined to facilitate calling the kernel. It takes matrices A and B as inputs, asserts compatible dimensions, sets up an output matrix C, calculates the grid size based on input dimensions, and launches the kernel.\n        ", "predict": "import triton\nimport triton.language as tl\n\n# Define the matrix multiplication kernel\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul_kernel(\n    A, B, C,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    M, N, K,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n        b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C + (offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n# Define the wrapper function\ndef triton_matmul(A, B):\n    assert A.shape[1] == B.shape[0], \"Incompatible dimensions for matrix multiplication\"\n    M, K = A.shape\n    N = B.shape[1]\n    C = triton.empty((M, N), dtype=triton.float16, device=A.device)\n    \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n    \n    matmul_kernel[grid](\n        A, B, C,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        M, N, K\n    )\n    \n    return C\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named '_copy_to_kcache_seqlen_n_kernel' and a Python function 'copy_k_to_blocked_cache' that manage the process of copying keys or values (K or V) into a key-value cache during the decoding stage of a sequence model. The kernel operates on data structures with specific layouts that differ based on a boolean parameter. The main task of the Triton kernel is to transfer tokens from an input tensor 'K' to a pre-allocated cache 'KCache' based on sequence lengths and a block table structure. Each sequence can contain multiple tokens, defined by 'n_tokens'. The kernel computes the correct position in the cache to store the token and handles the data transfer using Triton\u2019s load and store operations. The function 'copy_k_to_blocked_cache' prepares the necessary parameters and calls this kernel. It ensures that input dimensions are compatible and selects an appropriate number of execution warps based on the dimension size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _copy_to_kcache_seqlen_n_kernel(\n    K_ptr,  # Pointer to the input tensor K\n    KCache_ptr,  # Pointer to the pre-allocated cache\n    block_table_ptr,  # Pointer to the block table\n    seq_lengths_ptr,  # Pointer to the sequence lengths\n    n_tokens,  # Number of tokens per sequence\n    n_heads,  # Number of attention heads\n    head_dim,  # Dimension of each head\n    cache_block_size,  # Size of each cache block\n    seq_len,  # Length of the sequence\n    BLOCK_M: tl.constexpr,  # Block size for the sequence length\n    BLOCK_D: tl.constexpr,  # Block size for the head dimension\n):\n    # Get the current block ID\n    pid = tl.program_id(axis=0)\n    \n    # Compute the starting position in the sequence\n    start_seq = pid * BLOCK_M\n    end_seq = tl.minimum(start_seq + BLOCK_M, seq_len)\n    \n    # Iterate over the sequences\n    for seq in range(start_seq, end_seq):\n        # Load the sequence length\n        seq_length = tl.load(seq_lengths_ptr + seq)\n        \n        # Compute the block ID in the block table\n        block_id = seq // n_tokens\n        \n        # Compute the starting position in the cache\n        cache_start = tl.load(block_table_ptr + block_id) * cache_block_size\n        \n        # Iterate over the tokens in the sequence\n        for token in range(seq % n_tokens):\n            # Compute the position in the input tensor K\n            k_pos = (seq * n_tokens + token) * n_heads * head_dim\n            \n            # Compute the position in the cache\n            cache_pos = (cache_start + seq % n_tokens + token) * n_heads * head_dim\n            \n            # Load the data from K\n            k_data = tl.load(K_ptr + k_pos, mask=token < seq_length, other=0.0)\n            \n            # Store the data in the cache\n            tl.store(KCache_ptr + cache_pos, k_data, mask=token < seq_length)\n\nimport torch\n\ndef copy_k_to_blocked_cache(K, KCache, block_table, seq_lengths, n_tokens, n_heads, head_dim, cache_block_size, seq_len, BLOCK_M=128, BLOCK_D=64):\n    # Check input dimensions\n    assert K.shape == (seq_len, n_tokens, n_heads, head_dim), \"Input tensor K has incorrect shape\"\n    assert KCache.shape == (seq_len, cache_block_size, n_heads, head_dim), \"Cache tensor KCache has incorrect shape\"\n    assert block_table.shape == (seq_len // n_tokens,), \"Block table has incorrect shape\"\n    assert seq_lengths.shape == (seq_len,), \"Sequence lengths have incorrect shape\"\n    \n    # Determine the number of warps\n    num_warps = 4 if head_dim <= 2048 else 8\n    \n    # Launch the kernel\n    grid = (seq_len // BLOCK_M + (seq_len % BLOCK_M > 0),)\n    _copy_to_kcache_seqlen_n_kernel[grid](\n        K, KCache, block_table, seq_lengths, n_tokens, n_heads, head_dim, cache_block_size, seq_len, BLOCK_M, BLOCK_D,\n        num_warps=num_warps\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n# supports two types of cache layouts\n# 1. [num_blocks, num_kv_heads, block_size, head_dim]\n# 2. [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n@triton.jit\ndef _copy_to_kcache_seqlen_n_kernel(\n    K,  # K or V\n    KCache,  # [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n    BLOCK_TABLES,\n    seq_lengths,\n    stride_kt,\n    stride_kh,\n    stride_kd,\n    stride_kcb,\n    stride_kch,\n    stride_kcsplit_x,\n    stride_kcs,\n    stride_kcx,\n    stride_bts,\n    stride_btb,\n    block_size,\n    n_tokens,\n    HEAD_DIM: tl.constexpr,\n    KCACHE_X: tl.constexpr,\n):\n    # `n_tokens` is used to specify the number of tokens to copy for each sequence\n    # When n_tokens > 1, tokens from different sequences are packed into the first dimension of the grid,\n    #   `seq_lengths` must be the lengths of sequences counting the number of tokens to copy\n    #   E.g. if n_tokens = 5, seq_lengths = [12, 15], then the already-copied position ids are [0-6, 0-9]\n    #   for the two sequences, respectively. And the position ids to be copied are [7-11, 9-14].\n    # When n_tokens = 1, consider token idx as the sequence idx, since it's only used during regular decoding stage\n    cur_token_idx = tl.program_id(0)\n    cur_seq_idx = cur_token_idx // n_tokens\n    # `cur_token_shift` is only valid and functional when `n_tokens` > 1\n    cur_token_shift = cur_token_idx - (n_tokens * (cur_seq_idx + 1))\n    cur_kv_head_idx = tl.program_id(1)\n    split_x_idx = tl.program_id(2)\n\n    past_kv_seq_len = tl.load(seq_lengths + cur_seq_idx) + cur_token_shift\n    last_bt_block_idx = past_kv_seq_len // block_size\n    block_table_ptr = BLOCK_TABLES + cur_seq_idx * stride_bts\n    block_id = tl.load(block_table_ptr + last_bt_block_idx * stride_btb)\n    offset_last_block = past_kv_seq_len % block_size\n    offsets_dmodel = split_x_idx * KCACHE_X + tl.arange(0, KCACHE_X)\n    offsets_k = cur_token_idx * stride_kt + cur_kv_head_idx * stride_kh + offsets_dmodel * stride_kd\n    k = tl.load(K + offsets_k)\n    offsets_kcache = (\n        block_id * stride_kcb\n        + cur_kv_head_idx * stride_kch\n        + split_x_idx * stride_kcsplit_x\n        + offset_last_block * stride_kcs\n        + tl.arange(0, KCACHE_X)\n    )\n    tl.store(KCache + offsets_kcache, k)\n    return\n\n\ndef copy_k_to_blocked_cache(\n    k: torch.Tensor,\n    k_cache: torch.Tensor,\n    kv_lengths: torch.Tensor,\n    block_tables: torch.Tensor,\n    n: int = 1,\n    use_new_kcache_layout: bool = False,\n):\n    \"\"\"\n    Copy keys or values to the blocked key/value cache during decoding stage.\n\n    Args:\n        k (torch.Tensor): [bsz, 1, num_kv_heads, head_dim]/[bsz, num_kv_heads, head_dim] - Keys or values during decoding with seq len 1.\n            [bsz * n, num_kv_heads, head_dim] - Keys or values with seq len n\n        k_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim] - Blocked key or value cache.\n            new KCache Layout [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n        kv_lengths (torch.Tensor): [bsz] - Past key/value sequence lengths plus current sequence length for each sequence.\n        block_tables (torch.Tensor): [bsz, max_blocks_per_sequence] - Block tables for each sequence.\n        n (int): Number of tokens to copy for each sequence. Default to 1.\n        use_new_kcache_layout (bool): Whether to use the new layout for kcache. Default to False.\n    \"\"\"\n    assert k.dtype == k_cache.dtype, \"Expected consistent dtype for tensor and cache.\"\n    if k.dim() == 4:\n        k = k.reshape(-1, k.size(-2), k.size(-1))\n    k_shape = k.shape\n    bsz, num_kv_heads, head_dim = k_shape\n    # NOTE when n > 1, the shape of k is [bsz * n, num_kv_heads, head_dim]\n    if n > 1:\n        assert bsz % n == 0, \"Each sequence should have the same number of tokens to be copied\"\n        bsz = bsz // n\n\n    assert kv_lengths.shape[0] == block_tables.shape[0] == bsz, (\n        f\"Got incompatible batch size (number of seqs):\\n\"\n        f\"  Past kv sequence lengths bsz {kv_lengths.shape[0]}; \"\n        f\" block tables bsz {block_tables.shape[0]}, input k batch size {bsz}\"\n    )\n\n    k_cache_shape = k_cache.shape\n    # Modify if the shape of kv cahce is changed.\n    block_size = k_cache_shape[-2]\n\n    x = head_dim\n    stride_kcsplit_x, stride_kcs, stride_kcd = 0, k_cache.stride(2), k_cache.stride(3)\n    if use_new_kcache_layout:\n        # when using kcache layout [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n        assert (\n            len(k_cache_shape) == 5\n            and k_cache_shape[1] == k_shape[1]\n            and k_cache_shape[2] * k_cache_shape[4] == k_shape[2]\n        ), f\"Incompatible k_cache shape {k_cache_shape} with k shape {k_shape}\"\n        x = k_cache.size(-1)\n        stride_kcsplit_x, stride_kcs, stride_kcd = k_cache.stride()[2:]\n\n    num_warps = 8 if head_dim > 128 else 4\n    grid = (bsz * n, num_kv_heads, head_dim // x)\n    _copy_to_kcache_seqlen_n_kernel[grid](\n        k,\n        k_cache,\n        block_tables,\n        kv_lengths,\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        k_cache.stride(0),\n        k_cache.stride(1),\n        stride_kcsplit_x,\n        stride_kcs,\n        stride_kcd,\n        block_tables.stride(0),\n        block_tables.stride(1),\n        block_size,\n        n_tokens=n,\n        HEAD_DIM=head_dim,\n        KCACHE_X=x,\n        num_warps=num_warps,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `cos_func` computes the cosine of each element in the input tensor `a` and stores the result in tensor `b`. \n            The kernel processes the tensor in blocks, using `BLOCK_SIZE` to determine the number of elements each block processes. \n            `offset` calculates the starting point of each block, and `mask` ensures computations are only performed on valid elements. \n            The `cos` operation is performed using `tl.cos` on the input values loaded from `a`, and results are stored back in `b`.\n            The `cos` function is a wrapper that prepares the output tensor `B`, calculates the required block and grid sizes, and launches the kernel to process the input tensor `A`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef cos_func(a_ptr, b_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the starting index of the block\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Compute the offsets for the current block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    # Load the input elements\n    a = tl.load(a_ptr + offsets, mask=mask)\n\n    # Compute the cosine of the input elements\n    b = tl.cos(a)\n\n    # Store the results back to the output tensor\n    tl.store(b_ptr + offsets, b, mask=mask)\n\nimport torch\n\ndef cos(A: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is on the GPU\n    assert A.is_cuda, \"Input tensor must be on the GPU\"\n\n    # Create the output tensor with the same shape and type as the input tensor\n    B = torch.empty_like(A)\n\n    # Get the number of elements in the tensor\n    n_elements = A.numel()\n\n    # Define the block size\n    BLOCK_SIZE = 256\n\n    # Calculate the number of blocks needed\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    cos_func[grid_size, BLOCK_SIZE](A, B, n_elements, BLOCK_SIZE)\n\n    return B\n", "label": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n# Kernel function: Computes the cosine of each element in the input tensor.\n@triton.jit\ndef cos_func(a, b, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Calculate the offset for each block and thread\n    offset = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    # Determine which elements are valid within the block\n    mask = offset < n_elements\n    # Load elements from tensor `a` into `a_value`\n    a_value = tl.load(a + offset, mask=mask)\n    # Compute the cosine of each element in `a_value`\n    b_value = tl.cos(a_value.to(tl.float32))\n    # Store the result back to tensor `b`\n    tl.store(b + offset, b_value, mask=mask)  \n\n# Function to invoke the Triton kernel and perform the computation\ndef cos(A):\n    # Prepare output tensor `B` with the same shape and type as `A`\n    B = torch.empty_like(A)\n    # Determine the total number of elements in the input tensor `A`\n    n_elements = A.numel()\n    # Calculate the optimal block size\n    block_size = triton.next_power_of_2(math.ceil(math.sqrt(n_elements)))\n    # Determine the grid size needed to cover all elements\n    grid_size = triton.cdiv(n_elements, block_size)\n    # Launch the Triton kernel\n    cos_func[(grid_size, 1, 1)](A, B, n_elements, block_size)\n    return B\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code provides a fast cross-entropy loss function using Triton kernels for efficient GPU computation. The `Fast_CrossEntropyLoss` class extends PyTorch's autograd functionality to implement the forward and backward passes of cross-entropy computation. \n            \n            The main kernel `_cross_entropy_forward` computes the log-sum-exp trick to stabilize the exponentials for calculating the loss and normalizes the logits. It supports optional features like softcapping and logit scaling. \n            \n            `_cross_entropy_backward` computes the gradient of the loss with respect to the logits for backpropagation. It uses similar optional features as the forward kernel. \n            \n            The kernels are launched in parallel over the number of input rows and chunks if necessary. \n            \n            `fast_cross_entropy_loss` is the user-facing function that reshapes inputs, calls the `Fast_CrossEntropyLoss` class, and averages the loss over the valid labels.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _cross_entropy_forward(\n    logits_ptr, labels_ptr, loss_ptr, num_classes, softcap, logit_scale, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    batch_size = logits_ptr.shape[0]\n    row_start = pid * BLOCK_SIZE\n    row_end = row_start + BLOCK_SIZE\n\n    for i in range(row_start, row_end):\n        if i < batch_size:\n            logits = tl.load(logits_ptr + i * num_classes, mask=tl.arange(0, num_classes) < num_classes)\n            if logit_scale > 1.0:\n                logits = logits / logit_scale\n            if softcap > 0.0:\n                logits = tl.log(1 + tl.exp(logits)) / softcap\n            max_logit = tl.max(logits, axis=0)\n            logits = logits - max_logit\n            exp_logits = tl.exp(logits)\n            sum_exp_logits = tl.sum(exp_logits, axis=0)\n            log_sum_exp = tl.log(sum_exp_logits)\n            label = tl.load(labels_ptr + i)\n            loss = log_sum_exp - logits[label]\n            tl.store(loss_ptr + i, loss)\n\n@triton.jit\ndef _cross_entropy_backward(\n    grad_output_ptr, logits_ptr, labels_ptr, grad_logits_ptr, num_classes, softcap, logit_scale, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    batch_size = logits_ptr.shape[0]\n    row_start = pid * BLOCK_SIZE\n    row_end = row_start + BLOCK_SIZE\n\n    for i in range(row_start, row_end):\n        if i < batch_size:\n            logits = tl.load(logits_ptr + i * num_classes, mask=tl.arange(0, num_classes) < num_classes)\n            if logit_scale > 1.0:\n                logits = logits / logit_scale\n            if softcap > 0.0:\n                logits = tl.log(1 + tl.exp(logits)) / softcap\n            max_logit = tl.max(logits, axis=0)\n            logits = logits - max_logit\n            exp_logits = tl.exp(logits)\n            sum_exp_logits = tl.sum(exp_logits, axis=0)\n            label = tl.load(labels_ptr + i)\n            grad_output = tl.load(grad_output_ptr + i)\n            grad = exp_logits / sum_exp_logits\n            grad[label] -= 1.0\n            grad *= grad_output\n            if logit_scale > 1.0:\n                grad *= logit_scale\n            if softcap > 0.0:\n                grad *= (1 - tl.exp(logits) / (1 + tl.exp(logits))) / softcap\n            tl.store(grad_logits_ptr + i * num_classes, grad, mask=tl.arange(0, num_classes) < num_classes)\n\nclass Fast_CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, labels, softcap=0.0, logit_scale=1.0):\n        batch_size, num_classes = logits.shape\n        loss = torch.empty(batch_size, device=logits.device, dtype=logits.dtype)\n        \n        # Launch the forward kernel\n        grid = (batch_size, )\n        _cross_entropy_forward[grid](logits, labels, loss, num_classes, softcap, logit_scale, BLOCK_SIZE=1024)\n        \n        ctx.save_for_backward(logits, labels)\n        ctx.softcap = softcap\n        ctx.logit_scale = logit_scale\n        return loss.mean()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        logits, labels = ctx.saved_tensors\n        batch_size, num_classes = logits.shape\n        grad_logits = torch.zeros_like(logits)\n        \n        # Launch the backward kernel\n        grid = (batch_size, )\n        _cross_entropy_backward[grid](grad_output, logits, labels, grad_logits, num_classes, ctx.softcap, ctx.logit_scale, BLOCK_SIZE=1024)\n        \n        return grad_logits, None, None, None\n\ndef fast_cross_entropy_loss(logits, labels, softcap=0.0, logit_scale=1.0):\n    logits = logits.view(-1, logits.size(-1))\n    labels = labels.view(-1)\n    valid_mask = labels >= 0\n    valid_logits = logits[valid_mask]\n    valid_labels = labels[valid_mask]\n    loss = Fast_CrossEntropyLoss.apply(valid_logits, valid_labels, softcap, logit_scale)\n    return loss\n\nimport torch\n\n# Example inputs\nlogits = torch.randn(10, 5, requires_grad=True, device='cuda')\nlabels = torch.randint(0, 5, (10,), device='cuda')\n\n# Compute the loss\nloss = fast_cross_entropy_loss(logits, labels, softcap=0.1, logit_scale=2.0)\nprint(loss)\n\n# Backward pass\nloss.backward()\nprint(logits.grad)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n# from .utils import triton_tanh\nfrom triton.language.extra import libdevice\n\ntriton_tanh = libdevice.tanh\nnext_power_of_2 = triton.next_power_of_2\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int):\n    BLOCK_SIZE : int = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.heuristics({\n    \"DO_SOFTCAPPING\": lambda args: args[\"DO_SOFTCAPPING\"],\n    \"DO_LOGIT_SCALING\": lambda args: args[\"DO_LOGIT_SCALING\"],\n})\n@triton.jit\ndef _cross_entropy_forward(\n    logits_ptr, logits_row_stride,\n    loss_ptr,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n\n    logits = logits.to(tl.float32)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx)\n        if DO_LOGIT_SCALING: x = LOGIT_SCALE * x\n        if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)\n        loss = logsumexp - x.to(tl.float32)\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)\n\n@triton.heuristics({\n    \"DO_SOFTCAPPING\": lambda args: args[\"DO_SOFTCAPPING\"],\n    \"DO_LOGIT_SCALING\": lambda args: args[\"DO_LOGIT_SCALING\"],\n})\n@triton.jit\ndef _chunked_cross_entropy_forward(\n    logits_ptr, logits_row_stride,\n    loss_ptr,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    N_CHUNKS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n\n    col_offsets = chunk_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n\n    logits = logits.to(tl.float32)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx).to(tl.float32)\n            if DO_LOGIT_SCALING: x = LOGIT_SCALE * x\n            if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)\n            loss = -1.0 * x.to(tl.float32)\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n        tl.store(logsumexp_ptr, logsumexp)\n\n\n@triton.heuristics({\n    \"DO_SOFTCAPPING\": lambda args: args[\"DO_SOFTCAPPING\"],\n    \"DO_LOGIT_SCALING\": lambda args: args[\"DO_LOGIT_SCALING\"],\n})\n@triton.jit\ndef _cross_entropy_backward(\n    logits_ptr, logits_row_stride,\n    dloss_ptr, dloss_row_stride,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\n\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING:\n        x = x * LOGIT_SCALE\n    \n    if DO_SOFTCAPPING:\n        partial = triton_tanh(x / SOFTCAP)\n        x = SOFTCAP * partial\n    \n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x.to(tl.float32) - logsumexp)\n    y = tl.where(\n        col_offsets == label_idx,\n        y - 1.0,\n        y,\n    )\n\n    if DO_LOGIT_SCALING:\n        y = y * LOGIT_SCALE\n    \n    if DO_SOFTCAPPING:\n        y = y * (1.0 - partial*partial)\n    \n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)\n\nMAX_FUSED_SIZE = 65536\n\nclass Fast_CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, labels, logit_softcapping=0, logit_scaling=0):\n        n_rows, vocab_size = logits.shape\n\n        div, mod = divmod(vocab_size, MAX_FUSED_SIZE)\n        n_chunks = div + (mod != 0)\n        losses = torch.empty(n_rows, dtype=torch.float32, device=\"cuda:0\")\n\n        DO_SOFTCAPPING = (logit_softcapping != 0)\n        DO_LOGIT_SCALING = (logit_scaling != 0)\n\n        if n_chunks == 1:\n            BLOCK_SIZE, num_warps = calculate_settings(vocab_size)\n            logsumexp = torch.empty(n_rows, dtype=torch.float32, device=\"cuda:0\")\n\n            _cross_entropy_forward[(n_rows,)](\n                logits, logits.stride(0),\n                losses,\n                logsumexp,\n                labels,\n                VOCAB_SIZE=vocab_size,\n                BLOCK_SIZE=BLOCK_SIZE,\n                DO_SOFTCAPPING=DO_SOFTCAPPING,\n                SOFTCAP=logit_softcapping,\n                DO_LOGIT_SCALING=DO_LOGIT_SCALING,\n                LOGIT_SCALE=logit_scaling,\n                num_warps=num_warps,\n            )\n        else:\n            logsumexp = torch.empty((n_rows, n_chunks,), dtype=torch.float32, device=\"cuda:0\")\n\n            _chunked_cross_entropy_forward[(n_rows, n_chunks,)](\n                logits, logits.stride(0),\n                losses,\n                logsumexp,\n                labels,\n                VOCAB_SIZE=vocab_size,\n                N_CHUNKS=n_chunks,\n                BLOCK_SIZE=MAX_FUSED_SIZE,\n                DO_SOFTCAPPING=DO_SOFTCAPPING,\n                SOFTCAP=logit_softcapping,\n                DO_LOGIT_SCALING=DO_LOGIT_SCALING,\n                LOGIT_SCALE=logit_scaling,\n                num_warps=32,\n            )\n            logsumexp = torch.logsumexp(logsumexp, dim=1)\n            losses += logsumexp\n            losses.masked_fill_(labels == -100, 0)\n        \n        ctx.save_for_backward(logits, logsumexp, labels)\n        ctx.DO_SOFTCAPPING = DO_SOFTCAPPING\n        ctx.logit_softcapping = logit_softcapping\n        ctx.DO_LOGIT_SCALING = DO_LOGIT_SCALING\n        ctx.logit_scaling = logit_scaling\n        return losses\n    \n    @staticmethod\n    def backward(ctx, dlosses):\n        logits, logsumexp, labels = ctx.saved_tensors\n        n_rows, vocab_size = logits.shape\n\n        BLOCK_SIZE = 4096\n        div, mod = divmod(vocab_size, BLOCK_SIZE)\n        n_blocks = div + (mod != 0)\n\n        _cross_entropy_backward[(n_rows, n_blocks,)](\n            logits, logits.stride(0),\n            dlosses, dlosses.stride(0),\n            logsumexp,\n            labels,\n            VOCAB_SIZE=vocab_size,\n            BLOCK_SIZE=BLOCK_SIZE,\n            DO_SOFTCAPPING=ctx.DO_SOFTCAPPING,\n            SOFTCAP=ctx.logit_softcapping,\n            DO_LOGIT_SCALING=ctx.DO_LOGIT_SCALING,\n            LOGIT_SCALE=ctx.logit_scaling,\n            num_warps=8,\n        )\n        return logits, None, None, None,\n    \ndef fast_cross_entropy_loss(\n    logits,\n    labels,\n    logit_softcapping=0,\n    logit_scaling=0,\n    n_items=None,\n):\n    batch, seq_len, d = logits.shape\n    assert(labels.shape == (batch, seq_len))\n\n    loss = Fast_CrossEntropyLoss.apply(\n        logits.view(batch*seq_len, d),\n        labels.view(-1),\n        logit_softcapping,\n        logit_scaling,\n    )\n    if n_items is None:\n        n_items = torch.count_nonzero(labels != -100)\n    return loss.sum() / n_items\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel named 'log_softmax_kernel' implements the log softmax operation over a matrix. It takes as input pointers to the input and output data, along with dimensions M, N, and K, where M is the number of rows, N is the number of columns, and K is an auxiliary dimension. The kernel loads data from the input pointer, computes the log softmax by first normalizing the input, and stores the result back in the output pointer. The kernel supports automatic tuning of its block sizes (BLOCK_M and BLOCK_N) and number of warps via heuristic functions to optimize performance based on the input size.\n        \n        The 'LogSoftmax' class is an autograd function for PyTorch, wrapping the Triton kernel for forward and backward passes. The 'forward' method sets up the kernel execution, calculates grid dimensions, and invokes the log softmax kernel. It saves necessary data for backward computation. The 'backward' method calculates the gradient using another Triton kernel, 'log_softmax_backward_kernel', which computes the gradient of the log softmax operation.\n\n        The 'log_softmax' function is a user-facing API that leverages 'LogSoftmax' to perform the log softmax operation on an input tensor, specifying the dimension and optionally the data type. It ensures the input is contiguous and provides an output tensor of the same shape.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef log_softmax_kernel(\n    X_ptr,  # Pointer to input matrix\n    Y_ptr,  # Pointer to output matrix\n    M,      # Number of rows\n    N,      # Number of columns\n    K,      # Auxiliary dimension (not used in this kernel)\n    stride_xm,  # Stride for rows in input matrix\n    stride_xn,  # Stride for columns in input matrix\n    stride_ym,  # Stride for rows in output matrix\n    stride_yn,  # Stride for columns in output matrix\n    BLOCK_M: tl.constexpr,  # Block size for rows\n    BLOCK_N: tl.constexpr   # Block size for columns\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) // num_pid_m\n\n    # Compute the block of the matrix to process\n    rm = pid_m * BLOCK_M\n    rn = pid_n * BLOCK_N\n    XM = min(M, rm + BLOCK_M)\n    XN = min(N, rn + BLOCK_N)\n\n    # Load the block of the matrix\n    X_block = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(BLOCK_M):\n        for j in range(BLOCK_N):\n            if rm + i < M and rn + j < N:\n                X_block[i, j] = tl.load(X_ptr + (rm + i) * stride_xm + (rn + j) * stride_xn)\n\n    # Compute the max value in the block\n    max_val = tl.max(X_block, 1)\n    max_val = tl.reshape(max_val, (BLOCK_M, 1))\n\n    # Subtract the max value for numerical stability\n    X_block = X_block - max_val\n\n    # Compute the exponentials\n    exp_X_block = tl.exp(X_block)\n\n    # Compute the sum of exponentials\n    sum_exp = tl.sum(exp_X_block, 1)\n    sum_exp = tl.reshape(sum_exp, (BLOCK_M, 1))\n\n    # Compute the log softmax\n    log_sum_exp = tl.log(sum_exp)\n    Y_block = X_block - log_sum_exp\n\n    # Store the result back to the output matrix\n    for i in range(BLOCK_M):\n        for j in range(BLOCK_N):\n            if rm + i < M and rn + j < N:\n                tl.store(Y_ptr + (rm + i) * stride_ym + (rn + j) * stride_yn, Y_block[i, j])\n\nimport torch\nimport triton\nimport triton.language as tl\n\nclass LogSoftmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, dim, dtype=None):\n        # Ensure the input is contiguous\n        X = X.contiguous()\n        M, N = X.shape\n        K = 1  # Auxiliary dimension (not used in this kernel)\n        Y = torch.empty_like(X, dtype=dtype if dtype is not None else X.dtype)\n\n        # Define the grid and block sizes\n        BLOCK_M = 16\n        BLOCK_N = 16\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n        # Launch the kernel\n        log_softmax_kernel[grid](\n            X, Y, M, N, K, X.stride(0), X.stride(1), Y.stride(0), Y.stride(1),\n            BLOCK_M, BLOCK_N\n        )\n\n        # Save necessary data for backward computation\n        ctx.save_for_backward(X, Y)\n        ctx.dim = dim\n\n        return Y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        X, Y = ctx.saved_tensors\n        dim = ctx.dim\n        M, N = X.shape\n        K = 1  # Auxiliary dimension (not used in this kernel)\n        grad_input = torch.empty_like(X)\n\n        # Define the grid and block sizes\n        BLOCK_M = 16\n        BLOCK_N = 16\n        grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n        # Launch the backward kernel\n        log_softmax_backward_kernel[grid](\n            grad_output, Y, grad_input, M, N, K, grad_output.stride(0), grad_output.stride(1),\n            Y.stride(0), Y.stride(1), grad_input.stride(0), grad_input.stride(1),\n            BLOCK_M, BLOCK_N\n        )\n\n        return grad_input, None, None\n\n@triton.jit\ndef log_softmax_backward_kernel(\n    grad_output_ptr,  # Pointer to gradient of the output\n    Y_ptr,           # Pointer to output matrix (log softmax result)\n    grad_input_ptr,  # Pointer to gradient of the input\n    M,               # Number of rows\n    N,               # Number of columns\n    K,               # Auxiliary dimension (not used in this kernel)\n    stride_gom,      # Stride for rows in grad_output\n    stride_gon,      # Stride for columns in grad_output\n    stride_ym,       # Stride for rows in Y\n    stride_yn,       # Stride for columns in Y\n    stride_gim,      # Stride for rows in grad_input\n    stride_gin,      # Stride for columns in grad_input\n    BLOCK_M: tl.constexpr,  # Block size for rows\n    BLOCK_N: tl.constexpr   # Block size for columns\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) // num_pid_m\n\n    # Compute the block of the matrix to process\n    rm = pid_m * BLOCK_M\n    rn = pid_n * BLOCK_N\n    XM = min(M, rm + BLOCK_M)\n    XN = min(N, rn + BLOCK_N)\n\n    # Load the block of the matrices\n    grad_output_block = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    Y_block = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(BLOCK_M):\n        for j in range(BLOCK_N):\n            if rm + i < M and rn + j < N:\n                grad_output_block[i, j] = tl.load(grad_output_ptr + (rm + i) * stride_gom + (rn + j) * stride_gon)\n                Y_block[i, j] = tl.load(Y_ptr + (rm + i) * stride_ym + (rn + j) * stride_yn)\n\n    # Compute the sum of the gradients\n    sum_grad_output = tl.sum(grad_output_block, 1)\n    sum_grad_output = tl.reshape(sum_grad_output, (BLOCK_M, 1))\n\n    # Compute the gradient of the input\n    grad_input_block = grad_output_block - tl.exp(Y_block) * sum_grad_output\n\n    # Store the result back to the gradient of the input matrix\n    for i in range(BLOCK_M):\n        for j in range(BLOCK_N):\n            if rm + i < M and rn + j < N:\n                tl.store(grad_input_ptr + (rm + i) * stride_gim + (rn + j) * stride_gin, grad_input_block[i, j])\n\ndef log_softmax(input, dim, dtype=None):\n    return LogSoftmax.apply(input, dim, dtype)\n\nimport torch\n\n# Create a random input tensor\ninput = torch.randn(1024, 1024, device='cuda')\n\n# Compute the log softmax\noutput = log_softmax(input, dim=1)\n\n# Print the output\nprint(output)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n\ndef heur_block_n(args):\n    return triton.next_power_of_2(args[\"N\"])\n\n\ndef heur_num_warps(args):\n    if args[\"N\"] <= 1024:\n        return 4\n    elif args[\"N\"] <= 2048:\n        return 8\n    else:\n        return 16\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 1}),\n        triton.Config({\"BLOCK_M\": 2}),\n        triton.Config({\"BLOCK_M\": 4}),\n        triton.Config({\"BLOCK_M\": 8}),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"BLOCK_N\": heur_block_n,\n        \"num_warps\": heur_num_warps,\n    }\n)\n@triton.jit\ndef log_softmax_kernel(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    input_ptrs = input_ptr + offset\n    inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(tl.float32)\n    row_minus_max = inp - tl.max(inp, axis=1)[:, None]\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    softmax_output = tl.log(numerator / denominator)\n    output_ptrs = output_ptr + offset\n    tl.store(output_ptrs, softmax_output, mask=mask)\n\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 1}),\n        triton.Config({\"BLOCK_M\": 2}),\n        triton.Config({\"BLOCK_M\": 4}),\n        triton.Config({\"BLOCK_M\": 8}),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"BLOCK_N\": heur_block_n,\n        \"num_warps\": heur_num_warps,\n    }\n)\n@triton.jit\ndef log_softmax_backward_kernel(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n\n    offsets = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    out_ptrs = out_ptr + offsets\n    out = tl.load(out_ptrs, mask=mask).to(tl.float32)\n    out_grad_ptrs = out_grad_ptr + offsets\n    out_grad = tl.load(out_grad_ptrs, mask=mask).to(tl.float32)\n\n    scale = tl.sum(out_grad, 1)\n    in_grad = out_grad - tl.exp(out.to(tl.float32)) * scale[:, None]\n\n    in_grad_ptrs = in_grad_ptr + offsets\n    tl.store(in_grad_ptrs, in_grad, mask=mask)\n\n\nclass LogSoftmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, dim, dtype):\n        logging.debug(\"GEMS LOG_SOFTMAX\")\n\n        assert dim >= -x.ndim and dim < x.ndim, \"Invalid dim\"\n        dim = dim % x.ndim\n        M = 1\n        N = x.shape[dim]\n        for i in range(dim):\n            M *= x.shape[i]\n        inp = x.contiguous()\n        if dtype is None:\n            dtype = x.dtype\n        out = torch.empty_like(inp, dtype=dtype)\n        K = inp.numel() // M // N\n\n        grid = lambda meta: (\n            triton.cdiv(M, meta[\"BLOCK_M\"]),\n            K,\n        )\n        with torch.cuda.device(inp.device):\n            log_softmax_kernel[grid](\n                out,\n                inp,\n                M,\n                N,\n                K,\n            )\n        ctx.save_for_backward(out)\n        ctx.dim = dim\n        return out\n\n    @staticmethod\n    def backward(ctx, out_grad):\n        logging.debug(\"GEMS LOG_SOFTMAX VJP\")\n\n        dim = ctx.dim\n        (out,) = ctx.saved_tensors\n\n        assert dim >= -out.ndim and dim < out.ndim, \"Invalid dim\"\n        dim = dim % out.ndim\n        M = 1\n        N = out.shape[dim]\n        for i in range(dim):\n            M *= out.shape[i]\n\n        out_grad = out_grad.contiguous()\n        in_grad = torch.empty_like(out)\n        K = out.numel() // M // N\n\n        grid = lambda meta: (\n            triton.cdiv(M, meta[\"BLOCK_M\"]),\n            K,\n        )\n        with torch.cuda.device(in_grad.device):\n            log_softmax_backward_kernel[grid](\n                out,\n                out_grad,\n                in_grad,\n                M,\n                N,\n                K,\n            )\n        return in_grad, None, None\n\n\ndef log_softmax(x, dim=-1, dtype=None):\n    return LogSoftmax.apply(x, dim, dtype)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.This Triton code defines a function `index_select_cat_fwd` that selects rows from a 2D CUDA tensor `source` based on indices from a 1D CUDA tensor `index` and stores the result in `output`. The kernel `index_select_cat_fwd_kernel` is launched in a 2D grid. Each block processes a block of indices (`BLOCK_SIZE_INDEX`) and a block of columns (`BLOCK_SIZE_COL`). Inside the kernel, it calculates offsets for source and output tensors based on indices and columns, loads the source data, and stores it into the output tensor. The `grid` function calculates the grid size based on the number of indices and columns.", "predict": "import triton\nimport triton.language as tl\n\n# Define the kernel\n@triton.jit\ndef index_select_cat_fwd_kernel(\n    source_ptr,  # Pointer to the source tensor\n    index_ptr,   # Pointer to the index tensor\n    output_ptr,  # Pointer to the output tensor\n    source_stride0,  # Stride of the source tensor in the row dimension\n    source_stride1,  # Stride of the source tensor in the column dimension\n    output_stride0,  # Stride of the output tensor in the row dimension\n    output_stride1,  # Stride of the output tensor in the column dimension\n    num_indices,     # Number of indices\n    num_cols,        # Number of columns in the source and output tensors\n    BLOCK_SIZE_INDEX: tl.constexpr,  # Block size for indices\n    BLOCK_SIZE_COL: tl.constexpr     # Block size for columns\n):\n    # Get the block indices\n    pid_index = tl.program_id(0)\n    pid_col = tl.program_id(1)\n\n    # Compute the range of indices and columns for this block\n    index_start = pid_index * BLOCK_SIZE_INDEX\n    index_end = min(index_start + BLOCK_SIZE_INDEX, num_indices)\n    col_start = pid_col * BLOCK_SIZE_COL\n    col_end = min(col_start + BLOCK_SIZE_COL, num_cols)\n\n    # Load the indices for this block\n    indices = tl.arange(0, BLOCK_SIZE_INDEX)\n    indices = tl.where(indices < (index_end - index_start), indices + index_start, 0)\n\n    # Iterate over the columns\n    for col in range(col_start, col_end):\n        # Load the source data for this column\n        source_offsets = indices * source_stride0 + col * source_stride1\n        source_data = tl.load(source_ptr + source_offsets, mask=indices < (index_end - index_start))\n\n        # Compute the output offsets\n        output_offsets = (indices - index_start + pid_index * BLOCK_SIZE_INDEX) * output_stride0 + col * output_stride1\n\n        # Store the data into the output tensor\n        tl.store(output_ptr + output_offsets, source_data, mask=indices < (index_end - index_start))\n\n# Define the grid function\ndef grid(meta):\n    return (\n        (meta['num_indices'] + meta['BLOCK_SIZE_INDEX'] - 1) // meta['BLOCK_SIZE_INDEX'],\n        (meta['num_cols'] + meta['BLOCK_SIZE_COL'] - 1) // meta['BLOCK_SIZE_COL']\n    )\n\n# Define the wrapper function\ndef index_select_cat_fwd(source, index, output):\n    assert source.dim() == 2, \"Source tensor must be 2D\"\n    assert index.dim() == 1, \"Index tensor must be 1D\"\n    assert output.dim() == 2, \"Output tensor must be 2D\"\n    assert source.shape[1] == output.shape[1], \"Source and output tensors must have the same number of columns\"\n\n    num_indices = index.shape[0]\n    num_cols = source.shape[1]\n\n    # Define the block sizes\n    BLOCK_SIZE_INDEX = 128\n    BLOCK_SIZE_COL = 32\n\n    # Launch the kernel\n    index_select_cat_fwd_kernel[grid({'num_indices': num_indices, 'num_cols': num_cols, 'BLOCK_SIZE_INDEX': BLOCK_SIZE_INDEX, 'BLOCK_SIZE_COL': BLOCK_SIZE_COL})](\n        source.data_ptr(), index.data_ptr(), output.data_ptr(),\n        source.stride(0), source.stride(1),\n        output.stride(0), output.stride(1),\n        num_indices, num_cols,\n        BLOCK_SIZE_INDEX, BLOCK_SIZE_COL\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef index_select_cat_fwd_kernel(\n    output_ptr,  # *Pointer* to output tensor.\n    source_ptr,  # *Pointer* to source tensor.\n    index_ptr,  # *Pointer* to index tensor.\n    num_indices,\n    num_cols,\n    stride0,  # Stride information of source tensor.\n    stride1,\n    BLOCK_SIZE_INDEX: tl.constexpr,  # Number of indices each program should process.\n    BLOCK_SIZE_COL: tl.constexpr,  # Number of cols each program should process.\n):\n    pid0 = tl.program_id(axis=0)  # We use 2D launch grid\n    pid1 = tl.program_id(axis=1)\n\n    indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    rows = tl.load(index_ptr + indices, mask=(indices < num_indices))\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n\n    source_offsets = source_ptr + rows[:, None] * stride0 + cols[None, :] * stride1\n    mask = (indices[:, None] < num_indices) & (cols[None, :] < num_cols)\n    output = tl.load(source_offsets, mask=mask)\n\n    output_offsets = output_ptr + indices[:, None] * stride0 + cols[None, :] * stride1\n    tl.store(output_offsets, output, mask=mask)\n\n\ndef index_select_cat_fwd(\n    output: torch.Tensor,\n    source: torch.Tensor,\n    index: torch.Tensor,\n):\n    if not (source.is_cuda and index.is_cuda):\n        raise ValueError(\"The index tensor and the source tensor must be of type CUDA!\")\n\n    if not source.ndim == 2:\n        raise ValueError(f\"Expected 2-dimensional tensor, got {source.ndim}.\")\n    if not index.ndim == 1:\n        raise ValueError(f\"Expected 1-dimensional tensor, got {index.ndim}.\")\n\n    num_rows, num_cols = source.shape\n    num_indices = index.shape[0]\n\n    if num_indices > num_rows:\n        print(f\"Warning: The number of indices exceeds the number of rows in the source tensor. Truncating indices.\")\n        num_indices = num_rows\n        index = index[:num_rows]\n\n    stride0, stride1 = source.stride(0), source.stride(1)\n\n    def grid(meta):\n        return (\n            triton.cdiv(num_indices, meta[\"BLOCK_SIZE_INDEX\"]),\n            triton.cdiv(num_cols, meta[\"BLOCK_SIZE_COL\"]),\n        )\n\n    index_select_cat_fwd_kernel[grid](\n        output,\n        source,\n        index,\n        num_indices,\n        num_cols,\n        stride0,\n        stride1,\n        BLOCK_SIZE_INDEX=1,\n        BLOCK_SIZE_COL=512,\n    )\n\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel '_fwd_kernel_token_att1' computes the scaled dot-product attention for given query and key tensors. The function 'token_att_fwd' is a wrapper around the Triton kernel, handling configurations and launching the kernel execution. Inputs include query (q), key (k), location information (B_Loc), sequence start locations (B_Start_Loc), sequence lengths (B_Seqlen), maximum input length (max_input_len), and the attention output buffer (att_out). The kernel calculates attention scores by performing a dot product between the query and key vectors, scaling the result, and storing it into 'att_out'. Key computation involves loading query and key elements, calculating the attention value by summing the element-wise product of q and k, and storing these results. Key tensor dimensions and execution configurations are defined and determined by input parameters and tensor shapes.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_att1(\n    q_ptr, k_ptr, att_out_ptr,\n    B_Loc, B_Start_Loc, B_Seqlen,\n    stride_qbs, stride_qd,\n    stride_kbs, stride_kd,\n    stride_att_bs, stride_att_n,\n    max_input_len: tl.constexpr,\n    n_head: tl.constexpr,\n    head_dim: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr\n):\n    # Program ID\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(max_input_len, BLOCK_M)\n    num_pid_n = tl.cdiv(max_input_len, BLOCK_N)\n    num_pid_in_batch = num_pid_m * num_pid_n\n    batch_pid = pid // num_pid_in_batch\n    pid = pid % num_pid_in_batch\n    nm = pid % num_pid_m\n    no = pid // num_pid_m\n\n    # Offsets for q and k\n    off_m = nm * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = no * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_d = tl.arange(0, BLOCK_DMODEL)\n\n    # Offsets for q and k\n    q_offsets = (batch_pid * stride_qbs + off_m[:, None] * stride_qd + off_d[None, :])\n    k_offsets = (batch_pid * stride_kbs + off_n[:, None] * stride_kd + off_d[None, :])\n\n    # Load q and k\n    q = tl.load(q_ptr + q_offsets)\n    k = tl.load(k_ptr + k_offsets)\n\n    # Compute attention scores\n    att_scores = tl.sum(q * k, axis=1) / tl.sqrt(head_dim)\n\n    # Offsets for att_out\n    att_offsets = (batch_pid * stride_att_bs + off_m * stride_att_n + off_n)\n    tl.store(att_out_ptr + att_offsets, att_scores)\n\nimport torch\n\ndef token_att_fwd(q, k, B_Loc, B_Start_Loc, B_Seqlen, max_input_len, att_out):\n    # Get tensor dimensions\n    batch_size, n_head, seq_len, head_dim = q.shape\n\n    # Define grid and block sizes\n    BLOCK_M = 16\n    BLOCK_N = 16\n    BLOCK_DMODEL = head_dim\n    grid = (batch_size * (seq_len // BLOCK_M) * (seq_len // BLOCK_N),)\n\n    # Launch the kernel\n    _fwd_kernel_token_att1[grid](\n        q, k, att_out,\n        B_Loc, B_Start_Loc, B_Seqlen,\n        q.stride(0), q.stride(2),\n        k.stride(0), k.stride(2),\n        att_out.stride(0), att_out.stride(1),\n        max_input_len,\n        n_head,\n        head_dim,\n        BLOCK_M,\n        BLOCK_N,\n        BLOCK_DMODEL\n    )\n\n    return att_out\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_token_att1(\n    Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    Att_Out,\n    stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd,\n    att_stride_h, att_stride_bs,\n    kv_group_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    \n    cur_kv_head = cur_head // kv_group_num\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s * offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] < cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index + offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new < cur_batch_end_index)\n    return\n\n@torch.no_grad()\ndef token_att_fwd(q, k, att_out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len):\n    BLOCK = 32\n    # shape constraints\n    Lq, Lk = q.shape[-1], k.shape[-1]\n    assert Lq == Lk\n    assert Lk in {16, 32, 64, 128}\n    sm_scale = 1.0 / (Lk ** 0.5)\n\n    batch, head_num = B_Loc.shape[0], q.shape[1]\n\n    grid = (batch, head_num, triton.cdiv(max_input_len, BLOCK))\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    num_warps = 4 if Lk <= 64 else 8\n    num_warps = 2\n\n    _fwd_kernel_token_att1[grid](\n        q, k, sm_scale, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n        att_out,\n        B_Loc.stride(0), B_Loc.stride(1),\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        att_out.stride(0), att_out.stride(1),\n        kv_group_num=kv_group_num,\n        BLOCK_DMODEL=Lk,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The Triton kernel function `_l2_norm_bwd_kernel` computes the backward pass for the L2 normalization operation. It processes each row of the input matrix `X` individually to calculate gradients (`DX`) based on the gradients of the output (`DY`). The function loads slices of `X` and `DY`, computes the variance, and applies the gradient formula incorporating the variance to store the computed gradient in `DX`. The outer function `_l2_norm_bwd` reshapes inputs for batch processing, configures block sizes, and calls the kernel function. The inputs are `x`, `dy` and an optional `eps` for numerical stability, while the output is `dx`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X,  # Input matrix\n    DY,  # Gradients of the output\n    DX,  # Output gradients\n    N,  # Number of columns (features)\n    eps,  # Small value for numerical stability\n    stride_x_row,  # Stride for rows in X\n    stride_x_col,  # Stride for columns in X\n    stride_dy_row,  # Stride for rows in DY\n    stride_dy_col,  # Stride for columns in DY\n    stride_dx_row,  # Stride for rows in DX\n    stride_dx_col,  # Stride for columns in DX\n    BLOCK_SIZE: tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, BLOCK_SIZE)\n\n    # Load slices of X and DY\n    x_slice = tl.load(X + row_idx * stride_x_row + col_idx * stride_x_col, mask=col_idx < N, other=0.0)\n    dy_slice = tl.load(DY + row_idx * stride_dy_row + col_idx * stride_dy_col, mask=col_idx < N, other=0.0)\n\n    # Compute the L2 norm of the row\n    norm = tl.sqrt(tl.sum(x_slice * x_slice, axis=0) + eps)\n\n    # Compute the variance\n    variance = tl.sum(x_slice * dy_slice, axis=0) / norm\n\n    # Compute the gradient\n    dx_slice = (dy_slice * norm - x_slice * variance) / (norm * norm)\n\n    # Store the computed gradient in DX\n    tl.store(DX + row_idx * stride_dx_row + col_idx * stride_dx_col, dx_slice, mask=col_idx < N)\n\nimport torch\n\ndef _l2_norm_bwd(x: torch.Tensor, dy: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n    assert x.dim() == 2, \"Input tensor must be 2D\"\n    assert dy.dim() == 2, \"Gradient tensor must be 2D\"\n    assert x.shape == dy.shape, \"Input and gradient tensors must have the same shape\"\n\n    M, N = x.shape\n    dx = torch.empty_like(x)\n\n    # Define block sizes\n    BLOCK_SIZE = 128\n\n    # Configure the grid and block sizes\n    grid = (M,)\n\n    # Launch the kernel\n    _l2_norm_bwd_kernel[grid](\n        x, dy, dx, N, eps,\n        x.stride(0), x.stride(1),\n        dy.stride(0), dy.stride(1),\n        dx.stride(0), dx.stride(1),\n        BLOCK_SIZE\n    )\n\n    return dx\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X,  # pointer to the input\n    DY,  # pointer to the output gradient\n    DX,  # pointer to the input gradient\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x) \n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0).to(tl.float32)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var+eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n\ndef _l2_norm_bwd(\n    x, dy, eps=1e-5,\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, dy.shape[-1])\n    dy = dy.reshape(-1, dy.shape[-1])\n    if dy.stride(-1) != 1:\n        dy = dy.contiguous()\n    dx = torch.empty_like(x)\n    N = x.shape[-1]\n    M = x.shape[0]\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _l2_norm_bwd_kernel[(M,)](\n            x,\n            dy,\n            dx,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return dx.reshape(x_shape_og)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code implements a custom softmax operation using Triton, designed to handle varying sequence lengths efficiently. The main function, `token_softmax_fwd`, prepares and launches the Triton kernel `_fwd_kernel_token_softmax`. It calculates the softmax for sequences of logits with different lengths in parallel, for batches and heads in attention mechanisms. Inputs include `Logics` (logits), `B_Start_Loc` (start indices of sequences), `B_Seqlen` (sequence lengths), and `Prob_Out` (output tensor for probabilities). The kernel computes the softmax along sequence elements by normalizing logits, subtracting the max for stability, exponentiating, and dividing by the sum. It handles masked values by using `-inf` for padding positions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logits,  # Input logits tensor\n    B_Start_Loc,  # Start indices of sequences\n    B_Seqlen,  # Sequence lengths\n    Prob_Out,  # Output tensor for probabilities\n    stride_logit_b,  # Stride for logits in batch dimension\n    stride_logit_h,  # Stride for logits in head dimension\n    stride_logit_s,  # Stride for logits in sequence dimension\n    stride_prob_b,  # Stride for output in batch dimension\n    stride_prob_h,  # Stride for output in head dimension\n    stride_prob_s,  # Stride for output in sequence dimension\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Get the batch and head indices\n    bid = tl.program_id(0)\n    hid = tl.program_id(1)\n\n    # Get the start index and sequence length for the current batch\n    start_loc = tl.load(B_Start_Loc + bid)\n    seqlen = tl.load(B_Seqlen + bid)\n\n    # Initialize the maximum value and sum for the current sequence\n    max_val = float('-inf')\n    sum_val = 0.0\n\n    # Iterate over the sequence elements\n    for i in range(start_loc, start_loc + seqlen):\n        logit = tl.load(Logits + bid * stride_logit_b + hid * stride_logit_h + i * stride_logit_s)\n        max_val = tl.max(max_val, logit)\n\n    # Compute the exponentials and sum\n    for i in range(start_loc, start_loc + seqlen):\n        logit = tl.load(Logits + bid * stride_logit_b + hid * stride_logit_h + i * stride_logit_s)\n        exp_val = tl.exp(logit - max_val)\n        sum_val += exp_val\n\n    # Normalize the probabilities\n    for i in range(start_loc, start_loc + seqlen):\n        logit = tl.load(Logits + bid * stride_logit_b + hid * stride_logit_h + i * stride_logit_s)\n        exp_val = tl.exp(logit - max_val)\n        prob = exp_val / sum_val\n        tl.store(Prob_Out + bid * stride_prob_b + hid * stride_prob_h + i * stride_prob_s, prob)\n\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=8),\n    ],\n    key=['B', 'H', 'S'],\n)\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logits,  # Input logits tensor\n    B_Start_Loc,  # Start indices of sequences\n    B_Seqlen,  # Sequence lengths\n    Prob_Out,  # Output tensor for probabilities\n    stride_logit_b,  # Stride for logits in batch dimension\n    stride_logit_h,  # Stride for logits in head dimension\n    stride_logit_s,  # Stride for logits in sequence dimension\n    stride_prob_b,  # Stride for output in batch dimension\n    stride_prob_h,  # Stride for output in head dimension\n    stride_prob_s,  # Stride for output in sequence dimension\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Get the batch and head indices\n    bid = tl.program_id(0)\n    hid = tl.program_id(1)\n\n    # Get the start index and sequence length for the current batch\n    start_loc = tl.load(B_Start_Loc + bid)\n    seqlen = tl.load(B_Seqlen + bid)\n\n    # Initialize the maximum value and sum for the current sequence\n    max_val = float('-inf')\n    sum_val = 0.0\n\n    # Iterate over the sequence elements\n    for i in range(start_loc, start_loc + seqlen):\n        logit = tl.load(Logits + bid * stride_logit_b + hid * stride_logit_h + i * stride_logit_s)\n        max_val = tl.max(max_val, logit)\n\n    # Compute the exponentials and sum\n    for i in range(start_loc, start_loc + seqlen):\n        logit = tl.load(Logits + bid * stride_logit_b + hid * stride_logit_h + i * stride_logit_s)\n        exp_val = tl.exp(logit - max_val)\n        sum_val += exp_val\n\n    # Normalize the probabilities\n    for i in range(start_loc, start_loc + seqlen):\n        logit = tl.load(Logits + bid * stride_logit_b + hid * stride_logit_h + i * stride_logit_s)\n        exp_val = tl.exp(logit - max_val)\n        prob = exp_val / sum_val\n        tl.store(Prob_Out + bid * stride_prob_b + hid * stride_prob_h + i * stride_prob_s, prob)\n\ndef token_softmax_fwd(Logits, B_Start_Loc, B_Seqlen, Prob_Out):\n    B, H, S = Logits.shape\n    assert B == B_Start_Loc.shape[0]\n    assert B == B_Seqlen.shape[0]\n\n    # Launch the kernel\n    grid = (B, H)\n    _fwd_kernel_token_softmax[grid](\n        Logits, B_Start_Loc, B_Seqlen, Prob_Out,\n        Logits.stride(0), Logits.stride(1), Logits.stride(2),\n        Prob_Out.stride(0), Prob_Out.stride(1), Prob_Out.stride(2),\n        BLOCK_SIZE=256,\n    )\n\n# Example usage\nB, H, S = 32, 8, 128\nLogits = torch.randn((B, H, S), device='cuda')\nB_Start_Loc = torch.randint(0, S, (B,), device='cuda')\nB_Seqlen = torch.randint(1, S, (B,), device='cuda')\nProb_Out = torch.empty((B, H, S), device='cuda')\n\ntoken_softmax_fwd(Logits, B_Start_Loc, B_Seqlen, Prob_Out)\nprint(Prob_Out)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logics, B_Start_Loc, B_Seqlen,\n    Prob_Out,\n    stride_logic_h, stride_logic_bs,\n    stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    row = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_in_all_start_index + col_offsets) * stride_logic_bs,\n                  mask=col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    tl.store(Prob_Out + cur_head * stride_prob_h + (cur_batch_in_all_start_index + col_offsets)\n             * stride_prob_bs, softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n\n\n@torch.no_grad()\ndef token_softmax_fwd(Logics, B_Start_Loc, B_Seqlen, Prob_Out, max_input_len):\n    BLOCK_SIZE = triton.next_power_of_2(max_input_len)\n    batch, head_num = B_Start_Loc.shape[0], Logics.shape[0]\n\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n\n    _fwd_kernel_token_softmax[(batch, head_num)](\n        Logics, B_Start_Loc, B_Seqlen,\n        Prob_Out,\n        Logics.stride(0), Logics.stride(1),\n        Prob_Out.stride(0), Prob_Out.stride(1),\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided code defines a Triton kernel and a wrapper function for computing a chunk-based cumulative sum across a vector in a tensor. The kernel `chunk_global_cumsum_vector_kernel` operates on 2D slices (blocks) of the input tensor `s` and computes the cumulative sum along a specified dimension. The wrapper function `chunk_global_cumsum_vector` sets up this kernel by configuring grid sizes and prepares input/output tensors.\n\n    The main kernel logic involves loading blocks of data, computing a cumulative sum using matrix multiplication with a lower triangular mask (to accumulate values), and storing the result back. The kernel is optimized with Triton's `autotune` feature, exploring different configurations of block sizes (`BT`) and the number of warps.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(\n    s_ptr,  # Pointer to the input tensor\n    s_cumsum_ptr,  # Pointer to the output tensor\n    N,  # Total number of elements in the tensor\n    chunk_size,  # Size of each chunk\n    stride,  # Stride along the specified dimension\n    BLOCK_SIZE: tl.constexpr,  # Block size for the kernel\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the block of data\n    mask = offsets < N\n    s_block = tl.load(s_ptr + offsets, mask=mask, other=0.0)\n\n    # Compute the lower triangular mask\n    lower_tri_mask = tl.arange(0, BLOCK_SIZE)[:, None] <= tl.arange(0, BLOCK_SIZE)[None, :]\n\n    # Compute the cumulative sum using matrix multiplication\n    s_cumsum_block = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for i in range(BLOCK_SIZE):\n        s_cumsum_block += tl.where(lower_tri_mask[i, :], s_block, 0.0)\n\n    # Store the result back\n    tl.store(s_cumsum_ptr + offsets, s_cumsum_block, mask=mask)\n\nimport torch\n\ndef chunk_global_cumsum_vector(s, chunk_size, dim=0):\n    N = s.numel()\n    s_cumsum = torch.zeros_like(s)\n\n    # Define the grid and block sizes\n    BLOCK_SIZE = 128  # This can be tuned for better performance\n    grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    chunk_global_cumsum_vector_kernel[grid, BLOCK_SIZE](\n        s,  # Input tensor\n        s_cumsum,  # Output tensor\n        N,  # Total number of elements\n        chunk_size,  # Size of each chunk\n        s.stride(dim),  # Stride along the specified dimension\n        BLOCK_SIZE,  # Block size\n    )\n\n    return s_cumsum\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 16}, num_warps=4),\n        triton.Config({'BT': 16}, num_warps=8),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=4),\n        triton.Config({'BT': 64}, num_warps=8),\n    ],\n    key=['S']\n)\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(\n    s,\n    z,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr\n):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1., 0.)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        # [BT, BS]\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n\ndef chunk_global_cumsum_vector(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T, S = s.shape\n    BS = 32\n    dtype = dtype or s.dtype\n    grid = (triton.cdiv(S, BS), B * H)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_cumsum_vector_kernel[grid](\n        s, z,\n        s.stride(1), s.stride(2), s.stride(3),\n        T=T, S=S, BS=BS\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel implementation involves forward and backward pass functions for state-space models (SSM). The primary functions are `diag_ssm_forward_kernel` and `diag_ssm_forward_kernel_complex` for real and complex forward passes, respectively. The `diag_ssm_backward_kernel` and `diag_ssm_backward_kernel_complex` handle the backward pass.\n\n    Each function processes a 3D tensor input representing sequences of data, computes the state-space model output or gradients, and writes the results into a specified output tensor. The kernels are optimized to handle batch processing.\n\n    The forward pass functions compute output sequences by iteratively applying a diagonal state-space transformation matrix. The backward pass functions calculate gradients by propagating errors backward through the sequence.\n\n    The Python class `_ssm_forward` wraps these Triton kernels to integrate with PyTorch's autograd system, handling both real and complex cases appropriately.\n    ", "predict": "Certainly! Below is a complete implementation of the Triton kernels and the Python wrapper class for the state-space model (SSM) forward and backward passes. This implementation includes both real and complex versions of the forward and backward kernels, and it integrates with PyTorch's autograd system.\n\n### Triton Kernels\n\nFirst, let's define the Triton kernels for the forward and backward passes.\n\n```python\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef diag_ssm_forward_kernel(\n    input_ptr,  # *input_ptr: pointer to input tensor of shape (B, T, D)\n    A_ptr,      # *A_ptr: pointer to diagonal state transition matrix of shape (B, D, D)\n    B_ptr,      # *B_ptr: pointer to input matrix of shape (B, D, 1)\n    C_ptr,      # *C_ptr: pointer to output matrix of shape (B, 1, D)\n    output_ptr, # *output_ptr: pointer to output tensor of shape (B, T, 1)\n    B,          # batch size\n    T,          # sequence length\n    D,          # state dimension\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // T\n    tid = pid % T\n\n    if bid >= B or tid >= T:\n        return\n\n    input_offset = bid * T * D + tid * D\n    output_offset = bid * T + tid\n    A_offset = bid * D * D\n    B_offset = bid * D\n    C_offset = bid * D\n\n    state = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for t in range(tid, T):\n        input_vec = tl.load(input_ptr + input_offset + tl.arange(0, BLOCK_SIZE))\n        A_vec = tl.load(A_ptr + A_offset + tl.arange(0, BLOCK_SIZE))\n        B_vec = tl.load(B_ptr + B_offset + tl.arange(0, BLOCK_SIZE))\n        C_vec = tl.load(C_ptr + C_offset + tl.arange(0, BLOCK_SIZE))\n\n        state = state * A_vec + input_vec * B_vec\n        output = tl.dot(C_vec, state)\n\n        tl.store(output_ptr + output_offset, output)\n        input_offset += D\n        output_offset += 1\n\n@triton.jit\ndef diag_ssm_forward_kernel_complex(\n    input_ptr,  # *input_ptr: pointer to input tensor of shape (B, T, D)\n    A_ptr,      # *A_ptr: pointer to diagonal state transition matrix of shape (B, D, D)\n    B_ptr,      # *B_ptr: pointer to input matrix of shape (B, D, 1)\n    C_ptr,      # *C_ptr: pointer to output matrix of shape (B, 1, D)\n    output_ptr, # *output_ptr: pointer to output tensor of shape (B, T, 1)\n    B,          # batch size\n    T,          # sequence length\n    D,          # state dimension\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // T\n    tid = pid % T\n\n    if bid >= B or tid >= T:\n        return\n\n    input_offset = bid * T * D + tid * D\n    output_offset = bid * T + tid\n    A_offset = bid * D * D\n    B_offset = bid * D\n    C_offset = bid * D\n\n    state_real = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    state_imag = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for t in range(tid, T):\n        input_real = tl.load(input_ptr + input_offset + tl.arange(0, BLOCK_SIZE))\n        input_imag = tl.load(input_ptr + input_offset + D + tl.arange(0, BLOCK_SIZE))\n        A_real = tl.load(A_ptr + A_offset + tl.arange(0, BLOCK_SIZE))\n        A_imag = tl.load(A_ptr + A_offset + D + tl.arange(0, BLOCK_SIZE))\n        B_real = tl.load(B_ptr + B_offset + tl.arange(0, BLOCK_SIZE))\n        B_imag = tl.load(B_ptr + B_offset + D + tl.arange(0, BLOCK_SIZE))\n        C_real = tl.load(C_ptr + C_offset + tl.arange(0, BLOCK_SIZE))\n        C_imag = tl.load(C_ptr + C_offset + D + tl.arange(0, BLOCK_SIZE))\n\n        state_real, state_imag = (\n            state_real * A_real - state_imag * A_imag + input_real * B_real - input_imag * B_imag,\n            state_real * A_imag + state_imag * A_real + input_real * B_imag + input_imag * B_real\n        )\n        output_real = tl.dot(C_real, state_real) - tl.dot(C_imag, state_imag)\n        output_imag = tl.dot(C_real, state_imag) + tl.dot(C_imag, state_real)\n\n        tl.store(output_ptr + output_offset, output_real)\n        tl.store(output_ptr + output_offset + D, output_imag)\n        input_offset += D\n        output_offset += 1\n\n@triton.jit\ndef diag_ssm_backward_kernel(\n    grad_output_ptr,  # *grad_output_ptr: pointer to gradient of output tensor of shape (B, T, 1)\n    A_ptr,            # *A_ptr: pointer to diagonal state transition matrix of shape (B, D, D)\n    B_ptr,            # *B_ptr: pointer to input matrix of shape (B, D, 1)\n    C_ptr,            # *C_ptr: pointer to output matrix of shape (B, 1, D)\n    grad_input_ptr,   # *grad_input_ptr: pointer to gradient of input tensor of shape (B, T, D)\n    B,                # batch size\n    T,                # sequence length\n    D,                # state dimension\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // T\n    tid = pid % T\n\n    if bid >= B or tid >= T:\n        return\n\n    grad_output_offset = bid * T + tid\n    grad_input_offset = bid * T * D + tid * D\n    A_offset = bid * D * D\n    B_offset = bid * D\n    C_offset = bid * D\n\n    grad_state = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for t in range(tid, -1, -1):\n        grad_output = tl.load(grad_output_ptr + grad_output_offset)\n        C_vec = tl.load(C_ptr + C_offset + tl.arange(0, BLOCK_SIZE))\n        A_vec = tl.load(A_ptr + A_offset + tl.arange(0, BLOCK_SIZE))\n        B_vec = tl.load(B_ptr + B_offset + tl.arange(0, BLOCK_SIZE))\n\n        grad_state = grad_state * A_vec + grad_output * C_vec\n        grad_input = grad_state * B_vec\n\n        tl.store(grad_input_ptr + grad_input_offset, grad_input)\n        grad_input_offset -= D\n        grad_output_offset -= 1\n\n@triton.jit\ndef diag_ssm_backward_kernel_complex(\n    grad_output_ptr,  # *grad_output_ptr: pointer to gradient of output tensor of shape (B, T, 1)\n    A_ptr,            # *A_ptr: pointer to diagonal state transition matrix of shape (B, D, D)\n    B_ptr,            # *B_ptr: pointer to input matrix of shape (B, D, 1)\n    C_ptr,            # *C_ptr: pointer to output matrix of shape (B, 1, D)\n    grad_input_ptr,   # *grad_input_ptr: pointer to gradient of input tensor of shape (B, T, D)\n    B,                # batch size\n    T,                # sequence length\n    D,                # state dimension\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    bid = pid // T\n    tid = pid % T\n\n    if bid >= B or tid >= T:\n        return\n\n    grad_output_offset = bid * T + tid\n    grad_input_offset = bid * T * D + tid * D\n    A_offset = bid * D * D\n    B_offset = bid * D\n    C_offset = bid * D\n\n    grad_state_real = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    grad_state_imag = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for t in range(tid, -1, -1):\n        grad_output_real = tl.load(grad_output_ptr + grad_output_offset)\n        grad_output_imag = tl.load(grad_output_ptr + grad_output_offset + D)\n        C_real = tl.load(C_ptr + C_offset + tl.arange(0, BLOCK_SIZE))\n        C_imag = tl.load(C_ptr + C_offset + D + tl.arange(0, BLOCK_SIZE))\n        A_real = tl.load(A_ptr + A_offset + tl.arange(0, BLOCK_SIZE))\n        A_imag = tl.load(A_ptr + A_offset + D + tl.arange(0, BLOCK_SIZE))\n        B_real = tl.load(B_ptr + B_offset + tl.arange(0, BLOCK_SIZE))\n        B_imag = tl.load(B_ptr + B_offset + D + tl.arange(0, BLOCK_SIZE))\n\n        grad_state_real, grad_state_imag = (\n            grad_state_real * A_real - grad_state_imag * A_imag + grad_output_real * C_real - grad_output_imag * C_imag,\n            grad_state_real * A_imag + grad_state_imag *", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef diag_ssm_forward_kernel(s_ptr, x_ptr, lambda_ptr, y_ptr, length,\n                            batch_size, dim, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u524d\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u5b9e\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim]\n        x_ptr: [length, batch_size, dim]\n        lambda_ptr: [dim]\n        y_ptr: [length, batch_size, dim]\n    \"\"\"\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n    s = tl.load(s_ptr + col_offsets, mask=mask, other=0)\n    Lambda = tl.load(lambda_ptr + col_offsets % dim, mask=mask, other=0)\n    for t in range(length):\n        offsets = t * batch_size * dim + col_offsets\n        x = tl.load(x_ptr + offsets, mask=mask, other=0)\n        s = s * Lambda + x\n        tl.store(y_ptr + offsets, s, mask=mask)\n\n@triton.jit\ndef diag_ssm_backward_kernel(\n        s_ptr, lambda_ptr, y_ptr, grad_s_ptr, grad_x_ptr, grad_lambda_ptr,\n        grad_y_ptr, length, batch_size, dim, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u53cd\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u5b9e\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim]\n        lambda_ptr: [dim]\n        y_ptr: [length, batch_size, dim]\n        grad_s_ptr: [batch_size, dim]\n        grad_x_ptr: [length, batch_size, dim]\n        grad_lambda_ptr: [batch_size, dim]\n        grad_y_ptr: [length, batch_size, dim]\n    \"\"\"\n\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n\n    Lambda = tl.load(lambda_ptr + col_offsets % dim, mask=mask, other=0)\n\n    # \u521d\u59cb\u5316\u68af\u5ea6\u4e3a\u96f6\n    grad_s = tl.zeros_like(Lambda)\n    grad_Lambda = tl.zeros_like(Lambda)\n\n    for i in range(length):\n        # Triton \u4e0d\u652f\u6301 range(length - 1, -1, -1)\n        t = length - 1 - i\n        offsets = t * batch_size * dim + col_offsets\n\n        grad_y = tl.load(grad_y_ptr + offsets, mask=mask, other=0)\n        if t > 0:\n            s = tl.load(\n                y_ptr + offsets - batch_size * dim, mask=mask, other=0)\n        else:\n            s = tl.load(s_ptr + col_offsets, mask=mask, other=0)\n\n        grad_s = grad_y + grad_s\n        grad_x = grad_s\n        grad_Lambda += grad_s * s\n        grad_s = grad_s * Lambda\n\n        tl.store(grad_x_ptr + offsets, grad_x, mask=mask)\n\n    tl.store(grad_s_ptr + col_offsets, grad_s, mask=mask)\n    tl.store(grad_lambda_ptr + col_offsets, grad_Lambda, mask=mask)\n\n@triton.jit\ndef diag_ssm_forward_kernel_complex(s_ptr, x_ptr, y_ptr, lambda_ptr,\n                                    length, batch_size, dim,\n                                    BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u524d\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u590d\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim, 2]\n        x_ptr: [length, batch_size, dim, 2]\n        lambda_ptr: [dim, 2]\n        y_ptr: [length, batch_size, dim, 2]\n    \"\"\"\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n\n    # \u52a0\u8f7d's'\u548c'Lambda'\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n    s_real = tl.load(s_ptr + col_offsets * 2, mask=mask, other=0)\n    s_imag = tl.load(s_ptr + col_offsets * 2 + 1, mask=mask, other=0)\n    lambda_real = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2, mask=mask, other=0)\n    lambda_imag = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2 + 1, mask=mask, other=0)\n\n    for t in range(length):\n        offsets = (t * batch_size * dim + col_offsets) * 2\n        # \u52a0\u8f7d'x'\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n        x_real = tl.load(x_ptr + offsets, mask=mask, other=0)\n        x_imag = tl.load(x_ptr + offsets + 1, mask=mask, other=0)\n\n        # \u590d\u6570\u7684\u4e58\u6cd5\u548c\u52a0\u6cd5\n        new_s_real = s_real * lambda_real - s_imag * lambda_imag + x_real\n        new_s_imag = s_real * lambda_imag + s_imag * lambda_real + x_imag\n\n        # \u5b58\u50a8\u66f4\u65b0\u540e\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n        tl.store(y_ptr + offsets, new_s_real, mask=mask)\n        tl.store(y_ptr + offsets + 1, new_s_imag, mask=mask)\n\n        # \u66f4\u65b0's'\u4ee5\u8fdb\u884c\u4e0b\u4e00\u6b21\u8fed\u4ee3\n        s_real, s_imag = new_s_real, new_s_imag\n\n@triton.jit\ndef diag_ssm_backward_kernel_complex(\n        s_ptr, lambda_ptr, y_ptr, grad_s_ptr, grad_x_ptr, grad_lambda_ptr,\n        grad_y_ptr, length, batch_size, dim, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u53cd\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u590d\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim, 2]\n        lambda_ptr: [dim, 2]\n        y_ptr: [length, batch_size, dim, 2]\n        grad_s_ptr: [batch_size, dim, 2]\n        grad_x_ptr: [length, batch_size, dim, 2]\n        grad_lambda_ptr: [batch_size, dim, 2]\n        grad_y_ptr: [length, batch_size, dim, 2]\n    \"\"\"\n\n    # \u590d\u6570\u81ea\u5bfc\u6570\u8ba1\u7b97 \\partial f / \\partial z^*\n    # \u56e0\u6b64\u5728\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u9700\u8981\u53d6\u5171\u8f6d\n    # \u53c2\u8003\uff1ahttps://pytorch.org/docs/stable/notes/autograd.html#autograd-for-complex-numbers\n    # \u6240\u4ee5\u5728\u52a0\u8f7d/\u5b58\u50a8\u68af\u5ea6\u7684\u865a\u90e8\u65f6\uff0c\u9700\u8981\u53d6\u53cd\n\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n\n    # \u52a0\u8f7d'Lambda'\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n    lambda_real = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2, mask=mask, other=0)\n    lambda_imag = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2 + 1, mask=mask, other=0)\n\n    # \u521d\u59cb\u5316\u68af\u5ea6\u4e3a\u96f6\n    grad_s_real = tl.zeros_like(lambda_real)\n    grad_s_imag = tl.zeros_like(lambda_imag)\n    grad_lambda_real = tl.zeros_like(lambda_real)\n    grad_lambda_imag = tl.zeros_like(lambda_imag)\n\n    for i in range(length):\n        # Triton \u4e0d\u652f\u6301 range(length - 1, -1, -1)\n        t = length - 1 - i\n        offsets = (t * batch_size * dim + col_offsets) * 2\n\n        grad_y_real = tl.load(grad_y_ptr + offsets, mask=mask, other=0)\n        grad_y_imag = -tl.load(\n            grad_y_ptr + offsets + 1, mask=mask, other=0)\n        if t > 0:\n            s_real = tl.load(\n                y_ptr + offsets - 2 * batch_size * dim, mask=mask, other=0)\n            s_imag = tl.load(\n                y_ptr + offsets - 2 * batch_size * dim + 1,\n                mask=mask,\n                other=0)\n        else:\n            s_real = tl.load(s_ptr + 2 * col_offsets, mask=mask, other=0)\n            s_imag = tl.load(\n                s_ptr + 2 * col_offsets + 1, mask=mask, other=0)\n\n        grad_s_real = grad_y_real + grad_s_real\n        grad_s_imag = grad_y_imag + grad_s_imag\n        grad_x_real = grad_s_real\n        grad_x_imag = grad_s_imag\n        grad_lambda_real += grad_s_real * s_real - grad_s_imag * s_imag\n        grad_lambda_imag += grad_s_real * s_imag + grad_s_imag * s_real\n        grad_s_real = grad_x_real * lambda_real - grad_x_imag * lambda_imag\n        grad_s_imag = grad_x_real * lambda_imag + grad_x_imag * lambda_real\n\n        tl.store(grad_x_ptr + offsets, grad_x_real, mask=mask)\n        tl.store(grad_x_ptr + offsets + 1, -grad_x_imag, mask=mask)\n\n    # \u5b58\u50a8\u6700\u7ec8\u7684\u68af\u5ea6\n    tl.store(grad_s_ptr + col_offsets * 2, grad_s_real, mask=mask)\n    tl.store(grad_s_ptr + col_offsets * 2 + 1, -grad_s_imag, mask=mask)\n    tl.store(\n        grad_lambda_ptr + col_offsets * 2, grad_lambda_real, mask=mask)\n    tl.store(\n        grad_lambda_ptr + col_offsets * 2 + 1,\n        -grad_lambda_imag,\n        mask=mask)\n\nclass _ssm_forward(torch.autograd.Function):\n    # TODO \u4f7f\u7528 @triton.autotune \u9009\u62e9\u6700\u4f73\u7684 BLOCK_SIZE\n    # \u5bf9\u4e8e3090\uff0cBLOCK_SIZE = 128\u4f3c\u4e4e\u6548\u679c\u826f\u597d\n    BLOCK_SIZE = 128\n\n    @staticmethod\n    def forward(ctx, s, x, Lambda):\n        assert s.is_contiguous() and x.is_contiguous() and Lambda.is_contiguous()\n        length, batch_size, dim = x.shape\n        n = batch_size * dim\n        y = torch.zeros_like(x)\n        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']), )\n\n        if Lambda.dtype == torch.complex64:\n            # \u786e\u4fdds\u548cx\u662f\u590d\u6570\u5f20\u91cf\n            if not torch.is_complex(s):\n                raise ValueError(\"\u5f53Lambda\u4e3a\u590d\u6570\u65f6\uff0cs\u5fc5\u987b\u662f\u590d\u6570\u5f20\u91cf\")\n            if not torch.is_complex(x):\n                raise ValueError(\"\u5f53Lambda\u4e3a\u590d\u6570\u65f6\uff0cx\u5fc5\u987b\u662f\u590d\u6570\u5f20\u91cf\")\n            diag_ssm_forward_kernel_complex[grid](\n                torch.view_as_real(s), torch.view_as_real(x),\n                torch.view_as_real(y), torch.view_as_real(Lambda), length,\n                batch_size, dim, _ssm_forward.BLOCK_SIZE)\n        elif Lambda.dtype.is_floating_point:\n            diag_ssm_forward_kernel[grid](s, x, Lambda, y, length,\n                                          batch_size, dim,\n                                          _ssm_forward.BLOCK_SIZE)\n        else:\n            raise ValueError(\"\u4e0d\u652f\u6301\u7684 dtype: %s\" % Lambda.dtype)\n        ctx.save_for_backward(s, y, Lambda)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_y):\n        s, y, Lambda = ctx.saved_tensors\n        length, batch_size, dim = y.shape\n        grad_y = grad_y.contiguous()\n        n = batch_size * dim\n        grad_s = torch.empty_like(s)\n        grad_x = torch.empty_like(grad_y)\n        # grad_lambda \u5b58\u50a8\u6bcf\u4e2a\u6279\u6b21\u4e2d Lambda \u7684\u68af\u5ea6\n        # \u6211\u4eec\u5c06\u5728\u5185\u6838\u5b8c\u6210\u540e\u8fdb\u884c\u6c42\u548c\n        grad_lambda = torch.empty_like(s)\n        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']), )\n        if Lambda.dtype == torch.complex64:\n            diag_ssm_backward_kernel_complex[grid](\n                torch.view_as_real(s), torch.view_as_real(Lambda),\n                torch.view_as_real(y), torch.view_as_real(grad_s),\n                torch.view_as_real(grad_x),\n                torch.view_as_real(grad_lambda),\n                torch.view_as_real(grad_y), length, batch_size, dim,\n                _ssm_forward.BLOCK_SIZE)\n        else:\n            diag_ssm_backward_kernel[grid](\n                s, Lambda, y, grad_s, grad_x, grad_lambda, grad_y, length,\n                batch_size, dim, _ssm_forward.BLOCK_SIZE)\n        return grad_s, grad_x, grad_lambda.sum(dim=0)\n\ndiag_ssm_forward_triton = _ssm_forward.apply\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThis Triton kernel `_quantize_global` is designed to perform global quantization on a CUDA tensor `x`. It takes input tensor `x`, computes its inverse maximum absolute value (`absmax_inv`), and uses this to scale the values of `x` into an 8-bit integer range, storing the results in the `output` tensor. The main parameters are the pointers `x_ptr`, `absmax_inv_ptr`, and `output_ptr` which point to the input, the scaling factor, and the output memory respectively. `n_elements` denotes the number of elements to process, and `BLOCK_SIZE` is a compile-time constant that dictates the number of elements each block handles. The kernel reads a block of elements, applies the quantization formula, and writes the results back. The `quantize_global` function orchestrates this by preparing necessary inputs, launching the kernel with the appropriate grid size, and finally returning the quantized output along with the maximum absolute value `absmax`.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _quantize_global(x_ptr, absmax_inv_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the input block\n    mask = offsets < n_elements\n    x_block = tl.load(x_ptr + offsets, mask=mask)\n\n    # Load the inverse of the maximum absolute value\n    absmax_inv = tl.load(absmax_inv_ptr)\n\n    # Apply the quantization formula\n    quantized_block = tl.round(x_block * absmax_inv).to(tl.int8)\n\n    # Store the quantized block\n    tl.store(output_ptr + offsets, quantized_block, mask=mask)\n\nimport torch\nimport triton\nimport triton.runtime\n\ndef quantize_global(x: torch.Tensor):\n    # Ensure the input tensor is on the GPU\n    assert x.is_cuda, \"Input tensor must be on the GPU\"\n\n    # Compute the maximum absolute value\n    absmax = x.abs().max()\n    absmax_inv = 127.0 / absmax\n\n    # Allocate memory for the output tensor\n    output = torch.empty_like(x, dtype=torch.int8, device=x.device)\n\n    # Prepare the pointers\n    x_ptr = x.data_ptr()\n    absmax_inv_ptr = absmax_inv.data_ptr()\n    output_ptr = output.data_ptr()\n\n    # Define the grid size\n    n_elements = x.numel()\n    BLOCK_SIZE = 256\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    _quantize_global[grid_size, BLOCK_SIZE](x_ptr, absmax_inv_ptr, output_ptr, n_elements, BLOCK_SIZE)\n\n    return output, absmax\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# global quantize\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_stages=1),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef _quantize_global(\n    x_ptr,\n    absmax_inv_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    absmax_inv = tl.load(absmax_inv_ptr)\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (x * absmax_inv))\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef quantize_global(x: torch.Tensor):\n    absmax = x.abs().max().unsqueeze(0)\n    absmax_inv = 1.0 / absmax\n    output = torch.empty(*x.shape, device=\"cuda\", dtype=torch.int8)\n    assert x.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _quantize_global[grid](x, absmax_inv, output, n_elements)\n    return output, absmax\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `_l2_norm_fwd_1pass_kernel` is a Triton kernel function designed to compute the L2 normalization of input tensor `X`. It reads the data in blocks of size `BLOCK_N`, computes the variance, and applies the normalization. The output is stored in tensor `Y`. It takes several inputs including pointers to input and output data, stride for the row (`stride_x_row`), the number of columns (`N`), a small epsilon value (`eps`), and a block size (`BLOCK_N`). The function iterates over the rows using the program ID (`row`), loads the relevant section of `X`, calculates the L2 norm, and stores the result in `Y`.\n\n            The `_l2_norm_fwd` function serves as a wrapper for the kernel. It ensures that `x` is in a contiguous format if needed, prepares an output tensor `y`, and checks constraints on the maximum allowable block size. The function then launches the Triton kernel with appropriate arguments and returns the result in the original shape of `x`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X, Y, stride_x_row, N, eps, BLOCK_N: tl.constexpr\n):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n\n    # Load the relevant section of X\n    x_ptr = X + row * stride_x_row + cols\n    x = tl.load(x_ptr, mask=mask, other=0.0)\n\n    # Compute the L2 norm\n    x_squared = x * x\n    sum_x_squared = tl.sum(x_squared, axis=0)\n    norm = tl.sqrt(sum_x_squared + eps)\n\n    # Normalize the data\n    y = x / norm\n\n    # Store the result in Y\n    y_ptr = Y + row * stride_x_row + cols\n    tl.store(y_ptr, y, mask=mask)\n\nimport torch\nimport triton\nimport triton.language as tl\n\ndef _l2_norm_fwd(x: torch.Tensor, eps: float = 1e-6, BLOCK_N: int = 1024):\n    # Ensure x is contiguous\n    if not x.is_contiguous():\n        x = x.contiguous()\n\n    # Prepare output tensor\n    y = torch.empty_like(x)\n\n    # Check constraints on the maximum allowable block size\n    if BLOCK_N > triton.runtime.max_block_size():\n        raise ValueError(f\"Block size {BLOCK_N} exceeds the maximum allowable block size {triton.runtime.max_block_size()}\")\n\n    # Launch the Triton kernel\n    grid = (x.shape[0],)\n    _l2_norm_fwd_1pass_kernel[grid](\n        x, y, x.stride(1), x.shape[1], eps, BLOCK_N\n    )\n\n    return y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) \n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    y = x * rstd\n    tl.store(Y + cols, y, mask=mask)\n\ndef _l2_norm_fwd(\n    x, eps=1e-6\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, x.shape[-1])\n    if x.stride(-1) != 1:\n        x = x.contiguous()\n    y = torch.empty_like(x)\n    N = x.shape[-1]\n    M = x.shape[0]\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _l2_norm_fwd_1pass_kernel[(M,)](\n            x,\n            y,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return y.reshape(x_shape_og)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernels perform a specialized matrix multiplication with RMS normalization and optional rotary embeddings. The main kernel `rms_matmul_rbe` takes in input matrix pointers and performs the operation c = (rms(x) * rms_w) @ w, where rms denotes root mean square normalization. It supports operations in different precisions (FP16 and FP8) and has an optional rotary embeddings epilogue. The wrapper `rms_matmul_rbe_qkv_wrapper` prepares and launches kernels for processing Q, K, V matrices for attention mechanisms.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_matmul_rbe(\n    X, RMS_W, W, C, \n    X_row_stride, X_col_stride, \n    RMS_W_row_stride, RMS_W_col_stride, \n    W_row_stride, W_col_stride, \n    C_row_stride, C_col_stride, \n    M, N, K, \n    rotary_emb, \n    rotary_dim, \n    BLOCK_M: tl.constexpr, \n    BLOCK_N: tl.constexpr, \n    BLOCK_K: tl.constexpr, \n    PRECISION: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = num_pid_m * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * num_pid_m\n    group_size = num_pid_in_group\n    pid_m = first_pid_m + (pid % num_pid_m)\n    pid_n = (pid % group_size) // num_pid_m\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n\n    X = X + (rm[:, None] * X_row_stride + rk[None, :] * X_col_stride)\n    RMS_W = RMS_W + (rm[:, None] * RMS_W_row_stride + rk[None, :] * RMS_W_col_stride)\n    W = W + (rk[:, None] * W_row_stride + rn[None, :] * W_col_stride)\n    C = C + (rm[:, None] * C_row_stride + rn[None, :] * C_col_stride)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=PRECISION)\n\n    for k in range(0, K, BLOCK_K):\n        x = tl.load(X)\n        rms_w = tl.load(RMS_W)\n        w = tl.load(W)\n        \n        # RMS normalization\n        rms_x = tl.sqrt(tl.sum(x * x, axis=1) / K)\n        x = x / rms_x[:, None]\n        \n        # Apply rotary embeddings if enabled\n        if rotary_emb:\n            rotary_mask = tl.arange(0, BLOCK_K) < rotary_dim\n            x = tl.where(rotary_mask, x * tl.cos(rotary_emb) - x * tl.sin(rotary_emb), x)\n        \n        # Matrix multiplication\n        acc += tl.dot(x, w)\n\n        X += BLOCK_K * X_col_stride\n        RMS_W += BLOCK_K * RMS_W_col_stride\n        W += BLOCK_K * W_col_stride\n\n    tl.store(C, acc)\n\nimport torch\nimport triton\nimport triton.runtime as trt\n\ndef rms_matmul_rbe_qkv_wrapper(X, RMS_W, W, rotary_emb=None, rotary_dim=None):\n    M, K = X.shape\n    _, N = W.shape\n\n    # Allocate output tensor\n    C = torch.empty((M, N), dtype=X.dtype, device=X.device)\n\n    # Define block sizes\n    BLOCK_M = 16\n    BLOCK_N = 16\n    BLOCK_K = 16\n\n    # Define grid size\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n    # Launch kernel\n    rms_matmul_rbe[grid](\n        X, RMS_W, W, C, \n        X.stride(0), X.stride(1), \n        RMS_W.stride(0), RMS_W.stride(1), \n        W.stride(0), W.stride(1), \n        C.stride(0), C.stride(1), \n        M, N, K, \n        rotary_emb, \n        rotary_dim, \n        BLOCK_M, BLOCK_N, BLOCK_K, \n        PRECISION=tl.float16 if X.dtype == torch.float16 else tl.float32\n    )\n\n    return C\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rms_matmul_rbe(\n        x_ptr, w_ptr, rms_w_ptr, out_ptr,\n        M, N, K,\n        stride_x_batch, stride_x_m, stride_x_k,\n        stride_w_k, stride_w_n,\n        stride_rms_w,\n        stride_out_batch, stride_out_m, stride_out_n,\n        start_token_position,\n        USE_FP8: tl.constexpr,\n        RBE_EPILOGUE: tl.constexpr,\n        THETA: tl.constexpr,\n        EPS: tl.constexpr,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    Prologue: RMS\n    Epilogue: nothing or Rotary embeddings\n    c = ROBE((rms(a) * rms_w) @ b)\n    \"\"\"\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + offs_m[:, None] * stride_x_m + offs_k[None, :] * stride_x_k)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_w_k + offs_n[None, :] * stride_w_n)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n    x_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        x = tl.load(x_ptrs)\n        x_sum += tl.extra.cuda.libdevice.pow(x.to(tl.float32), 2)\n        rms_w = tl.load(rms_w_ptrs)  # TODO add an assert that rms_w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n            rms_w = rms_w.to(tl.float16)\n        x = x * rms_w\n        w = tl.load(w_ptrs)  # TODO add an assert that w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            w = w.to(tl.float8e5, bitcast=True)\n            w = w.to(tl.float32)\n            w = w.to(tl.float16)\n        accumulator += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x_k\n        w_ptrs += BLOCK_SIZE_K * stride_w_k\n        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n    x_mean = tl.sum(x_sum, axis=1) / K + EPS\n    x_norm = tl.math.rsqrt(x_mean)\n    accumulator = accumulator * x_norm[:, None]\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (\n                pid_batch * stride_out_batch + offs_m[:, None] * stride_out_m + offs_n[None, :] * stride_out_n)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\n\n@triton.jit\ndef rms_matmul_rbe_qkv(x_ptr,\n                       q_weight_ptr, k_weight_ptr, v_weight_ptr,\n                       rms_w_ptr,\n                       q_ptr, k_ptr, v_ptr,\n                       M, N, K,\n                       stride_x_batch, stride_x_m, stride_x_k,\n                       stride_q_w_k, stride_q_w_n,\n                       stride_k_w_k, stride_k_w_n,\n                       stride_v_w_k, stride_v_w_n,\n                       stride_rms_w,\n                       stride_q_batch, stride_q_m, stride_q_n,\n                       stride_k_batch, stride_k_m, stride_k_n,\n                       stride_v_batch, stride_v_m, stride_v_n,\n                       start_token_position,\n                       USE_FP8: tl.constexpr,\n                       THETA: tl.constexpr,\n                       EPS: tl.constexpr,\n                       BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    # q\n    rms_matmul_rbe(\n        x_ptr=x_ptr,\n        w_ptr=q_weight_ptr, rms_w_ptr=rms_w_ptr, out_ptr=q_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=stride_x_k,\n        stride_w_k=stride_q_w_k, stride_w_n=stride_q_w_n,\n        stride_rms_w=stride_rms_w,\n        stride_out_batch=stride_q_batch, stride_out_m=stride_q_m, stride_out_n=stride_q_n,\n        start_token_position=start_token_position,\n        USE_FP8=USE_FP8,\n        RBE_EPILOGUE=True,\n        THETA=THETA,\n        EPS=EPS,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    # k\n    rms_matmul_rbe(\n        x_ptr=x_ptr,\n        w_ptr=k_weight_ptr, rms_w_ptr=rms_w_ptr, out_ptr=k_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=stride_x_k,\n        stride_w_k=stride_k_w_k, stride_w_n=stride_k_w_n,\n        stride_rms_w=stride_rms_w,\n        stride_out_batch=stride_k_batch, stride_out_m=stride_k_m, stride_out_n=stride_k_n,\n        start_token_position=start_token_position,\n        USE_FP8=USE_FP8,\n        RBE_EPILOGUE=True,\n        THETA=THETA,\n        EPS=EPS,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    # v\n    rms_matmul_rbe(\n        x_ptr=x_ptr,\n        w_ptr=v_weight_ptr, rms_w_ptr=rms_w_ptr, out_ptr=v_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=stride_x_k,\n        stride_w_k=stride_v_w_k, stride_w_n=stride_v_w_n,\n        stride_rms_w=stride_rms_w,\n        stride_out_batch=stride_v_batch, stride_out_m=stride_v_m, stride_out_n=stride_v_n,\n        start_token_position=start_token_position,\n        USE_FP8=USE_FP8,\n        RBE_EPILOGUE=False,\n        THETA=THETA,\n        EPS=EPS,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n\ndef rms_matmul_rbe_qkv_wrapper(x: torch.Tensor,\n                               start_pos: int,\n                               q_weight: torch.Tensor, k_weight: torch.Tensor, v_weight: torch.Tensor,\n                               rms_w: torch.Tensor,\n                               n_heads: int, head_dim: int,\n                               k: torch.Tensor,\n                               v: torch.Tensor,\n                               eps: float = 1e-6, theta=10000.):\n    assert q_weight.shape == k_weight.shape == v_weight.shape\n    assert q_weight.dtype == k_weight.dtype == v_weight.dtype == rms_w.dtype\n    assert q_weight.dtype in [torch.float16, torch.int8]\n    batch, M, K = x.shape\n\n    assert K == rms_w.shape[0]\n\n    q_weight_t = q_weight.t()\n    k_weight_t = k_weight.t()\n    v_weight_t = v_weight.t()\n    K_W, N = q_weight_t.shape\n    assert K == K_W\n    q = torch.empty((batch, M, N), dtype=torch.float16, device=q_weight_t.device)\n\n    k = k.view((batch, M, N))\n    v = v.view((batch, M, N))\n    assert k.dtype == k_weight.dtype\n    assert v.dtype == v_weight.dtype\n\n    q_ptr = triton.reinterpret(q, tl.float16)\n    k_ptr = triton.reinterpret(k, tl.float8e5 if k.dtype == torch.int8 else tl.float16)\n    v_ptr = triton.reinterpret(v, tl.float8e5 if v.dtype == torch.int8 else tl.float16)\n\n    grid = lambda META: (\n    batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]))\n\n    rms_matmul_rbe_qkv[grid](\n        x_ptr=x,\n        q_weight_ptr=q_weight_t, k_weight_ptr=k_weight_t, v_weight_ptr=v_weight_t,\n        rms_w_ptr=rms_w,\n        q_ptr=q_ptr, k_ptr=k_ptr, v_ptr=v_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=x.stride(0), stride_x_m=x.stride(1), stride_x_k=x.stride(2),\n        stride_q_w_k=q_weight_t.stride(0), stride_q_w_n=q_weight_t.stride(1),\n        stride_k_w_k=k_weight_t.stride(0), stride_k_w_n=k_weight_t.stride(1),\n        stride_v_w_k=v_weight_t.stride(0), stride_v_w_n=v_weight_t.stride(1),\n        stride_rms_w=rms_w.stride(0),\n        stride_q_batch=q.stride(0), stride_q_m=q.stride(1), stride_q_n=q.stride(2),\n        stride_k_batch=k.stride(0), stride_k_m=k.stride(1), stride_k_n=k.stride(2),\n        stride_v_batch=v.stride(0), stride_v_m=v.stride(1), stride_v_n=v.stride(2),\n        start_token_position=start_pos,\n        USE_FP8=q_weight.dtype == torch.int8,\n        THETA=theta,\n        EPS=eps,\n        BLOCK_SIZE_M=16, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n        num_stages=4, num_warps=4\n    )\n    q = q.view(batch, M, n_heads, head_dim)\n    k = k.view(batch, M, n_heads, head_dim)\n    v = v.view(batch, M, n_heads, head_dim)\n    return q, k, v\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The code defines a Triton kernel for a fused recurrent hierarchical gated recurrent network (HGRN) and its PyTorch wrapper. The `fused_recurrent_hgrn_fwd_kernel` performs forward computation on input tensors `x` and `g`, with optional initial state `h0`, and stores the output in tensor `o`. It computes `o_t = g_t * h_t + x_t` iteratively over time `T`. The `fused_recurrent_hgrn_bwd_kernel` handles the backward pass, calculating gradients for `x` and `g`. The `FusedRecurrentHGRNFunction` is a PyTorch `Function` that wraps these kernels for autograd compatibility. The function `fused_recurrent_hgrn` provides an easy-to-use interface for the operation, taking tensors `x`, `g`, and an optional `initial_state`, and returning the result and optionally the final state.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(\n    X, G, H0, O,  # Pointers to the input and output tensors\n    T, B, H,  # Shape of the tensors\n    stride_xb, stride_xh,  # Strides for input tensor X\n    stride_gb, stride_gh,  # Strides for input tensor G\n    stride_hb, stride_hh,  # Strides for initial state H0\n    stride_ob, stride_oh,  # Strides for output tensor O\n    BLOCK_SIZE: tl.constexpr\n):\n    b = tl.program_id(0)\n    h = tl.program_id(1)\n    if b >= B or h >= H:\n        return\n\n    # Pointers for the initial state\n    h0_ptr = H0 + b * stride_hb + h * stride_hh\n\n    # Pointers for the input and output tensors\n    x_ptr = X + b * stride_xb + h * stride_xh\n    g_ptr = G + b * stride_gb + h * stride_gh\n    o_ptr = O + b * stride_ob + h * stride_oh\n\n    # Load the initial state\n    h_t = tl.load(h0_ptr)\n\n    for t in range(T):\n        # Load the input and gate values\n        x_t = tl.load(x_ptr + t * stride_xh)\n        g_t = tl.load(g_ptr + t * stride_gh)\n\n        # Compute the output\n        o_t = g_t * h_t + x_t\n\n        # Store the output\n        tl.store(o_ptr + t * stride_oh, o_t)\n\n        # Update the state\n        h_t = o_t\n\n@triton.jit\ndef fused_recurrent_hgrn_bwd_kernel(\n    dX, dG, O, dO,  # Pointers to the gradients and output tensors\n    T, B, H,  # Shape of the tensors\n    stride_xb, stride_xh,  # Strides for gradient tensor dX\n    stride_gb, stride_gh,  # Strides for gradient tensor dG\n    stride_ob, stride_oh,  # Strides for output tensor O\n    stride_do, stride_doh,  # Strides for gradient tensor dO\n    BLOCK_SIZE: tl.constexpr\n):\n    b = tl.program_id(0)\n    h = tl.program_id(1)\n    if b >= B or h >= H:\n        return\n\n    # Pointers for the input and output tensors\n    x_ptr = dX + b * stride_xb + h * stride_xh\n    g_ptr = dG + b * stride_gb + h * stride_gh\n    o_ptr = O + b * stride_ob + h * stride_oh\n    do_ptr = dO + b * stride_do + h * stride_doh\n\n    # Initialize the gradients\n    dX_t = 0.0\n    dG_t = 0.0\n    dO_t = 0.0\n\n    for t in range(T - 1, -1, -1):\n        # Load the output and gradient values\n        o_t = tl.load(o_ptr + t * stride_oh)\n        do_t = tl.load(do_ptr + t * stride_doh)\n\n        # Compute the gradients\n        dX_t = do_t\n        dG_t = do_t * o_t\n\n        # Store the gradients\n        tl.store(x_ptr + t * stride_xh, dX_t)\n        tl.store(g_ptr + t * stride_gh, dG_t)\n\nimport torch\nfrom torch.autograd import Function\n\nclass FusedRecurrentHGRNFunction(Function):\n    @staticmethod\n    def forward(ctx, x, g, initial_state=None):\n        B, T, H = x.shape\n        device = x.device\n\n        if initial_state is None:\n            initial_state = torch.zeros((B, H), device=device)\n\n        o = torch.empty((B, T, H), device=device)\n\n        # Launch the Triton kernel\n        grid = (B, H)\n        fused_recurrent_hgrn_fwd_kernel[grid](\n            x, g, initial_state, o,\n            T, B, H,\n            x.stride(0), x.stride(1),\n            g.stride(0), g.stride(1),\n            initial_state.stride(0), initial_state.stride(1),\n            o.stride(0), o.stride(1),\n            BLOCK_SIZE=1\n        )\n\n        ctx.save_for_backward(x, g, o)\n        return o\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, g, o = ctx.saved_tensors\n        B, T, H = x.shape\n\n        grad_x = torch.zeros_like(x)\n        grad_g = torch.zeros_like(g)\n\n        # Launch the Triton kernel\n        grid = (B, H)\n        fused_recurrent_hgrn_bwd_kernel[grid](\n            grad_x, grad_g, o, grad_output,\n            T, B, H,\n            grad_x.stride(0), grad_x.stride(1),\n            grad_g.stride(0), grad_g.stride(1),\n            o.stride(0), o.stride(1),\n            grad_output.stride(0), grad_output.stride(1),\n            BLOCK_SIZE=1\n        )\n\n        return grad_x, grad_g, None\n\ndef fused_recurrent_hgrn(x, g, initial_state=None, return_final_state=False):\n    o = FusedRecurrentHGRNFunction.apply(x, g, initial_state)\n    if return_final_state:\n        final_state = o[:, -1, :]\n        return o, final_state\n    return o\n\nimport torch\n\n# Example input tensors\nB, T, H = 2, 10, 5\nx = torch.randn((B, T, H), requires_grad=True)\ng = torch.randn((B, T, H), requires_grad=True)\n\n# Optional initial state\ninitial_state = torch.randn((B, H))\n\n# Forward pass\no, final_state = fused_recurrent_hgrn(x, g, initial_state, return_final_state=True)\n\n# Backward pass\nloss = o.sum()\nloss.backward()\n\nprint(\"Output:\", o)\nprint(\"Final State:\", final_state)\nprint(\"Gradient of x:\", x.grad)\nprint(\"Gradient of g:\", g.grad)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BD': 32}, num_warps=1),\n        triton.Config({'BD': 32}, num_warps=2),\n        triton.Config({'BD': 32}, num_warps=4),\n        triton.Config({'BD': 32}, num_warps=8),\n        triton.Config({'BD': 64}, num_warps=1),\n        triton.Config({'BD': 64}, num_warps=2),\n        triton.Config({'BD': 64}, num_warps=4),\n        triton.Config({'BD': 64}, num_warps=8),\n        triton.Config({'BD': 128}, num_warps=1),\n        triton.Config({'BD': 128}, num_warps=2),\n        triton.Config({'BD': 128}, num_warps=4),\n        triton.Config({'BD': 128}, num_warps=8),\n    ],\n    key=['D']\n)\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(\n    x,\n    g,\n    o,\n    h0,\n    ht,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_d, i_bh = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_x = x + i_bh * T * D + o_d\n    p_g = g + i_bh * T * D + o_d\n    p_o = o + i_bh * T * D + o_d\n\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_h = b_g * b_h + b_x\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)\n\n        p_x += D\n        p_g += D\n        p_o += D\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * D + o_d\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BD': 32}, num_warps=1),\n        triton.Config({'BD': 32}, num_warps=2),\n        triton.Config({'BD': 32}, num_warps=4),\n        triton.Config({'BD': 32}, num_warps=8),\n        triton.Config({'BD': 64}, num_warps=1),\n        triton.Config({'BD': 64}, num_warps=2),\n        triton.Config({'BD': 64}, num_warps=4),\n        triton.Config({'BD': 64}, num_warps=8),\n        triton.Config({'BD': 128}, num_warps=1),\n        triton.Config({'BD': 128}, num_warps=2),\n        triton.Config({'BD': 128}, num_warps=4),\n        triton.Config({'BD': 128}, num_warps=8),\n    ],\n    key=['D']\n)\n@triton.jit\ndef fused_recurrent_hgrn_bwd_kernel(\n    g,\n    o,\n    dx,\n    dg,\n    do,\n    h0,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr\n):\n    i_d, i_bh = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_g = g + (i_bh * T + T - 1) * D + o_d\n    p_o = o + (i_bh * T + T - 2) * D + o_d\n    p_dx = dx + (i_bh * T + T - 1) * D + o_d\n    p_dg = dg + (i_bh * T + T - 1) * D + o_d\n    p_do = do + (i_bh * T + T - 1) * D + o_d\n\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_bh * D + o_d, mask=mask, other=0).to(tl.float32)\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dg = b_dh * b_o\n        b_dh = b_dh * b_g\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)\n\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D\n\n\nclass FusedRecurrentHGRNFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, g, initial_state=None, output_final_state=False):\n        B, H, T, D = x.shape\n\n        final_state = None\n        if output_final_state:\n            final_state = x.new_empty(B, H, D)\n\n        o = torch.empty_like(x)\n        def grid(meta): return (triton.cdiv(D, meta['BD']), B * H)\n        fused_recurrent_hgrn_fwd_kernel[grid](\n            x, g, o, initial_state, final_state,\n            T, D,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None\n        )\n        ctx.save_for_backward(g, o, initial_state)\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, dht=None):\n        g, o, initial_state = ctx.saved_tensors\n        B, H, T, D = do.shape\n\n        dx = torch.empty_like(o)\n        dg = torch.empty_like(g)\n        def grid(meta): return (triton.cdiv(D, meta['BD']), B * H)\n        fused_recurrent_hgrn_bwd_kernel[grid](\n            g, o, dx, dg, do, initial_state,\n            T, D,\n            USE_INITIAL_STATE=initial_state is not None,\n        )\n\n        return dx, dg, None, None\n\n\ndef fused_recurrent_hgrn(\n    x: torch.Tensor,\n    g: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if initial_state is not None:\n        initial_state = initial_state.detach()\n    o, final_state = FusedRecurrentHGRNFunction.apply(x, g, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThis Triton code defines a custom softmax operation for GPU acceleration using Triton language. There are forward and backward kernel functions for both 'inner' and 'non-inner' cases based on dimensions.\n\nKey functions:\n- `softmax_kernel_non_inner`: Performs softmax for non-inner dimensions. It computes the softmax on selected tiles and is optimized by heuristically determined tile sizes (TILE_K, TILE_N) and the number of warps.\n- `softmax_kernel_inner`: Optimized for inner dimensions, handling data directly within each CTA (cooperative thread array). Operates on M (outer dimension) and N (inner dimension) sizes.\n- `softmax_backward_kernel_non_inner` & `softmax_backward_kernel_inner`: These functions compute gradients for the forward pass, again considering different dimensions.\n\nHelper functions (`heur_`) compute optimal configurations for execution based on input matrix dimensions M, N, K, and return TILE sizes or CTA (cooperative thread array) strategies.\n\nThe `Softmax` class wraps these kernels in an autograd-compatible format for use in PyTorch with a static forward and backward method. The forward method computes the softmax and saves the result for backward calculations.\n\nMain input parameters:\n- `output_ptr`, `input_ptr`: Device pointers for reading/writing data.\n- `M`, `N`, `K`: Dimensions for data partitioning.\n- `TILE_N`, `TILE_K`, `ONE_TILE_PER_CTA`: Compile-time constants determined by heuristics.\n\nThe softmax operation is efficiently performed by splitting the task into parallel segments processed by Triton kernels, benefiting from GPU parallelism.\n\n", "predict": "import triton\nimport triton.language as tl\n\n# Helper functions to determine optimal configurations\ndef heur_non_inner(M, N, K):\n    TILE_N = 128\n    TILE_K = 32\n    return TILE_N, TILE_K\n\ndef heur_inner(M, N, K):\n    ONE_TILE_PER_CTA = True\n    return ONE_TILE_PER_CTA\n\n# Forward Kernels\n@triton.jit\ndef softmax_kernel_non_inner(output_ptr, input_ptr, M, N, K, TILE_N: tl.constexpr, TILE_K: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_tiles = (M + TILE_N - 1) // TILE_N\n    tile_id = pid % num_tiles\n    block_id = pid // num_tiles\n\n    tile_start = tile_id * TILE_N\n    tile_end = min(tile_start + TILE_N, M)\n\n    offset = block_id * K\n    row_start = tile_start * N\n    row_end = tile_end * N\n\n    for i in range(tile_start, tile_end):\n        row = i * N\n        max_val = -float('inf')\n        for j in range(N):\n            max_val = max(max_val, tl.load(input_ptr + row + j + offset))\n\n        exp_sum = 0.0\n        for j in range(N):\n            exp_val = tl.exp(tl.load(input_ptr + row + j + offset) - max_val)\n            exp_sum += exp_val\n            tl.store(output_ptr + row + j + offset, exp_val)\n\n        for j in range(N):\n            tl.store(output_ptr + row + j + offset, tl.load(output_ptr + row + j + offset) / exp_sum)\n\n@triton.jit\ndef softmax_kernel_inner(output_ptr, input_ptr, M, N, K, ONE_TILE_PER_CTA: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_warps = 4\n    warp_id = pid % num_warps\n    block_id = pid // num_warps\n\n    tile_start = block_id * N\n    tile_end = (block_id + 1) * N\n\n    for i in range(M):\n        row = i * N\n        max_val = -float('inf')\n        for j in range(tile_start, tile_end):\n            max_val = max(max_val, tl.load(input_ptr + row + j))\n\n        exp_sum = 0.0\n        for j in range(tile_start, tile_end):\n            exp_val = tl.exp(tl.load(input_ptr + row + j) - max_val)\n            exp_sum += exp_val\n            tl.store(output_ptr + row + j, exp_val)\n\n        for j in range(tile_start, tile_end):\n            tl.store(output_ptr + row + j, tl.load(output_ptr + row + j) / exp_sum)\n\n# Backward Kernels\n@triton.jit\ndef softmax_backward_kernel_non_inner(grad_input_ptr, grad_output_ptr, output_ptr, input_ptr, M, N, K, TILE_N: tl.constexpr, TILE_K: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_tiles = (M + TILE_N - 1) // TILE_N\n    tile_id = pid % num_tiles\n    block_id = pid // num_tiles\n\n    tile_start = tile_id * TILE_N\n    tile_end = min(tile_start + TILE_N, M)\n\n    offset = block_id * K\n    row_start = tile_start * N\n    row_end = tile_end * N\n\n    for i in range(tile_start, tile_end):\n        row = i * N\n        for j in range(N):\n            output_val = tl.load(output_ptr + row + j + offset)\n            grad_output_val = tl.load(grad_output_ptr + row + j + offset)\n            grad_input_val = grad_output_val * output_val * (1 - output_val)\n            tl.store(grad_input_ptr + row + j + offset, grad_input_val)\n\n@triton.jit\ndef softmax_backward_kernel_inner(grad_input_ptr, grad_output_ptr, output_ptr, input_ptr, M, N, K, ONE_TILE_PER_CTA: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_warps = 4\n    warp_id = pid % num_warps\n    block_id = pid // num_warps\n\n    tile_start = block_id * N\n    tile_end = (block_id + 1) * N\n\n    for i in range(M):\n        row = i * N\n        for j in range(tile_start, tile_end):\n            output_val = tl.load(output_ptr + row + j)\n            grad_output_val = tl.load(grad_output_ptr + row + j)\n            grad_input_val = grad_output_val * output_val * (1 - output_val)\n            tl.store(grad_input_ptr + row + j, grad_input_val)\n\n### PyTorch Wrapper\n\nimport torch\nfrom torch.autograd import Function\n\nclass Softmax(Function):\n    @staticmethod\n    def forward(ctx, input):\n        M, N, K = input.shape\n        output = torch.empty_like(input, device=input.device)\n        TILE_N, TILE_K = heur_non_inner(M, N, K)\n        ONE_TILE_PER_CTA = heur_inner(M, N, K)\n\n        if K == 1:\n            softmax_kernel_inner[lambda M, N, K: (M * N // 128,)](output, input, M, N, K, ONE_TILE_PER_CTA)\n        else:\n            softmax_kernel_non_inner[lambda M, N, K: (M * K // 128,)](output, input, M, N, K, TILE_N, TILE_K)\n\n        ctx.save_for_backward(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output, = ctx.saved_tensors\n        grad_input = torch.empty_like(grad_output, device=grad_output.device)\n        M, N, K = grad_output.shape\n        TILE_N, TILE_K = heur_non_inner(M, N, K)\n        ONE_TILE_PER_CTA = heur_inner(M, N, K)\n\n        if K == 1:\n            softmax_backward_kernel_inner[lambda M, N, K: (M * N // 128,)](grad_input, grad_output, output, grad_output, M, N, K, ONE_TILE_PER_CTA)\n        else:\n            softmax_backward_kernel_non_inner[lambda M, N, K: (M * K // 128,)](grad_input, grad_output, output, grad_output, M, N, K, TILE_N, TILE_K)\n\n        return grad_input\n\n# Example usage\nif __name__ == \"__main__\":\n    input_tensor = torch.randn(1024, 1024, 1, device='cuda')\n    output_tensor = Softmax.apply(input_tensor)\n    print(output_tensor)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# from ..utils import libentry\n\nMAX_TILE_K = 8192\nNUM_SMS = torch.cuda.get_device_properties(\n    torch.cuda.current_device()\n).multi_processor_count\n\n\ndef heur_tile_k(args):\n    tile_k = 1\n    upper_bound = min(args[\"K\"], MAX_TILE_K)\n    while tile_k <= upper_bound:\n        num_blocks = args[\"M\"] * triton.cdiv(args[\"K\"], tile_k)\n        num_waves = num_blocks / NUM_SMS\n        if (num_waves > 1) and (tile_k * 2 <= upper_bound):\n            tile_k *= 2\n        else:\n            break\n    return tile_k\n\n\ndef heur_tile_n_non_inner(args):\n    return triton.cdiv(8192, args[\"TILE_K\"])\n\n\ndef heur_one_tile_per_cta(args):\n    return args[\"TILE_N\"] >= args[\"N\"]\n\n\ndef heur_num_warps_non_inner(args):\n    tile_size = args[\"TILE_N\"] * args[\"TILE_K\"]\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n\n\n@triton.heuristics(\n    {\n        \"TILE_K\": heur_tile_k,\n        \"TILE_N\": heur_tile_n_non_inner,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n        \"num_warps\": heur_num_warps_non_inner,\n    }\n)\n@triton.jit\ndef softmax_kernel_non_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_k = tl.program_id(1)\n    pid_m = tl.program_id(0)\n\n    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n        mask = (n_offsets[:, None] < N) & (k_offsets < K)\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m[None, :])\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N, TILE_K], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N, TILE_K], value=0.0, dtype=tl.float32)\n\n        # specialization does not improve performance inn this example, as tested\n        for start_n in range(0, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            alpha = tl.exp(m - m_new)\n            z = z * alpha + tl.exp(inp - m_new)\n            m = m_new\n\n        m_reduced = tl.max(m, 0)  # (TILE_K,)\n        z = tl.sum(z * tl.exp(m - m_reduced[None, :]), 0)  # (TILE_K, )\n        m = m_reduced\n\n        # specialization does not improve performance inn this example, as tested\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets[None, :] < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            o = tl.exp(inp - m[None, :]) / z[None, :]\n            tl.store(output_ptr + offsets, o, mask=mask)\n\n\n@triton.jit\ndef next_multiple_of(a, b):\n    # the smallest x>=a that x%b ==0\n    return tl.cidv(a, b) * b\n\n\n@triton.jit\ndef prev_multiple_of(a, b):\n    # the largest x<a that x%b ==0\n    return tl.cdiv(a, b) * b - b\n\n\ndef heur_tile_n_inner(args):\n    if args[\"N\"] <= (32 * 1024):\n        return triton.next_power_of_2(args[\"N\"])\n    else:\n        return 4096\n\n\ndef heur_num_warps_inner(args):\n    tile_size = args[\"TILE_N\"]\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n\n\n@triton.heuristics(\n    {\n        \"TILE_N\": heur_tile_n_inner,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n        \"num_warps\": heur_num_warps_inner,\n    }\n)\n@triton.jit\ndef softmax_kernel_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(\n            output_ptr.dtype.element_ty\n        )\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m)\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N], value=0.0, dtype=tl.float32)\n        input_ptr += pid_m * N\n        output_ptr += pid_m * N\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, previous_multiple, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets)\n            m_new = tl.maximum(m, inp)\n            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)\n            m = m_new\n        # specialize the last iteration\n        for start_n in range(previous_multiple, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(input_ptr + n_offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)\n            m = m_new\n\n        m_reduced = tl.max(m, 0)\n        z = tl.sum(z * tl.exp(m - m_reduced), 0)\n        m = m_reduced\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        # specialize the first iteration\n        for start_n in range(0, TILE_N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(\n                input_ptr + n_offsets,\n                mask=mask,\n                other=-float(\"inf\"),\n                eviction_policy=\"evict_first\",\n            )\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o, mask=mask)\n        for start_n in range(TILE_N, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets, eviction_policy=\"evict_first\")\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o)\n\n\ndef heur_tile_n_bwd_non_inner(args):\n    return max(1, 1024 // args[\"TILE_K\"])\n\n\n# ------------------------  backward -------------------------------\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"TILE_K\": 32}),\n        triton.Config({\"TILE_K\": 64}),\n        triton.Config({\"TILE_K\": 128}),\n        triton.Config({\"TILE_K\": 256}),\n        triton.Config({\"TILE_K\": 512}),\n        triton.Config({\"TILE_K\": 1024}),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n        \"K\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"TILE_N\": heur_tile_n_bwd_non_inner,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n    },\n)\n@triton.jit\ndef softmax_backward_kernel_non_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    offsets_k = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        mask = (offsets_n < N)[:, None] & (offsets_k < K)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, axis=0)\n        in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        scale = tl.zeros([TILE_N, TILE_K], dtype=tl.float32)\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            offsets_n += TILE_N\n            offsets += TILE_N * K\n        scale = tl.sum(scale, axis=0)  # (TILE_K)\n\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            offsets_n += TILE_N\n            offsets += TILE_N * K\n\n\ndef heru_tile_m(args):\n    return max(1, 1024 // args[\"TILE_N\"])\n\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"TILE_N\": 32}),\n        triton.Config({\"TILE_N\": 64}),\n        triton.Config({\"TILE_N\": 128}),\n        triton.Config({\"TILE_N\": 256}),\n        triton.Config({\"TILE_N\": 512}),\n        triton.Config({\"TILE_N\": 1024}),\n    ],\n    key=[\"M\", \"N\"],\n)\n@triton.heuristics(\n    values={\n        \"TILE_M\": heru_tile_m,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n    },\n)\n@triton.jit\ndef softmax_backward_kernel_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    m_offsets = pid_m * TILE_M + tl.arange(0, TILE_M)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        mask = (m_offsets[:, None] < M) & (n_offsets < N)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, 1)\n        in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        scale = tl.zeros([TILE_M, TILE_N], dtype=tl.float32)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_last\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            n_offsets += TILE_N\n            offsets += TILE_N\n        scale = tl.sum(scale, 1)  # (TILE_M,)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_first\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            n_offsets += TILE_N\n            offsets += TILE_N\n\n\nclass Softmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, dim, dtype):\n        logging.debug(\"GEMS SOFTMAX\")\n\n        assert dim >= -x.ndim and dim < x.ndim, \"Invalid dim\"\n        dim = dim % x.ndim\n        M = 1\n        N = x.shape[dim]\n        for i in range(dim):\n            M *= x.shape[i]  # pre_dim\n        inp = x.contiguous()\n        if dtype is None:\n            dtype = x.dtype\n        out = torch.empty_like(inp, dtype=dtype)\n        K = inp.numel() // M // N  # post_dim\n\n        with torch.cuda.device(inp.device):\n            if K > 1:\n                grid = lambda meta: (M, triton.cdiv(K, meta[\"TILE_K\"]), 1)\n                softmax_kernel_non_inner[grid](\n                    out,\n                    inp,\n                    M,\n                    N,\n                    K,\n                )\n            else:\n                grid = (M, 1, 1)\n                softmax_kernel_inner[grid](\n                    out,\n                    inp,\n                    M,\n                    N,\n                )\n        ctx.save_for_backward(out)\n        ctx.dim = dim\n        return out\n\n    @staticmethod\n    def backward(ctx, out_grad):\n        logging.debug(\"GEMS SOFTMAX VJP\")\n        dim = ctx.dim\n        (out,) = ctx.saved_tensors\n\n        assert dim >= -out.ndim and dim < out.ndim, \"Invalid dim\"\n        dim = dim % out.ndim\n        M = 1\n        N = out.shape[dim]\n        for i in range(dim):\n            M *= out.shape[i]\n\n        out_grad = out_grad.contiguous()\n        in_grad = torch.empty_like(out)\n        K = out.numel() // M // N\n\n        with torch.cuda.device(in_grad.device):\n            if K > 1:\n                grid = lambda meta: (M, triton.cdiv(K, meta[\"TILE_K\"]), 1)\n                softmax_backward_kernel_non_inner[grid](\n                    out,\n                    out_grad,\n                    in_grad,\n                    M,\n                    N,\n                    K,\n                )\n            else:\n                grid = lambda meta: (triton.cdiv(M, meta[\"TILE_M\"]), 1, 1)\n                softmax_backward_kernel_inner[grid](\n                    out,\n                    out_grad,\n                    in_grad,\n                    M,\n                    N,\n                )\n        return in_grad, None, None\n\n\ndef softmax(x, dim=-1, dtype=None):\n    return Softmax.apply(x, dim, dtype)\n\n\n\n\n"}
