{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton code implements a custom attention mechanism with forward and backward kernels for computing gradients. There are two main functions, `LightningAttention2NoDecay.forward` for the forward pass and `LightningAttention2NoDecay.backward` for the backward pass. In the forward pass, it computes the output using queries (Q), keys (K), and values (V) tensors. The backward pass computes gradients with respect to Q, K, and V.\n\n            The `_fwd_kernel` function computes the attention output using block processing. It loads blocks of Q, K, and V, computes the QK product, and then computes the output using matrix multiplication.\n\n            The `_bwd_intra_kernel` function calculates gradients within each block by backpropagating the error from the output to the inputs. It updates gradients of Q, K, and V.\n\n            The `_bwd_inter_kernel` handles inter-block computations for gradients. It iterates over blocks to compute and accumulate gradients for K and V.\n\n            The block size for forward and backward computations is 64, while the compute block size (CBLOCK) in the backward intra part is 32.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Forward kernel\n@triton.jit\ndef _fwd_kernel(Q_ptr, K_ptr, V_ptr, Out_ptr, stride_q, stride_k, stride_v, stride_out, n_heads, head_dim, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the starting position of each block\n    q_start = pid * BLOCK_SIZE\n    k_start = 0\n    \n    # Load Q and K blocks\n    Q = tl.load(Q_ptr + q_start * stride_q + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n    K = tl.load(K_ptr + k_start * stride_k + tl.arange(0, BLOCK_SIZE)[None, :] * head_dim)\n    \n    # Compute QK^T\n    QK = tl.dot(Q, K)\n    \n    # Softmax along the last dimension\n    QK = tl.softmax(QK, axis=-1)\n    \n    # Load V block\n    V = tl.load(V_ptr + k_start * stride_v + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n    \n    # Compute the output\n    Out = tl.dot(QK, V)\n    \n    # Store the result\n    tl.store(Out_ptr + q_start * stride_out + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim, Out)\n\n# Backward intra-block kernel\n@triton.jit\ndef _bwd_intra_kernel(GradOut_ptr, Q_ptr, K_ptr, V_ptr, GradQ_ptr, GradK_ptr, GradV_ptr, stride_gradout, stride_q, stride_k, stride_v, stride_gradq, stride_gradk, stride_gradv, n_heads, head_dim, BLOCK_SIZE: tl.constexpr, CBLOCK: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the starting position of each block\n    q_start = pid * BLOCK_SIZE\n    k_start = 0\n    \n    # Load GradOut, Q, K, V blocks\n    GradOut = tl.load(GradOut_ptr + q_start * stride_gradout + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n    Q = tl.load(Q_ptr + q_start * stride_q + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n    K = tl.load(K_ptr + k_start * stride_k + tl.arange(0, BLOCK_SIZE)[None, :] * head_dim)\n    V = tl.load(V_ptr + k_start * stride_v + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n    \n    # Compute gradients\n    GradQ = tl.dot(GradOut, V.T)\n    GradK = tl.dot(Q.T, GradOut)\n    GradV = tl.dot(GradOut.T, Q)\n    \n    # Store gradients\n    tl.store(GradQ_ptr + q_start * stride_gradq + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim, GradQ)\n    tl.store(GradK_ptr + k_start * stride_gradk + tl.arange(0, BLOCK_SIZE)[None, :] * head_dim, GradK)\n    tl.store(GradV_ptr + k_start * stride_gradv + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim, GradV)\n\n# Backward inter-block kernel\n@triton.jit\ndef _bwd_inter_kernel(GradOut_ptr, Q_ptr, K_ptr, V_ptr, GradK_ptr, GradV_ptr, stride_gradout, stride_q, stride_k, stride_v, stride_gradk, stride_gradv, n_heads, head_dim, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    \n    # Iterate over blocks to accumulate gradients\n    for k_start in range(0, head_dim, BLOCK_SIZE):\n        # Load blocks\n        GradOut = tl.load(GradOut_ptr + pid * stride_gradout + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n        Q = tl.load(Q_ptr + pid * stride_q + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n        K = tl.load(K_ptr + k_start * stride_k + tl.arange(0, BLOCK_SIZE)[None, :] * head_dim)\n        V = tl.load(V_ptr + k_start * stride_v + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim)\n        \n        # Compute and accumulate gradients\n        GradK = tl.dot(Q.T, GradOut)\n        GradV = tl.dot(GradOut.T, Q)\n        \n        # Accumulate gradients\n        tl.atomic_add(GradK_ptr + k_start * stride_gradk + tl.arange(0, BLOCK_SIZE)[None, :] * head_dim, GradK)\n        tl.atomic_add(GradV_ptr + k_start * stride_gradv + tl.arange(0, BLOCK_SIZE)[:, None] * head_dim, GradV)\n\n# Wrapper class\nclass LightningAttention2NoDecay:\n    @staticmethod\n    def forward(Q, K, V):\n        # Define output tensor\n        Out = torch.empty_like(Q)\n        \n        # Define grid size\n        grid = (Q.shape[0] // 64, )\n        \n        # Launch forward kernel\n        _fwd_kernel[grid](Q, K, V, Out, Q.stride(0), K.stride(0), V.stride(0), Out.stride(0), Q.shape[1], Q.shape[2], BLOCK_SIZE=64)\n        \n        return Out\n\n    @staticmethod\n    def backward(GradOut, Q, K, V):\n        # Define gradient tensors\n        GradQ = torch.empty_like(Q)\n        GradK = torch.empty_like(K)\n        GradV = torch.empty_like(V)\n        \n        # Define grid size\n        grid = (Q.shape[0] // 64, )\n        \n        # Launch backward intra-block kernel\n        _bwd_intra_kernel[grid](GradOut, Q, K, V, GradQ, GradK, GradV, GradOut.stride(0), Q.stride(0), K.stride(0), V.stride(0), GradQ.stride(0), GradK.stride(0), GradV.stride(0), Q.shape[1], Q.shape[2], BLOCK_SIZE=64, CBLOCK=32)\n        \n        # Launch backward inter-block kernel\n        _bwd_inter_kernel[grid](GradOut, Q, K, V, GradK, GradV, GradOut.stride(0), Q.stride(0), K.stride(0), V.stride(0), GradK.stride(0), GradV.stride(0), Q.shape[1], Q.shape[2], BLOCK_SIZE=64)\n        \n        return GradQ, GradK, GradV\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    Out,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    BLOCK_MODEL: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_bh % h\n    off_e = tl.program_id(1)\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    # channel offset\n    e_offset = off_e * BLOCK_MODEL\n\n    ##### get block ptr\n    Q_block_ptr = Q + qk_offset + tl.arange(0, d)[None, :]\n    K_trans_block_ptr = K + qk_offset + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    O_block_ptr = Out + o_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n\n    ##### init diag decay(Lambda); q, k decay; kv\n    # q, k decay\n    off_block = tl.arange(\n        0, BLOCK\n    )  # Not bug, this is a bit different from algorithm 1, but is mathematically equivalent\n    # diag decay\n    index = off_block[:, None] - off_block[None, :]\n    kv = tl.zeros([d, BLOCK_MODEL], dtype=tl.float32)\n\n    ##### compute\n    for i in range(NUM_BLOCK):\n        # load\n        q = tl.load(\n            Q_block_ptr + off_block[:, None] * d, mask=off_block[:, None] < n, other=0.0\n        ).to(tl.float32)\n        k_trans = tl.load(\n            K_trans_block_ptr + off_block[None, :] * d,\n            mask=off_block[None, :] < n,\n            other=0.0,\n        ).to(tl.float32)\n        v = tl.load(\n            V_block_ptr + off_block[:, None] * e, mask=off_block[:, None] < n, other=0.0\n        ).to(tl.float32)\n\n        # compute\n        qk = tl.dot(q, k_trans)\n        qk = tl.where(index >= 0, qk, 0)\n        o_intra = tl.dot(qk, v)\n        o_inter = tl.dot(q, kv)\n        o = o_intra + o_inter\n\n        # save and update\n        tl.store(\n            O_block_ptr + off_block[:, None] * e,\n            o.to(O_block_ptr.dtype.element_ty),\n            mask=off_block[:, None] < n,\n        )\n        kv += tl.dot(k_trans, v)\n        off_block += BLOCK\n\n\n@triton.jit\ndef _bwd_intra_kernel(\n    Q,\n    K,\n    V,\n    DO,\n    DQ,\n    DK,\n    DV,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    CBLOCK: tl.constexpr,\n    NUM_CBLOCK: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK + tl.arange(0, BLOCK)\n\n    ##### get block ptr\n    Q_trans_block_ptr = (\n        Q + qk_offset + block_offset[None, :] * d + tl.arange(0, d)[:, None]\n    )\n    K_block_ptr = K + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = (\n        V + v_offset + block_offset[None, :] * e + tl.arange(0, e)[:, None]\n    )\n\n    DQ_block_ptr = DQ + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[None, :]\n    DK_trans_block_ptr = (\n        DK + qk_offset + block_offset[None, :] * d + tl.arange(0, d)[:, None]\n    )\n    DV_block_ptr = DV + v_offset + block_offset[:, None] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + block_offset[:, None] * e + tl.arange(0, e)[None, :]\n\n    ##### init diag decay(Lambda)\n    array = tl.arange(0, BLOCK).to(tl.float32)\n    # diag\n    index = array[:, None] - array[None, :]\n\n    ##### load block\n    k = tl.load(K_block_ptr, mask=block_offset[:, None] < n, other=0.0).to(tl.float32)\n    v_trans = tl.load(V_trans_block_ptr, mask=block_offset[None, :] < n, other=0.0).to(\n        tl.float32\n    )\n    do = tl.load(DO_block_ptr, mask=block_offset[:, None] < n, other=0.0).to(tl.float32)\n    q_trans = tl.load(Q_trans_block_ptr, mask=block_offset[None, :] < n, other=0.0).to(\n        tl.float32\n    )\n\n    ##### compute\n    dqk = tl.dot(do, v_trans)\n    dqk = tl.where(index >= 0, dqk, 0)\n    dq_intra = tl.dot(dqk, k)\n\n    dk_intra_trans = tl.dot(q_trans, dqk)\n\n    qk_trans = tl.dot(k, q_trans)\n    qk_trans = tl.where(index <= 0, qk_trans, 0)\n    dv_intra = tl.dot(qk_trans, do)\n\n    dq = dq_intra\n    dk_trans = dk_intra_trans\n    dv = dv_intra\n\n    # save\n    tl.store(\n        DQ_block_ptr,\n        dq.to(DQ_block_ptr.dtype.element_ty),\n        mask=block_offset[:, None] < n,\n    )\n    tl.store(\n        DK_trans_block_ptr,\n        dk_trans.to(DK_trans_block_ptr.dtype.element_ty),\n        mask=block_offset[None, :] < n,\n    )\n    tl.store(\n        DV_block_ptr,\n        dv.to(DV_block_ptr.dtype.element_ty),\n        mask=block_offset[:, None] < n,\n    )\n\n\n@triton.jit\ndef _bwd_inter_kernel(\n    Q,\n    K,\n    V,\n    DO,\n    DQ,\n    DK,\n    DV,\n    b: tl.constexpr,\n    h: tl.constexpr,\n    n: tl.constexpr,\n    d: tl.constexpr,\n    e: tl.constexpr,\n    BLOCK: tl.constexpr,\n    NUM_BLOCK: tl.constexpr,\n    CBLOCK: tl.constexpr,\n    NUM_CBLOCK: tl.constexpr,\n):\n    ##### get offset\n    off_bh = tl.program_id(0)\n    off_bh % h\n\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n\n    ##### get block ptr\n    DQ_block_ptr = (\n        DQ + qk_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :]\n    )\n    K_block_ptr = (\n        K + qk_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :]\n    )\n    V_trans_block_ptr = (\n        V + v_offset + tl.arange(0, CBLOCK)[None, :] * e + tl.arange(0, e)[:, None]\n    )\n    DO_block_ptr = (\n        DO + o_offset + tl.arange(0, CBLOCK)[:, None] * e + tl.arange(0, e)[None, :]\n    )\n    # mask\n    off_block1 = tl.arange(0, CBLOCK)\n    off_block2 = tl.arange(0, CBLOCK)\n\n    ##### init lambda; kv\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n\n    ##### compute dq inter\n    for i in range(NUM_BLOCK):\n        # compute in subblock\n        for j in range(NUM_CBLOCK):\n            if i > 0:  # if not add this, may have bug\n                do = tl.load(DO_block_ptr, mask=off_block1[:, None] < n, other=0.0).to(\n                    tl.float32\n                )\n                dq_inter = tl.dot(do, kv_trans)\n                dq = dq_inter + tl.load(\n                    DQ_block_ptr, mask=off_block1[:, None] < n, other=0.0\n                )\n                tl.store(\n                    DQ_block_ptr,\n                    dq.to(DQ_block_ptr.dtype.element_ty),\n                    mask=off_block1[:, None] < n,\n                )\n\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n            off_block1 += CBLOCK\n\n        # update kv in subblock\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(\n                V_trans_block_ptr, mask=off_block2[None, :] < n, other=0.0\n            ).to(tl.float32)\n            k = tl.load(K_block_ptr, mask=off_block2[:, None] < n, other=0.0).to(\n                tl.float32\n            )\n            kv_trans_current += tl.dot(v_trans, k)\n\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n            off_block2 += CBLOCK\n\n        kv_trans += kv_trans_current\n\n    ##### get block ptr\n    m = NUM_BLOCK * BLOCK\n    off_block1 = m + tl.arange(0, CBLOCK)\n    off_block2 = m + tl.arange(0, CBLOCK)\n\n    Q_trans_block_ptr = (\n        Q\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[None, :] * d\n        + tl.arange(0, d)[:, None]\n    )\n    K_block_ptr = (\n        K\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[:, None] * d\n        + tl.arange(0, d)[None, :]\n    )\n    V_trans_block_ptr = (\n        V\n        + v_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[None, :] * e\n        + tl.arange(0, e)[:, None]\n    )\n\n    DK_trans_block_ptr = (\n        DK\n        + qk_offset\n        + m * d\n        + tl.arange(0, CBLOCK)[None, :] * d\n        + tl.arange(0, d)[:, None]\n    )\n    DV_block_ptr = (\n        DV\n        + v_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[:, None] * e\n        + tl.arange(0, e)[None, :]\n    )\n    DO_block_ptr = (\n        DO\n        + o_offset\n        + m * e\n        + tl.arange(0, CBLOCK)[:, None] * e\n        + tl.arange(0, e)[None, :]\n    )\n\n    ##### init dkv\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n\n    ##### compute dk, dv inter\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        # compute in subblock\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            off_block1 -= CBLOCK\n\n            if i < NUM_BLOCK - 1:  # if not add this, may have bug\n                k = tl.load(K_block_ptr, mask=off_block1[:, None] < n, other=0.0).to(\n                    tl.float32\n                )\n                v_trans = tl.load(\n                    V_trans_block_ptr, mask=off_block1[None, :] < n, other=0.0\n                ).to(tl.float32)\n\n                dk_inter_trans = tl.dot(dkv, v_trans)\n                dv_inter = tl.dot(k, dkv)\n\n                dk_trans = dk_inter_trans + tl.load(\n                    DK_trans_block_ptr, mask=off_block1[None, :] < n, other=0.0\n                )\n                dv = dv_inter + tl.load(\n                    DV_block_ptr, mask=off_block1[:, None] < n, other=0.0\n                )\n\n                tl.store(\n                    DK_trans_block_ptr,\n                    dk_trans.to(DK_trans_block_ptr.dtype.element_ty),\n                    mask=off_block1[None, :] < n,\n                )\n                tl.store(\n                    DV_block_ptr,\n                    dv.to(DV_block_ptr.dtype.element_ty),\n                    mask=off_block1[:, None] < n,\n                )\n\n        # update dkv in subblock\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            off_block2 -= CBLOCK\n\n            do = tl.load(DO_block_ptr, mask=off_block2[:, None] < n, other=0.0).to(\n                tl.float32\n            )\n            q_trans = tl.load(\n                Q_trans_block_ptr, mask=off_block2[None, :] < n, other=0.0\n            ).to(tl.float32)\n            dkv_current += tl.dot(q_trans, do)\n\n        dkv += dkv_current\n\n\nclass LightningAttention2NoDecay(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v):\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n\n        b, h, n, d = q.shape\n        e = v.shape[-1]\n        o = torch.empty((b, h, n, e), dtype=q.dtype, device=q.device)\n\n        BLOCK = 64\n        NUM_BLOCK = triton.cdiv(q.shape[2], BLOCK)\n        # parallel over channel\n        BLOCK_MODEL = min(triton.next_power_of_2(e), 32)\n        grid = (b * h, triton.cdiv(e, BLOCK_MODEL))\n\n        _fwd_kernel[grid](\n            q,\n            k,\n            v,\n            o,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            BLOCK_MODEL=BLOCK_MODEL,\n        )\n\n        ctx.save_for_backward(q, k, v)\n\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        q, k, v = ctx.saved_tensors\n\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        do = do.contiguous()\n\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n\n        b, h, n, d = q.shape\n        e = v.shape[-1]\n\n        # block size\n        BLOCK = 64\n        NUM_BLOCK = triton.cdiv(n, BLOCK)\n        # compute block size\n        CBLOCK = 32\n        NUM_CBLOCK = BLOCK // CBLOCK\n\n        # for intra part, compute in parallel\n        grid = (b * h, NUM_BLOCK)\n        _bwd_intra_kernel[grid](\n            q,\n            k,\n            v,\n            do,\n            dq,\n            dk,\n            dv,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            CBLOCK=CBLOCK,\n            NUM_CBLOCK=NUM_CBLOCK,\n        )\n\n        # for inter part, compute in sequencial\n        grid = (b * h,)\n        _bwd_inter_kernel[grid](\n            q,\n            k,\n            v,\n            do,\n            dq,\n            dk,\n            dv,\n            b,\n            h,\n            n,\n            d,\n            e,\n            BLOCK=BLOCK,\n            NUM_BLOCK=NUM_BLOCK,\n            CBLOCK=CBLOCK,\n            NUM_CBLOCK=NUM_CBLOCK,\n        )\n\n        return dq, dk, dv\n\n\nlightning_attn2_no_decay = LightningAttention2NoDecay.apply\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel '_dequantize_rowwise' performs row-wise dequantization on a given input tensor 'x'. Each row is processed individually by multiplying each element with a stored maximum value from 'state_x' and a constant inverse of 127. The result is stored in 'output_ptr'. The kernel operates over blocks of data defined by 'BLOCK_SIZE'. The 'dequantize_rowwise' function initializes the output tensor and launches the Triton kernel. It calculates the nearest power of two for the number of columns and sets the execution grid to iterate over the number of rows.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef _dequantize_rowwise(x_ptr, state_x_ptr, output_ptr, num_cols, BLOCK_SIZE: tl.constexpr):\n    # Get the program id\n    pid = tl.program_id(0)\n    \n    # Calculate the row start index\n    row_start = pid * BLOCK_SIZE\n    \n    # Create a range of column indices\n    col_idx = tl.arange(0, BLOCK_SIZE)\n    \n    # Check if the column index is within bounds\n    mask = col_idx < num_cols\n    \n    # Load the maximum value for the current row from state_x\n    max_val = tl.load(state_x_ptr + pid)\n    \n    # Load the input values for the current row\n    x = tl.load(x_ptr + row_start * num_cols + col_idx, mask=mask, other=0)\n    \n    # Define the inverse of 127\n    inv_127 = 1.0 / 127.0\n    \n    # Perform the dequantization\n    dequantized = x * max_val * inv_127\n    \n    # Store the result in the output tensor\n    tl.store(output_ptr + row_start * num_cols + col_idx, dequantized, mask=mask)\n\n# Define the wrapper function\ndef dequantize_rowwise(x, state_x, BLOCK_SIZE=128):\n    # Ensure x and state_x are on the GPU\n    x = x.cuda()\n    state_x = state_x.cuda()\n    \n    # Get the shape of the input tensor\n    num_rows, num_cols = x.shape\n    \n    # Calculate the nearest power of two for the number of columns\n    nearest_pow2 = 1 << (num_cols - 1).bit_length()\n    \n    # Allocate the output tensor\n    output = torch.empty_like(x, dtype=torch.float32)\n    \n    # Launch the Triton kernel\n    grid = (num_rows,)\n    _dequantize_rowwise[grid](x, state_x, output, num_cols, BLOCK_SIZE=nearest_pow2)\n    \n    return output\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create example input data\n    x = torch.randint(0, 128, (10, 128), dtype=torch.int32)\n    state_x = torch.rand(10, dtype=torch.float32)\n    \n    # Perform dequantization\n    output = dequantize_rowwise(x, state_x)\n    print(output)\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for rowwise dequantization\n@triton.jit\ndef _dequantize_rowwise(\n    x_ptr,\n    state_x,\n    output_ptr,\n    inv_127,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n    max_val = tl.load(state_x + pid)\n    output = max_val * x * inv_127\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n\n# Function to call the Triton kernel\ndef dequantize_rowwise(x: torch.Tensor, state_x: torch.Tensor):\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.float16)\n\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n\n    assert x.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (x.shape[0],)\n    _dequantize_rowwise[grid](x, state_x, output, 1.0 / 127, n_elements, BLOCK_SIZE=x.shape[1], P2=P2)\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `kldivergence_kernel` function is a Triton kernel designed to calculate the Kullback-Leibler (KL) divergence between two input vectors `x` and `y`. It takes pointers to these vectors and an output vector where results will be stored. The kernel processes data in blocks, each of which contains `BLOCK_SIZE` elements. It uses Triton's `tl.program_id` to identify the program's ID and compute the block's starting index. The kernel then loads elements from `x` and `y`, computes the KL divergence for each element using `x * tl.log(x / y)`, and stores the results in the output vector. The `kldivergence` function serves as a Python wrapper around this kernel, taking PyTorch tensors `x` and `y` as input and returning the KL divergence as output.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef kldivergence_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Get the program's ID\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the start index for this block\n    start_idx = pid * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n    \n    # Load x and y vectors for this block\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Compute KL divergence for each element\n    # Avoid division by zero by using a small epsilon value\n    epsilon = 1e-10\n    kl_div = x * tl.log((x + epsilon) / (y + epsilon))\n    \n    # Store the result in the output vector\n    tl.store(output_ptr + offsets, kl_div, mask=mask)\n\n# Python wrapper function\ndef kldivergence(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    # Ensure x and y are on the same device and have the same shape\n    assert x.device == y.device, \"x and y must be on the same device\"\n    assert x.shape == y.shape, \"x and y must have the same shape\"\n    \n    # Get the number of elements\n    n_elements = x.numel()\n    \n    # Allocate output tensor\n    output = torch.empty_like(x)\n    \n    # Define the block size\n    BLOCK_SIZE = 1024  # You can adjust this based on your GPU's capabilities\n    \n    # Launch the Triton kernel\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE  # Calculate the number of blocks\n    kldivergence_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return output\n\n# Example usage\nx = torch.tensor([0.1, 0.2, 0.7], device='cuda')\ny = torch.tensor([0.1, 0.3, 0.6], device='cuda')\nkl_divergence = kldivergence(x, y)\nprint(kl_divergence)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef kldivergence_kernel(x_ptr,  # *Pointer* to first input vector.\n                        y_ptr,  # *Pointer* to second input vector.\n                        output_ptr,  # *Pointer* to output vector.\n                        n_elements,  # Size of the vector.\n                        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n                        # NOTE: `constexpr` so it can be used as a shape value.\n                        ):\n\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * tl.log(x / y)\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef kldivergence(x: torch.Tensor, y: torch.Tensor):\n    output = torch.empty_like(x)\n    assert x.is_cuda and y.is_cuda and output.is_cuda\n    n_elements = output.numel()\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n\n    kldivergence_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton kernel implements the forward pass of a context attention mechanism. It is a custom implementation of a scaled dot-product attention, optimized for the GPU using Triton. The key function here is '_fwd_kernel' which computes attention scores and applies them to values to produce the output.\n\n            The '_fwd_kernel' function takes several tensor inputs: Q (queries), K (keys), V (values), and outputs the result to the Out tensor. It also takes various indexing and stride parameters to handle batched and multi-headed inputs.\n\n            The function computes the dot product of Q and K, applies scaling, and uses the softmax function to normalize these scores. Then it applies these normalized scores to V to get the final output. This entire process is batched and operates over multiple attention heads in parallel.\n\n            The 'context_attention_fwd' function acts as a wrapper to prepare arguments and call the Triton kernel, handling grid and block size configurations, and ensuring optimal performance based on input dimensions and GPU architecture.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel(Q_ptr, K_ptr, V_ptr, Out_ptr,\n                stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n                stride_vb, stride_vh, stride_vn, stride_om, stride_oh, stride_ob,\n                B, H, M, N, D, scale, BLOCK_SIZE: tl.constexpr):\n    # Get the program index for this kernel\n    pid = tl.program_id(axis=0)\n    \n    # Compute batch and head indices\n    batch_id = pid // H\n    head_id = pid % H\n    \n    # Compute the offset for this batch and head\n    Q_offset = batch_id * stride_qb + head_id * stride_qh\n    K_offset = batch_id * stride_kb + head_id * stride_kh\n    V_offset = batch_id * stride_vb + head_id * stride_vh\n    Out_offset = batch_id * stride_ob + head_id * stride_oh\n    \n    # Create pointers for this block\n    Q = tl.load(Q_ptr + Q_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_qm)\n    K = tl.load(K_ptr + K_offset + tl.arange(0, BLOCK_SIZE)[None, :] * stride_kn)\n    V = tl.load(V_ptr + V_offset + tl.arange(0, BLOCK_SIZE)[None, :] * stride_vn)\n    \n    # Compute QK^T\n    QK = tl.dot(Q, K, trans_b=True) * scale\n    \n    # Apply softmax to get attention scores\n    attn_scores = tl.softmax(QK, axis=1)\n    \n    # Compute output by weighting values with attention scores\n    Out = tl.dot(attn_scores, V)\n    \n    # Store the result\n    tl.store(Out_ptr + Out_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_om, Out)\n\ndef context_attention_fwd(Q, K, V, scale):\n    # Extract dimensions\n    B, H, M, D = Q.shape\n    _, _, N, _ = K.shape\n    \n    # Allocate output tensor\n    Out = torch.empty((B, H, M, D), device=Q.device, dtype=Q.dtype)\n    \n    # Define block size\n    BLOCK_SIZE = 128  # Adjust based on your hardware\n    \n    # Define grid size\n    grid = (B * H,)\n    \n    # Launch the Triton kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(2), Out.stride(1), Out.stride(0),\n        B, H, M, N, D, scale,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return Out\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\nimport math\nimport torch.nn.functional as F\n\nTESLA = \"Tesla\" in torch.cuda.get_device_name(0)\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    B_Start_Loc,\n    B_Seqlen,  # B_LOC \u5185\u90e8\u8bb0\u5f55\u6bcf\u4e2abatch \u8f93\u5165\u7684\u771f\u5b9e\u4f4d\u7f6e\uff0c B_SEQ_len \u8bb0\u5f55\u5f53\u524d\u8f93\u5165\u7684\u771f\u5b9e\u957f\u5ea6\n    Out,\n    Req_to_tokens,\n    B_req_idx,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    kv_group_num,\n    b_prompt_cache_len,\n    head_dim: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n\n    cur_kv_head = cur_head // kv_group_num\n\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (offs_d[None, :] < head_dim), other=0.0)\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + prompt_cache_len, cur_batch_seq_len + prompt_cache_len)\n\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        kv_loc = tl.load(\n            Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=(start_n + offs_n) < block_end_loc,\n            other=0,\n        )\n        off_k = kv_loc[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd\n        k = tl.load(\n            K + off_k, mask=((start_n + offs_n[None, :]) < block_end_loc) & (offs_d[:, None] < head_dim), other=0.0\n        )\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] + prompt_cache_len >= start_n + offs_n[None, :], qk, float(\"-100000000.0\"))\n\n        # -- compute m_ij, p, l_ij\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        # -- update m_i and l_i\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        # -- update output accumulator --\n        # scale p\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        # scale acc\n        acc_scale = l_i / l_i_new * alpha\n        acc_scale = tl.where(offs_m + prompt_cache_len >= start_n, acc_scale, 1.0)\n        acc = acc * acc_scale[:, None]\n        # update acc\n        off_v = kv_loc[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n        v = tl.load(\n            V + off_v, mask=((start_n + offs_n[:, None]) < block_end_loc) & (offs_d[None, :] < head_dim), other=0.0\n        )\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_i = l_i_new\n        m_i = m_i_new\n    # initialize pointers to output\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (offs_d[None, :] < head_dim))\n    return\n\n\n@torch.no_grad()\ndef context_attention_fwd(\n    q, k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len, req_to_token_indexs\n):\n    BLOCK = 128 if not TESLA else 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    head_dim = Lq\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n\n    sm_scale = 1.0 / (Lq ** 0.5)  # \u8ba1\u7b97scale\u7cfb\u6570\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n\n    num_warps = 4 if Lk <= 64 else 8\n    _fwd_kernel[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        b_start_loc,\n        b_seq_len,\n        o,\n        req_to_token_indexs,\n        b_req_idx,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        req_to_token_indexs.stride(0),\n        req_to_token_indexs.stride(1),\n        kv_group_num=kv_group_num,\n        b_prompt_cache_len=b_prompt_cache_len,\n        head_dim=head_dim,\n        BLOCK_M=BLOCK,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton kernel is named `_swiglu_fwd_kernel`. It performs the forward computation for the Swish-Gated Linear Units (Swiglu). The kernel operates on two input matrices `X` and `Y`, producing an output matrix `OUT`. The main computation involves element-wise multiplication of `x` with the sigmoid activation of `x`, further multiplied by `y`. The function `_swiglu_fwd` acts as a wrapper to set up and invoke the Triton kernel. The inputs are split into two parts: `x` and `y`. The function configures grid dimensions and ensures data is contiguous before execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel for Swiglu forward computation\n@triton.jit\ndef _swiglu_fwd_kernel(X, Y, OUT, stride_xm, stride_ym, stride_om, N, **meta):\n    # Obtain the program ID for parallel execution\n    pid = tl.program_id(axis=0)\n\n    # Calculate the starting index for this program instance\n    offs = pid * meta['BLOCK_SIZE_N'] + tl.arange(0, meta['BLOCK_SIZE_N'])\n\n    # Ensure we don't read/write out of bounds\n    mask = offs < N\n\n    # Load elements from X and Y\n    x = tl.load(X + offs * stride_xm, mask=mask, other=0.0)\n    y = tl.load(Y + offs * stride_ym, mask=mask, other=0.0)\n\n    # Compute the Swish activation: x * sigmoid(x)\n    swish_x = x * tl.sigmoid(x)\n\n    # Compute the output: swish_x * y\n    out = swish_x * y\n\n    # Store the result in the output matrix\n    tl.store(OUT + offs * stride_om, out, mask=mask)\n\n# Wrapper function to set up and invoke the Triton kernel\ndef _swiglu_fwd(x, y):\n    # Ensure the input tensors are contiguous\n    x = x.contiguous()\n    y = y.contiguous()\n\n    # Check that the dimensions of x and y match\n    assert x.shape == y.shape, \"Input tensors must have the same shape\"\n\n    # Get the number of elements in the input tensors\n    N = x.numel()\n\n    # Create an output tensor with the same shape\n    out = torch.empty_like(x)\n\n    # Define block size\n    BLOCK_SIZE_N = 1024\n\n    # Calculate the number of blocks needed\n    grid = (triton.cdiv(N, BLOCK_SIZE_N),)\n\n    # Launch the Triton kernel\n    _swiglu_fwd_kernel[grid](\n        x, y, out,\n        x.stride(0), y.stride(0), out.stride(0),\n        N,\n        BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n\n    return out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Forward Triton kernel for Swish-Gated Linear Units (Swiglu)\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 32}),\n        triton.Config({'BLOCK_N': 64}),\n        triton.Config({'BLOCK_N': 128}),\n        triton.Config({'BLOCK_N': 256}),\n        triton.Config({'BLOCK_N': 512}),\n        triton.Config({'BLOCK_N': 1024}),\n    ],\n    key=['ncols'],\n)\n@triton.jit\ndef _swiglu_fwd_kernel(\n    X, Y, OUT, stride_x_row, stride_y_row, stride_out_row, ncols, BLOCK_N: tl.constexpr\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)\n\n# Function to invoke the forward kernel\ndef _swiglu_fwd(xy, out=None):\n    if xy.stride(-1) != 1:\n        xy = xy.contiguous()\n    batch_shape = xy.shape[:-1]\n    xy = xy.reshape(-1, xy.shape[-1])\n    x, y = xy.chunk(2, dim=-1)\n    if out is None:\n        out = torch.empty_like(x)\n    else:\n        out = out.reshape(-1, out.shape[-1])\n        assert out.shape == x.shape\n    assert out.stride(-1) == 1\n    M, N = x.shape\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_N']))\n    with torch.cuda.device(x.device.index):\n        _swiglu_fwd_kernel[grid](x, y, out, x.stride(0), y.stride(0), out.stride(0), N)\n    return out.reshape(*batch_shape, out.shape[-1])\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe code defines a Triton kernel named `matmul_kernel` for performing matrix multiplication and a function `matmul` to call this kernel. The main functionality is to multiply two matrices `a` and `b` with specific blocking dimensions using Triton's parallel execution capabilities. The input matrices must satisfy the condition that the number of columns in `a` is four times the number of rows in `b`. The `matmul` function initializes an output matrix `c` and calculates the appropriate grid size for launching the kernel. `matmul_kernel` reads blocks of data from `a` and `b`, performs dot product operations, and writes the result into `c`.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bn, stride_bk, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr):\n    # Get the program ID and compute the starting indices for this block\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the start of the block in the output matrix\n    start_m = pid_m * BLOCK_M\n    start_n = pid_n * BLOCK_N\n\n    # Initialize an accumulator for the block\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Loop over the K dimension in blocks\n    for k in range(0, K, BLOCK_K):\n        # Load a block of A and B into shared memory\n        a_block = tl.load(a_ptr + (start_m + tl.arange(0, BLOCK_M))[:, None] * stride_am + (k + tl.arange(0, BLOCK_K))[None, :] * stride_ak, mask=(start_m + tl.arange(0, BLOCK_M))[:, None] < M)\n        b_block = tl.load(b_ptr + (k + tl.arange(0, BLOCK_K))[:, None] * stride_bk + (start_n + tl.arange(0, BLOCK_N))[None, :] * stride_bn, mask=(start_n + tl.arange(0, BLOCK_N))[None, :] < N)\n\n        # Perform the dot product\n        acc += tl.dot(a_block, b_block)\n\n    # Write the result to the output matrix\n    c_block_ptr = c_ptr + (start_m + tl.arange(0, BLOCK_M))[:, None] * stride_cm + (start_n + tl.arange(0, BLOCK_N))[None, :] * stride_cn\n    tl.store(c_block_ptr, acc, mask=(start_m + tl.arange(0, BLOCK_M))[:, None] < M)\n\ndef matmul(a, b):\n    # Check input dimensions\n    assert a.shape[1] == 4 * b.shape[0], \"The number of columns in 'a' must be four times the number of rows in 'b'.\"\n\n    # Define matrix dimensions\n    M, K = a.shape\n    K, N = b.shape\n\n    # Create an output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n\n    # Define the block sizes\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_K = 32\n\n    # Calculate grid sizes\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        a_ptr=a,\n        b_ptr=b,\n        c_ptr=c,\n        M=M, N=N, K=K,\n        stride_am=a.stride(0), stride_ak=a.stride(1),\n        stride_bn=b.stride(1), stride_bk=b.stride(0),\n        stride_cm=c.stride(0), stride_cn=c.stride(1),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n    )\n\n    return c\n\n# Example usage:\n# a = torch.randn((M, 4*K), device='cuda', dtype=torch.float32)\n# b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n# c = matmul(a, b)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef get_autotune_config():\n    return [\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 256,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 256,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 128,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 32,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 4,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n    ]\n\n@triton.autotune(\n    configs=get_autotune_config(),\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    M,\n    N,\n    K: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    tl.static_assert(\n        K % (4 * BLOCK_SIZE_K) == 0,\n        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",\n    )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0).to(tl.int8)  # \u8f6c\u6362\u4e3a int8 \u7c7b\u578b\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << (2 * i)\n            b = ((b_uint8 & mask) >> (2 * i)).to(tl.int8)  # \u8f6c\u6362\u4e3a int8 \u7c7b\u578b\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)  # \u4f7f\u7528 int8 \u7c7b\u578b\n            accumulator += tl.dot(a, (b - tensor_full), out_dtype=tl.int32)  # \u4fdd\u6301 a \u548c b \u4e3a int8\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef matmul(a, b):\n    assert (\n        a.shape[1] == b.shape[0] * 4\n    ), \"Incompatible dimensions, the weight matrix need to be packed\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    _, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.int32)\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n    )\n    matmul_kernel[grid](\n        a,\n        b,\n        c,\n        M,\n        N,\n        K,\n        a.stride(0),\n        a.stride(1),\n        b.stride(0),\n        b.stride(1),\n        c.stride(0),\n        c.stride(1),\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel `_fwd_kernel_aligned` that performs a forward pass for an attention mechanism with relative positional embeddings, and a wrapper function `_attention_rel_h_rel_w_kernel_aligned_device` to execute this kernel on a device. The kernel processes the input tensors Q, K, V (queries, keys, values) and an additional tensor B0 that contains relative positional information, performing scaled dot-product attention. The result is stored in the output tensor Out. The computation is parallelized across grid dimensions, where `start_m` and `off_hz` define the computation space for each block.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(Q, K, V, B0, Out, \n                        stride_qz, stride_qh, stride_qm, stride_qk, \n                        stride_kz, stride_kh, stride_kn, stride_kk, \n                        stride_vz, stride_vh, stride_vn, stride_vk, \n                        stride_b0z, stride_b0h, stride_b0m, stride_b0n, \n                        stride_outz, stride_outh, stride_outm, stride_outk,\n                        start_m, off_hz, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Load input blocks\n    pid_m = tl.program_id(0) + start_m\n    pid_hz = tl.program_id(1) + off_hz\n\n    # Compute the position in the batch and the sequence\n    q_offset = pid_hz * stride_qh + pid_m * BLOCK_M * stride_qm\n    k_offset = pid_hz * stride_kh\n    v_offset = pid_hz * stride_vh\n    b0_offset = pid_hz * stride_b0h + pid_m * BLOCK_M * stride_b0m\n\n    # Load Q, K, V, B0 blocks\n    Q_block = tl.load(Q + q_offset + tl.arange(0, BLOCK_M)[:, None] * stride_qm + tl.arange(0, BLOCK_N)[None, :] * stride_qk)\n    K_block = tl.load(K + k_offset + tl.arange(0, BLOCK_N)[:, None] * stride_kn + tl.arange(0, BLOCK_N)[None, :] * stride_kk)\n    V_block = tl.load(V + v_offset + tl.arange(0, BLOCK_N)[:, None] * stride_vn + tl.arange(0, BLOCK_N)[None, :] * stride_vk)\n    B0_block = tl.load(B0 + b0_offset + tl.arange(0, BLOCK_M)[:, None] * stride_b0m + tl.arange(0, BLOCK_N)[None, :] * stride_b0n)\n\n    # Compute scaled dot-product attention\n    scale = 1.0 / tl.sqrt(tl.float32(BLOCK_N))\n    logits = tl.dot(Q_block, K_block) * scale + B0_block\n    weights = tl.softmax(logits, axis=1)\n\n    # Compute output\n    Out_block = tl.dot(weights, V_block)\n\n    # Store the result\n    out_offset = pid_hz * stride_outh + pid_m * BLOCK_M * stride_outm\n    tl.store(Out + out_offset + tl.arange(0, BLOCK_M)[:, None] * stride_outm + tl.arange(0, BLOCK_N)[None, :] * stride_outk, Out_block)\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(Q, K, V, B0, Out, BLOCK_M=128, BLOCK_N=64):\n    # Define grid dimensions\n    grid = (Q.shape[1] // BLOCK_M, Q.shape[0])  # Assuming Q.shape = (batch_size, seq_len, dim)\n\n    # Launch kernel\n    triton.kernel(\n        _fwd_kernel_aligned,\n        grid=grid,\n        num_warps=4,  # Choose appropriate number of warps\n        args=[Q, K, V, B0, Out, \n              Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n              K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n              V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n              B0.stride(0), B0.stride(1), B0.stride(2), B0.stride(3),\n              Out.stride(0), Out.stride(1), Out.stride(2), Out.stride(3),\n              0, 0, BLOCK_M, BLOCK_N],\n        stream=triton.Stream()\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(\n    Q, K, V, B0, sm_scale,\n    Out,\n    stride_qh, stride_qm, stride_qk,\n    stride_kh, stride_kn, stride_kk,\n    stride_vh, stride_vk, stride_vn,\n    stride_oh, stride_om, stride_on,\n    stride_b0h, stride_b0m,\n    Z,\n    H,\n    N_CTX,\n    P_SEQ,\n    OUT_DTYPE: tl.constexpr,\n    BIAS_LAST_SIZE: tl.constexpr,\n    B0_NUMEL: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + kv_offset,\n        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + kv_offset,\n        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(OUT_DTYPE)\n    lo = 0\n    hi = N_CTX + P_SEQ\n\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = (tl.arange(0, BLOCK_N) %\n                         BIAS_LAST_SIZE) + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                 * stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k, out_dtype=OUT_DTYPE)\n\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                     * stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += (b0 + b1)\n\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p.to(OUT_DTYPE), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    acc = acc / l_i[:, None]\n\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    tl.store(O_block_ptr, acc.to(OUT_DTYPE))\n\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(q, k, v, rel_h_w, sm_scale, o,\n                                                 BLOCK_M,\n                                                 BLOCK_N,\n                                                 num_warps,\n                                                 num_stages):\n    _, Lk, _ = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert q.size() == k.size()\n    assert q.size() == v.size()\n    assert q.size(-2) == rel_h_w.size(-2)\n    assert (q.dtype == torch.bfloat16 or q.dtype == torch.float16)\n    assert k.dtype == q.dtype\n    assert v.dtype == k.dtype\n    assert o.dtype == v.dtype\n    assert rel_h_w.dtype == q.dtype\n    assert rel_h_w.size(-1) == 128\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n    assert P_SEQ == 0\n    assert rel_h_w.is_contiguous(), str(rel_h_w.stride())\n    _fwd_kernel_aligned[grid](\n        q, k, v,\n        rel_h_w,\n        sm_scale,\n        o,\n        q.stride(1), q.stride(2), q.stride(3),\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        o.stride(1), o.stride(2), o.stride(3),\n        rel_h_w.stride(1), rel_h_w.stride(2),\n        q.shape[0],\n        q.shape[1],\n        q.shape[2],\n        P_SEQ,\n        OUT_DTYPE=tl.float16 if q.dtype == torch.float16 else tl.bfloat16,\n        BIAS_LAST_SIZE=(rel_h_w.size(-1) // 2),\n        B0_NUMEL=rel_h_w.size(-1),\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        BLOCK_DMODEL=Lk,\n        num_warps=num_warps,\n        num_stages=num_stages)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel 'ff_llama' is designed to perform a fused operation involving matrix multiplications and element-wise operations. It multiplies input 'x' with two weight matrices 'w1' and 'w3', applies an element-wise SILU activation on the result of 'w1', and scales the inputs using a root mean square (RMS) weight. The operation also normalizes the input using an L2-norm.\n\nThe kernel function is invoked by 'kernel_ff', which takes in four tensors: 'x', 'w1', 'w3', and 'rms_w'. It expects these tensors to be of type float16 or int8 for weights and performs the computations accordingly. The result is stored in an output tensor which is reshaped to match the batch and sequence dimensions of 'x'.\n\nKey parameters like 'BLOCK_SIZE_M', 'BLOCK_SIZE_N', and 'BLOCK_SIZE_K' determine the tiling size for the computations, optimizing memory access and parallel execution.\n", "predict": "import triton\nimport triton.language as tl\n\n# Define block sizes for tiling\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef ff_llama_kernel(\n    x_ptr, w1_ptr, w3_ptr, rms_w_ptr, out_ptr,\n    M, N, K,\n    stride_xm, stride_xk,\n    stride_w1k, stride_w1n,\n    stride_w3k, stride_w3n,\n    stride_outm, stride_outn,\n    **meta\n):\n    # Program ID and offsets\n    pid = tl.program_id(axis=0)\n    offs_m = pid * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Pointers to input and output data\n    x = tl.load(x_ptr + offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk, mask=offs_m[:, None] < M)\n    w1 = tl.load(w1_ptr + offs_k[:, None] * stride_w1k + offs_n[None, :] * stride_w1n)\n    w3 = tl.load(w3_ptr + offs_k[:, None] * stride_w3k + offs_n[None, :] * stride_w3n)\n\n    # Matrix multiplication\n    x_w1 = tl.dot(x, w1)\n    x_w3 = tl.dot(x, w3)\n\n    # Element-wise SILU activation\n    silu_x_w1 = x_w1 * tl.sigmoid(x_w1)\n\n    # Root Mean Square (RMS) scaling\n    rms_scale = tl.sqrt(tl.sum(x * x, axis=1) / K)\n    scaled_x = x / rms_scale[:, None]\n\n    # Combine results\n    result = silu_x_w1 + x_w3\n\n    # Store result\n    tl.store(out_ptr + offs_m[:, None] * stride_outm + offs_n[None, :] * stride_outn, result, mask=offs_m[:, None] < M)\n\ndef kernel_ff(x, w1, w3, rms_w):\n    # Ensure input types are correct\n    assert x.dtype in [triton.float16, triton.int8], \"x must be float16 or int8\"\n    assert w1.dtype in [triton.float16, triton.int8], \"w1 must be float16 or int8\"\n    assert w3.dtype in [triton.float16, triton.int8], \"w3 must be float16 or int8\"\n\n    # Get shapes and strides\n    M, K = x.shape\n    _, N = w1.shape\n    x_strides = x.strides\n    w1_strides = w1.strides\n    w3_strides = w3.strides\n    out = triton.empty((M, N), dtype=triton.float16)\n\n    # Launch kernel\n    grid = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M\n    ff_llama_kernel[grid](\n        x, w1, w3, rms_w, out,\n        M, N, K,\n        x_strides[0], x_strides[1],\n        w1_strides[0], w1_strides[1],\n        w3_strides[0], w3_strides[1],\n        out.strides[0], out.strides[1]\n    )\n\n    return out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef ff_llama(\n    a_ptr, w1_ptr, w3_ptr, out_ptr, rms_w_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_w1k, stride_w1n,\n    stride_w3k, stride_w3n,\n    stride_outm, stride_outn,\n    stride_rms_w,\n    USE_FP8: tl.constexpr,\n    EPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    w1 and w3 are weights (linear layers)\n    F.silu(w1(x)) * w3(x)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    w1_ptrs = w1_ptr + (offs_k[:, None] * stride_w1k + offs_bn[None, :] * stride_w1n)\n    w3_ptrs = w3_ptr + (offs_k[:, None] * stride_w3k + offs_bn[None, :] * stride_w3n)\n    acc1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    acc2 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n    a_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs)\n        a_sum += tl.extra.cuda.libdevice.pow(a.to(tl.float32), 2)\n        rms_w = tl.load(rms_w_ptrs)\n        if USE_FP8:\n            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n            rms_w = rms_w.to(tl.float16)\n        a = a * rms_w\n        b = tl.load(w1_ptrs)\n        if USE_FP8:\n            b = b.to(tl.float8e5, bitcast=True)\n            b = b.to(tl.float32)\n            b = b.to(tl.float16)\n        acc1 += tl.dot(a, b)\n        c = tl.load(w3_ptrs)\n        if USE_FP8:\n            c = c.to(tl.float8e5, bitcast=True)\n            c = c.to(tl.float32)\n            c = c.to(tl.float16)\n        acc2 += tl.dot(a, c)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        w1_ptrs += BLOCK_SIZE_K * stride_w1k\n        w3_ptrs += BLOCK_SIZE_K * stride_w3k\n\n        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n\n    a_mean = tl.sum(a_sum, axis=1) / K + EPS\n    a_norm = tl.math.rsqrt(a_mean)\n    acc1 = acc1 * a_norm[:, None]\n    acc2 = acc2 * a_norm[:, None]\n    accumulator = (acc1 * tl.sigmoid(acc1)) * acc2\n\n    offs_outm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_outn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (stride_outm * offs_outm[:, None] + stride_outn * offs_outn[None, :])\n    out_mask = (offs_outm[:, None] < M) & (offs_outn[None, :] < N)\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\n\ndef kernel_ff(x: torch.Tensor, w1: torch.Tensor, w3: torch.Tensor, rms_w: torch.Tensor) -> torch.Tensor:\n    assert x.dtype == torch.float16\n    assert w1.dtype == w3.dtype == rms_w.dtype\n    assert w1.dtype in [torch.int8, torch.float16]\n    assert w1.shape == w3.shape\n\n    w1_t = w1.t()\n    w3_t = w3.t()\n\n    batch, seq_len, dim = x.shape\n    M, K = batch * seq_len, dim\n\n    N = w1_t.shape[1]\n    assert K == w1_t.shape[0]\n    assert w1_t.shape == w3_t.shape\n    x_reshape = x.reshape(M, K)\n    out = torch.empty((M, N), dtype=x.dtype, device=x.device)\n    grid = lambda META: (triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]),)\n    ff_llama[grid](\n        x_reshape, w1_t, w3_t, out, rms_w,\n        M, N, K,\n        *x_reshape.stride(),\n        *w1_t.stride(),\n        *w3_t.stride(),\n        *out.stride(),\n        *rms_w.stride(),\n        USE_FP8=w1_t.dtype != torch.float16,\n        EPS=1e-6,\n        BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=64,\n        num_stages=2, num_warps=4\n    )\n    out = out.view(batch, seq_len, -1)\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton kernel `_fwd_kernel_destindex_copy_quantize_kv` performs quantization and copies key-value pairs from a source to a destination tensor. It uses index remapping specified in `Dest_loc` to determine where the quantized data should be stored. The kernel operates over a grid defined by sequences and heads, divides data into groups, computes scaling factors for quantization, and stores both quantized data and scaling factors. The function `destindex_copy_quantize_kv` is a wrapper that sets up inputs and configuration for the kernel execution.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    src_ptr, dest_ptr, dest_loc_ptr, scale_ptr, \n    num_groups, num_heads, num_sequences, \n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the group, head, and sequence indices\n    group_id = pid // (num_heads * num_sequences)\n    head_id = (pid // num_sequences) % num_heads\n    seq_id = pid % num_sequences\n\n    # Calculate offset\n    offset = (group_id * num_heads * num_sequences + head_id * num_sequences + seq_id) * BLOCK_SIZE\n\n    # Load source data\n    src = tl.load(src_ptr + offset + tl.arange(0, BLOCK_SIZE))\n\n    # Compute scaling factor for quantization\n    max_val = tl.max(tl.abs(src), axis=0)\n    scale = max_val / 127.0  # Assuming 8-bit quantization\n\n    # Quantize the source data\n    quantized = tl.cast(src / scale, tl.int8)\n\n    # Load destination index\n    dest_index = tl.load(dest_loc_ptr + seq_id)\n\n    # Calculate destination offset\n    dest_offset = (group_id * num_heads * num_sequences + head_id * num_sequences + dest_index) * BLOCK_SIZE\n\n    # Store quantized data and scaling factor\n    tl.store(dest_ptr + dest_offset + tl.arange(0, BLOCK_SIZE), quantized)\n    tl.store(scale_ptr + dest_offset // BLOCK_SIZE, scale)\n\n\nimport torch\n\ndef destindex_copy_quantize_kv(src, dest, dest_loc, num_groups, num_heads, num_sequences):\n    BLOCK_SIZE = 128  # Define a block size that fits your problem\n    grid = (num_groups * num_heads * num_sequences,)\n\n    # Allocate scale tensor\n    scale = torch.empty((num_groups, num_heads, num_sequences), dtype=torch.float32, device=src.device)\n\n    # Launch the kernel\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        src_ptr=src, \n        dest_ptr=dest, \n        dest_loc_ptr=dest_loc, \n        scale_ptr=scale,\n        num_groups=num_groups, \n        num_heads=num_heads, \n        num_sequences=num_sequences,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return dest, scale\n\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K,\n    Dest_loc,\n    Out,\n    Out_scale,\n    stride_k_bs,\n    stride_k_h,\n    stride_k_g,\n    stride_k_d,\n    stride_o_bs,\n    stride_o_h,\n    stride_o_g,\n    stride_o_d,\n    stride_os_bs,\n    stride_os_h,\n    stride_os_g,\n    group_size,\n    BLOCK_GROUP_NUM: tl.constexpr,\n    BLOCK_GROUP_DIM: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    offs_g = tl.arange(0, BLOCK_GROUP_NUM)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    src_data = tl.load(\n        K + cur_index * stride_k_bs + cur_head * stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :],\n        mask=offs_g[:, None] < group_size,\n        other=0.0,\n    )\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.element_ty)\n    q_src_data = (src_data / data_scale[:, None]).to(tl.int8)\n\n    o_ptrs = Out + dest_index * stride_o_bs + cur_head * stride_o_h + offs_g[:, None] * stride_o_g + offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + cur_head * stride_os_h + offs_g\n    tl.store(o_ptrs, q_src_data, mask=offs_g[:, None] < group_size)\n    tl.store(os_ptrs, data_scale, mask=offs_g < group_size)\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    quant_group_dim = 8\n\n    assert head_dim % quant_group_dim == 0, \"error head dim, can not been supported to copy quant kv\"\n    grid = (seq_len, head_num)\n    num_warps = 1\n\n    group_size = head_dim // quant_group_dim\n    group_dim = quant_group_dim\n\n    K = K.view((K.shape[0], K.shape[1], group_size, group_dim))\n    Out = Out.view(Out.shape[0], Out.shape[1], group_size, group_dim)\n\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K,\n        DestLoc,\n        Out,\n        Out_scale,\n        K.stride(0),\n        K.stride(1),\n        K.stride(2),\n        K.stride(3),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        Out.stride(3),\n        Out_scale.stride(0),\n        Out_scale.stride(1),\n        Out_scale.stride(2),\n        group_size,\n        BLOCK_GROUP_NUM=triton.next_power_of_2(group_size),\n        BLOCK_GROUP_DIM=group_dim,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton code provided contains a kernel function for computing RoPE (Rotary Position Embeddings) and two main Python functions to facilitate this. The kernel `_rope_embedding` calculates the RoPE embedding by utilizing trigonometric properties to transform the input tensor. The functions `_rope_embedding_forward_impl` and `_rope_embedding_backward_impl` handle the forward and backward passes of the embedding respectively, using the kernel.\n\n- `_rope_embedding` is the Triton JIT-compiled kernel that performs the RoPE transformation on the input matrix `Q` using precomputed cosine and sine matrices. It applies the mathematical operation `Q * cos + rotate_half(Q) * sin` where `rotate_half` splits and rotates the matrix.\n\n- `_rope_embedding_forward_impl` is a Python function that prepares inputs for the forward operation by reshaping and transposing the input `Q`. It calculates optimal settings using `calculate_settings` and invokes the Triton kernel with these settings.\n\n- `_rope_embedding_backward_impl` prepares the input for the backward pass, similarly reshaping and transposing, and calls the Triton kernel to compute gradients based on the modified RoPE operation.\n\nCommon variables:\n- `Q` represents the input tensor.\n- `cos` and `sin` are tensors storing cosine and sine values.\n- `head_dim` and `n_heads` define the dimensions and number of heads for the input tensor.\n- `BLOCK_SIZE` and `num_warps` optimize computation on the GPU.\n", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel for RoPE embedding\n@triton.jit\ndef _rope_embedding(Q_ptr, cos_ptr, sin_ptr, output_ptr,\n                    head_dim, n_heads, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Calculate the indices for the block\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load data from global memory\n    Q = tl.load(Q_ptr + offsets)\n    cos = tl.load(cos_ptr + offsets)\n    sin = tl.load(sin_ptr + offsets)\n    \n    # Rotate half of the Q matrix\n    Q_half = tl.cat([Q[head_dim:], Q[:head_dim]], axis=0)\n    \n    # Compute the RoPE transformation\n    rope_result = Q * cos + Q_half * sin\n    \n    # Store the result back to global memory\n    tl.store(output_ptr + offsets, rope_result)\n\n# Forward pass function\ndef _rope_embedding_forward_impl(Q, cos, sin, head_dim, n_heads):\n    BLOCK_SIZE = 128  # Define block size for Triton kernel\n    num_warps = 4     # Define number of warps for optimal performance\n    \n    # Reshape and transpose input tensors as needed\n    Q = Q.view(-1)\n    cos = cos.view(-1)\n    sin = sin.view(-1)\n    \n    # Prepare output tensor\n    output = torch.empty_like(Q)\n    \n    # Launch the Triton kernel\n    grid = (Q.numel() // BLOCK_SIZE,)\n    _rope_embedding[grid](Q, cos, sin, output, head_dim, n_heads, BLOCK_SIZE=BLOCK_SIZE)\n    \n    # Reshape the output to the original shape\n    return output.view_as(Q)\n\n# Backward pass function\ndef _rope_embedding_backward_impl(Q, cos, sin, head_dim, n_heads):\n    # This function would typically compute the gradient of the input with respect to some loss\n    # Here, we'll demonstrate a simple example of how you might set up such a function\n    \n    BLOCK_SIZE = 128  # Define block size for Triton kernel\n    num_warps = 4     # Define number of warps for optimal performance\n    \n    # Reshape and transpose input tensors as needed\n    Q = Q.view(-1)\n    cos = cos.view(-1)\n    sin = sin.view(-1)\n    \n    # Prepare output tensor\n    output_grad = torch.empty_like(Q)\n    \n    # Launch the Triton kernel\n    grid = (Q.numel() // BLOCK_SIZE,)\n    _rope_embedding[grid](Q, cos, sin, output_grad, head_dim, n_heads, BLOCK_SIZE=BLOCK_SIZE)\n    \n    # Reshape the output to the original shape\n    return output_grad.view_as(Q)\n\n# Example usage:\n# Q = torch.randn(batch_size, seq_len, n_heads, head_dim)\n# cos = torch.cos(torch.arange(head_dim).float())\n# sin = torch.sin(torch.arange(head_dim).float())\n# result = _rope_embedding_forward_impl(Q, cos, sin, head_dim, n_heads)\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\nMAX_FUSED_SIZE = 65536  # 2**16\nnext_power_of_2 = triton.next_power_of_2\n\ndef calculate_settings(n):\n    BLOCK_SIZE = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n\nROPE_GROUP_SIZE = 4\n\n@triton.heuristics({\"BACKWARD_PASS\": lambda args: args[\"BACKWARD_PASS\"],})\n@triton.jit\ndef _rope_embedding(\n    Q,     Q_row_stride,\n    cos, cos_row_stride,\n    sin, sin_row_stride,\n    seqlen,\n    head_dim        : tl.constexpr,\n    n_heads         : tl.constexpr,\n    BACKWARD_PASS   : tl.constexpr,\n    BLOCK_SIZE      : tl.constexpr,\n    ROPE_GROUP_SIZE : tl.constexpr = 4,\n):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    row_position  = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets  = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n\n    sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n    cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n\n    if BACKWARD_PASS:\n        # See our blog post for more info.\n        sin1 = -sin1\n    \n    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\n\n    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n\n        # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\n        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\n\n        tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\n        tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\n    \n\n\ndef _rope_embedding_forward_impl(Q, cos, sin):\n    Q = Q.transpose(1, 2).clone()\n    cos, sin = cos.squeeze(), sin.squeeze()\n    batch, seq_len, n_heads, head_dim = Q.shape\n    Q = Q.reshape(batch*seq_len, n_heads*head_dim)\n    n_rows, n_cols = Q.shape\n    assert(seq_len <= cos.shape[0])\n\n    # [TODO] Changing blocksize to head_dim//2 seems to have\n    # some concurrency / un-deterministic issues.\n    BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\n\n    # group_size = 4 # 4 or 8, too large group_size can hurt performance.\n    div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\n    n_groups = div + (mod != 0)\n\n    _rope_embedding[(n_rows, n_groups, )](\n          Q,   Q.stride(0),\n        cos, cos.stride(0),\n        sin, sin.stride(0),\n        seq_len,\n        head_dim, n_heads,\n        BACKWARD_PASS = False,\n        BLOCK_SIZE = BLOCK_SIZE,\n        num_warps  = num_warps,\n    )\n    Q = Q.view(batch, seq_len, n_heads, head_dim)\n    Q = Q.transpose(1, 2)\n    return Q, cos, sin, n_groups, BLOCK_SIZE, num_warps\n\n\ndef _rope_embedding_backward_impl(dY, cos, sin, n_groups, BLOCK_SIZE, num_warps):\n    dY = dY.transpose(1, 2)\n    batch, seq_len, n_heads, head_dim = dY.shape\n    dY = dY.reshape(batch*seq_len, n_heads*head_dim)\n    # Must be reshape not view\n    n_rows, n_cols = dY.shape\n\n    _rope_embedding[(n_rows, n_groups, )](\n        dY,  dY .stride(0),\n        cos, cos.stride(0),\n        sin, sin.stride(0),\n        seq_len, head_dim, n_heads,\n        BACKWARD_PASS = True,\n        BLOCK_SIZE = BLOCK_SIZE,\n        num_warps  = num_warps,\n    )\n    dY = dY.view(batch, seq_len, n_heads, head_dim)\n    dY = dY.transpose(1, 2)\n    return dY\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton operator code is for a matrix-vector multiplication kernel, named `mv_kernel`, optimized for GPU execution. It takes a matrix `A`, a vector `B`, and computes their product storing the result in vector `C`. The key parameters are `N` and `M`, representing the dimensions of matrix `A` (N x M). The kernel utilizes configurable block sizes (`BLOCK_N`, `BLOCK_M`) to process sub-portions of the matrix and vector, enabling efficient parallel computation. It uses Triton's `@autotune` decorator to optimize configurations based on the input sizes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for matrix-vector multiplication\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 64, 'BLOCK_M': 64}),\n        triton.Config({'BLOCK_N': 128, 'BLOCK_M': 32}),\n        triton.Config({'BLOCK_N': 32, 'BLOCK_M': 128}),\n    ],\n    key=['N', 'M']\n)\n@triton.jit\ndef mv_kernel(A_ptr, B_ptr, C_ptr, N, M, BLOCK_N: tl.constexpr, BLOCK_M: tl.constexpr):\n    # Define the block index\n    pid = tl.program_id(axis=0)\n    \n    # Define the starting indices for this block\n    block_n_start = pid * BLOCK_N\n    \n    # Create a pointer to the start of the block in C\n    C_block_ptr = C_ptr + block_n_start\n    \n    # Create an accumulator for the results\n    C_acc = tl.zeros([BLOCK_N], dtype=tl.float32)\n    \n    # Iterate over the blocks of the matrix\n    for block_m_start in range(0, M, BLOCK_M):\n        # Load the sub-matrix A and sub-vector B\n        A_block_ptr = A_ptr + block_n_start * M + block_m_start\n        B_block_ptr = B_ptr + block_m_start\n        \n        A_block = tl.load(A_block_ptr + tl.arange(0, BLOCK_N)[:, None] * M + tl.arange(0, BLOCK_M)[None, :], mask=True)\n        B_block = tl.load(B_block_ptr + tl.arange(0, BLOCK_M), mask=True)\n        \n        # Perform the matrix-vector multiplication for this block\n        C_acc += tl.dot(A_block, B_block)\n    \n    # Store the result in C\n    tl.store(C_block_ptr + tl.arange(0, BLOCK_N), C_acc)\n\n# Python wrapper to call the Triton kernel\ndef matrix_vector_multiply(A, B, N, M):\n    # Ensure input dimensions are correct\n    assert A.shape == (N, M)\n    assert B.shape == (M,)\n    \n    # Allocate output vector C\n    C = torch.empty((N,), dtype=A.dtype, device=A.device)\n    \n    # Launch the Triton kernel\n    grid = (triton.cdiv(N, 64),)  # Assuming BLOCK_N=64 for the grid size\n    mv_kernel[grid](A, B, C, N, M)\n    \n    return C\n\n# Example usage\nimport torch\n\n# Define matrix A and vector B\nN, M = 128, 256\nA = torch.randn((N, M), dtype=torch.float32, device='cuda')\nB = torch.randn((M,), dtype=torch.float32, device='cuda')\n\n# Perform matrix-vector multiplication\nC = matrix_vector_multiply(A, B, N, M)\nprint(C)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": m, \"BLOCK_N\": n}, num_stages=s, num_warps=w)\n        for m in [32, 64, 128]\n        for n in [1, 2, 4, 8]\n        for s in [3, 4]\n        for w in [4, 8]\n    ],\n    key=[\"M\", \"N\"],\n)\n@triton.jit\ndef mv_kernel(\n    A,\n    B,\n    C,\n    N,\n    M,\n    stride_an,\n    stride_am,\n    stride_bm,\n    stride_cn,\n    BLOCK_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offset_n = pid * BLOCK_N + tl.arange(0, BLOCK_N)[:, None]\n    offset_m = tl.arange(0, BLOCK_M)[None, :]\n    n_mask = offset_n < N\n    A_ptrs = A + offset_n * stride_an + offset_m * stride_am\n    B_ptrs = B + offset_m * stride_bm\n    acc = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    for m in range(0, M, BLOCK_M):\n        m_mask = m + offset_m < M\n        a = tl.load(A_ptrs, mask=n_mask & m_mask, other=0.0).to(tl.float32)\n        b = tl.load(B_ptrs, mask=m_mask, other=0.0).to(tl.float32)\n        acc += a * b\n        A_ptrs += BLOCK_M * stride_am\n        B_ptrs += BLOCK_M * stride_bm\n\n    acc = tl.sum(acc, axis=1)\n    C_ptrs = C + offset_n * stride_cn\n    tl.store(C_ptrs, acc[:, None], mask=n_mask)\n\n\ndef mv(inp, vec):\n    logging.debug(\"GEMS MV\")\n    assert inp.shape[1] == vec.shape[0], \"incompatible dimensions\"\n    N, M = inp.shape\n    out = torch.empty((N,), device=inp.device, dtype=inp.dtype)\n    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_N\"]),)\n    with torch.cuda.device(inp.device):\n        mv_kernel[grid](\n            inp,\n            vec,\n            out,\n            N,\n            M,\n            inp.stride(0),\n            inp.stride(1),\n            vec.stride(0),\n            out.stride(0),\n        )\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The given Triton kernel, `rms_matmul_rbe`, performs a matrix multiplication between an input tensor `x` and a weight matrix `weight`, with an RMS normalization and optional Rotary Embeddings (RBE) applied. It is intended for transformer models, handling operations in an efficient, parallel manner. The wrapper function `rms_matmul_rbe_wrapper` orchestrates this kernel, ensuring type compatibility and shaping the output tensor appropriately. It facilitates matrix multiplication with support for FP16 and INT8 data types, adjusting computation based on parameters like number of heads and head dimension.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_matmul_rbe_kernel(\n    x_ptr, weight_ptr, out_ptr, rotary_ptr,\n    n, m, k,  # Dimensions\n    stride_xn, stride_xk,  # Strides for input matrix x\n    stride_wn, stride_wk,  # Strides for weight matrix\n    stride_on, stride_om,  # Strides for output matrix\n    apply_rotary,  # Flag to apply rotary embeddings\n    BLOCK_SIZE: tl.constexpr  # Block size for tiling\n):\n    # Compute the row and column indices\n    row = tl.program_id(0)\n    col = tl.program_id(1)\n\n    # Compute the pointers to the current block\n    x_ptrs = x_ptr + row * stride_xn + tl.arange(0, BLOCK_SIZE) * stride_xk\n    w_ptrs = weight_ptr + col * stride_wk + tl.arange(0, BLOCK_SIZE) * stride_wn\n\n    # Initialize accumulation\n    acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n\n    # Loop over the k dimension\n    for k_block in range(0, k, BLOCK_SIZE):\n        # Load x and w\n        x = tl.load(x_ptrs + k_block * stride_xk)\n        w = tl.load(w_ptrs + k_block * stride_wk)\n\n        # Compute partial product\n        acc += x * w\n\n    # RMS normalization\n    rms = tl.sqrt(tl.sum(acc * acc) / m)\n    acc /= rms\n\n    # Apply rotary embeddings if required\n    if apply_rotary:\n        rotary_emb = tl.load(rotary_ptr + row * stride_on + col * stride_om)\n        acc = acc * rotary_emb\n\n    # Store the result\n    out_ptrs = out_ptr + row * stride_on + col * stride_om\n    tl.store(out_ptrs, acc)\n\nimport torch\n\ndef rms_matmul_rbe_wrapper(x, weight, rotary_emb=None, apply_rotary=False):\n    # Ensure the data types are compatible\n    assert x.dtype in [torch.float16, torch.int8]\n    assert weight.dtype in [torch.float16, torch.int8]\n\n    # Get dimensions\n    n, k = x.shape\n    m, _ = weight.shape\n\n    # Allocate output tensor\n    out = torch.empty((n, m), dtype=torch.float16, device=x.device)\n\n    # Convert to pointers\n    x_ptr = x.data_ptr()\n    weight_ptr = weight.data_ptr()\n    out_ptr = out.data_ptr()\n    rotary_ptr = rotary_emb.data_ptr() if rotary_emb is not None else 0\n\n    # Define block size\n    BLOCK_SIZE = 128  # Example block size, can be tuned\n\n    # Launch kernel\n    grid = (n // BLOCK_SIZE, m // BLOCK_SIZE)\n    rms_matmul_rbe_kernel[grid](\n        x_ptr, weight_ptr, out_ptr, rotary_ptr,\n        n, m, k,\n        x.stride(0), x.stride(1),\n        weight.stride(0), weight.stride(1),\n        out.stride(0), out.stride(1),\n        apply_rotary,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return out\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rbe_triton(x_ptr, out_ptr,\n               M, K,\n               stride_x_batch, stride_x_m, stride_x_n,\n               stride_out_batch, stride_out_m, stride_out_n,\n               start_token_position,\n               THETA: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_n = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K // 2) * 2  # take only even numbers\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + stride_x_m * offs_m[:, None] + stride_x_n * offs_n[None, :])\n    x_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    real = tl.load(x_ptrs, mask=x_real_mask, other=0.0)\n    x_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    imag = tl.load(x_ptrs + 1, mask=x_imag_mask, other=0.0)\n    tl.debug_barrier()\n    start_block = start_token_position + pid_m * BLOCK_SIZE_M\n    cos, sin = get_freq_multi_tokens(offs_cn=offs_n, starting_idx=start_block, theta=THETA, NB_TOKENS=BLOCK_SIZE_M)\n\n    out_real = real * cos - imag * sin\n    out_imag = real * sin + imag * cos\n    tl.debug_barrier()\n    out_ptrs = out_ptr + (\n            pid_batch * stride_out_batch + stride_out_m * offs_m[:, None] + stride_out_n * offs_n[None, :])\n    out_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    tl.store(out_ptrs, out_real, mask=out_real_mask)\n    out_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    tl.store(out_ptrs + 1, out_imag, mask=out_imag_mask)\n\n\n@triton.jit\ndef rms_matmul_rbe(\n        x_ptr, w_ptr, rms_w_ptr, out_ptr,\n        M, N, K,\n        stride_x_batch, stride_x_m, stride_x_k,\n        stride_w_k, stride_w_n,\n        stride_rms_w,\n        stride_out_batch, stride_out_m, stride_out_n,\n        start_token_position,\n        USE_FP8: tl.constexpr,\n        RBE_EPILOGUE: tl.constexpr,\n        THETA: tl.constexpr,\n        EPS: tl.constexpr,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    Prologue: RMS\n    Epilogue: nothing or Rotary embeddings\n    c = ROBE((rms(a) * rms_w) @ b)\n    \"\"\"\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + offs_m[:, None] * stride_x_m + offs_k[None, :] * stride_x_k)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_w_k + offs_n[None, :] * stride_w_n)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n    x_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        x = tl.load(x_ptrs)\n        x_sum += tl.extra.cuda.libdevice.pow(x.to(tl.float32), 2)\n        rms_w = tl.load(rms_w_ptrs)  # TODO add an assert that rms_w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n            rms_w = rms_w.to(tl.float16)\n        x = x * rms_w\n        w = tl.load(w_ptrs)  # TODO add an assert that w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            w = w.to(tl.float8e5, bitcast=True)\n            w = w.to(tl.float32)\n            w = w.to(tl.float16)\n        accumulator += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x_k\n        w_ptrs += BLOCK_SIZE_K * stride_w_k\n        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n    x_mean = tl.sum(x_sum, axis=1) / K + EPS\n    x_norm = tl.math.rsqrt(x_mean)\n    accumulator = accumulator * x_norm[:, None]\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (\n                pid_batch * stride_out_batch + offs_m[:, None] * stride_out_m + offs_n[None, :] * stride_out_n)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\n\ndef rms_matmul_rbe_wrapper(x: torch.Tensor, weight: torch.Tensor, rms_w: torch.Tensor, use_rbe: bool, start_pos: int,\n                           n_heads: int, head_dim: int):\n    # \u786e\u4fdd weight \u548c rms_w \u7684\u6570\u636e\u7c7b\u578b\u4e00\u81f4\n    assert weight.dtype in [torch.float16, torch.int8], \"Only torch.float16 or torch.int8 are supported for weight\"\n    \n    # \u786e\u4fdd rms_w \u548c weight \u7684 dtype \u4e00\u81f4\n    if rms_w.dtype != weight.dtype:\n        # print(f\"rms_w dtype: {rms_w.dtype}, weight dtype: {weight.dtype}\")\n        rms_w = rms_w.to(weight.dtype)  # \u5982\u679c\u7c7b\u578b\u4e0d\u4e00\u81f4\uff0c\u5c06 rms_w \u8f6c\u6362\u4e3a\u4e0e weight \u4e00\u81f4\u7684\u7c7b\u578b\n\n    # \u521b\u5efa\u8f93\u51fa\u5f20\u91cf\n    batch, M, K = x.shape\n    weight_t = weight.t()\n    K_W, N = weight_t.shape\n    assert K == K_W\n\n    out = torch.empty((batch, M, N), dtype=weight_t.dtype, device=weight_t.device)\n    out_ptr = triton.reinterpret(out, tl.float8e5 if out.dtype == torch.int8 else tl.float16)\n\n    grid = lambda META: (\n        batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]))\n\n    rms_matmul_rbe[grid](\n        x_ptr=x,\n        w_ptr=weight_t, rms_w_ptr=rms_w, out_ptr=out_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=x.stride(0), stride_x_m=x.stride(1), stride_x_k=x.stride(2),\n        stride_w_k=weight_t.stride(0), stride_w_n=weight_t.stride(1),\n        stride_rms_w=rms_w.stride(0),\n        stride_out_batch=out.stride(0), stride_out_m=out.stride(1), stride_out_n=out.stride(2),\n        start_token_position=start_pos,\n        USE_FP8=weight_t.dtype == torch.int8,\n        RBE_EPILOGUE=use_rbe,\n        THETA=10000.,\n        EPS=1e-6,\n        BLOCK_SIZE_M=16, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n        num_stages=4, num_warps=4\n    )\n    out = out.view(batch, M, n_heads, head_dim)\n    return out\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton code implements a layer normalization operation in a custom manner using Triton kernels for both forward and backward passes. The primary functions are `layer_norm_forward` and `layer_norm_backward`, and they leverage two Triton kernels: `_layer_norm_forward_kernel` and `_layer_norm_backward_kernel`. \n\n    `layer_norm_forward` accepts input tensors `X`, `W`, `B` and a small constant `eps`. It reshapes `X`, calculates appropriate settings, and initializes output tensors `Y`, `Mean`, and `RSTD`. The `_layer_norm_forward_kernel` is then launched, which computes mean and variance for each row, applies normalization, and stores results in `Y`.\n\n    `layer_norm_backward` computes gradients for inputs, weights, and biases given a gradient `dY`. It utilizes `_layer_norm_backward_kernel` to iteratively compute these gradients across rows.\n\n    `LigerLayerNormFunction` is an autograd function wrapping these operations for PyTorch differentiation, ensuring input tensors are contiguous for efficient computation. The forward method calls `layer_norm_forward` and saves relevant tensors for backward computation, while the backward method computes gradients using `layer_norm_backward`.\n\n    Key variables:\n    - `n_cols`, `n_rows`: represent matrix dimensions.\n    - `BLOCK_SIZE`, `num_warps`: Triton-specific execution settings.\n    - `tl.constexpr`: Triton constant expressions for kernel parameters.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for forward pass of layer normalization\n@triton.jit\ndef _layer_norm_forward_kernel(X_ptr, W_ptr, B_ptr, Y_ptr, Mean_ptr, RSTD_ptr, \n                               n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    # Compute row index\n    row_idx = tl.program_id(0)\n    \n    # Load input row\n    X = tl.load(X_ptr + row_idx * n_cols + tl.arange(0, BLOCK_SIZE))\n    \n    # Compute mean\n    mean = tl.sum(X, axis=0) / n_cols\n    \n    # Compute variance\n    var = tl.sum((X - mean) ** 2, axis=0) / n_cols\n    \n    # Compute reciprocal of standard deviation\n    rstd = 1 / tl.sqrt(var + eps)\n    \n    # Normalize\n    Y = (X - mean) * rstd\n    \n    # Apply weight and bias\n    W = tl.load(W_ptr + tl.arange(0, BLOCK_SIZE))\n    B = tl.load(B_ptr + tl.arange(0, BLOCK_SIZE))\n    Y = Y * W + B\n    \n    # Store results\n    tl.store(Y_ptr + row_idx * n_cols + tl.arange(0, BLOCK_SIZE), Y)\n    tl.store(Mean_ptr + row_idx, mean)\n    tl.store(RSTD_ptr + row_idx, rstd)\n\n# Triton kernel for backward pass of layer normalization\n@triton.jit\ndef _layer_norm_backward_kernel(dY_ptr, X_ptr, W_ptr, Mean_ptr, RSTD_ptr, \n                                dX_ptr, dW_ptr, dB_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n    # Compute row index\n    row_idx = tl.program_id(0)\n    \n    # Load data\n    dY = tl.load(dY_ptr + row_idx * n_cols + tl.arange(0, BLOCK_SIZE))\n    X = tl.load(X_ptr + row_idx * n_cols + tl.arange(0, BLOCK_SIZE))\n    W = tl.load(W_ptr + tl.arange(0, BLOCK_SIZE))\n    mean = tl.load(Mean_ptr + row_idx)\n    rstd = tl.load(RSTD_ptr + row_idx)\n    \n    # Compute dX\n    dX = W * rstd * (dY - tl.sum(dY, axis=0) / n_cols - (X - mean) * rstd * tl.sum(dY * (X - mean), axis=0) / n_cols)\n    tl.store(dX_ptr + row_idx * n_cols + tl.arange(0, BLOCK_SIZE), dX)\n    \n    # Compute dW and dB\n    dW = tl.sum(dY * (X - mean) * rstd, axis=0)\n    dB = tl.sum(dY, axis=0)\n    \n    # Store gradients\n    tl.atomic_add(dW_ptr + tl.arange(0, BLOCK_SIZE), dW)\n    tl.atomic_add(dB_ptr + tl.arange(0, BLOCK_SIZE), dB)\n\ndef layer_norm_forward(X, W, B, eps):\n    n_rows, n_cols = X.shape\n    BLOCK_SIZE = 128\n    Y = torch.empty_like(X)\n    Mean = torch.empty(n_rows, device=X.device)\n    RSTD = torch.empty(n_rows, device=X.device)\n    \n    grid = (n_rows,)\n    _layer_norm_forward_kernel[grid](X, W, B, Y, Mean, RSTD, n_cols, eps, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return Y, Mean, RSTD\n\ndef layer_norm_backward(dY, X, W, Mean, RSTD):\n    n_rows, n_cols = X.shape\n    BLOCK_SIZE = 128\n    dX = torch.empty_like(X)\n    dW = torch.zeros_like(W)\n    dB = torch.zeros_like(B)\n    \n    grid = (n_rows,)\n    _layer_norm_backward_kernel[grid](dY, X, W, Mean, RSTD, dX, dW, dB, n_cols, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return dX, dW, dB\n\nclass LigerLayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, B, eps):\n        X = X.contiguous()\n        W = W.contiguous()\n        B = B.contiguous()\n        \n        Y, Mean, RSTD = layer_norm_forward(X, W, B, eps)\n        \n        ctx.save_for_backward(X, W, Mean, RSTD)\n        ctx.eps = eps\n        \n        return Y\n\n    @staticmethod\n    def backward(ctx, dY):\n        X, W, Mean, RSTD = ctx.saved_tensors\n        eps = ctx.eps\n        \n        dX, dW, dB = layer_norm_backward(dY, X, W, Mean, RSTD)\n        \n        return dX, dW, dB, None\n\n# Example usage:\n# X, W, B = torch.randn(64, 128, device='cuda'), torch.randn(128, device='cuda'), torch.randn(128, device='cuda')\n# eps = 1e-5\n# Y = LigerLayerNormFunction.apply(X, W, B, eps)\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\nimport functools\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\ndef ensure_contiguous(fn):\n    @functools.wraps(fn)\n    def wrapper(ctx, *args, **kwargs):\n        def maybe_to_contiguous(x):\n            return x.contiguous() if isinstance(x, torch.Tensor) else x\n\n        args = [maybe_to_contiguous(arg) for arg in args]\n        kwargs = {k: maybe_to_contiguous(v) for k, v in kwargs.items()}\n        return fn(ctx, *args, **kwargs)\n\n    return wrapper\n\n\ndef calculate_settings(n):\n    MAX_FUSED_SIZE = 65536\n    BLOCK_SIZE = triton.next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(\n            f\"Cannot launch Triton kernel since n = {n} exceeds \"\n            f\"the recommended Triton blocksize = {MAX_FUSED_SIZE}.\"\n        )\n\n    num_warps = 4\n    if BLOCK_SIZE >= 32768:\n        num_warps = 32 if not is_hip() else 16\n    elif BLOCK_SIZE >= 8192:\n        num_warps = 16\n    elif BLOCK_SIZE >= 2048:\n        num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _layer_norm_forward_kernel(\n    Y_ptr,  # pointer to output, shape (n_rows, n_cols)\n    Y_row_stride,  # stride of each row in output\n    X_ptr,  # pointer to input, shape (n_rows, n_cols)\n    X_row_stride,  # stride of each row in input\n    W_ptr,  # pointer to weights, shape (n_cols,)\n    W_row_stride,  # stride of each row in weights\n    B_ptr,  # pointer to bias, shape (n_cols,)\n    B_row_stride,  # stride of each row in bias\n    Mean_ptr,  # pointer to mean, shape (n_rows,)\n    Mean_row_stride,  # stride of each row in mean\n    RSTD_ptr,  # pointer to rstd, shape (n_rows,)\n    RSTD_row_stride,  # stride of each row in rstd\n    n_cols,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    Mean_ptr += row_idx * Mean_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    B_row = tl.load(B_ptr + col_offsets, mask=mask, other=0)\n\n    mean = tl.sum(X_row, axis=0) / n_cols\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / n_cols\n    rstd = tl.rsqrt(var + eps)\n\n    tl.store(Mean_ptr, mean)\n    tl.store(RSTD_ptr, rstd)\n\n    Y_row = (X_row - mean) * rstd * W_row + B_row\n\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n\n@triton.jit\ndef _layer_norm_backward_kernel(\n    X_ptr,  # pointer to input, shape (n_rows, n_cols)\n    W_ptr,  # pointer to weights, shape (n_cols,)\n    Mean_ptr,  # pointer to mean, shape (n_rows,)\n    RSTD_ptr,  # pointer to rstd, shape (n_rows,)\n    DX_ptr,  # pointer to input grad, shape (n_rows, n_cols)\n    DW_ptr,  # pointer to weights grad, shape (n_cols,)\n    DB_ptr,  # pointer to bias grad, shape (n_cols,)\n    DY_ptr,  # pointer to output grad, shape (n_rows, n_cols)\n    stride_x,  # stride of each row in input\n    stride_dx,  # stride of each row in input grad\n    stride_dw,  # stride of each row in weights grad\n    stride_db,  # stride of each row in bias grad\n    stride_dy,  # stride of each row in output grad\n    n_rows,\n    n_cols,\n    rows_per_program: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    dtype: tl.constexpr,\n):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < n_cols\n\n    dw_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    db_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    X_ptr += row_start * stride_x\n    Mean_ptr += row_start\n    RSTD_ptr += row_start\n    DX_ptr += row_start * stride_dx\n    DY_ptr += row_start * stride_dy\n\n    for _ in range(row_start, row_end):\n        x = tl.load(X_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(W_ptr + cols, mask=mask, other=0.0)\n        dy = tl.load(DY_ptr + cols, mask=mask, other=0.0)\n        mean = tl.load(Mean_ptr)\n        rstd = tl.load(RSTD_ptr)\n\n        x_hat = (x - mean) * rstd\n        wdy = w * dy\n        c1 = tl.sum(x_hat * wdy, axis=0) / n_cols\n        c2 = tl.sum(wdy, axis=0) / n_cols\n        dx = (wdy - (x_hat * c1 + c2)) * rstd\n        tl.store(DX_ptr + cols, dx.to(dtype), mask=mask)\n\n        dw_row += dy * x_hat\n        db_row += dy\n\n        X_ptr += stride_x\n        Mean_ptr += 1\n        RSTD_ptr += 1\n        DX_ptr += stride_dx\n        DY_ptr += stride_dy\n\n    tl.store(DW_ptr + row_block_id * stride_dw + cols, dw_row.to(dtype), mask=mask)\n    tl.store(DB_ptr + row_block_id * stride_db + cols, db_row.to(dtype), mask=mask)\n\ndef layer_norm_forward(X, W, B, eps):\n    shape = X.shape\n    dim = shape[-1]\n    X = X.view(-1, dim)\n    n_rows, n_cols = X.shape\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n    Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)\n    Mean = torch.empty(n_rows, dtype=X.dtype, device=X.device)\n    RSTD = torch.empty(n_rows, dtype=X.dtype, device=X.device)\n    assert (\n        X.shape[1] == W.shape[0]\n    ), f\"Incompatible hidden size dimension between input tensor with shape[1] = {X.shape[1]} and weight tensor with shape[0] = {W.shape[0]}\"\n\n    _layer_norm_forward_kernel[(n_rows,)](\n        Y,\n        Y.stride(0),\n        X,\n        X.stride(0),\n        W,\n        W.stride(0),\n        B,\n        B.stride(0),\n        Mean,\n        Mean.stride(0),\n        RSTD,\n        RSTD.stride(0),\n        n_cols,\n        eps,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return Y.view(*shape), X, Mean, RSTD, BLOCK_SIZE, num_warps\n\ndef layer_norm_backward(dY, X, W, B, Mean, RSTD):\n    shape = dY.shape\n    dim = shape[-1]\n    dY = dY.view(-1, dim)\n    n_rows, n_cols = dY.shape\n\n    DX = torch.empty((n_rows, n_cols), dtype=X.dtype, device=X.device)\n    sm_count = torch.cuda.get_device_properties(X.device).multi_processor_count\n    _DW = torch.empty((sm_count, n_cols), dtype=W.dtype, device=W.device)\n    _DB = torch.empty((sm_count, n_cols), dtype=W.dtype, device=W.device)\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n    if n_cols > BLOCK_SIZE:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n\n    rows_per_program = math.ceil(n_rows / sm_count)\n    grid = (sm_count,)\n    triton_dtype = tl.float32 if X.dtype == torch.float32 else tl.bfloat16\n    _layer_norm_backward_kernel[grid](\n        X,\n        W,\n        Mean,\n        RSTD,\n        DX,\n        _DW,\n        _DB,\n        dY,\n        X.stride(0),\n        DX.stride(0),\n        _DW.stride(0),\n        _DB.stride(0),\n        dY.stride(0),\n        n_rows,\n        n_cols,\n        rows_per_program,\n        BLOCK_SIZE=BLOCK_SIZE,\n        dtype=triton_dtype,\n    )\n\n    DW = _DW.sum(dim=0).to(W.dtype)\n    DB = _DB.sum(dim=0).to(W.dtype)\n\n    DX = DX.view(*shape)\n    return DX, DW, DB\n\nclass LigerLayerNormFunction(torch.autograd.Function):\n    @staticmethod\n    @ensure_contiguous\n    def forward(ctx, X, W, B, eps):\n        Y, X, Mean, RSTD, BLOCK_SIZE, num_warps = layer_norm_forward(X, W, B, eps)\n        ctx.save_for_backward(X, W, B, Mean, RSTD)\n        return Y\n\n    @staticmethod\n    @ensure_contiguous\n    def backward(ctx, dY):\n        X, W, B, Mean, RSTD = ctx.saved_tensors\n        DX, DW, DB = layer_norm_backward(dY, X, W, B, Mean, RSTD)\n        return DX, DW, DB, None\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel `_fwd_kernel` performs a context attention mechanism on the given input tensors Q, K, and V. \n        It scales the dot product of Q and K by `sm_scale` to compute attention scores and applies a softmax function. \n        The result is used to weight the values in V, producing the output stored in `Out`. \n        The kernel uses parameters like `B_Start_Loc`, `B_Seqlen`, and `B_req_idx` to manage variable sequence lengths and batching. \n        It operates over blocks of data with dimensions defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_DMODEL`, adapting for different head groups using `kv_group_num`.\n\n        The Python function `context_attention_fwd` sets up the Triton kernel for execution. \n        It configures the grid and block dimensions based on input shapes and Tesla architecture considerations. \n        The function calculates a scaling factor for attention scores and passes necessary strides and shape parameters to the Triton kernel for execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(Q, K, V, Out, B_Start_Loc, B_Seqlen, B_req_idx, sm_scale, \n                stride_qz, stride_qh, stride_qm, stride_qd,\n                stride_kz, stride_kh, stride_kn, stride_kd,\n                stride_vz, stride_vh, stride_vn, stride_vd,\n                stride_oz, stride_oh, stride_om, stride_od,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n                kv_group_num: tl.constexpr):\n    # Define the position of the block in the grid\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    pid_d = tl.program_id(2)\n\n    # Compute the starting indices for this block\n    offset_m = pid_m * BLOCK_M\n    offset_n = pid_n * BLOCK_N\n    offset_d = pid_d * BLOCK_DMODEL\n\n    # Load Q, K, V for this block\n    q = tl.load(Q + offset_m * stride_qm + offset_d * stride_qd)\n    k = tl.load(K + offset_n * stride_kn + offset_d * stride_kd)\n    v = tl.load(V + offset_n * stride_vn + offset_d * stride_vd)\n\n    # Compute scaled dot product attention\n    qk = tl.dot(q, k) * sm_scale\n\n    # Apply softmax to the attention scores\n    attn_scores = tl.softmax(qk)\n\n    # Compute the weighted sum of V\n    out = tl.dot(attn_scores, v)\n\n    # Store the result\n    tl.store(Out + offset_m * stride_om + offset_d * stride_od, out)\n\ndef context_attention_fwd(Q, K, V, B_Start_Loc, B_Seqlen, B_req_idx, sm_scale, kv_group_num):\n    # Extract dimensions\n    B, H, M, D = Q.shape\n    _, _, N, _ = K.shape\n\n    # Define block sizes\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_DMODEL = 64\n\n    # Calculate grid size\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N), triton.cdiv(D, BLOCK_DMODEL))\n\n    # Launch the Triton kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out, B_Start_Loc, B_Seqlen, B_req_idx, sm_scale,\n        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n        K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n        V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n        Out.stride(0), Out.stride(1), Out.stride(2), Out.stride(3),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL,\n        kv_group_num=kv_group_num\n    )\n\n# Example usage\n# context_attention_fwd(Q, K, V, B_Start_Loc, B_Seqlen, B_req_idx, sm_scale, kv_group_num)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\nTESLA = \"Tesla\" in torch.cuda.get_device_name(0)\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    Out,\n    B_Start_Loc,\n    B_Seqlen,\n    Req_to_tokens,\n    B_req_idx,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    kv_group_num,\n    b_prompt_cache_len,\n    H: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n\n    cur_kv_head = cur_head // kv_group_num\n\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len, cur_batch_seq_len + prompt_cache_len)\n\n    # causal mask\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        kv_loc = tl.load(\n            Req_to_tokens + stride_req_to_tokens_b * cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=(start_n + offs_n) < block_end_loc,\n            other=0,\n        )\n        off_k = kv_loc[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :]) < block_end_loc, other=0.0)\n        qk = tl.dot(q, k)\n\n        mask = offs_m[:, None] + prompt_cache_len >= (start_n + offs_n[None, :])\n        qk = tl.where(mask, qk * sm_scale, -1.0e8)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n\n        # -- update m_i and l_i\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        # -- update output accumulator --\n        acc = acc * alpha[:, None]\n        # update acc\n        off_v = kv_loc[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None]) < block_end_loc, other=0.0)\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        # update m_i and l_i\n        m_i = m_ij\n\n    acc = acc / l_i[:, None]\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n\n\n@torch.no_grad()\ndef context_attention_fwd(\n    q, k, v, o, b_req_idx, b_start_loc, b_seq_len, b_prompt_cache_len, max_input_len, req_to_token_indexs\n):\n    BLOCK_M = 128 if not TESLA else 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128, 256}\n\n    # \u8ba1\u7b97scale\u7cfb\u6570, \u5e76\u4e58\u4ee5 1/log(2) = 1.4426950408889634,\n    # \u7b97\u5b50\u5185\u90e8\u4f7f\u7528 tl.math.exp2 \u6765\u4f7f\u8ba1\u7b97\u4e0e\u6807\u51c6attention\u7b49\u4ef7\u3002\n    sm_scale = 1.0 / (Lq ** 0.5) * 1.4426950408889634\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = lambda meta: (triton.cdiv(max_input_len, meta[\"BLOCK_M\"]), batch * head, 1)\n\n    BLOCK_N = BLOCK_M\n    num_warps = 4 if Lk <= 64 else 8\n    num_stages = 1\n\n    _fwd_kernel[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        o,\n        b_start_loc,\n        b_seq_len,\n        req_to_token_indexs,\n        b_req_idx,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        req_to_token_indexs.stride(0),\n        req_to_token_indexs.stride(1),\n        kv_group_num=kv_group_num,\n        b_prompt_cache_len=b_prompt_cache_len,\n        H=head,\n        BLOCK_DMODEL=Lk,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        num_warps=num_warps,\n        num_stages=num_stages,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This script defines a Triton-based custom cross-entropy loss function for PyTorch, supporting features like label smoothing and large-scale parallelization across distributed systems. It leverages two main Triton kernels: `cross_entropy_fwd_kernel` for the forward pass and `cross_entropy_bwd_kernel` for the backward pass.\n\n    The forward kernel (`cross_entropy_fwd_kernel`) computes the log-sum-exp (LSE) and cross-entropy loss, considering label smoothing if applicable. The backward kernel (`cross_entropy_bwd_kernel`) computes the gradients with respect to the logits using the pre-computed LSE and the provided gradient of the losses. These kernels are invoked by the `CrossEntropyLoss` class, which defines the forward and backward static methods.\n\n    The `CrossEntropyLoss` class manages data pointers, block size definitions, and control flow for managing grid dimensions and kernel execution, including communication between distributed systems when necessary.\n\n    The `cross_entropy_loss` function provides a simple interface for users to apply this custom loss function by wrapping around the `CrossEntropyLoss.apply` call.\n\n    Input tensors include:\n    - `logits`: the predicted logits.\n    - `labels`: the true labels.\n    Optional parameters allow for configuration of label smoothing, ignored indices, and distributed settings.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the forward kernel for cross-entropy loss\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    logits_ptr, labels_ptr, output_ptr, lse_ptr,\n    n_classes, label_smoothing, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    # Offsets for the current block\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Load logits and labels\n    logits = tl.load(logits_ptr + offs)\n    labels = tl.load(labels_ptr + offs, mask=offs < n_classes)\n    \n    # Compute Log-Sum-Exp (LSE)\n    max_logits = tl.max(logits, axis=0)\n    logits = logits - max_logits\n    exp_logits = tl.exp(logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n    lse = max_logits + tl.log(sum_exp_logits)\n    \n    # Store LSE for backward pass\n    tl.store(lse_ptr + offs, lse)\n    \n    # Compute cross-entropy loss with label smoothing\n    if label_smoothing > 0:\n        smooth_factor = label_smoothing / n_classes\n        one_hot_labels = (1 - label_smoothing) * tl.eye(n_classes)[labels] + smooth_factor\n    else:\n        one_hot_labels = tl.eye(n_classes)[labels]\n    \n    loss = -tl.sum(one_hot_labels * logits, axis=0) + lse\n    tl.store(output_ptr + offs, loss)\n\n# Define the backward kernel for cross-entropy loss\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    logits_ptr, labels_ptr, grad_output_ptr, lse_ptr, grad_logits_ptr,\n    n_classes, label_smoothing, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    # Offsets for the current block\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Load logits, labels, and LSE\n    logits = tl.load(logits_ptr + offs)\n    labels = tl.load(labels_ptr + offs, mask=offs < n_classes)\n    lse = tl.load(lse_ptr + offs)\n    \n    # Compute softmax probabilities\n    exp_logits = tl.exp(logits - lse)\n    probs = exp_logits / tl.sum(exp_logits, axis=0)\n    \n    # Compute gradients\n    if label_smoothing > 0:\n        smooth_factor = label_smoothing / n_classes\n        one_hot_labels = (1 - label_smoothing) * tl.eye(n_classes)[labels] + smooth_factor\n    else:\n        one_hot_labels = tl.eye(n_classes)[labels]\n    \n    grad_logits = probs - one_hot_labels\n    grad_output = tl.load(grad_output_ptr + offs)\n    grad_logits *= grad_output\n    \n    # Store gradients\n    tl.store(grad_logits_ptr + offs, grad_logits)\n\nclass CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, labels, label_smoothing=0.0):\n        n_classes = logits.size(-1)\n        batch_size = logits.size(0)\n        \n        # Allocate output tensors\n        output = torch.empty(batch_size, device=logits.device, dtype=logits.dtype)\n        lse = torch.empty(batch_size, device=logits.device, dtype=logits.dtype)\n        \n        # Launch forward kernel\n        grid = lambda meta: (triton.cdiv(batch_size, meta['BLOCK_SIZE']),)\n        cross_entropy_fwd_kernel[grid](\n            logits, labels, output, lse,\n            n_classes, label_smoothing, BLOCK_SIZE=128\n        )\n        \n        ctx.save_for_backward(logits, labels, lse)\n        ctx.label_smoothing = label_smoothing\n        return output.sum()\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        logits, labels, lse = ctx.saved_tensors\n        n_classes = logits.size(-1)\n        batch_size = logits.size(0)\n        \n        # Allocate gradient tensor\n        grad_logits = torch.empty_like(logits)\n        \n        # Launch backward kernel\n        grid = lambda meta: (triton.cdiv(batch_size, meta['BLOCK_SIZE']),)\n        cross_entropy_bwd_kernel[grid](\n            logits, labels, grad_output, lse, grad_logits,\n            n_classes, ctx.label_smoothing, BLOCK_SIZE=128\n        )\n        \n        return grad_logits, None, None\n\ndef cross_entropy_loss(logits, labels, label_smoothing=0.0):\n    return CrossEntropyLoss.apply(logits, labels, label_smoothing)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    loss_ptr,  # data ptrs\n    lse_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    n_rows,\n    logits_row_stride,  # strides\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    # if SPLIT (e.g. tensor parallel), don't include the LSE in the loss since it's not the final LSE\n    SPLIT: tl.constexpr,\n):\n    # Triton kernel implementation for the forward pass of cross-entropy with label smoothing.\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(tl.float32)\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols, (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx)\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n            else:\n                loss = 0.0\n        if not SPLIT:\n            loss += lse_square_scale * lse * lse\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    dlogits_ptr,  # data ptrs\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    logits_row_stride,  # strides\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    # Triton kernel implementation for the backward pass of cross-entropy with label smoothing.\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(tl.float32)\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * probs, mask=col_offsets < n_cols)\n\nclass CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        logits,\n        labels,\n        smoothing,\n        lse_square_scale=0.0,\n        ignored_index=-100,\n        inplace_backward=False,\n        process_group=None,\n    ):\n        # CrossEntropyLoss forward function leveraging the Triton kernel.\n        n_rows, n_cols = logits.shape\n        assert labels.shape == (n_rows,)\n        world_size = 1 if process_group is None else torch.distributed.get_world_size(process_group)\n        total_classes = world_size * n_cols\n        rank = 0 if process_group is None else torch.distributed.get_rank(process_group)\n        class_start_idx = rank * n_cols\n\n        if logits.stride(-1) != 1:\n            logits = logits.contiguous()\n        MAX_BLOCK_SIZE = 64 * 1024\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), MAX_BLOCK_SIZE)\n        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else (16 if BLOCK_SIZE < 128 * 1024 else 32))\n        split = world_size > 1 or n_cols > MAX_BLOCK_SIZE\n        n_splits = (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE\n        loss_shape = (n_splits, n_rows) if n_splits > 1 else (n_rows,)\n        losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        lse = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_fwd_kernel[(n_rows, n_splits)](\n                losses,  # data ptrs\n                lse,\n                logits,\n                labels,\n                smoothing,\n                lse_square_scale,\n                ignored_index,\n                total_classes,\n                class_start_idx,\n                n_cols,  # shapes\n                n_rows,\n                logits.stride(0),  # strides\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n                SPLIT=split,\n            )\n\n        if split:\n            if world_size > 1:\n                lse_allgather = torch.empty(world_size, n_rows, dtype=lse.dtype, device=lse.device)\n                torch.distributed.all_gather_into_tensor(lse_allgather, lse, group=process_group)\n                handle_losses = torch.distributed.all_reduce(\n                    losses, op=torch.distributed.ReduceOp.SUM, group=process_group, async_op=True\n                )\n                lse = torch.logsumexp(lse_allgather, dim=0)\n                handle_losses.wait()\n            else:\n                lse = torch.logsumexp(lse, dim=0)\n                losses = losses.sum(dim=0)\n            losses += lse\n            if lse_square_scale != 0.0:\n                losses += lse_square_scale * lse.square()\n            losses.masked_fill_(labels == ignored_index, 0.0)\n\n        ctx.save_for_backward(logits, lse, labels)\n        ctx.smoothing = smoothing\n        ctx.lse_square_scale = lse_square_scale\n        ctx.ignored_index = ignored_index\n        ctx.total_classes = total_classes\n        ctx.class_start_idx = class_start_idx\n        ctx.inplace_backward = inplace_backward\n        return losses\n\n    @staticmethod\n    def backward(ctx, grad_losses):\n        logits, lse, labels = ctx.saved_tensors\n        dlogits = logits if ctx.inplace_backward else torch.empty_like(logits)\n        n_rows, n_cols = logits.shape\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 4 * 1024)\n        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else 16)\n        grid = lambda META: (n_rows, triton.cdiv(n_cols, META[\"BLOCK_SIZE\"]))  # noqa\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_bwd_kernel[grid](\n                dlogits,  # data ptrs\n                grad_losses,\n                logits,\n                lse,\n                labels,\n                ctx.smoothing,\n                ctx.lse_square_scale,\n                ctx.ignored_index,\n                ctx.total_classes,\n                ctx.class_start_idx,\n                n_cols,  # shapes\n                logits.stride(0),  # strides\n                dlogits.stride(0),\n                grad_losses.stride(0),\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n            )\n        return dlogits, None, None, None, None, None, None, None\n\ndef cross_entropy_loss(\n    logits: torch.Tensor,\n    labels: torch.Tensor,\n    label_smoothing: float = 0.0,\n    lse_square_scale: float = 0.0,\n    ignored_index=-100,\n    inplace_backward: bool = False,\n    process_group=None,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    CrossEntropyLoss wrapper function for calling the custom autograd Function.\n    \"\"\"\n    return CrossEntropyLoss.apply(\n        logits,\n        labels,\n        label_smoothing,\n        lse_square_scale,\n        ignored_index,\n        inplace_backward,\n        process_group,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a masked selection operation using Triton, a language designed for high-performance GPU computing. The main function, `masked_select`, takes two tensors, `inp` and `mask`, and selects elements from `inp` based on `mask`. If `mask[i]` is true, `inp[i]` is included in the output.\n\n        The kernel `masked_select_kernel` is written in Triton. It processes data in blocks defined by `BLOCK_SIZE`. For each block, it computes offsets and checks if they are within `n_elements`. It loads input data and selection masks, computes output offsets using a prefix sum, and stores the selected input elements to the output buffer if the corresponding `mask` element is true.\n\n        The `cfggen` function generates a list of Triton configuration objects, varying the block size and number of warps to optimize the kernel execution. The `broadcastable` function checks whether two shapes are compatible for broadcasting, a feature that aligns tensors of different shapes for operations like addition or multiplication.\n        ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef masked_select_kernel(inp_ptr, mask_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the block index and the starting offset\n    block_start = tl.program_id(0) * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Mask to ensure we don't read out of bounds\n    mask = offsets < n_elements\n\n    # Load input and mask data\n    inp = tl.load(inp_ptr + offsets, mask=mask, other=0.0)\n    mask_vals = tl.load(mask_ptr + offsets, mask=mask, other=False)\n\n    # Compute prefix sum to determine output positions\n    selected_offsets = tl.zeros([BLOCK_SIZE], dtype=tl.int32)\n    prefix_sum = tl.zeros([BLOCK_SIZE], dtype=tl.int32)\n    prefix_sum[0] = mask_vals[0]\n    for i in range(1, BLOCK_SIZE):\n        prefix_sum[i] = prefix_sum[i - 1] + mask_vals[i]\n\n    # Store selected elements\n    for i in range(BLOCK_SIZE):\n        if mask_vals[i]:\n            selected_offsets[i] = prefix_sum[i] - 1\n\n    out_indices = block_start + selected_offsets\n    tl.store(out_ptr + out_indices, inp, mask=mask_vals)\n\ndef masked_select(inp, mask):\n    assert inp.shape == mask.shape, \"Input and mask must have the same shape\"\n    n_elements = inp.numel()\n\n    # Allocate output buffer\n    out_size = mask.sum().item()\n    out = torch.empty(out_size, dtype=inp.dtype, device=inp.device)\n\n    # Launch kernel\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    masked_select_kernel[grid](inp, mask, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return out\n\ndef cfggen():\n    configs = []\n    for block_size in [128, 256, 512, 1024]:\n        for num_warps in [1, 2, 4]:\n            configs.append(triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps))\n    return configs\n\ndef broadcastable(shape1, shape2):\n    # Reverse shapes for easier alignment\n    shape1, shape2 = shape1[::-1], shape2[::-1]\n    for s1, s2 in zip(shape1, shape2):\n        if s1 != s2 and s1 != 1 and s2 != 1:\n            return False\n    return True\n\n# Example usage\ninp = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, device='cuda')\nmask = torch.tensor([True, False, True, False, True], dtype=torch.bool, device='cuda')\nout = masked_select(inp, mask)\nprint(out)  # Output: tensor([1., 3., 5.], device='cuda:0')\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n\nShape = Tuple[int]\n\ndef broadcastable(s1: Shape, s2: Shape) -> bool:\n    r1 = len(s1)\n    if r1 == 0:\n        return True\n    r2 = len(s2)\n    if r2 == 0:\n        return True\n\n    s1, s2 = (s1, s2) if r1 >= r2 else (s2, s1)\n    r1, r2 = (r1, r2) if r1 >= r2 else (r2, r1)\n\n    d = r1 - r2\n    for i in range(r2):\n        if s1[d + i] == 1 or s2[i] == 1 or s1[d + i] == s2[i]:\n            continue\n        return False\n    return True\n\n\ndef cfggen():\n    configs = [\n        triton.Config({\"BLOCK_SIZE\": bs}, num_warps=w)\n        for w in [4, 8, 16, 32]\n        for bs in [256, 512, 1024, 2048, 4096]\n    ]\n    return configs\n\n\n\n@triton.autotune(configs=cfggen(), key=[\"n_elements\"])\n@triton.jit\ndef masked_select_kernel(\n    inp_ptr,\n    select_mask_ptr,\n    prefix_sum_ptr,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n\n    inp = tl.load(inp_ptr + offsets, mask=mask, other=0.0)\n    select_mask = tl.load(select_mask_ptr + offsets, mask=mask, other=0.0).to(tl.int1)\n    out_offset = tl.load(prefix_sum_ptr + offsets, mask=mask, other=0.0) - 1\n\n    tl.store(out_ptr + out_offset, inp, mask=(select_mask and mask))\n\n\ndef masked_select(inp, mask):\n    logging.debug(\"GEMS MASKED SELECT\")\n\n    inp_shape = tuple(inp.shape)\n    mask_shape = tuple(mask.shape)\n\n    assert broadcastable(\n        inp_shape, mask_shape\n    ), \"The shapes of the `mask` and the `input` tensor must be broadcastable\"\n    inp, mask = torch.broadcast_tensors(inp, mask)\n\n    inp = inp.contiguous()\n    mask = mask.contiguous()\n\n    mask_flattened = mask.ravel()\n\n    prefix_sum = mask_flattened.cumsum(axis=0)\n    out = torch.empty(prefix_sum[-1].item(), dtype=inp.dtype, device=inp.device)\n\n    n_elements = inp.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    with torch.cuda.device(inp.device):\n        masked_select_kernel[grid](inp, mask_flattened, prefix_sum, out, n_elements)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton operator `_fwd_kernel` is designed to perform a custom scaled dot-product attention computation, which is a key component in Transformer models. The operator works with batched inputs of query (`Q`), key (`K`), and value (`V`) tensors, scaling factor (`sm_scale`), and batch metadata for sequence positions (`B_Start_Loc`) and sequence lengths (`B_Seqlen`). The function outputs results to `Out`.\n\nThe `context_attention_fwd` function is a wrapper that prepares the inputs and launches the `_fwd_kernel` with appropriate configurations.\n\nThe main logic of `_fwd_kernel` includes:\n- Iterating over blocks of keys and values, based on a block size `BLOCK`.\n- Computing the dot-product `qk` between `Q` and `K` blocks, and scaling by `sm_scale`.\n- Applying a sliding window attention mask to limit the attention score computation to a specific range.\n- Updating accumulators for the output using numerically stable techniques for exponentiation and scaling.\n- Writing the computed attention outputs back to `Out`.\n\nThe grid and number of warps are chosen based on input sizes to optimize parallel execution.\n\n`context_attention_fwd` configures the execution of `_fwd_kernel` and ensures the correct computation across the batch and attention heads.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the kernel\n@triton.jit\ndef _fwd_kernel(Q, K, V, Out, B_Start_Loc, B_Seqlen, sm_scale, BLOCK: tl.constexpr):\n    # Get the program ID for parallel execution\n    pid = tl.program_id(axis=0)\n\n    # Calculate the batch index and head index\n    batch_idx = pid // B_Seqlen.shape[0]\n    head_idx = pid % B_Seqlen.shape[0]\n\n    # Calculate start location and sequence length for this batch\n    start_loc = B_Start_Loc[batch_idx]\n    seqlen = B_Seqlen[batch_idx]\n\n    # Load the query for this block\n    q = tl.load(Q + pid * BLOCK)\n\n    # Initialize accumulators for attention scores and outputs\n    acc_scores = tl.zeros([BLOCK], dtype=tl.float32)\n    acc_out = tl.zeros([BLOCK, V.shape[-1]], dtype=tl.float32)\n\n    # Iterate over blocks of keys and values\n    for i in range(0, seqlen, BLOCK):\n        # Load key and value blocks\n        k = tl.load(K + (start_loc + i) * BLOCK)\n        v = tl.load(V + (start_loc + i) * BLOCK * V.shape[-1])\n\n        # Compute dot-product qk and apply scaling\n        qk = tl.dot(q, k) * sm_scale\n\n        # Apply attention mask (e.g., causal mask for autoregressive models)\n        mask = tl.arange(0, BLOCK) < (seqlen - i)\n        qk = tl.where(mask, qk, float('-inf'))\n\n        # Compute softmax scores using numerically stable technique\n        max_qk = tl.max(qk, axis=0)\n        exp_qk = tl.exp(qk - max_qk)\n        sum_exp_qk = tl.sum(exp_qk, axis=0)\n\n        # Update accumulators\n        acc_scores += sum_exp_qk\n        acc_out += tl.dot(exp_qk, v)\n\n    # Normalize the accumulated outputs by the sum of scores\n    acc_out /= acc_scores\n\n    # Write the result to the output tensor\n    tl.store(Out + pid * BLOCK * V.shape[-1], acc_out)\n\n# Wrapper function to launch the kernel\ndef context_attention_fwd(Q, K, V, B_Start_Loc, B_Seqlen, sm_scale, BLOCK=128):\n    # Calculate grid size based on batch and head dimensions\n    grid = (Q.shape[0] * Q.shape[1],)\n    \n    # Launch the Triton kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out, B_Start_Loc, B_Seqlen, sm_scale,\n        BLOCK=BLOCK,\n        num_warps=4  # Choose number of warps based on performance tuning\n    )\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\nimport math\nimport torch.nn.functional as F\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    B_Start_Loc,\n    B_Seqlen,  # B_LOC \u5185\u90e8\u8bb0\u5f55\u6bcf\u4e2abatch \u8f93\u5165\u7684\u771f\u5b9e\u4f4d\u7f6e\uff0c B_SEQ_len \u8bb0\u5f55\u5f53\u524d\u8f93\u5165\u7684\u771f\u5b9e\u957f\u5ea6\n    Out,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    sliding_window,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n\n    cur_kv_head = cur_head // kv_group_num\n\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n            start_n = tl.multiple_of(start_n, BLOCK_N)\n            # -- compute qk ----\n            k = tl.load(\n                k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,\n                mask=(start_n + offs_n[None, :]) < cur_batch_seq_len,\n                other=0.0,\n            )\n            \n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, k)\n            qk *= sm_scale\n            # [SYM] mask outside of windows\uff0c\u4f7f\u7528\u5927\u8d1f\u6570\u4ee3\u66ff -inf\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, -1e9)\n            qk = tl.where((start_n + offs_n[None, :]) > (offs_m[:, None] - sliding_window), qk, -1e9)\n\n            # -- compute m_ij, p, l_ij\n            m_ij = tl.max(qk, 1)\n            # \u9632\u6b62 m_ij \u4e3a -1e9 \u5bfc\u81f4\u7684\u6570\u503c\u95ee\u9898\n            m_ij = tl.where(m_ij == -1e9, 0.0, m_ij)\n            p = tl.exp(qk - m_ij[:, None])\n            l_ij = tl.sum(p, 1)\n            \n            # -- update m_i and l_i\n            m_i_new = tl.maximum(m_i, m_ij)\n            alpha = tl.exp(m_i - m_i_new)\n            beta = tl.exp(m_ij - m_i_new)\n            l_i_new = alpha * l_i + beta * l_ij\n            l_i_new = tl.where(l_i_new == 0.0, 1e-9, l_i_new)  # \u9632\u6b62\u9664\u96f6\n            \n            # -- update output accumulator --\n            # scale p\n            p_scale = beta / l_i_new\n            p = p * p_scale[:, None]\n            # scale acc\n            acc_scale = l_i / l_i_new * alpha\n            acc = acc * acc_scale[:, None]\n            # update acc\n            v = tl.load(\n                v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,\n                mask=(start_n + offs_n[:, None]) < cur_batch_seq_len,\n                other=0.0,\n            )\n\n            p = p.to(v.dtype)\n            acc += tl.dot(p, v)\n            # update m_i and l_i\n            l_i = l_i_new\n            m_i = m_i_new\n    # initialize pointers to output\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n    return\n\n\n@torch.no_grad()\ndef context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, sliding_window):\n    BLOCK = 128\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n\n    sm_scale = 1.0 / (Lq ** 0.5)  # \u8ba1\u7b97scale\u7cfb\u6570\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n\n    num_warps = 4 if Lk <= 64 else 8\n    _fwd_kernel[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        b_start_loc,\n        b_seq_len,\n        o,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        kv_group_num=kv_group_num,\n        sliding_window=sliding_window,\n        BLOCK_M=BLOCK,\n        BLOCK_DMODEL=Lk,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton operator implements a forward pass of a multi-head attention mechanism optimized for block processing on GPUs. The main kernel `_attn_fwd` takes as input query (Q), key (K), and value (V) matrices along with their scaling factors (Q_scale, K_scale), and computes the attention output (Out).\n\n            - `_attn_fwd_inner`: This function performs the core computation of attention mechanism within a certain range defined by BLOCK_M and BLOCK_N. It calculates attention scores, applies masks, performs softmax normalization, and accumulates the results. It handles different stages of computation as defined by STAGE parameter, which affects the processing range and logic.\n\n            - `_attn_fwd`: This function orchestrates the overall attention computation by preparing input offsets and pointers, invoking `_attn_fwd_inner` for block processing, and storing the results back into the output tensor. It initializes the necessary variables and handles tensor strides for multi-dimensional input data. It computes results in stages to handle large sequence lengths efficiently.\n\n            - `forward`: This is a wrapper function to set up and launch the Triton kernel `_attn_fwd`. It initializes the output tensor, computes the grid for kernel launch, and manages input dimensions and strides.\n\n            The code is optimized for performance by processing blocks of data at a time and leveraging GPU parallelism through Triton. Key constants include BLOCK_M, BLOCK_N, HEAD_DIM, and STAGE which dictate the computational granularity and flow.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants for block processing\nBLOCK_M = 128\nBLOCK_N = 128\nHEAD_DIM = 64\nSTAGE = 2  # Number of stages for handling large sequences\n\n@triton.jit\ndef _attn_fwd_inner(Q, K, V, Out, Q_scale, K_scale, stride_qz, stride_qh, stride_qm,\n                    stride_kz, stride_kh, stride_kn, stride_vz, stride_vh, stride_vm,\n                    stride_oz, stride_oh, stride_om, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr):\n    # Program ID and offsets\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    pid_h = tl.program_id(2)\n\n    # Offsets for query, key, value, and output\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_h = pid_h\n\n    # Load Q, K, V blocks\n    Q_block = tl.load(Q + offs_h * stride_qh + offs_m[:, None] * stride_qm)\n    K_block = tl.load(K + offs_h * stride_kh + offs_n[None, :] * stride_kn)\n    V_block = tl.load(V + offs_h * stride_vh + offs_n[None, :] * stride_vm)\n\n    # Scale and compute attention scores\n    Q_scaled = Q_block * Q_scale\n    K_scaled = K_block * K_scale\n    attn_scores = tl.dot(Q_scaled, K_scaled.T)\n\n    # Softmax normalization\n    attn_scores = attn_scores - tl.max(attn_scores, axis=1, keepdims=True)\n    attn_weights = tl.exp(attn_scores)\n    attn_weights = attn_weights / tl.sum(attn_weights, axis=1, keepdims=True)\n\n    # Compute output\n    Out_block = tl.dot(attn_weights, V_block)\n\n    # Store results\n    tl.store(Out + offs_h * stride_oh + offs_m[:, None] * stride_om, Out_block)\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Out, Q_scale, K_scale, stride_qz, stride_qh, stride_qm,\n              stride_kz, stride_kh, stride_kn, stride_vz, stride_vh, stride_vm,\n              stride_oz, stride_oh, stride_om, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr):\n    # Launch the inner kernel in a grid of blocks\n    grid = (tl.cdiv(Q.shape[0], BLOCK_M), tl.cdiv(K.shape[1], BLOCK_N), Q.shape[1])\n    _attn_fwd_inner[grid](Q, K, V, Out, Q_scale, K_scale, stride_qz, stride_qh, stride_qm,\n                          stride_kz, stride_kh, stride_kn, stride_vz, stride_vh, stride_vm,\n                          stride_oz, stride_oh, stride_om, BLOCK_M, BLOCK_N, HEAD_DIM)\n\ndef forward(Q, K, V, Q_scale, K_scale):\n    # Determine output dimensions and allocate output tensor\n    Z, H, M, N = Q.shape[0], Q.shape[1], Q.shape[2], K.shape[2]\n    Out = torch.empty((Z, H, M, HEAD_DIM), device=Q.device, dtype=Q.dtype)\n\n    # Strides for each dimension\n    stride_qz, stride_qh, stride_qm = Q.stride(0), Q.stride(1), Q.stride(2)\n    stride_kz, stride_kh, stride_kn = K.stride(0), K.stride(1), K.stride(2)\n    stride_vz, stride_vh, stride_vm = V.stride(0), V.stride(1), V.stride(2)\n    stride_oz, stride_oh, stride_om = Out.stride(0), Out.stride(1), Out.stride(2)\n\n    # Launch the Triton kernel\n    grid = (tl.cdiv(M, BLOCK_M), tl.cdiv(N, BLOCK_N), H)\n    _attn_fwd[grid](Q, K, V, Out, Q_scale, K_scale, stride_qz, stride_qh, stride_qm,\n                    stride_kz, stride_kh, stride_kn, stride_vz, stride_vh, stride_vm,\n                    stride_oz, stride_oh, stride_om, BLOCK_M, BLOCK_N, HEAD_DIM)\n\n    return Out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n        K_scale_ptr += lo // BLOCK_N\n        K_ptrs += HEAD_DIM * lo\n        V_ptrs += HEAD_DIM * lo\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask = k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        if STAGE == 2:\n            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n            qk = qk + tl.where(mask, 0, -1.0e6)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask = (offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        acc += tl.dot(p, v, out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i, m_i\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr  \n              ):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N) \n    \n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX \n                                    )\n    acc, l_i, _ = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    2, offs_m, offs_n, N_CTX \n                                    )\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.float16)\n    stage = 3\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=4)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `_bgmv_shrink_kernel` performs a batched generalized matrix-vector multiplication (BGMV) with a \"shrink\" operation using a lora (low-rank adaptation) mechanism. The kernel takes inputs and scales the product of the inputs with a set of lora weights, storing the results in an output tensor.\n\n            The inputs to the kernel are:\n            - `input_ptr`: pointer to the input data.\n            - `lora_ptr`: pointer to the lora weight matrix.\n            - `out_ptr`: pointer to the output tensor.\n            - `N`: the number of columns in the lora matrix.\n            - `K`: the number of rows in the lora matrix.\n            - `lora_indices`: indices indicating which lora matrix to use for each batch.\n            - `scaling`: a scaling factor applied to the result.\n            - Strides (`xm_stride`, `xk_stride`, etc.) define how to traverse the input, lora, and output tensors.\n            - `BLOCK_N`, `BLOCK_K`, `SPLIT_K`: these constants define the tile size for loading data and parallelism.\n\n            The kernel iterates over the input tensor in blocks of `BLOCK_K` rows. For each block, it multiplies the input block by the corresponding lora weight block, accumulates the results, applies the scaling, and stores the result in the output tensor. If `SPLIT_K` is greater than 1, it performs an atomic add to handle reduction across multiple kernel instances.\n\n            The wrapper function `_bgmv_shrink` sets up the kernel execution environment. It ensures the inputs are contiguous, determines batch size, and prepares kernel grid dimensions based on `SPLIT_K` and batch size. Then, it launches the kernel with the given parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_shrink_kernel(input_ptr, lora_ptr, out_ptr, lora_indices, scaling,\n                        N, K, xm_stride, xk_stride, lm_stride, lk_stride, om_stride,\n                        BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr):\n    pid = tl.program_id(0)\n    batch_id = tl.program_id(1)\n\n    # Determine start of the batch\n    lora_idx = lora_indices[batch_id]\n    lora_offset = lora_idx * lm_stride\n\n    # Determine the starting positions\n    n_start = pid * BLOCK_N\n    k_start = 0\n\n    # Create accumulators for the results\n    acc = tl.zeros([BLOCK_N], dtype=tl.float32)\n\n    # Iterate over K dimension in blocks\n    for k_offset in range(0, K, BLOCK_K):\n        # Load input and lora blocks\n        input_block = tl.load(input_ptr + batch_id * xm_stride + k_offset * xk_stride + tl.arange(0, BLOCK_K))\n        lora_block = tl.load(lora_ptr + lora_offset + k_offset * lk_stride + tl.arange(0, BLOCK_K))\n\n        # Perform the matrix-vector multiplication\n        acc += tl.dot(input_block, lora_block)\n\n    # Apply scaling\n    acc *= scaling\n\n    # Store results\n    out_offset = batch_id * om_stride + n_start\n    if SPLIT_K > 1:\n        # Use atomic add for reductions across multiple kernel instances\n        tl.atomic_add(out_ptr + out_offset, acc)\n    else:\n        tl.store(out_ptr + out_offset, acc)\n\nimport torch\n\ndef _bgmv_shrink(input_tensor, lora_tensor, output_tensor, lora_indices, scaling,\n                 BLOCK_N=128, BLOCK_K=128, SPLIT_K=1):\n    # Ensure inputs are contiguous\n    input_tensor = input_tensor.contiguous()\n    lora_tensor = lora_tensor.contiguous()\n    output_tensor = output_tensor.contiguous()\n\n    # Determine dimensions\n    batch_size = input_tensor.shape[0]\n    N = lora_tensor.shape[1]\n    K = lora_tensor.shape[0]\n\n    # Strides\n    xm_stride = input_tensor.stride(0)\n    xk_stride = input_tensor.stride(1)\n    lm_stride = lora_tensor.stride(0)\n    lk_stride = lora_tensor.stride(1)\n    om_stride = output_tensor.stride(0)\n\n    # Grid dimensions\n    grid = (triton.cdiv(N, BLOCK_N), batch_size)\n\n    # Launch the kernel\n    _bgmv_shrink_kernel[grid](\n        input_tensor, lora_tensor, output_tensor, lora_indices, scaling,\n        N, K, xm_stride, xk_stride, lm_stride, lk_stride, om_stride,\n        BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, SPLIT_K=SPLIT_K\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n# from .utils import get_lora_op_configs\n\n@triton.jit\ndef _bgmv_shrink_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    lora_indices,\n    scaling,\n    xm_stride,\n    xk_stride,\n    l0_stride,\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n):\n    pid_sk = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n\n    offset_n = tl.arange(0, BLOCK_N)\n    offset_k = tl.arange(0, BLOCK_K) + pid_sk * BLOCK_K\n    a_ptr = input_ptr + cur_batch * xm_stride\n    b_ptr = lora_ptr + l0_stride * lora_index\n    accumulator = tl.zeros((BLOCK_N, ), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K * SPLIT_K):\n        current_k = k + offset_k\n        current_k_c = tl.max_contiguous(current_k, BLOCK_K)\n        tiled_a = tl.load(\n            a_ptr + current_k_c,\n            mask=current_k < K,\n            other=0.0,\n        )\n        b_ptr_mask = (offset_n[:, None] < N) & (current_k[None, :] < K)\n\n        tiled_b = tl.load(\n            b_ptr + offset_n[:, None] * lora_k_stride +\n            current_k[None, :] * lora_n_stride,\n            mask=b_ptr_mask,\n            other=0.0,\n        )\n\n        accumulator += tl.sum(tiled_a * tiled_b, 1)\n    accumulator *= scaling\n    offset_cn = tl.arange(0, BLOCK_N)\n    c_ptr = out_ptr + cur_batch * cm_stride + offset_cn * cn_stride\n    c_mask = offset_cn < N\n    if SPLIT_K == 1:\n        tl.store(c_ptr, accumulator, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptr, accumulator, mask=c_mask)\n\n\n@torch.inference_mode()\ndef _bgmv_shrink(\n    inputs: torch.Tensor,\n    lora_a_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    scaling: float = 1.0,\n) -> None:\n    assert inputs.dtype == lora_a_weights.dtype\n    assert inputs.dtype in [torch.float16, torch.bfloat16]\n    assert lora_a_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(1) == lora_a_weights.size(-1)\n    assert inputs.is_contiguous()\n\n    if lora_a_weights.ndim == 4:\n        assert lora_a_weights.size(1) == 1\n        lora_a_weights = lora_a_weights.squeeze(dim=1)\n    else:\n        assert lora_a_weights.ndim == 3\n    assert lora_a_weights.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    batches = lora_indices_tensor.size(0)\n    N, K = lora_a_weights.shape[-2:]\n    BLOCK_N = triton.next_power_of_2(N)\n    # config = get_lora_op_configs(\"bgmv_shrink\", batches, K)\n\n    grid = lambda META: (\n        META[\"SPLIT_K\"],\n        batches,\n    )\n    _bgmv_shrink_kernel[grid](\n        inputs,\n        lora_a_weights,\n        output_tensor,\n        N,\n        K,\n        lora_indices_tensor,\n        scaling,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_a_weights.stride(0),\n        lora_a_weights.stride(1),\n        lora_a_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        BLOCK_N=BLOCK_N,\n        BLOCK_K=256,\n        SPLIT_K=64,\n\n        # **config,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator performs a forward softmax operation on token logits. The kernel function '_fwd_kernel_token_softmax' computes the softmax on a per-head, per-batch basis for each token sequence. It reads the logits from 'Logics', uses a block of size 'BLOCK_SIZE' to load and process data, applies the softmax normalization, and writes the output probabilities to 'Prob_Out'. The stride parameters manage how data is laid out in memory. The utility function 'token_softmax_fwd' prepares the data and configures the kernel launch, handling batched and multi-headed data.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 128  # This can be tuned based on your hardware and problem size\n\n@triton.jit\ndef _fwd_kernel_token_softmax(Logits, Prob_Out, stride_logits_m, stride_logits_h, stride_logits_t, stride_prob_m, stride_prob_h, stride_prob_t, BLOCK_SIZE: tl.constexpr):\n    # Get the program ID for the current block\n    pid = tl.program_id(0)\n    \n    # Compute the batch, head, and token indices\n    batch_idx = pid // (stride_logits_h * stride_logits_t)\n    head_idx = (pid // stride_logits_t) % stride_logits_h\n    token_idx = pid % stride_logits_t\n\n    # Compute the offset in the logits and output tensors\n    logits_offset = batch_idx * stride_logits_m + head_idx * stride_logits_h + token_idx * stride_logits_t\n    prob_offset = batch_idx * stride_prob_m + head_idx * stride_prob_h + token_idx * stride_prob_t\n\n    # Load the logits for the current token sequence\n    logits = tl.load(Logits + logits_offset + tl.arange(0, BLOCK_SIZE))\n\n    # Compute the max for numerical stability\n    max_logits = tl.max(logits, axis=0)\n    logits = logits - max_logits\n\n    # Compute the exponentials\n    exp_logits = tl.exp(logits)\n\n    # Compute the sum of exponentials\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n\n    # Compute the probabilities\n    probabilities = exp_logits / sum_exp_logits\n\n    # Store the result\n    tl.store(Prob_Out + prob_offset + tl.arange(0, BLOCK_SIZE), probabilities)\n\ndef token_softmax_fwd(logits, prob_out, batch_size, num_heads, seq_length):\n    # Calculate the strides for the logits and output tensors\n    stride_logits_m = num_heads * seq_length\n    stride_logits_h = seq_length\n    stride_logits_t = 1\n\n    stride_prob_m = num_heads * seq_length\n    stride_prob_h = seq_length\n    stride_prob_t = 1\n\n    # Calculate the number of blocks needed\n    num_blocks = batch_size * num_heads * seq_length // BLOCK_SIZE\n\n    # Launch the kernel\n    _fwd_kernel_token_softmax[(num_blocks,)](\n        logits, prob_out,\n        stride_logits_m, stride_logits_h, stride_logits_t,\n        stride_prob_m, stride_prob_h, stride_prob_t,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for forward token softmax\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logics, B_Start_Loc, B_Seqlen,\n    Prob_Out,\n    stride_logic_h, stride_logic_bs,\n    stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    row = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_in_all_start_index + col_offsets) * stride_logic_bs,\n                  mask=col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    tl.store(Prob_Out + cur_head * stride_prob_h + (cur_batch_in_all_start_index + col_offsets)\n             * stride_prob_bs, softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n\n# Function to launch the Triton kernel\n@torch.no_grad()\ndef token_softmax_fwd(Logics, B_Start_Loc, B_Seqlen, Prob_Out, max_input_len):\n    BLOCK_SIZE = triton.next_power_of_2(max_input_len)\n    batch, head_num = B_Start_Loc.shape[0], Logics.shape[0]\n\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n\n    _fwd_kernel_token_softmax[(batch, head_num)](\n        Logics, B_Start_Loc, B_Seqlen,\n        Prob_Out,\n        Logics.stride(0), Logics.stride(1),\n        Prob_Out.stride(0), Prob_Out.stride(1),\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton-based kernel is a matrix multiplication operator implemented as a function `matmul_kernel`. \n        The kernel performs matrix multiplication of two input matrices `a` and `b`, and writes the result into matrix `c`. \n        The function `matmul` serves as a wrapper around the kernel to manage configuration and dispatch execution.\n\n        `matmul_kernel` operates with block-wise parallelism, leveraging Triton's capabilities to divide work across multiple GPU threads. \n        The key parameters include BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K, which define the dimensions of the computational blocks \n        used for tiling the input matrices and optimizing data throughput. \n\n        Inputs to the kernel include pointers to the matrices (`a_ptr`, `b_ptr`, `c_ptr`), dimensions (`M`, `N`, `K`), and strides for each matrix. \n        The `stride_am`, `stride_ak`, etc., define how to traverse the matrix elements linearly in memory. \n\n        The outer function `matmul` defines configuration settings based on the data type, ensuring compatibility \n        and optimizing execution by choosing suitable block sizes, group sizes, and other meta parameters \n        that influence performance like number of stages and warps.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    # Define block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the start of the block in each dimension\n    block_m_start = pid_m * BLOCK_SIZE_M\n    block_n_start = pid_n * BLOCK_SIZE_N\n\n    # Create a range for the block\n    range_m = block_m_start + tl.arange(0, BLOCK_SIZE_M)\n    range_n = block_n_start + tl.arange(0, BLOCK_SIZE_N)\n\n    # Initialize the accumulator for the result block\n    c_accum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over the K dimension in BLOCK_SIZE_K chunks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Create a range for the K dimension\n        range_k = k + tl.arange(0, BLOCK_SIZE_K)\n\n        # Load blocks of A and B\n        a_block = tl.load(a_ptr + range_m[:, None] * stride_am + range_k[None, :] * stride_ak, mask=(range_m[:, None] < M) & (range_k[None, :] < K), other=0.0)\n        b_block = tl.load(b_ptr + range_k[:, None] * stride_bk + range_n[None, :] * stride_bn, mask=(range_k[:, None] < K) & (range_n[None, :] < N), other=0.0)\n\n        # Perform the matrix multiplication for this block\n        c_accum += tl.dot(a_block, b_block)\n\n    # Write the result to the output matrix C\n    c_ptr = c_ptr + range_m[:, None] * stride_cm + range_n[None, :] * stride_cn\n    tl.store(c_ptr, c_accum, mask=(range_m[:, None] < M) & (range_n[None, :] < N))\n\ndef matmul(a, b, c, M, N, K):\n    # Define block sizes\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 128\n    BLOCK_SIZE_K = 32\n\n    # Calculate strides for the matrices\n    stride_am = a.stride(0)\n    stride_ak = a.stride(1)\n    stride_bk = b.stride(0)\n    stride_bn = b.stride(1)\n    stride_cm = c.stride(0)\n    stride_cn = c.stride(1)\n\n    # Launch the kernel\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n    matmul_kernel[grid](\n        a_ptr=a, b_ptr=b, c_ptr=c,\n        M=M, N=N, K=K,\n        stride_am=stride_am, stride_ak=stride_ak,\n        stride_bk=stride_bk, stride_bn=stride_bn,\n        stride_cm=stride_cm, stride_cn=stride_cn,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    ret[\"name\"] = f\"{kernel.name} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                  M, N, K,  #\n                  stride_am, stride_ak,  #\n                  stride_bk, stride_bn,  #\n                  stride_cm, stride_cn,  #\n                  BLOCK_SIZE_M: tl.constexpr,  #\n                  BLOCK_SIZE_N: tl.constexpr,  #\n                  BLOCK_SIZE_K: tl.constexpr,  #\n                  GROUP_SIZE_M: tl.constexpr,  #\n                  ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if (c_ptr.dtype.element_ty == tl.float8e4nv):\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef matmul(a, b):\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )\n    matmul_kernel[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel named `bmm_kernel` performs batched matrix multiplication (BMM) for 3D tensors A and B, producing an output tensor O. It operates in blocks defined by tile sizes (TILE_M, TILE_N, TILE_K), allowing efficient parallel computation on the GPU. The inputs are:\n        - `A`, `B`: Input matrices for multiplication with shapes (batch, M, K) and (batch, K, N) respectively.\n        - `O`: Output matrix with shape (batch, M, N).\n        - `M`, `N`, `K`: Dimensions of the matrices.\n        The kernel computes partial results in tiles and accumulates them. The `bmm` function wraps the kernel call, handling tensor contiguity and preparing output storage.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the kernel for batched matrix multiplication\n@triton.jit\ndef bmm_kernel(\n    A_ptr, B_ptr, O_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_om, stride_on,\n    TILE_M: tl.constexpr, TILE_N: tl.constexpr, TILE_K: tl.constexpr\n):\n    # Define the block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    pid_batch = tl.program_id(2)\n\n    # Compute the starting indices of the tiles\n    offs_am = pid_m * TILE_M + tl.arange(0, TILE_M)\n    offs_bn = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n\n    # Initialize the accumulator\n    acc = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, TILE_K):\n        # Load tiles from A and B\n        A_tile = tl.load(A_ptr + pid_batch * stride_am * M + offs_am[:, None] * stride_am + (k + offs_k)[None, :] * stride_ak, mask=(offs_am[:, None] < M) & ((k + offs_k)[None, :] < K), other=0.0)\n        B_tile = tl.load(B_ptr + pid_batch * stride_bk * K + (k + offs_k)[:, None] * stride_bk + offs_bn[None, :] * stride_bn, mask=((k + offs_k)[:, None] < K) & (offs_bn[None, :] < N), other=0.0)\n\n        # Accumulate the results\n        acc += tl.dot(A_tile, B_tile)\n\n    # Store the result\n    O_ptr += pid_batch * stride_om * M + offs_am[:, None] * stride_om + offs_bn[None, :] * stride_on\n    tl.store(O_ptr, acc, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n\n# Wrapper function for batched matrix multiplication\ndef bmm(A, B, M, N, K):\n    # Get the batch size\n    batch = A.shape[0]\n\n    # Define the tile sizes\n    TILE_M = 128\n    TILE_N = 128\n    TILE_K = 32\n\n    # Allocate output tensor\n    O = torch.empty((batch, M, N), device='cuda', dtype=torch.float32)\n\n    # Launch the kernel\n    grid = (triton.cdiv(M, TILE_M), triton.cdiv(N, TILE_N), batch)\n    bmm_kernel[grid](\n        A, B, O,\n        M, N, K,\n        A.stride(1), A.stride(2),\n        B.stride(1), B.stride(2),\n        O.stride(1), O.stride(2),\n        TILE_M=TILE_M, TILE_N=TILE_N, TILE_K=TILE_K\n    )\n\n    return O\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\ndef heur_divisible_m(args):\n    return args[\"M\"] % args[\"TILE_M\"] == 0\n\n\ndef heur_divisible_n(args):\n    return args[\"N\"] % args[\"TILE_N\"] == 0\n\n\ndef heur_divisible_k(args):\n    return args[\"K\"] % args[\"TILE_K\"] == 0\n\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\"TILE_M\": 32, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 1},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 128, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=2,\n        ),\n        triton.Config(\n            {\"TILE_M\": 32, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 1},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 64, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 32, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 64, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n        triton.Config(\n            {\"TILE_M\": 128, \"TILE_N\": 128, \"TILE_K\": 32, \"GROUP_M\": 2},\n            num_warps=4,\n            num_stages=3,\n        ),\n    ],\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.heuristics(\n    {\n        \"DIVISIBLE_M\": heur_divisible_m,\n        \"DIVISIBLE_N\": heur_divisible_n,\n        \"DIVISIBLE_K\": heur_divisible_k,\n    }\n)\n@triton.jit\ndef bmm_kernel(\n    A,\n    B,\n    O,\n    M,\n    N,\n    K,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    DIVISIBLE_M: tl.constexpr,\n    DIVISIBLE_N: tl.constexpr,\n    DIVISIBLE_K: tl.constexpr,\n):\n    # batch offsets\n    pid_b = tl.program_id(2)\n    A += pid_b * M * K\n    B += pid_b * K * N\n    O += pid_b * M * N\n\n    pidx = tl.program_id(0)\n    pidy = tl.program_id(1)\n\n    if GROUP_M == 1:\n        pid_m, pid_n = pidx, pidy\n    else:\n        # reorder CTAs\n        gridx = tl.num_programs(0)\n        gridy = tl.num_programs(1)\n        pid = pidx + pidy * gridx\n\n        num_CTA_per_group = gridy * GROUP_M\n\n        group_id = pid // num_CTA_per_group\n        inner_group_id = pid % num_CTA_per_group\n        if (group_id * GROUP_M + GROUP_M) > gridx:\n            GROUP_SIZE = gridx % GROUP_M\n        else:\n            GROUP_SIZE = GROUP_M\n        pid_m = group_id * GROUP_M + inner_group_id % GROUP_SIZE\n        pid_n = inner_group_id // GROUP_SIZE\n\n    offs_m = pid_m * TILE_M + tl.arange(0, TILE_M)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n\n    if not DIVISIBLE_M:\n        mask_m = offs_m < M\n    if not DIVISIBLE_N:\n        mask_n = offs_n < N\n\n    a_ptrs = A + offs_m[:, None] * K + offs_k[None, :]\n    b_ptrs = B + offs_k[:, None] * N + offs_n[None, :]\n    o_ptrs = O + offs_m[:, None] * N + offs_n[None, :]\n\n    num_iters = tl.cdiv(K, TILE_K)\n    o = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\n    for _ in range(num_iters):\n        if DIVISIBLE_K:\n            if DIVISIBLE_M:\n                mask_a = None\n            else:\n                mask_a = mask_m[:, None]\n            if DIVISIBLE_N:\n                mask_b = None\n            else:\n                mask_b = mask_n[None, :]\n        else:\n            mask_k = offs_k < K\n            if DIVISIBLE_M:\n                mask_a = mask_k[None, :]\n            else:\n                mask_a = mask_m[:, None] & mask_k[None, :]\n            if DIVISIBLE_N:\n                mask_b = mask_k[:, None]\n            else:\n                mask_b = mask_k[:, None] & mask_n[None, :]\n\n        a = tl.load(a_ptrs, mask_a)\n        b = tl.load(b_ptrs, mask_b)\n\n        offs_k += TILE_K\n        a_ptrs += TILE_K\n        b_ptrs += TILE_K * N\n\n        o += tl.dot(a, b, allow_tf32=False)\n\n    if DIVISIBLE_M and DIVISIBLE_N:\n        mask_c = None\n    elif DIVISIBLE_M and not DIVISIBLE_N:\n        mask_c = mask_n[None, :]\n    elif not DIVISIBLE_M and DIVISIBLE_N:\n        mask_c = mask_m[:, None]\n    else:\n        mask_c = mask_m[:, None] & mask_n[None, :]\n    tl.store(o_ptrs, o, mask_c)\n\n\ndef bmm(A, B):\n    logging.debug(\"GEMS BMM\")\n    batch, M, K = A.shape\n    _, _, N = B.shape\n    A = A.contiguous()\n    B = B.contiguous()\n    out = torch.empty((batch, M, N), dtype=A.dtype, device=A.device)\n\n    grid_fn = lambda meta: (\n        triton.cdiv(meta[\"M\"], meta[\"TILE_M\"]),\n        triton.cdiv(meta[\"N\"], meta[\"TILE_N\"]),\n        batch,\n    )\n    with torch.cuda.device(A.device):\n        bmm_kernel[grid_fn](A, B, out, M, N, K)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implementation aims to accelerate the attention mechanism commonly used in transformer architectures. The primary functions are `_fwd_kernel` and `_bwd_kernel`, handling the forward and backward passes respectively.\n\n            The `_fwd_kernel` function computes the scaled dot-product attention. It takes as input query `Q`, key `K`, and value `V` tensors, along with a scale factor `sm_scale`. It outputs the attention result `Out`. The function divides the computation into blocks defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_DMODEL`. Within each block, it computes the dot product between `Q` and `K`, applies the softmax to get attention scores, and uses these scores to weight the `V` tensor. The results are stored in `Out`. The function also computes intermediate normalization constants `L` and `M` for stability.\n\n            The `_bwd_kernel` handles the backward pass for the attention mechanism. It calculates gradients for `Q`, `K`, and `V` using the chain rule and updates these gradients incrementally by processing the input in blocks similar to the forward pass.\n\n            The `attention` class serves as a wrapper for these kernel operations, supporting both forward and backward computations using PyTorch's autograd functionality. It handles grid setup and memory allocations for intermediate results.\n\n            The `BLOCK` constant typically set to 128 defines the tile size for the computation, which directly influences performance and memory usage. The context `ctx` stores necessary information across forward and backward calls to allow efficient gradient computation.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Constants for block sizes\nBLOCK_M = 128\nBLOCK_N = 128\nBLOCK_DMODEL = 128\n\n@triton.jit\ndef _fwd_kernel(Q, K, V, Out, L, M, sm_scale, stride_qm, stride_qk, stride_km, stride_kn, stride_vm, stride_vn, stride_om, stride_on, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    # Compute block indices\n    pid = tl.program_id(0)\n    # Compute block start indices\n    start_m = pid * BLOCK_M\n    # Load Q, K, V tiles\n    q = tl.load(Q + start_m * stride_qm, mask=start_m + tl.arange(0, BLOCK_M) < Q.shape[0])\n    k = tl.load(K + tl.arange(0, BLOCK_N) * stride_kn, mask=tl.arange(0, BLOCK_N) < K.shape[1])\n    v = tl.load(V + tl.arange(0, BLOCK_N) * stride_vn, mask=tl.arange(0, BLOCK_N) < V.shape[1])\n    # Compute QK^T\n    qk = tl.dot(q, k)\n    # Scale by sm_scale\n    qk_scaled = qk * sm_scale\n    # Apply softmax\n    qk_softmax = tl.softmax(qk_scaled, axis=1)\n    # Compute weighted sum\n    out = tl.dot(qk_softmax, v)\n    # Store result\n    tl.store(Out + start_m * stride_om, out, mask=start_m + tl.arange(0, BLOCK_M) < Out.shape[0])\n\n@triton.jit\ndef _bwd_kernel(dOut, Q, K, V, dQ, dK, dV, L, M, sm_scale, stride_qm, stride_qk, stride_km, stride_kn, stride_vm, stride_vn, stride_dom, stride_don, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    # Compute block indices\n    pid = tl.program_id(0)\n    # Compute block start indices\n    start_m = pid * BLOCK_M\n    # Load gradients and inputs\n    dout = tl.load(dOut + start_m * stride_dom, mask=start_m + tl.arange(0, BLOCK_M) < dOut.shape[0])\n    q = tl.load(Q + start_m * stride_qm, mask=start_m + tl.arange(0, BLOCK_M) < Q.shape[0])\n    k = tl.load(K + tl.arange(0, BLOCK_N) * stride_kn, mask=tl.arange(0, BLOCK_N) < K.shape[1])\n    v = tl.load(V + tl.arange(0, BLOCK_N) * stride_vn, mask=tl.arange(0, BLOCK_N) < V.shape[1])\n    # Compute gradients\n    dq = tl.dot(dout, v.T)\n    dk = tl.dot(q.T, dout)\n    dv = tl.dot(qk_softmax.T, dout)\n    # Store gradients\n    tl.store(dQ + start_m * stride_qm, dq, mask=start_m + tl.arange(0, BLOCK_M) < dQ.shape[0])\n    tl.store(dK + tl.arange(0, BLOCK_N) * stride_kn, dk, mask=tl.arange(0, BLOCK_N) < dK.shape[1])\n    tl.store(dV + tl.arange(0, BLOCK_N) * stride_vn, dv, mask=tl.arange(0, BLOCK_N) < dV.shape[1])\n\nclass Attention(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, K, V, sm_scale):\n        # Allocate output tensor\n        Out = torch.empty((Q.shape[0], V.shape[1]), device=Q.device, dtype=Q.dtype)\n        # Launch Triton kernel\n        grid = (triton.cdiv(Q.shape[0], BLOCK_M),)\n        _fwd_kernel[grid](Q, K, V, Out, None, None, sm_scale, Q.stride(0), Q.stride(1), K.stride(0), K.stride(1), V.stride(0), V.stride(1), Out.stride(0), Out.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL)\n        # Save tensors for backward\n        ctx.save_for_backward(Q, K, V, Out)\n        ctx.sm_scale = sm_scale\n        return Out\n\n    @staticmethod\n    def backward(ctx, dOut):\n        Q, K, V, Out = ctx.saved_tensors\n        sm_scale = ctx.sm_scale\n        # Allocate gradient tensors\n        dQ = torch.empty_like(Q)\n        dK = torch.empty_like(K)\n        dV = torch.empty_like(V)\n        # Launch Triton kernel for backward pass\n        grid = (triton.cdiv(Q.shape[0], BLOCK_M),)\n        _bwd_kernel[grid](dOut, Q, K, V, dQ, dK, dV, None, None, sm_scale, Q.stride(0), Q.stride(1), K.stride(0), K.stride(1), V.stride(0), V.stride(1), dOut.stride(0), dOut.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL)\n        return dQ, dK, dV, None\n\n# Usage\ndef attention(Q, K, V, sm_scale):\n    return Attention.apply(Q, K, V, sm_scale)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale,\n    L, M,\n    Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on,\n    Z, H, N_CTX, D0,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    # initialize offsets for store\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    # initialize pointer to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    stride_qh_2d = stride_qh // stride_qm // stride_qk\n\n    q_tile_ptr = tl.make_block_ptr(base=Q,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_qm, stride_qk),\n                                   offsets=(\n                                       off_hz * stride_qh_2d + start_m * BLOCK_M, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    k_tile_ptr = tl.make_block_ptr(base=K,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_kn, stride_kk),\n                                   offsets=(off_hz * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_N, BLOCK_DMODEL),\n                                   order=(1, 0))\n    v_tile_ptr = tl.make_block_ptr(base=V,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_vk, stride_vn),\n                                   offsets=(off_hz * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_N, BLOCK_DMODEL),\n                                   order=(1, 0))\n    out_tile_ptr = tl.make_block_ptr(base=Out,\n                                     shape=(D0, BLOCK_DMODEL),\n                                     strides=(stride_om, stride_on),\n                                     offsets=(off_hz * stride_qh_2d + start_m * BLOCK_M, 0),\n                                     block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                     order=(1, 0))\n    # load q: it will stay in SRAM throughout\n    q = tl.load(q_tile_ptr)\n\n    # loop over k, v and update accumulators\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        # -- compute qk ----\n        k = tl.load(k_tile_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= (\n            start_n + offs_n[None, :]), qk, float(\"-inf\"))\n        # compute new m\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        # correct old l\n        l_prev *= tl.exp(m_prev - m_curr)\n        # attention weights\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        # rescale operands of matmuls\n        l_rcp = 1. / l_curr\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None]\n        # update acc\n        p = p.to(tl.float16)\n        v = tl.load(v_tile_ptr, boundary_check=(0, 1))\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n        # update pointers\n        k_tile_ptr = tl.advance(k_tile_ptr, [BLOCK_N, 0])\n        v_tile_ptr = tl.advance(v_tile_ptr, [BLOCK_N, 0])\n    # rematerialize offsets to save registers\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    # write back l and m\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_prev)\n    tl.store(m_ptrs, m_prev)\n\n    acc = acc.to(tl.float16)\n    tl.store(out_tile_ptr, acc, boundary_check=(0, 1))\n\n\n@triton.jit\ndef _bwd_preprocess(\n    Out, DO, L,\n    NewDO, Delta,\n    BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    # load\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n    denom = tl.load(L + off_m).to(tl.float32)\n    # compute\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    # write-back\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n    tl.store(Delta + off_m, delta)\n\n\n@triton.jit\ndef _bwd_kernel(\n    Q, K, V, sm_scale, Out, DO,\n    DQ, DK, DV,\n    L, M,\n    D,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    Z, H, N_CTX, D0,\n    num_block,\n    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    # init tile_ptr\n    stride_qz_2d = stride_qz // stride_qm // stride_qk\n    stride_qh_2d = stride_qh // stride_qm // stride_qk\n\n    q_tile_ptr = tl.make_block_ptr(base=Q,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_qm, stride_qk),\n                                   offsets=(\n                                       off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    k_tile_ptr = tl.make_block_ptr(base=K,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_kn, stride_kk),\n                                   offsets=(\n                                       off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    v_tile_ptr = tl.make_block_ptr(base=V,\n                                   shape=(D0, BLOCK_DMODEL),\n                                   strides=(stride_vk, stride_vn),\n                                   offsets=(\n                                       off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                   block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                   order=(1, 0))\n    do_tile_ptr = tl.make_block_ptr(base=DO,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    dq_tile_ptr = tl.make_block_ptr(base=DQ,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    dk_tile_ptr = tl.make_block_ptr(base=DK,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    dv_tile_ptr = tl.make_block_ptr(base=DV,\n                                    shape=(D0, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(\n                                        off_z * stride_qz_2d + off_h * stride_qh_2d, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    # offset pointers for batch/head\n    DQ += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = start_n * BLOCK_M\n        # initialize row/col offsets\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        # initialize pointers to value-like data\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n        # pointer to row-wise quantities in value-like data\n        D_ptrs = D + off_hz * N_CTX\n        m_ptrs = M + off_hz * N_CTX\n        # initialize dv amd dk\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        # k and v stay in SRAM throughout\n        k = tl.load(k_tile_ptr, boundary_check=(0, 1))\n        v = tl.load(v_tile_ptr, boundary_check=(0, 1))\n        # loop over rows\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            # load q, k, v, do on-chip\n            q = tl.load(q_tile_ptr, boundary_check=(0, 1))\n            # recompute p = softmax(qk, dim=-1).T\n            # NOTE: `do` is pre-divided by `l`; no normalization here\n            qk = tl.dot(q, tl.trans(k))\n            qk = tl.where(offs_m_curr[:, None] >= (\n                offs_n[None, :]), qk, float(\"-inf\"))\n            m = tl.load(m_ptrs + offs_m_curr)\n            p = tl.exp(qk * sm_scale - m[:, None])\n            # compute dv\n            do = tl.load(do_tile_ptr, boundary_check=(0, 1))\n            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n            # compute dp = dot(v, do)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            # compute ds = p * (dp - delta[:, None])\n            ds = p * dp * sm_scale\n            # compute dk = dot(ds.T, q)\n            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n            # compute dq\n            dq = tl.load(dq_tile_ptr)\n            dq += tl.dot(ds.to(tl.float16), k)\n            tl.store(dq_tile_ptr, dq)\n            # increment pointers\n            dq_ptrs += BLOCK_M * stride_qm\n            q_tile_ptr = tl.advance(q_tile_ptr, [BLOCK_M, 0])\n            do_tile_ptr = tl.advance(do_tile_ptr, [BLOCK_M, 0])\n            dq_tile_ptr = tl.advance(dq_tile_ptr, [BLOCK_M, 0])\n        q_tile_ptr = tl.advance(q_tile_ptr, [lo + (1 - num_block) * BLOCK_M, 0])\n        do_tile_ptr = tl.advance(do_tile_ptr, [lo + (1 - num_block) * BLOCK_M, 0])\n        dq_tile_ptr = tl.advance(dq_tile_ptr, [lo + (1 - num_block) * BLOCK_M, 0])\n        # increment tile pointers\n        k_tile_ptr = tl.advance(k_tile_ptr, [BLOCK_M, 0])\n        v_tile_ptr = tl.advance(v_tile_ptr, [BLOCK_M, 0])\n        # write-back\n        tl.store(dv_tile_ptr, dv.to(tl.float16), boundary_check=(0, 1))\n        tl.store(dk_tile_ptr, dk.to(tl.float16), boundary_check=(0, 1))\n        dv_tile_ptr = tl.advance(dv_tile_ptr, [BLOCK_M, 0])\n        dk_tile_ptr = tl.advance(dk_tile_ptr, [BLOCK_M, 0])\n\n\nclass _attention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, sm_scale):\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        assert Lk in {16, 32, 64, 128}\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n        L = torch.empty(\n            (q.shape[0] * q.shape[1], q.shape[2]),\n            device=q.device,\n            dtype=torch.float32)\n        m = torch.empty(\n            (q.shape[0] * q.shape[1], q.shape[2]),\n            device=q.device,\n            dtype=torch.float32)\n        num_warps = 4 if Lk <= 64 else 8\n        D0 = q.shape[0] * q.shape[1] * q.shape[2]\n        _fwd_kernel[grid](\n            q, k, v, sm_scale,\n            L, m,\n            o,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], D0,\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n            BLOCK_DMODEL=Lk, num_warps=num_warps,\n            num_stages=2,\n        )\n\n        ctx.save_for_backward(q, k, v, o, L, m)\n        ctx.grid = grid\n        ctx.sm_scale = sm_scale\n        ctx.BLOCK_DMODEL = Lk\n        return o\n\n    @staticmethod\n    def backward(ctx, do):\n        BLOCK = 128\n        q, k, v, o, l, m = ctx.saved_tensors\n        do = do.contiguous()\n        dq = torch.zeros_like(q, dtype=torch.float32)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        do_scaled = torch.empty_like(do)\n        delta = torch.empty_like(l)\n        D0 = q.shape[0] * q.shape[1] * q.shape[2]\n        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n            o, do, l,\n            do_scaled, delta,\n            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n        )\n        _bwd_kernel[(ctx.grid[1],)](\n            q, k, v, ctx.sm_scale,\n            o, do_scaled,\n            dq, dk, dv,\n            l, m,\n            delta,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n            q.shape[0], q.shape[1], q.shape[2], D0,\n            ctx.grid[0],\n            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n            num_stages=1,\n        )\n        return dq, dk, dv, None\n\n\nattention = _attention.apply\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel code provides a function `rotary_embedding` that applies rotary positional embeddings to query and key tensors used in transformers. The function handles two cases based on whether a key cache is provided. In the first case, without a cache, the `rotary_embedding_kernel` is used, while in the second case, with a cache, `fused_rotary_embedding_kernel_v2` is employed. The kernels perform similar operations: loading cosine and sine embeddings, applying them to query and key tensors to modify their values using the rotary embedding technique, and then storing the modified values. Inputs include the tensors for query (q) and key (k), cosine and sine tables for the embeddings, and optionally, a key cache (`k_cache`), block tables, and context lengths. Outputs are the rotated query and possibly updated key tensor in the cache. Key constants like `Q_HEAD_NUM` and `HEAD_DIM` define the number of heads and dimension of each head.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Constants defining the number of heads and the dimension of each head\nQ_HEAD_NUM = 12\nHEAD_DIM = 64\n\n@triton.jit\ndef rotary_embedding_kernel(\n    q_ptr, k_ptr, cos_ptr, sin_ptr, \n    stride_qz, stride_qh, stride_qd,\n    stride_kz, stride_kh, stride_kd,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Program ID\n    pid = tl.program_id(0)\n\n    # Compute the indices\n    block_idx = pid // Q_HEAD_NUM\n    head_idx = pid % Q_HEAD_NUM\n\n    # Offsets for query and key\n    q_offset = block_idx * stride_qz + head_idx * stride_qh\n    k_offset = block_idx * stride_kz + head_idx * stride_kh\n\n    # Load query and key\n    q = tl.load(q_ptr + q_offset + tl.arange(0, BLOCK_SIZE) * stride_qd)\n    k = tl.load(k_ptr + k_offset + tl.arange(0, BLOCK_SIZE) * stride_kd)\n\n    # Load cosine and sine embeddings\n    cos = tl.load(cos_ptr + tl.arange(0, BLOCK_SIZE))\n    sin = tl.load(sin_ptr + tl.arange(0, BLOCK_SIZE))\n\n    # Apply rotary embeddings\n    q_rot = q * cos - tl.swizzle(q, 'transpose') * sin\n    k_rot = k * cos - tl.swizzle(k, 'transpose') * sin\n\n    # Store the results back\n    tl.store(q_ptr + q_offset + tl.arange(0, BLOCK_SIZE) * stride_qd, q_rot)\n    tl.store(k_ptr + k_offset + tl.arange(0, BLOCK_SIZE) * stride_kd, k_rot)\n\n@triton.jit\ndef fused_rotary_embedding_kernel_v2(\n    q_ptr, k_cache_ptr, cos_ptr, sin_ptr, \n    block_table_ptr, context_length_ptr,\n    stride_qz, stride_qh, stride_qd,\n    stride_kz, stride_kh, stride_kd,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Program ID\n    pid = tl.program_id(0)\n\n    # Compute the indices\n    block_idx = pid // Q_HEAD_NUM\n    head_idx = pid % Q_HEAD_NUM\n\n    # Offsets for query and key\n    q_offset = block_idx * stride_qz + head_idx * stride_qh\n    k_offset = block_idx * stride_kz + head_idx * stride_kh\n\n    # Load query\n    q = tl.load(q_ptr + q_offset + tl.arange(0, BLOCK_SIZE) * stride_qd)\n\n    # Load cosine and sine embeddings\n    cos = tl.load(cos_ptr + tl.arange(0, BLOCK_SIZE))\n    sin = tl.load(sin_ptr + tl.arange(0, BLOCK_SIZE))\n\n    # Apply rotary embeddings to query\n    q_rot = q * cos - tl.swizzle(q, 'transpose') * sin\n\n    # Store the result back\n    tl.store(q_ptr + q_offset + tl.arange(0, BLOCK_SIZE) * stride_qd, q_rot)\n\n    # Handle key cache if provided\n    if k_cache_ptr is not None:\n        # Load key from cache\n        k = tl.load(k_cache_ptr + k_offset + tl.arange(0, BLOCK_SIZE) * stride_kd)\n\n        # Apply rotary embeddings to key\n        k_rot = k * cos - tl.swizzle(k, 'transpose') * sin\n\n        # Store the rotated key back in the cache\n        tl.store(k_cache_ptr + k_offset + tl.arange(0, BLOCK_SIZE) * stride_kd, k_rot)\n\ndef rotary_embedding(q, k, cos, sin, k_cache=None):\n    # Get the shapes and strides\n    q_shape = q.shape\n    k_shape = k.shape\n    q_strides = q.strides\n    k_strides = k.strides\n\n    # Launch the appropriate kernel\n    if k_cache is None:\n        rotary_embedding_kernel[(q_shape[0] * Q_HEAD_NUM,)](\n            q, k, cos, sin, \n            q_strides[0], q_strides[1], q_strides[2],\n            k_strides[0], k_strides[1], k_strides[2],\n            BLOCK_SIZE=HEAD_DIM\n        )\n    else:\n        fused_rotary_embedding_kernel_v2[(q_shape[0] * Q_HEAD_NUM,)](\n            q, k_cache, cos, sin, \n            None, None,  # Assuming block table and context length are not used in this example\n            q_strides[0], q_strides[1], q_strides[2],\n            k_strides[0], k_strides[1], k_strides[2],\n            BLOCK_SIZE=HEAD_DIM\n        )\n", "label": "from typing import Optional\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef rotary_embedding_kernel(\n    q,\n    k,\n    cos,\n    sin,\n    q_token_stride,\n    q_head_stride,\n    k_token_stride,\n    k_head_stride,\n    head_dim_stride,\n    cos_token_stride,\n    cos_stride,\n    q_total_tokens,\n    Q_HEAD_NUM: tl.constexpr,\n    KV_GROUP_NUM: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n    BLOCK_TOKENS: tl.constexpr,  # token range length\n):\n    cur_head_idx = tl.program_id(0)\n    cur_token_block_idx = tl.program_id(1)\n\n    tokens_range = cur_token_block_idx * BLOCK_TOKENS + tl.arange(0, BLOCK_TOKENS)\n    dim_range0 = tl.arange(0, HEAD_DIM // 2)\n    dim_range1 = tl.arange(HEAD_DIM // 2, HEAD_DIM)\n\n    off_cos_sin = tokens_range[:, None] * cos_token_stride + dim_range0[None, :] * cos_stride\n    loaded_cos = tl.load(cos + off_cos_sin, mask=(tokens_range[:, None] < q_total_tokens), other=0.0)\n    loaded_sin = tl.load(sin + off_cos_sin, mask=(tokens_range[:, None] < q_total_tokens), other=0.0)\n\n    off_q0 = (\n        tokens_range[:, None, None] * q_token_stride\n        + cur_head_idx * q_head_stride\n        + dim_range0[None, None, :] * head_dim_stride\n    )\n    off_q1 = (\n        tokens_range[:, None, None] * q_token_stride\n        + cur_head_idx * q_head_stride\n        + dim_range1[None, None, :] * head_dim_stride\n    )\n    loaded_q0 = tl.load(\n        q + off_q0,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n        other=0.0,\n    )\n    loaded_q1 = tl.load(\n        q + off_q1,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n        other=0.0,\n    )\n    out_q0 = loaded_q0 * loaded_cos[:, None, :] - loaded_q1 * loaded_sin[:, None, :]\n    out_q1 = loaded_q0 * loaded_sin[:, None, :] + loaded_q1 * loaded_cos[:, None, :]\n\n    tl.store(\n        q + off_q0,\n        out_q0,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n    )\n    tl.store(\n        q + off_q1,\n        out_q1,\n        mask=((cur_head_idx < Q_HEAD_NUM) & (tokens_range[:, None, None] < q_total_tokens)),\n    )\n\n    handle_kv = cur_head_idx % KV_GROUP_NUM == 0\n    if handle_kv:\n        k_head_idx = cur_head_idx // KV_GROUP_NUM\n        off_k0 = (\n            tokens_range[:, None, None] * k_token_stride\n            + k_head_idx * k_head_stride\n            + dim_range0[None, None, :] * head_dim_stride\n        )\n        off_k1 = (\n            tokens_range[:, None, None] * k_token_stride\n            + k_head_idx * k_head_stride\n            + dim_range1[None, None, :] * head_dim_stride\n        )\n        loaded_k0 = tl.load(\n            k + off_k0,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n            other=0.0,\n        )\n        loaded_k1 = tl.load(\n            k + off_k1,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n            other=0.0,\n        )\n        out_k0 = loaded_k0 * loaded_cos[:, None, :] - loaded_k1 * loaded_sin[:, None, :]\n        out_k1 = loaded_k0 * loaded_sin[:, None, :] + loaded_k1 * loaded_cos[:, None, :]\n        tl.store(\n            k + off_k0,\n            out_k0,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n        )\n        tl.store(\n            k + off_k1,\n            out_k1,\n            mask=(tokens_range[:, None, None] < q_total_tokens),\n        )\n\n\n@triton.jit\ndef fused_rotary_embedding_kernel_v2(\n    q,\n    k,\n    cos,\n    sin,\n    kv_cache,\n    BLOCK_TABLES,\n    context_lengths,\n    q_token_stride,\n    q_head_stride,\n    k_token_stride,\n    k_head_stride,\n    head_dim_stride,\n    cos_token_stride,\n    cos_stride,\n    cacheb_stride,\n    cacheh_stride,\n    cachebs_stride,\n    cached_stride,\n    bts_stride,\n    btb_stride,\n    block_size,\n    q_total_tokens,\n    Q_HEAD_NUM: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n):\n    block_head_index = tl.program_id(0)\n    if block_head_index >= Q_HEAD_NUM:\n        return\n    block_token_index = tl.program_id(1)\n\n    dim_range0 = tl.arange(0, HEAD_DIM // 2)\n    dim_range1 = tl.arange(HEAD_DIM // 2, HEAD_DIM)\n\n    off_q0 = block_token_index * q_token_stride + block_head_index * q_head_stride + dim_range0 * head_dim_stride\n    off_q1 = block_token_index * q_token_stride + block_head_index * q_head_stride + dim_range1 * head_dim_stride\n    off_k0 = block_token_index * k_token_stride + block_head_index * k_head_stride + dim_range0 * head_dim_stride\n    off_k1 = block_token_index * k_token_stride + block_head_index * k_head_stride + dim_range1 * head_dim_stride\n\n    loaded_q0 = tl.load(\n        q + off_q0,\n    )\n    loaded_q1 = tl.load(\n        q + off_q1,\n    )\n\n    loaded_k0 = tl.load(\n        k + off_k0,\n    )\n\n    loaded_k1 = tl.load(\n        k + off_k1,\n    )\n\n    off_cos_sin = block_token_index * cos_token_stride + dim_range0 * cos_stride\n\n    loaded_cos = tl.load(cos + off_cos_sin, mask=(block_token_index < q_total_tokens), other=0.0)\n    loaded_sin = tl.load(sin + off_cos_sin, mask=(block_token_index < q_total_tokens), other=0.0)\n\n    out_q0 = loaded_q0 * loaded_cos - loaded_q1 * loaded_sin\n    out_q1 = loaded_q0 * loaded_sin + loaded_q1 * loaded_cos\n\n    out_k0 = loaded_k0 * loaded_cos - loaded_k1 * loaded_sin\n    out_k1 = loaded_k0 * loaded_sin + loaded_k1 * loaded_cos  # total_tokens, head_num, head_dim\n\n    past_kv_seq_len = tl.load(context_lengths + block_token_index) - 1\n\n    last_block_idx = past_kv_seq_len // block_size\n    block_table_ptr = BLOCK_TABLES + block_token_index * bts_stride\n    block_ids = tl.load(block_table_ptr + last_block_idx * btb_stride, mask=(block_token_index < q_total_tokens))\n    offsets_in_last_block = (past_kv_seq_len % block_size) * cachebs_stride\n\n    kv_range0 = (\n        block_ids * cacheb_stride\n        + block_head_index * cacheh_stride\n        + offsets_in_last_block\n        + dim_range0 * cached_stride\n    )\n    kv_range1 = (\n        block_ids * cacheb_stride\n        + block_head_index * cacheh_stride\n        + offsets_in_last_block\n        + dim_range1 * cached_stride\n    )\n\n    tl.store(\n        kv_cache + kv_range0,\n        out_k0,\n    )\n    tl.store(\n        kv_cache + kv_range1,\n        out_k1,\n    )\n\n    # concat\n    tl.store(\n        q + off_q0,\n        out_q0,\n    )\n    tl.store(\n        q + off_q1,\n        out_q1,\n    )\n\n\ndef rotary_embedding(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    k_cache: Optional[torch.Tensor] = None,\n    block_tables: Optional[torch.Tensor] = None,\n    kv_lengths: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Args:\n        q: query tensor, [total_tokens, head_num, head_dim]\n        k: key tensor, [total_tokens, kv_head_num, head_dim]\n        cos: cosine for rotary embedding, [max_position_len, head_dim]\n        sin: sine for rotary embedding, [max_position_len, head_dim]\n        k_cache (torch.Tensor):  Blocked key cache. [num_blocks, num_kv_heads, block_size, head_dim]\n        kv_lengths, Past key/value sequence lengths plus current sequence length for each sequence. [bsz]\n        block_tables: Block tables for each sequence. [bsz, max_blocks_per_sequence]\n    \"\"\"\n    q_total_tokens, q_head_num, head_dim = q.shape\n    assert q.size(0) == k.size(0)\n    BLOCK_TOKENS = 4\n\n    if head_dim >= 512:\n        num_warps = 16\n    elif head_dim >= 256:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    k_head_num = k.size(1)\n    q_token_stride, q_head_stride, head_dim_stride = q.stride()\n    k_token_stride, k_head_stride, _ = k.stride()\n    cos_token_stride, cos_stride = cos.stride()\n\n    assert q_head_num % k_head_num == 0\n    kv_group_num = q_head_num // k_head_num\n\n    if k_cache == None:\n        grid = lambda META: (\n            q_head_num,\n            triton.cdiv(q_total_tokens, META[\"BLOCK_TOKENS\"]),\n        )\n        rotary_embedding_kernel[grid](\n            q,\n            k,\n            cos,\n            sin,\n            q_token_stride,\n            q_head_stride,\n            k_token_stride,\n            k_head_stride,\n            head_dim_stride,\n            cos_token_stride,\n            cos_stride,\n            q_total_tokens,\n            Q_HEAD_NUM=q_head_num,\n            KV_GROUP_NUM=kv_group_num,\n            HEAD_DIM=head_dim,\n            BLOCK_TOKENS=BLOCK_TOKENS,\n            num_warps=num_warps,\n        )\n    else:\n        grid = (triton.next_power_of_2(q_head_num), q_total_tokens)\n        fused_rotary_embedding_kernel_v2[grid](\n            q,\n            k,\n            cos,\n            sin,\n            k_cache,\n            block_tables,\n            kv_lengths,\n            q_token_stride,\n            q_head_stride,\n            k_token_stride,\n            k_head_stride,\n            head_dim_stride,\n            cos_token_stride,\n            cos_stride,\n            k_cache.stride(0),\n            k_cache.stride(1),\n            k_cache.stride(2),\n            k_cache.stride(3),\n            block_tables.stride(0),\n            block_tables.stride(1),\n            k_cache.size(-2),\n            q_total_tokens,\n            Q_HEAD_NUM=q_head_num,\n            HEAD_DIM=head_dim,\n            num_warps=num_warps,\n        )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel `chunk_global_reversed_cumsum_scalar_kernel` performs a reversed cumulative sum operation on a 3D input tensor `s` with dimensions (B, H, T). The kernel processes each (B, H) independently, iterating backwards over the last dimension T in chunks of size `BT`. During each iteration, it computes the cumulative sum from the current chunk to the beginning of the dimension and stores the result in the output tensor `o`. The input tensor `s` is assumed to be of shape (B, H, T), and the output tensor `z` is of the same shape as `s`. The `chunk_global_reversed_cumsum_scalar` function wraps this kernel, handling the initialization of the output tensor and launching the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Kernel function\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(s_ptr, z_ptr, B, H, T, BT, stride_b, stride_h, stride_t):\n    pid = tl.program_id(axis=0)\n    # Calculate the batch and head indices\n    b_idx = pid // H\n    h_idx = pid % H\n\n    # Offset pointers to the start of the batch and head\n    s_offset = b_idx * stride_b + h_idx * stride_h\n    z_offset = b_idx * stride_b + h_idx * stride_h\n\n    # Create a pointer to the start of the last chunk\n    t_start = T - BT\n\n    # Initialize a cumulative sum variable\n    cumsum = tl.zeros((BT,), dtype=tl.float32)\n\n    # Iterate over chunks in reverse\n    for t in range(t_start, -BT, -BT):\n        # Load a chunk of data\n        s_chunk = tl.load(s_ptr + s_offset + t * stride_t + tl.arange(0, BT))\n        \n        # Update the cumulative sum\n        cumsum = cumsum + s_chunk\n        \n        # Store the cumulative sum\n        tl.store(z_ptr + z_offset + t * stride_t + tl.arange(0, BT), cumsum)\n\n# Wrapper function\ndef chunk_global_reversed_cumsum_scalar(s, BT):\n    B, H, T = s.shape\n    z = torch.empty_like(s)\n\n    # Launch the kernel\n    grid = (B * H,)\n    chunk_global_reversed_cumsum_scalar_kernel[grid](\n        s_ptr=s,\n        z_ptr=z,\n        B=B,\n        H=H,\n        T=T,\n        BT=BT,\n        stride_b=s.stride(0),\n        stride_h=s.stride(1),\n        stride_t=s.stride(2),\n    )\n    return z\n\n# Example usage\nB, H, T = 4, 3, 16  # Example dimensions\nBT = 4  # Chunk size\ns = torch.randn((B, H, T), device='cuda')  # Input tensor on GPU\nz = chunk_global_reversed_cumsum_scalar(s, BT)\nprint(z)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=4),\n    ],\n    key=[]\n)\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(\n    s,\n    o,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n\n\ndef chunk_global_reversed_cumsum_scalar(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T = s.shape\n    dtype = dtype or s.dtype\n    grid = (B * H,)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_reversed_cumsum_scalar_kernel[grid](\n        s, z,\n        T=T\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The function `flash_attn_triton` implements a flash attention mechanism using Triton. The key inputs are matrices `q`, `k`, and `v`, representing queries, keys, and values. It performs scaled dot-product attention optionally with causal masking. The function uses a Triton kernel `_fwd_kernel` to efficiently compute the attention output. The result is stored in `o`, which has the same shape as `q`. Major parameters include `BLOCK_M` and `BLOCK_N`, determining the block sizes for computations.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(Q, K, V, O, M, N, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, causal: tl.constexpr):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    q = tl.load(Q + offs_m[:, None] * N + offs_n[None, :])\n    k = tl.load(K + offs_m[:, None] * N + offs_n[None, :])\n    v = tl.load(V + offs_m[:, None] * N + offs_n[None, :])\n\n    # Compute the scaled dot-product attention\n    scale = 1.0 / tl.sqrt(float(N))\n    qk = tl.dot(q, k, trans_b=True) * scale\n\n    # Apply causal masking if required\n    if causal:\n        mask = tl.arange(0, BLOCK_M)[:, None] >= tl.arange(0, BLOCK_N)[None, :]\n        qk = tl.where(mask, qk, float('-inf'))\n\n    # Softmax along the last dimension\n    qk_max = tl.max(qk, axis=1)\n    qk_exp = tl.exp(qk - qk_max[:, None])\n    qk_sum = tl.sum(qk_exp, axis=1)\n    attn = qk_exp / qk_sum[:, None]\n\n    # Compute the output\n    o = tl.dot(attn, v)\n    tl.store(O + offs_m[:, None] * N + offs_n[None, :], o)\n\n### Python Wrapper\n\nThe Python wrapper `flash_attn_triton` will handle the setup and execution of the Triton kernel.\n\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef flash_attn_triton(q, k, v, causal=True, sm_scale=1):\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n\n    o = torch.empty_like(q)\n\n    BLOCK_M = 128\n    BLOCK_N = 64\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    num_warps = 4 if Lk <= 64 else 8\n    _fwd_kernel[grid](\n        q, k, v, sm_scale,\n        L,\n        o,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n        q.shape[0], q.shape[1], q.shape[2],\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, DIM=Lk,\n        IS_CAUSAL=causal,\n        num_warps=num_warps,\n        num_stages=4)\n\n    return o\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale,\n    L,\n    O,\n    stride_q_bs, stride_q_head, stride_q_seqlen, stride_q_dim,\n    stride_k_bs, stride_k_head, stride_k_seqlen, stride_k_dim,\n    stride_v_bs, stride_v_head, stride_v_seqlen, stride_v_dim,\n    stride_o_bs, stride_o_head, stride_o_seqlen, stride_o_dim,\n    BS, HEAD, SEQLEN,\n    BLOCK_M: tl.constexpr,\n    DIM: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_bs_head = tl.program_id(1)\n\n    qkv_base_offset = off_bs_head * stride_q_head\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + qkv_base_offset,\n        shape=(SEQLEN, DIM),\n        strides=(stride_q_seqlen, stride_q_dim),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, DIM),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + qkv_base_offset,\n        shape=(DIM, SEQLEN),\n        strides=(stride_k_dim, stride_k_seqlen),\n        offsets=(0, 0),\n        block_shape=(DIM, BLOCK_N),\n        order=(0, 1),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + qkv_base_offset,\n        shape=(SEQLEN, DIM),\n        strides=(stride_k_seqlen, stride_v_dim),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, DIM),\n        order=(1, 0),\n    )\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, BLOCK_N)\n    max = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    denom = tl.zeros([BLOCK_M], dtype=tl.float32)\n    out_buffer = tl.zeros([BLOCK_M, DIM], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(tl.float16)\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else SEQLEN\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(off_m[:, None] >= (start_n + off_n[None, :]), qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n\n        max_new = tl.maximum(max, tl.max(qk, 1))\n        alpha = tl.math.exp2(max - max_new)\n        nume = tl.math.exp2(qk - max_new[:, None])\n        out_scale = denom * 0 + alpha\n        out_buffer *= out_scale[:, None]\n        out_buffer += tl.dot(nume.to(tl.float16), v)\n        denom = denom * alpha + tl.sum(nume, 1)\n        max = max_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    out_buffer = out_buffer / denom[:, None]\n    l_ptr = L + off_bs_head * SEQLEN + off_m\n    tl.store(l_ptr, max + tl.math.log2(denom))\n    O_block_ptr = tl.make_block_ptr(\n        base=O + qkv_base_offset,\n        shape=(SEQLEN, DIM),\n        strides=(stride_o_seqlen, stride_o_dim),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, DIM),\n        order=(1, 0),\n    )\n    tl.store(O_block_ptr, out_buffer.to(tl.float16))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton-based code efficiently processes query (q) and key (k) matrices by converting them to int8 format for reduced precision and space saving. The process involves two primary Triton kernels: q_kernel_per_block_int8 and k_kernel_per_block_int8. Each kernel takes input tensors, scales them, quantizes them into int8, and stores both the quantized data and scaling factors.\n\n        The per_block_int8 function manages this conversion process. It takes q and k tensors, prepares int8 tensor outputs and corresponding scaling factors, reshapes the inputs for uniform processing, and then launches the kernels.\n\n        Main variables/functions:\n        - q_kernel_per_block_int8: Converts the query matrix block by block into int8.\n        - k_kernel_per_block_int8: Converts the key matrix block by block into int8.\n        - per_block_int8: Wrapper function that handles input reshaping, kernel execution, and output storage.\n\n        Inputs:\n        - q: Query matrix (Tensor).\n        - k: Key matrix (Tensor).\n        - BLKQ: Block size for query processing.\n        - BLKK: Block size for key processing.\n\n        Outputs:\n        - q_int8: Int8 representation of query matrix.\n        - q_scale: Scaling factors for query blocks.\n        - k_int8: Int8 representation of key matrix.\n        - k_scale: Scaling factors for key blocks.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef q_kernel_per_block_int8(Q, Q_INT8, Q_SCALE, BLKQ, stride_q, stride_q_int8, stride_q_scale, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE\n\n    q_block = tl.load(Q + offset * stride_q, mask=offset + tl.arange(0, BLOCK_SIZE) < BLKQ, other=0.0)\n    max_val = tl.max(tl.abs(q_block), axis=0)\n    scale = 127.0 / max_val\n    q_int8_block = tl.cast(q_block * scale, tl.int8)\n\n    tl.store(Q_INT8 + offset * stride_q_int8, q_int8_block, mask=offset + tl.arange(0, BLOCK_SIZE) < BLKQ)\n    tl.store(Q_SCALE + pid * stride_q_scale, scale)\n\n@triton.jit\ndef k_kernel_per_block_int8(K, K_INT8, K_SCALE, BLKK, stride_k, stride_k_int8, stride_k_scale, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE\n\n    k_block = tl.load(K + offset * stride_k, mask=offset + tl.arange(0, BLOCK_SIZE) < BLKK, other=0.0)\n    max_val = tl.max(tl.abs(k_block), axis=0)\n    scale = 127.0 / max_val\n    k_int8_block = tl.cast(k_block * scale, tl.int8)\n\n    tl.store(K_INT8 + offset * stride_k_int8, k_int8_block, mask=offset + tl.arange(0, BLOCK_SIZE) < BLKK)\n    tl.store(K_SCALE + pid * stride_k_scale, scale)\n\ndef per_block_int8(q, k, BLKQ, BLKK):\n    BLOCK_SIZE = 128  # This can be adjusted based on your GPU's capability\n\n    # Prepare output tensors\n    q_int8 = torch.empty_like(q, dtype=torch.int8)\n    q_scale = torch.empty((q.size(0) // BLKQ,), dtype=torch.float32)\n    k_int8 = torch.empty_like(k, dtype=torch.int8)\n    k_scale = torch.empty((k.size(0) // BLKK,), dtype=torch.float32)\n\n    # Launch Triton kernels\n    grid_q = (q.size(0) // BLOCK_SIZE,)\n    grid_k = (k.size(0) // BLOCK_SIZE,)\n\n    q_kernel_per_block_int8[grid_q](\n        q, q_int8, q_scale, BLKQ,\n        q.stride(0), q_int8.stride(0), q_scale.stride(0),\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    k_kernel_per_block_int8[grid_k](\n        k, k_int8, k_scale, BLKK,\n        k.stride(0), k_int8.stride(0), k_scale.stride(0),\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return q_int8, q_scale, k_int8, k_scale\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef q_kernel_per_block_int8(X, X_int8, BLK: tl.constexpr, Scale, L, C: tl.constexpr, scale_stride):\n    # Triton kernel for processing block of query matrix in int8\n    off_b = tl.program_id(1) \n    off_blk = tl.program_id(0)\n    x_offset = off_b * L * C \n    offs_m = off_blk*BLK + tl.arange(0, BLK)\n    offs_k = tl.arange(0, C)\n\n    x_ptrs = X + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    x_int8_ptrs = X_int8 + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    scale_ptrs = Scale + off_b * scale_stride + off_blk  \n\n    x = tl.load(x_ptrs, mask=offs_m[:, None] < L)\n    x *= (C**-0.5 * 1.44269504)\n    scale = tl.max(tl.abs(x)) / 127.\n    x_int8 = x / scale\n    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)\n    x_int8 = x_int8.to(tl.int8)\n    tl.store(x_int8_ptrs, x_int8, mask=offs_m[:, None] < L)\n    tl.store(scale_ptrs, scale)\n\n@triton.jit\ndef k_kernel_per_block_int8(X, X_int8, BLK: tl.constexpr, Scale, L, C: tl.constexpr, scale_stride):\n    # Triton kernel for processing block of key matrix in int8\n    off_b = tl.program_id(1) \n    off_blk = tl.program_id(0)\n    x_offset = off_b * L * C \n    offs_m = off_blk*BLK + tl.arange(0, BLK)\n    offs_k = tl.arange(0, C)\n\n    x_ptrs = X + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    x_int8_ptrs = X_int8 + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    scale_ptrs = Scale + off_b * scale_stride + off_blk  \n\n    x = tl.load(x_ptrs, mask=offs_m[:, None] < L)\n    scale = tl.max(tl.abs(x)) / 127.\n    x_int8 = x / scale\n    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)\n    x_int8 = x_int8.to(tl.int8)\n    tl.store(x_int8_ptrs, x_int8, mask=offs_m[:, None] < L)\n    tl.store(scale_ptrs, scale)\n\n\ndef per_block_int8(q, k, BLKQ=128, BLKK=64):\n    # Function to initialize and launch Triton kernels for processing q and k\n    q_int8 = torch.empty_like(q, dtype=torch.int8)\n    k_int8 = q_int8.clone()\n\n    if q.dim() == 3:\n        q_scale = torch.empty((q.shape[-3], (q.shape[-2] + BLKQ - 1) // BLKQ, 1), device=q.device, dtype=torch.float32)\n        k_scale = torch.empty((k.shape[-3], (k.shape[-2] + BLKK - 1) // BLKK, 1), device=q.device, dtype=torch.float32)\n    elif q.dim() == 4:\n        q_scale = torch.empty((q.shape[-4], q.shape[-3], (q.shape[-2] + BLKQ - 1) // BLKQ, 1), device=q.device, dtype=torch.float32)\n        k_scale = torch.empty((k.shape[-4], k.shape[-3], (k.shape[-2] + BLKK - 1) // BLKK, 1), device=q.device, dtype=torch.float32)\n\n    q = q.view(-1, q.shape[-2], q.shape[-1])\n    k = k.view(-1, k.shape[-2], k.shape[-1])\n\n    B, L, C = q.shape\n    grid = ((L+BLKQ-1)//BLKQ, B, )\n    q_kernel_per_block_int8[grid](\n        q, \n        q_int8,\n        BLKQ,\n        q_scale,\n        L, C, q_scale.stride(0) if q_scale.dim() == 3 else q_scale.stride(1),\n    )\n\n    grid = ((L+BLKK-1)//BLKK, B, )\n    k_kernel_per_block_int8[grid](\n        k, \n        k_int8,\n        BLKK,\n        k_scale,\n        L, C, k_scale.stride(0) if k_scale.dim() == 3 else k_scale.stride(1),\n    )\n\n    return q_int8, q_scale, k_int8, k_scale\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel for performing an attention forward pass, specifically implementing the scaled dot-product attention mechanism. The main function, `forward(q, k, v, q_scale, k_scale)`, takes query (`q`), key (`k`), and value (`v`) tensors, along with scaling factors for query and key. It uses Triton to parallelize the computation over a grid, where each block computes partial results for the attention mechanism, accumulating weighted values into an output tensor `o`. The `_attn_fwd` kernel operates on blocks of size `BLOCK_M` by `BLOCK_N` and adjusts the attention scores using scaling factors and masks to ensure numerical stability and attention weight correctness. It relies on helper function `_attn_fwd_inner` to process stages of attention computation, updating accumulated values (`acc`), normalization factors (`l_i`), and maximum scores (`m_i`) iteratively.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_M = 128\nBLOCK_N = 128\n\n@triton.jit\ndef _attn_fwd_inner(Q, K, V, acc, l_i, m_i, q_scale, k_scale, stride_q, stride_k, stride_v, stride_o, BLOCK_M, BLOCK_N, start_idx):\n    pid = tl.program_id(axis=0)\n    \n    # Compute block indices\n    row_start = pid * BLOCK_M\n    col_start = start_idx\n\n    # Load Q, K, V for the block\n    q = tl.load(Q + row_start * stride_q, mask=row_start < Q.shape[0])\n    k = tl.load(K + col_start * stride_k, mask=col_start < K.shape[1])\n    v = tl.load(V + col_start * stride_v, mask=col_start < V.shape[1])\n\n    # Compute scaled dot-product\n    q_scaled = q * q_scale\n    k_scaled = k * k_scale\n    score = tl.dot(q_scaled, k_scaled)\n\n    # Apply mask and softmax normalization\n    m_i = tl.maximum(m_i, score)\n    exp_score = tl.exp(score - m_i)\n    l_i = l_i + exp_score\n    acc = acc + tl.dot(exp_score, v)\n\n    # Store results back\n    tl.store(l_i, l_i)\n    tl.store(m_i, m_i)\n    tl.store(acc, acc)\n\n@triton.jit\ndef _attn_fwd(Q, K, V, O, q_scale, k_scale, stride_q, stride_k, stride_v, stride_o, BLOCK_M, BLOCK_N):\n    pid = tl.program_id(axis=0)\n    \n    # Initialize accumulation and normalization variables\n    acc = tl.zeros([BLOCK_M, V.shape[1]], dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M, 1], dtype=tl.float32)\n    m_i = tl.full([BLOCK_M, 1], -float('inf'), dtype=tl.float32)\n\n    # Iterate over columns of K, V\n    for start_idx in range(0, K.shape[1], BLOCK_N):\n        _attn_fwd_inner(Q, K, V, acc, l_i, m_i, q_scale, k_scale, stride_q, stride_k, stride_v, stride_o, BLOCK_M, BLOCK_N, start_idx)\n\n    # Normalize accumulated values\n    acc = acc / l_i\n    tl.store(O, acc)\n\ndef forward(q, k, v, q_scale, k_scale):\n    # Determine the grid size\n    grid = (q.shape[0] // BLOCK_M,)\n\n    # Define strides\n    stride_q = q.stride(0)\n    stride_k = k.stride(0)\n    stride_v = v.stride(0)\n    stride_o = q.stride(0)\n\n    # Allocate output tensor\n    o = torch.empty((q.shape[0], v.shape[1]), dtype=torch.float32, device=q.device)\n\n    # Launch Triton kernel\n    _attn_fwd[grid](q, k, v, o, q_scale, k_scale, stride_q, stride_k, stride_v, stride_o, BLOCK_M, BLOCK_N)\n\n    return o\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n        K_scale_ptr += lo // BLOCK_N\n        K_ptrs += HEAD_DIM * lo\n        V_ptrs += HEAD_DIM * lo\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask=k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        if STAGE == 2:\n            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n            qk = qk + tl.where(mask, 0, -1.0e6)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask=(offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        \n        acc += tl.dot(p, v.to(tl.float16), out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i, m_i\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N)\n    \n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask=(offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX)\n    acc, l_i, _ = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                  start_m,  \n                                  BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                  2, offs_m, offs_n, N_CTX)\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask=(offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.bfloat16)\n    stage = 3\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=3)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel 'rotary_kernel' performs rotary positional encoding on input tensor X using cosine and sine values. It operates with varying sequence lengths (varlen) or fixed lengths. The kernel processes data in blocks, handling interleaved or non-interleaved data layouts, and considers optional conjugate transformation. The 'apply_rotary' function is a wrapper that sets up the necessary parameters and configurations for the kernel to execute on the input tensors, handling various input shapes and conditions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rotary_kernel(\n    X_ptr, cos_ptr, sin_ptr, out_ptr,\n    n_heads, head_dim, seq_len,\n    interleaved: tl.constexpr, conjugate: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Compute block indices\n    pid = tl.program_id(0)\n    bid = tl.program_id(1)\n\n    # Compute position in sequence and head dimension\n    seq_idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    head_idx = bid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n\n    # Load input data\n    if interleaved:\n        # Interleaved layout\n        x_idx = seq_idx[:, None] * n_heads * head_dim + head_idx[None, :]\n    else:\n        # Non-interleaved layout\n        x_idx = bid * seq_len * head_dim + seq_idx[:, None] * head_dim + head_idx[None, :]\n\n    # Load X, cos, sin values\n    X = tl.load(X_ptr + x_idx, mask=(seq_idx[:, None] < seq_len) & (head_idx[None, :] < head_dim), other=0.0)\n    cos = tl.load(cos_ptr + head_idx, mask=head_idx < head_dim, other=1.0)\n    sin = tl.load(sin_ptr + head_idx, mask=head_idx < head_dim, other=0.0)\n\n    # Apply rotary transformation\n    X_rotated_real = X * cos - X * sin\n    X_rotated_imag = X * sin + X * cos\n\n    if conjugate:\n        X_rotated_imag = -X_rotated_imag\n\n    # Store result\n    tl.store(out_ptr + x_idx, X_rotated_real + 1j * X_rotated_imag, mask=(seq_idx[:, None] < seq_len) & (head_idx[None, :] < head_dim))\n\n\ndef apply_rotary(X, cos, sin, interleaved=False, conjugate=False):\n    # Determine input shapes\n    batch_size, seq_len, n_heads, head_dim = X.shape\n\n    # Allocate output tensor\n    out = torch.empty_like(X, dtype=torch.complex64)\n\n    # Define grid size\n    grid = (triton.cdiv(seq_len, BLOCK_SIZE), n_heads)\n\n    # Launch Triton kernel\n    rotary_kernel[grid](\n        X_ptr=X,\n        cos_ptr=cos,\n        sin_ptr=sin,\n        out_ptr=out,\n        n_heads=n_heads,\n        head_dim=head_dim,\n        seq_len=seq_len,\n        interleaved=interleaved,\n        conjugate=conjugate,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Union, Optional\n\n\n@triton.jit\ndef rotary_kernel(\n    OUT,  # Pointers to matrices\n    X,\n    COS,\n    SIN,\n    CU_SEQLENS,\n    SEQLEN_OFFSETS,  # this could be int or a pointer\n    # Matrix dimensions\n    seqlen,\n    rotary_dim,\n    seqlen_ro,\n    # strides\n    stride_out_batch,\n    stride_out_seqlen,\n    stride_out_nheads,\n    stride_out_headdim,\n    stride_x_batch,\n    stride_x_seqlen,\n    stride_x_nheads,\n    stride_x_headdim,\n    # Meta-parameters\n    BLOCK_K: tl.constexpr,\n    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n    INTERLEAVED: tl.constexpr,\n    CONJUGATE: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads\n\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(\n            COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0\n        ).to(tl.float32)\n        sin = tl.load(\n            SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0\n        ).to(tl.float32)\n        x0 = tl.load(\n            X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0\n        ).to(tl.float32)\n        x1 = tl.load(\n            X + rotary_dim_half * stride_x_headdim,\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\n            other=0.0,\n        ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n        tl.store(\n            OUT + rotary_dim_half * stride_out_headdim,\n            o1,\n            mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half),\n        )\n    else:\n        rk_swap = rk + ((rk + 1) % 2) * 2 - 1  # 1, 0, 3, 2, 5, 4, ...\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(\n            COS,\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\n            other=1.0,\n        ).to(tl.float32)\n        sin = tl.load(\n            SIN,\n            mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half),\n            other=0.0,\n        ).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(\n            tl.float32\n        )\n        x1 = tl.load(\n            X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0\n        ).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))\n\ndef apply_rotary(\n    x: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    seqlen_offsets: Union[int, torch.Tensor] = 0,\n    cu_seqlens: Optional[torch.Tensor] = None,\n    max_seqlen: Optional[int] = None,\n    interleaved=False,\n    inplace=False,\n    conjugate=False,\n) -> torch.Tensor:\n    is_varlen = cu_seqlens is not None\n    if not is_varlen:\n        batch, seqlen, nheads, headdim = x.shape\n    else:\n        assert max_seqlen is not None, \"If cu_seqlens is passed in, then max_seqlen must be passed\"\n        total_seqlen, nheads, headdim = x.shape\n        batch_p_1 = cu_seqlens.shape[0]\n        batch = batch_p_1 - 1\n        seqlen = max_seqlen\n    seqlen_ro, rotary_dim = cos.shape\n    assert sin.shape == cos.shape\n    rotary_dim *= 2\n    assert rotary_dim <= headdim, \"rotary_dim must be <= headdim\"\n    assert headdim <= 256, \"Only support headdim <= 256\"\n    assert seqlen_ro >= seqlen, \"seqlen_ro must be >= seqlen\"\n\n    assert (\n        cos.dtype == sin.dtype\n    ), f\"cos and sin must have the same dtype, got {cos.dtype} and {sin.dtype}\"\n    assert (\n        x.dtype == cos.dtype\n    ), f\"Input and cos/sin must have the same dtype, got {x.dtype} and {cos.dtype}\"\n\n    cos, sin = cos.contiguous(), sin.contiguous()\n    if isinstance(seqlen_offsets, torch.Tensor):\n        assert seqlen_offsets.shape == (batch,)\n        assert seqlen_offsets.dtype in [torch.int32, torch.int64]\n        seqlen_offsets = seqlen_offsets.contiguous()\n    else:\n        assert seqlen_offsets + seqlen <= seqlen_ro\n\n    output = torch.empty_like(x) if not inplace else x\n    if rotary_dim < headdim and not inplace:\n        output[..., rotary_dim:].copy_(x[..., rotary_dim:])\n\n    BLOCK_K = (\n        32\n        if rotary_dim <= 32\n        else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\n    )\n    grid = lambda META: (triton.cdiv(seqlen, META[\"BLOCK_M\"]), batch, nheads)\n    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 128 else 4)\n\n    with torch.cuda.device(x.device.index):\n        rotary_kernel[grid](\n            output,  # data ptrs\n            x,\n            cos,\n            sin,\n            cu_seqlens,\n            seqlen_offsets,\n            seqlen,  # shapes\n            rotary_dim,\n            seqlen_ro,\n            output.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\n            output.stride(-3),  # seqlen_stride or total_seqlen_stride\n            output.stride(-2),  # nheads_stride\n            output.stride(-1),  # headdim_stride\n            x.stride(0) if not is_varlen else 0,  # batch_strides if not varlen else 0\n            x.stride(-3),  # seqlen stride or total_seqlen_stride\n            x.stride(-2),  # nheads stride\n            x.stride(-1),  # headdim stride\n            BLOCK_K,\n            isinstance(seqlen_offsets, torch.Tensor),\n            is_varlen,\n            interleaved,\n            conjugate,\n            BLOCK_M,\n        )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is designed to compute the softmax function for each row of a given input matrix. The kernel is named `softmax_kernel` and is annotated with `@triton.jit` for just-in-time compilation, enabling it to run on GPUs efficiently. \n\n            The `softmax_kernel` function takes several inputs: `output_ptr`, `input_ptr`, `input_row_stride`, `output_row_stride`, `n_cols`, and a constant `BLOCK_SIZE`. It computes the softmax for each row independently, exploiting parallel execution by launching one kernel instance per row of the matrix. \n\n            The kernel loads a row of data into a local variable, applies numerical stabilization by subtracting the maximum value of the row, computes the exponential of each element, normalizes the results, and stores them back into the output matrix.\n\n            The `softmax` function is a wrapper that sets up the parameters for the kernel execution, such as determining the `BLOCK_SIZE` and `num_warps` based on the matrix dimensions. It allocates space for the output and calls the `softmax_kernel` with a 1D launch grid corresponding to the number of rows in the input.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr, input_ptr,\n    input_row_stride, output_row_stride,\n    n_cols, BLOCK_SIZE: tl.constexpr\n):\n    # Row index of the current program.\n    row_idx = tl.program_id(0)\n\n    # Compute the start of the row in the input and output matrices.\n    input_offset = row_idx * input_row_stride\n    output_offset = row_idx * output_row_stride\n\n    # Create a pointer for the start of the row.\n    input_ptr_row = input_ptr + input_offset\n    output_ptr_row = output_ptr + output_offset\n\n    # Load data for the current row.\n    row_data = tl.load(input_ptr_row + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < n_cols, other=-float('inf'))\n\n    # Numerical stability: subtract max from row elements.\n    row_max = tl.max(row_data, axis=0)\n    row_data = row_data - row_max\n\n    # Exponentiate the stabilized values.\n    exp_row_data = tl.exp(row_data)\n\n    # Sum of exponentials for normalization.\n    sum_exp_row_data = tl.sum(exp_row_data, axis=0)\n\n    # Normalize to get softmax.\n    softmax_row_data = exp_row_data / sum_exp_row_data\n\n    # Store the result in the output matrix.\n    tl.store(output_ptr_row + tl.arange(0, BLOCK_SIZE), softmax_row_data, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\n\ndef softmax(input_tensor):\n    # Get the dimensions of the input matrix.\n    n_rows, n_cols = input_tensor.shape\n\n    # Determine BLOCK_SIZE and number of warps.\n    BLOCK_SIZE = 128  # This can be tuned based on the hardware.\n    num_warps = 4     # Also tunable.\n\n    # Allocate output tensor.\n    output_tensor = torch.empty_like(input_tensor)\n\n    # Launch the kernel with a 1D grid where each block handles one row.\n    grid = (n_rows,)\n\n    # Call the Triton kernel.\n    softmax_kernel[grid](\n        output_tensor, input_tensor,\n        input_tensor.stride(0), output_tensor.stride(0),\n        n_cols, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return output_tensor\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a random input tensor.\n    input_tensor = torch.randn(1024, 512, device='cuda', dtype=torch.float32)\n\n    # Compute softmax using the Triton-based implementation.\n    output_tensor = softmax(input_tensor)\n\n    # Verify the result using PyTorch's softmax.\n    expected_output = torch.nn.functional.softmax(input_tensor, dim=1)\n\n    # Check if the results are close.\n    assert torch.allclose(output_tensor, expected_output, atol=1e-5)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # The rows of the softmax are independent, so we parallelize across those\n    row_idx = tl.program_id(0)\n    # The stride represents how much we need to increase the pointer to advance 1 row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    # The block size is the next power of two greater than n_cols, so we can fit each\n    # row in a single block\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    # Subtract maximum for numerical stability\n    row_minus_max = row - tl.max(row, axis=0)\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    # Write back output to DRAM\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n    # The block size is the smallest power of two greater than the number of columns in `x`\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Another trick we can use is to ask the compiler to use more threads per row by\n    # increasing the number of warps (`num_warps`) over which each row is distributed.\n    # You will see in the next tutorial how to auto-tune this value in a more natural\n    # way so you don't have to come up with manual heuristics yourself.\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    y = torch.empty_like(x)\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n    # f the input matrix\n    softmax_kernel[(n_rows, )](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_cols,\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return y\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `fused_native_layer_norm_no_welford` function performs layer normalization on input tensors using Triton. The Triton kernel, `triton_red_fused_native_layer_norm_no_welford`, computes the mean and standard deviation across a specified dimension and normalizes the input tensor accordingly. The kernel uses block-based parallelism controlled by `XBLOCK` and `RBLOCK` constants for efficient computation on GPUs. Inputs are `primals_1`, `primals_2`, and `primals_3`. The function outputs the normalized tensor, a tensor storing mean values, and a tensor storing inverse standard deviation values.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants defining block sizes for parallelism\nXBLOCK = 128  # Number of elements processed in parallel per block in X dimension\nRBLOCK = 128  # Number of elements processed in parallel per block in R dimension\n\n@triton.jit\ndef triton_red_fused_native_layer_norm_no_welford(\n    primals_1_ptr, primals_2_ptr, primals_3_ptr,\n    mean_ptr, inv_std_ptr, norm_ptr,\n    stride_x, stride_r, N,\n    BLOCK: tl.constexpr\n):\n    # Program ID\n    pid = tl.program_id(0)\n\n    # Create a block of indices for the reduction dimension\n    r_offset = pid * RBLOCK + tl.arange(0, RBLOCK)\n    mask_r = r_offset < N\n\n    # Load input data\n    primals_1 = tl.load(primals_1_ptr + r_offset * stride_r, mask=mask_r, other=0.0)\n    primals_2 = tl.load(primals_2_ptr + r_offset * stride_r, mask=mask_r, other=0.0)\n    primals_3 = tl.load(primals_3_ptr + r_offset * stride_r, mask=mask_r, other=0.0)\n\n    # Compute mean\n    mean = (primals_1 + primals_2 + primals_3) / 3.0\n    tl.store(mean_ptr + r_offset * stride_r, mean, mask=mask_r)\n\n    # Compute variance\n    var = ((primals_1 - mean) ** 2 + (primals_2 - mean) ** 2 + (primals_3 - mean) ** 2) / 3.0\n\n    # Compute inverse standard deviation\n    inv_std = 1.0 / tl.sqrt(var + 1e-5)\n    tl.store(inv_std_ptr + r_offset * stride_r, inv_std, mask=mask_r)\n\n    # Normalize inputs\n    norm_1 = (primals_1 - mean) * inv_std\n    norm_2 = (primals_2 - mean) * inv_std\n    norm_3 = (primals_3 - mean) * inv_std\n\n    # Store normalized outputs\n    tl.store(norm_ptr + r_offset * stride_r, norm_1, mask=mask_r)\n    tl.store(norm_ptr + r_offset * stride_r + stride_x, norm_2, mask=mask_r)\n    tl.store(norm_ptr + r_offset * stride_r + 2 * stride_x, norm_3, mask=mask_r)\n\ndef fused_native_layer_norm_no_welford(primals_1, primals_2, primals_3):\n    # Assume primals_1, primals_2, and primals_3 are Triton-compatible tensors\n    N = primals_1.shape[0]\n    stride_x = primals_1.stride(0)\n    stride_r = primals_1.stride(1)\n\n    # Allocate output tensors\n    mean = torch.empty_like(primals_1)\n    inv_std = torch.empty_like(primals_1)\n    norm = torch.empty((3, *primals_1.shape), dtype=primals_1.dtype, device=primals_1.device)\n\n    # Launch Triton kernel\n    grid = (triton.cdiv(N, RBLOCK),)\n    triton_red_fused_native_layer_norm_no_welford[grid](\n        primals_1, primals_2, primals_3,\n        mean, inv_std, norm,\n        stride_x, stride_r, N,\n        BLOCK=RBLOCK\n    )\n\n    return norm, mean, inv_std\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime.triton_helpers import libdevice\n\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch.ops.inductor._reinterpret_tensor\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 1024,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 2048,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n    ],\n    key=[\"xnumel\", \"rnumel\"],\n)\n@triton.jit\ndef triton_red_fused_native_layer_norm_no_welford(\n    in_out_ptr0,\n    in_out_ptr1,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = _tmp3 + tmp2\n        _tmp3 = tmp4\n    tmp3 = tl.sum(_tmp3, 1)[:, None]\n    tmp5 = rnumel  # 4096.0\n    tmp6 = tmp3 / tmp5\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp6, None)\n    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp8 = tmp7.to(tl.float32)\n        tmp9 = tmp8 - tmp6\n        tmp10 = tmp9 * tmp9\n        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])\n        tmp13 = _tmp12 + tmp11\n        _tmp12 = tmp13\n    tmp12 = tl.sum(_tmp12, 1)[:, None]\n    tmp14 = rnumel  # 4096.0\n    tmp15 = tmp12 / tmp14\n    tmp16 = 1e-05\n    tmp17 = tmp15 + tmp16\n    tmp18 = libdevice.rsqrt(tmp17)\n    tl.debug_barrier()\n    tl.store(in_out_ptr1 + (x0), tmp18, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp19 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp20 = tmp19.to(tl.float32)\n        tmp21 = tmp20 - tmp6\n        tmp22 = tmp21 * tmp18\n        tmp24 = tmp23.to(tl.float32)\n        tmp25 = tmp22 * tmp24\n        tmp27 = tmp26.to(tl.float32)\n        tmp28 = tmp25 + tmp27\n        tmp29 = tmp28.to(tl.float32)\n        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)\n\ndef fused_native_layer_norm_no_welford(primals_1, primals_2, primals_3):\n    S, D = primals_3.shape\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((S, 1), (1, S), torch.float32)\n        buf1 = buf0\n        del buf0  # reuse\n        buf2 = empty_strided_cuda((S, 1), (1, S), torch.float32)\n        buf3 = reinterpret_tensor(buf2, (S, 1), (1, 1), 0)\n        del buf2  # reuse\n        buf4 = empty_strided_cuda((S, D), (D, 1), torch.bfloat16)\n        stream0 = get_raw_stream(0)\n        grid = lambda META: (triton.cdiv(S, META[\"XBLOCK\"]),)\n        triton_red_fused_native_layer_norm_no_welford[grid](\n            buf1, buf3, primals_3, primals_1, primals_2, buf4, S, D\n        )\n    return (\n        buf4,\n        primals_3,\n        buf1,\n        buf3,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel named `mul_kernel` takes a source tensor `src` and multiplies each of its elements by a constant exponent compensator. The result is stored in a destination tensor `dst`. The operation is parallelized over blocks of data of size `BLOCK_SIZE`. The constant exponent compensator is set to `2.0 ** (127 - 15)`. The main functionality is to perform element-wise multiplication with the compensator for a segment of the tensor defined by `BLOCK_SIZE` and the current program ID.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef mul_kernel(src_ptr, dst_ptr, BLOCK_SIZE: tl.constexpr, N: tl.constexpr):\n    # Define the exponent compensator\n    EXPONENT_COMPENSATOR = 2.0 ** (127 - 15)\n    \n    # Program ID determines which block of data this instance of the kernel will process\n    pid = tl.program_id(0)\n    \n    # Compute the start index for this block\n    start = pid * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't read/write out of bounds\n    mask = offsets < N\n    \n    # Load a block of data from the source tensor\n    src = tl.load(src_ptr + offsets, mask=mask)\n    \n    # Perform the element-wise multiplication with the compensator\n    result = src * EXPONENT_COMPENSATOR\n    \n    # Store the result in the destination tensor\n    tl.store(dst_ptr + offsets, result, mask=mask)\n\n# Wrapper function to call the Triton kernel\ndef multiply_with_compensator(src: torch.Tensor, BLOCK_SIZE: int):\n    # Ensure the source tensor is on the GPU\n    assert src.is_cuda, \"Source tensor must be on the GPU\"\n    \n    # Create the destination tensor\n    dst = torch.empty_like(src)\n    \n    # Calculate the number of blocks needed\n    N = src.numel()\n    num_blocks = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the Triton kernel\n    mul_kernel[(num_blocks,)](src, dst, BLOCK_SIZE, N)\n    \n    return dst\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a source tensor on the GPU\n    src = torch.rand(1024, device='cuda')\n    \n    # Define the block size\n    BLOCK_SIZE = 256\n    \n    # Call the wrapper function\n    dst = multiply_with_compensator(src, BLOCK_SIZE)\n    \n    # Print the result\n    print(dst)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel to multiply each element in the source tensor by a constant exponent compensator\n@triton.jit\ndef mul_kernel(src, dst, BLOCK_SIZE: tl.constexpr):\n    # Define a constant exponent compensator\n    exponent_compensator: tl.constexpr = 2.0 ** (127 - 15)\n    # Calculate the indices for the current program ID\n    idxs = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    # Load elements from the source tensor\n    x = tl.load(src + idxs)\n    # Multiply each element by the exponent compensator\n    y = x * exponent_compensator\n    # Store the result in the destination tensor\n    tl.store(dst + idxs, y)\n\n# Function to launch the Triton kernel\ndef launch_mul_kernel(src, BLOCK_SIZE=1):\n    # Create an empty tensor for the result\n    dst = torch.empty(src.shape, dtype=torch.float32, device='cuda')\n    # Launch the Triton kernel\n    mul_kernel[(src.shape[0] // BLOCK_SIZE,)](src, dst, BLOCK_SIZE)\n    return dst\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The function `dropout` applies the dropout operation using a custom Triton kernel `_dropout`. The main logic of `_dropout` involves reading input data from `x_ptr` and a mask from `x_keep_ptr`, scaling non-zero entries by `1/(1-p)`, and storing the result in `output_ptr`. It operates on data in blocks of size `BLOCK_SIZE`. The `dropout` function prepares inputs, defines grid size, and calls `_dropout` with the specified probability `p`.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel for dropout\n@triton.jit\ndef _dropout(x_ptr, x_keep_ptr, output_ptr, n_elements, p, BLOCK_SIZE: tl.constexpr):\n    # Calculate the program's grid index\n    pid = tl.program_id(0)\n    \n    # Calculate the range of indices this program will process\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load data from x_ptr and mask from x_keep_ptr\n    x = tl.load(x_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    mask = tl.load(x_keep_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    \n    # Apply dropout: scale non-zero entries by 1/(1-p)\n    scale = 1.0 / (1.0 - p)\n    dropout_result = tl.where(mask, x * scale, 0.0)\n    \n    # Store the result back to output_ptr\n    tl.store(output_ptr + offsets, dropout_result, mask=offsets < n_elements)\n\n# Wrapper function for dropout\ndef dropout(x, mask, p, BLOCK_SIZE=1024):\n    assert x.shape == mask.shape, \"Input and mask must have the same shape\"\n    \n    # Allocate output tensor\n    output = torch.empty_like(x)\n    \n    # Get number of elements\n    n_elements = x.numel()\n    \n    # Calculate the number of blocks\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    \n    # Launch the Triton kernel\n    _dropout[grid](x, mask, output, n_elements, p, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return output\n\n# Example usage\nif __name__ == \"__main__\":\n    # Set random seed for reproducibility\n    torch.manual_seed(0)\n    \n    # Create input tensor and dropout mask\n    x = torch.rand(4096, device='cuda')\n    mask = (torch.rand(4096, device='cuda') > 0.5).float()  # Example mask with p=0.5\n    \n    # Apply dropout\n    p = 0.5\n    output = dropout(x, mask, p)\n    \n    print(\"Input:\", x)\n    print(\"Mask:\", mask)\n    print(\"Output:\", output)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _dropout(\n    x_ptr,  # pointer to the input\n    x_keep_ptr,  # pointer to a mask of 0s and 1s\n    output_ptr,  # pointer to the output\n    n_elements,  # number of elements in the `x` tensor\n    p,  # probability that an element of `x` is changed to zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    # Load data\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n    # The line below is the crucial part, described in the paragraph above!\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    # Write-back output\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef dropout(x, x_keep, p):\n    output = torch.empty_like(x)\n    assert x.is_contiguous()\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe function `destindex_copy_quantize_kv` leverages a Triton kernel `_fwd_kernel_destindex_copy_quantize_kv` to process an input tensor `K`, performing an index-based copy and quantization operation. The inputs to the kernel include `K`, `DestLoc`, `Out`, and `Out_scale`. The kernel copies data from `K` to `Out` at specified indices from `DestLoc`, scaling and quantizing it to int8 format. The scale is calculated to allow the quantized data to utilize the range of int8 efficiently. The grid is defined by the sequence length `seq_len`, and uses one warp.\n", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K_ptr, DestLoc_ptr, Out_ptr, Out_scale_ptr, seq_len, BLOCK_SIZE: tl.constexpr):\n    # Calculate the program ID\n    pid = tl.program_id(0)\n    \n    # Calculate the block start index\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range for the current block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load the indices from DestLoc\n    dest_indices = tl.load(DestLoc_ptr + offsets, mask=offsets < seq_len, other=0)\n    \n    # Load the values from K using the offsets\n    values = tl.load(K_ptr + offsets, mask=offsets < seq_len, other=0.0)\n    \n    # Calculate the scale for quantization\n    # Assume that Out_scale is a precomputed scalar value for simplicity\n    scale = tl.load(Out_scale_ptr)\n    \n    # Quantize the values to int8\n    quantized_values = tl.libdevice.rint(values * scale).to(tl.int8)\n    \n    # Store the quantized values in the output tensor at the specified destination indices\n    tl.store(Out_ptr + dest_indices, quantized_values, mask=offsets < seq_len)\n\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    # Assume K, DestLoc, and Out are 1D tensors for simplicity\n    seq_len = K.shape[0]\n    \n    # Define the block size\n    BLOCK_SIZE = 128  # This can be tuned based on your GPU architecture\n    \n    # Launch the Triton kernel\n    grid = (triton.cdiv(seq_len, BLOCK_SIZE),)\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K_ptr=K,\n        DestLoc_ptr=DestLoc,\n        Out_ptr=Out,\n        Out_scale_ptr=Out_scale,\n        seq_len=seq_len,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage\nseq_len = 1024\nK = torch.rand(seq_len, dtype=torch.float32, device='cuda')\nDestLoc = torch.randint(0, seq_len, (seq_len,), dtype=torch.int32, device='cuda')\nOut = torch.empty(seq_len, dtype=torch.int8, device='cuda')\nOut_scale = torch.tensor(127.0 / K.abs().max(), dtype=torch.float32, device='cuda')  # Example scale calculation\n\ndestindex_copy_quantize_kv(K, DestLoc, Out, Out_scale)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d,\n    stride_o_bs, stride_o_h, stride_o_d,\n    stride_os_bs, stride_os_h, stride_os_d,\n    head_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] * stride_k_h + stride_k_d * offs_d[None, :], \n                       mask=offs_h[:, None] < head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.).to(tl.float16)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[:, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n\n\n@torch.no_grad()\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K, DestLoc, Out, Out_scale,\n        K.stride(0), K.stride(1), K.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        Out_scale.stride(0), Out_scale.stride(1), Out_scale.stride(2),\n        head_num,\n        BLOCK_DMODEL=head_dim,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `_fwd_kernel_token_att2` and a PyTorch wrapper function `token_att_fwd2`. The kernel computes attention over tokens using probability and value tensors to produce an output tensor. It takes into account batch and head dimensions, and uses sliding window attention. The wrapper function prepares the kernel execution by setting up grid dimensions and other parameters. \n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    prob_ptr,  # Pointer to the probability tensor\n    value_ptr, # Pointer to the value tensor\n    output_ptr, # Pointer to the output tensor\n    batch_size, # Number of batches\n    num_heads,  # Number of attention heads\n    seq_len,    # Sequence length\n    head_dim,   # Dimension of each head\n    BLOCK_SIZE: tl.constexpr, # Block size for processing\n):\n    # Compute program ID\n    pid = tl.program_id(0)\n    \n    # Calculate indices for the current block\n    batch_id = pid // (num_heads * seq_len)\n    head_id = (pid // seq_len) % num_heads\n    token_id = pid % seq_len\n    \n    # Offset pointers to the start of the current batch and head\n    prob_offset = batch_id * num_heads * seq_len * seq_len + head_id * seq_len * seq_len + token_id * seq_len\n    value_offset = batch_id * num_heads * seq_len * head_dim + head_id * seq_len * head_dim\n    output_offset = batch_id * num_heads * seq_len * head_dim + head_id * seq_len * head_dim + token_id * head_dim\n    \n    # Load probabilities and values\n    probs = tl.load(prob_ptr + prob_offset + tl.arange(0, seq_len))\n    values = tl.load(value_ptr + value_offset + tl.arange(0, head_dim))\n    \n    # Initialize accumulator\n    acc = tl.zeros([head_dim], dtype=tl.float32)\n    \n    # Compute attention output\n    for i in range(seq_len):\n        value = tl.load(value_ptr + value_offset + i * head_dim + tl.arange(0, head_dim))\n        acc += probs[i] * value\n    \n    # Store result\n    tl.store(output_ptr + output_offset + tl.arange(0, head_dim), acc)\n\ndef token_att_fwd2(prob, value):\n    batch_size, num_heads, seq_len, head_dim = value.shape\n    \n    # Allocate output tensor\n    output = torch.empty((batch_size, num_heads, seq_len, head_dim), device=prob.device, dtype=prob.dtype)\n    \n    # Define grid dimensions\n    grid = (batch_size * num_heads * seq_len,)\n    \n    # Launch Triton kernel\n    _fwd_kernel_token_att2[grid](\n        prob,\n        value,\n        output,\n        batch_size,\n        num_heads,\n        seq_len,\n        head_dim,\n        BLOCK_SIZE=128  # Example block size\n    )\n    \n    return output\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    Prob,\n    V,\n    Out,\n    Req_to_tokens,\n    B_req_idx,\n    B_Start_Loc,\n    B_Seqlen,\n    B_Att_Start_Loc,\n    B_Att_Seqlen,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    stride_ph,\n    stride_pbs,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    sliding_window,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    # Triton kernel for computing token attention\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n\n    v_loc_off = (\n        cur_batch_req_idx * stride_req_to_tokens_b + (cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    )\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_att_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=(start_n + offs_n) < cur_att_seq_len, other=0.0)\n        v_loc = tl.load(\n            Req_to_tokens + v_loc_off + start_n * stride_req_to_tokens_s,\n            mask=(start_n + offs_n + cur_batch_start_index) < cur_batch_seq_len,\n            other=0.0,\n        )\n        v_value = tl.load(\n            V + v_offs + v_loc[:, None] * stride_vbs,\n            mask=(start_n + offs_n[:, None] + cur_batch_start_index) < cur_batch_seq_len,\n            other=0.0,\n        )\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_att_fwd2(\n    prob, v, out, Req_to_tokens, B_req_idx, B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, sliding_window\n):\n    # Launch the Triton kernel for token attention\n    BLOCK = 128\n    batch, head = B_req_idx.shape[0], prob.shape[0]\n    grid = (batch, head)\n    num_warps = 4\n    dim = v.shape[-1]\n\n    kv_group_num = prob.shape[0] // v.shape[1]\n\n    _fwd_kernel_token_att2[grid](\n        prob,\n        v,\n        out,\n        Req_to_tokens,\n        B_req_idx,\n        B_Start_Loc,\n        B_Seqlen,\n        B_Att_Start_Loc,\n        B_Att_Seqlen,\n        Req_to_tokens.stride(0),\n        Req_to_tokens.stride(1),\n        prob.stride(0),\n        prob.stride(1),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        out.stride(0),\n        out.stride(1),\n        out.stride(2),\n        kv_group_num=kv_group_num,\n        sliding_window=sliding_window,\n        BLOCK_DMODEL=dim,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel named `var_len_copy_kernel_triton` copies variable-length segments from one array to another. It takes the starting indices and lengths of these segments for both the source and destination arrays. It uses `BLOCK_SIZE` to divide the work into manageable chunks. The kernel is launched by the `launch_var_len_copy_triton` function, which sets the block size and grid size based on the number of segments. It passes memory pointers and block size to the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef var_len_copy_kernel_triton(\n    src_ptr, dst_ptr, src_indices_ptr, dst_indices_ptr, lengths_ptr, \n    BLOCK_SIZE: tl.constexpr\n):\n    # Get the program ID for this kernel instance\n    pid = tl.program_id(axis=0)\n\n    # Load the starting index and length for the current segment\n    src_index = tl.load(src_indices_ptr + pid)\n    dst_index = tl.load(dst_indices_ptr + pid)\n    length = tl.load(lengths_ptr + pid)\n\n    # Create a range for the current block\n    offsets = tl.arange(0, BLOCK_SIZE)\n\n    # Calculate the effective indices for the source and destination\n    src_offsets = src_index + offsets\n    dst_offsets = dst_index + offsets\n\n    # Load from source and store to destination within the segment length\n    mask = offsets < length\n    src_vals = tl.load(src_ptr + src_offsets, mask=mask)\n    tl.store(dst_ptr + dst_offsets, src_vals, mask=mask)\n\n# Wrapper function to launch the Triton kernel\ndef launch_var_len_copy_triton(src, dst, src_indices, dst_indices, lengths, block_size=128):\n    assert len(src_indices) == len(dst_indices) == len(lengths), \"Segment arrays must be of the same length\"\n\n    # Determine the number of segments\n    num_segments = len(src_indices)\n\n    # Allocate memory on the device\n    src_ptr = triton.device_pointer(src)\n    dst_ptr = triton.device_pointer(dst)\n    src_indices_ptr = triton.device_pointer(src_indices)\n    dst_indices_ptr = triton.device_pointer(dst_indices)\n    lengths_ptr = triton.device_pointer(lengths)\n\n    # Launch the Triton kernel\n    grid = (num_segments,)\n    var_len_copy_kernel_triton[grid](\n        src_ptr, dst_ptr, src_indices_ptr, dst_indices_ptr, lengths_ptr, \n        BLOCK_SIZE=block_size\n    )\n", "label": "\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location, new_a_start, new_a_location,\n                               BLOCK_SIZE: tl.constexpr):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=new_offset < length)\n\ndef launch_var_len_copy_triton(old_a_start, old_a_len, old_location, new_a_start, new_a_location):\n    BLOCK_SIZE = 256\n    grid_size = (len(old_a_start),)\n\n    var_len_copy_kernel_triton[grid_size](\n        old_a_start, old_a_len, old_location, new_a_start, new_a_location, BLOCK_SIZE)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel named `_score_kernel` is used to compute the attention score matrix for a transformer model. It uses block-level matrix multiplication and an optimized approach for performance. The function `get_score` is a wrapper around the kernel, managing input parameters and kernel execution.\n            \n            The kernel takes matrices `Q` (queries), `K` (keys), `M` (masking values), and produces the output `Out`. It operates on blocks of size `BLOCK_M x BLOCK_N`, with parameters such as strides for Q, K, and Out, along with constants like `BLOCK_M`, `BLOCK_N`, and `BLOCK_DMODEL`. The main computation involves loading the blocks of Q and K, computing their dot product, applying scale and mask, and storing results in `Out`.\n\n            The `get_score` function manages grid configuration, sets the scaling factor, and handles exceptions like resource constraints by reducing block sizes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes and model dimension\nBLOCK_M = 128\nBLOCK_N = 128\nBLOCK_DMODEL = 64\n\n@triton.jit\ndef _score_kernel(Q, K, M, Out, stride_qm, stride_qk, stride_kn, stride_om, stride_on, scale, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    # Get the program ID\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate the starting indices for the blocks\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    # Load blocks of Q and K\n    Q_block = tl.load(Q + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk)\n    K_block = tl.load(K + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_qk)\n\n    # Compute the dot product\n    score = tl.dot(Q_block, K_block)\n\n    # Apply the scale and mask\n    score = score * scale\n    mask = tl.load(M + offs_m[:, None] * stride_qm + offs_n[None, :] * stride_on)\n    score = tl.where(mask, score, float('-inf'))\n\n    # Store the result\n    tl.store(Out + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on, score)\n\ndef get_score(Q, K, M, scale):\n    # Get dimensions\n    num_queries, d_model = Q.shape\n    num_keys, _ = K.shape\n\n    # Define grid size\n    grid = (triton.cdiv(num_queries, BLOCK_M), triton.cdiv(num_keys, BLOCK_N))\n\n    # Allocate output tensor\n    Out = torch.empty((num_queries, num_keys), device=Q.device, dtype=Q.dtype)\n\n    # Calculate strides\n    stride_qm = Q.stride(0)\n    stride_qk = Q.stride(1)\n    stride_kn = K.stride(1)\n    stride_om = Out.stride(0)\n    stride_on = Out.stride(1)\n\n    # Launch the kernel\n    _score_kernel[grid](\n        Q, K, M, Out,\n        stride_qm, stride_qk, stride_kn, stride_om, stride_on,\n        scale,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL\n    )\n\n    return Out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n_BLOCK_N = 64\n_BLOCK_M = 64\n\n@triton.heuristics(\n    {\n        \"IS_EVEN_M\": lambda args: args[\"N_CTX\"] % args[\"BLOCK_M\"] == 0,\n        \"IS_EVEN_N\": lambda args: args[\"NKV_CTX\"] % args[\"BLOCK_N\"] == 0,\n    }\n)\n@triton.jit\ndef _score_kernel(\n    Q, K, M, sm_scale, Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,  #\n    stride_kz, stride_kh, stride_kn, stride_kk,  #\n    stride_oz, stride_oh, stride_on,\n    Z, H, H_KV, #\n    N_CTX,  #\n    ROUND_CTX,\n    NKV_CTX,\n    sliding_window_offset,\n    sliding_window_size,\n    SLIDING_WINDOW: tl.constexpr,\n    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,\n    IS_EVEN_M: tl.constexpr,\n    IS_EVEN_N: tl.constexpr,\n    BLOCK_M: tl.constexpr,  #\n    BLOCK_DMODEL: tl.constexpr,  #\n    BLOCK_N: tl.constexpr,  #\n):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H//H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, NKV_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, start_n * BLOCK_N),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634   # 1/log(2)\n\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0,1), padding_option=\"zero\")\n\n        m = tl.load(m_ptrs)\n\n        # calc qk\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[None, :] \\\n                 + start_m - start_n * BLOCK_N + sliding_window_offset\n\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = (dist >= sliding_window_size)\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk) # (BLOCK_M, BLOCK_N)\n\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n\n        if not IS_EVEN_N:\n            p = tl.where(\n                ((tl.arange(0, BLOCK_M) + start_m) < N_CTX)[:, None],\n                p, 0\n            )\n\n        o += tl.sum(p, axis=0)\n\n\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N # orange\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o.to(Out.type.element_ty), mask = o_range < NKV_CTX)\n\ndef get_score(q, k, m, sliding_window, complement_sliding_window):\n    N_CTX = q.size(-2)\n    NKV_CTX = k.size(-2)\n    ROUND_CTX = m.size(-1)\n    ret = torch.zeros(\n        (q.size(0), q.size(1), k.size(2)),\n        dtype=k.dtype, device=k.device\n    )\n    if sliding_window is not None:\n        sliding_window_offset, sliding_window_size = sliding_window\n    else:\n        sliding_window_offset, sliding_window_size = None, None\n\n    grid = lambda META: (\n        triton.cdiv(k.shape[2], META[\"BLOCK_N\"]),\n        q.shape[0] * q.shape[1]\n    )\n    sm_scale = 1 / math.sqrt(q.size(-1))\n\n    global _BLOCK_N\n    global _BLOCK_M\n\n    try:\n        _score_kernel[grid](\n            q, k, m, sm_scale, ret,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            ret.stride(0), ret.stride(1), ret.stride(2),\n            q.size(0), q.size(1), k.size(1),\n            N_CTX, ROUND_CTX, NKV_CTX,\n            sliding_window_offset,\n            sliding_window_size,\n            SLIDING_WINDOW=(sliding_window is not None),\n            COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n            BLOCK_M=_BLOCK_M,\n            BLOCK_N=_BLOCK_N,\n            BLOCK_DMODEL=q.size(-1)\n        )\n    except triton.OutOfResources as E:\n        from warnings import warn\n        _BLOCK_N = _BLOCK_N // 2\n        _BLOCK_M = _BLOCK_M // 2\n        warn(f\"Triton Attention Output Resources. {E}\\nUse smaller block size {_BLOCK_N}.\")\n        _score_kernel[grid](\n            q, k, m, sm_scale, ret,\n            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n            ret.stride(0), ret.stride(1), ret.stride(2),\n            q.size(0), q.size(1), k.size(1),\n            N_CTX, ROUND_CTX, NKV_CTX,\n            sliding_window_offset,\n            sliding_window_size,\n            SLIDING_WINDOW=(sliding_window is not None),\n            COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n            BLOCK_M=_BLOCK_M,\n            BLOCK_N=_BLOCK_N,\n            BLOCK_DMODEL=q.size(-1)\n        )\n\n    return ret\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This code defines a Triton kernel `batched_vecmat_kernel` that performs a batched vector-matrix multiplication on GPUs using Triton. The kernel takes matrices A and B as input, computes the vector-matrix product, and stores the result in the output matrix. The function `batched_vecmat` wraps around this kernel to set up the inputs and the computation grid and manages the invocation of the kernel. The main parameters are the matrix dimensions M, N, K, and block sizes for each dimension. The kernel iterates over the K dimension in blocks to compute partial products and accumulates the results in a tile of the output matrix.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef batched_vecmat_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    # Compute the tile indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the starting index for each tile\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Iterate over K dimension in blocks\n    for k in range(0, K, BLOCK_K):\n        # Load a block of A and B\n        a = tl.load(A_ptr + (offs_m[:, None] * stride_am + (k + offs_k) * stride_ak))\n        b = tl.load(B_ptr + ((k + offs_k)[:, None] * stride_bk + offs_n * stride_bn))\n\n        # Compute the product and accumulate\n        acc += tl.dot(a, b)\n\n    # Store the result\n    tl.store(C_ptr + (offs_m[:, None] * stride_cm + offs_n * stride_cn), acc)\n\ndef batched_vecmat(A, B, M, N, K, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32):\n    # Ensure the input matrices are contiguous\n    A = A.contiguous()\n    B = B.contiguous()\n\n    # Allocate output matrix\n    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n    # Compute strides\n    stride_am, stride_ak = A.stride()\n    stride_bk, stride_bn = B.stride()\n    stride_cm, stride_cn = C.stride()\n\n    # Define grid size\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n\n    # Launch the kernel\n    batched_vecmat_kernel[grid](\n        A, B, C,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n    )\n\n    return C\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef batched_vecmat_kernel(\n        A,  # shape: [dim_m, dim_k]\n        B,  # shape: [dim_m, dim_n, dim_k]\n        dim_m, dim_n, dim_k,\n        output,\n        block_m: tl.constexpr, block_n: tl.constexpr, block_k: tl.constexpr):\n    m_index = tl.program_id(0)\n    n_index = tl.program_id(1)\n    output_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_n \\\n        + (n_index * block_n + tl.arange(0, block_n))[None, :]\n\n    vecmat = tl.zeros([block_m, block_n], dtype=A.dtype.element_ty)\n    k_blocks = dim_k // block_k\n    for k_index in range(k_blocks):\n        a_tile = (m_index * block_m + tl.arange(0, block_m))[:, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, :]\n        a = tl.load(A + a_tile)\n\n        b_tile = (m_index * block_m + tl.arange(0, block_m))[None, :, None] * dim_n * dim_k \\\n            + (n_index * block_n + tl.arange(0, block_n))[:, None, None] * dim_k \\\n            + (k_index * block_k + tl.arange(0, block_k))[None, None, :]\n        b = tl.load(B + b_tile)\n\n        expanded_a, _ = tl.broadcast(a, b)\n        vecmat += tl.trans(tl.sum(expanded_a * b, axis=2))\n\n    tl.store(output + output_tile, vecmat)\n\n\ndef batched_vecmat(\n    M, N, K, block_m, block_n, block_k, num_warps=4, num_stages=1\n):\n\n    A = torch.randn(M, K, device='cuda', dtype=torch.float32)  # shape: [M, K]\n    B = torch.randn(M, N, K, device='cuda', dtype=torch.float32)  # shape: [M, N, K]\n    output = torch.zeros(M, N, device='cuda', dtype=torch.float32)  # \u8f93\u51fa\u5f20\u91cf\uff0cshape: [M, N]\n\n    assert K % block_k == 0, \"\"\n    assert M % block_m == 0, \"\"\n    assert N % block_n == 0, \"\"\n\n    grid = (M // block_m, N // block_n)\n\n    # \u8c03\u7528 Triton Kernel\n    batched_vecmat_kernel[grid](\n        A,\n        B,\n        M, N, K,\n        output,\n        block_m=block_m,\n        block_n=block_n,\n        block_k=block_k,\n        num_warps=num_warps,\n        num_stages=num_stages\n    )\n\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton operator computes the cross-entropy loss and its gradient for a given set of logits and labels. The implementation includes two kernels: one for the forward pass (`cross_entropy_fwd_kernel`) and one for the backward pass (`cross_entropy_bwd_kernel`). \n\n    The `cross_entropy_fwd_kernel` function calculates the loss, the log-sum-exp (lse), and the z_loss from the logits and labels. It iterates over each row and processes a block of columns at a time. It handles optional label smoothing and scaling of logits. \n\n    The `cross_entropy_bwd_kernel` function computes gradients of the logits (dlogits) based on the loss gradient (dloss) from the forward pass. It adjusts probabilities according to the logits and applies optional label smoothing.\n\n    The `cross_entropy_fwd` function sets up and launches the forward kernel, and it returns the computed loss, lse, and z_loss. The `cross_entropy_bwd` function launches the backward kernel to compute and return the dlogits.\n\n    Inputs include: logits, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, and flags like HAS_SMOOTHING and SPLIT. Outputs include loss, lse, z_loss for forward, and dlogits for backward.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    logits_ptr, labels_ptr, loss_ptr, lse_ptr, z_loss_ptr,\n    num_classes, num_rows, smoothing, logit_scale, BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr, SPLIT: tl.constexpr\n):\n    # Define the row and column index\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, BLOCK_SIZE)\n\n    # Load the logits for this row\n    logits = tl.load(logits_ptr + row_idx * num_classes + col_idx, mask=col_idx < num_classes, other=-float('inf'))\n\n    # Apply scaling to logits if necessary\n    if logit_scale != 1.0:\n        logits *= logit_scale\n\n    # Compute log-sum-exp for normalization\n    max_logits = tl.max(logits, axis=0)\n    logits_exp = tl.exp(logits - max_logits)\n    lse = tl.log(tl.sum(logits_exp, axis=0)) + max_logits\n\n    # Store lse\n    tl.store(lse_ptr + row_idx, lse)\n\n    # Get the label for this row\n    label = tl.load(labels_ptr + row_idx)\n\n    # Compute z_loss and loss\n    if label != -1:  # Assuming -1 is the ignored_index\n        log_prob = logits[label] - lse\n        z_loss = -log_prob\n        if HAS_SMOOTHING:\n            smooth_loss = -tl.sum(logits_exp / num_classes) * smoothing\n            z_loss = (1.0 - smoothing) * z_loss + smooth_loss\n        tl.store(loss_ptr + row_idx, z_loss)\n        tl.store(z_loss_ptr + row_idx, z_loss)\n\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    logits_ptr, labels_ptr, dloss_ptr, dlogits_ptr,\n    num_classes, num_rows, smoothing, logit_scale, BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr, SPLIT: tl.constexpr\n):\n    # Define the row and column index\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, BLOCK_SIZE)\n\n    # Load the logits and lse for this row\n    logits = tl.load(logits_ptr + row_idx * num_classes + col_idx, mask=col_idx < num_classes, other=-float('inf'))\n    lse = tl.load(lse_ptr + row_idx)\n\n    # Apply scaling to logits if necessary\n    if logit_scale != 1.0:\n        logits *= logit_scale\n\n    # Compute probabilities\n    max_logits = tl.max(logits, axis=0)\n    logits_exp = tl.exp(logits - max_logits)\n    probabilities = logits_exp / tl.sum(logits_exp, axis=0)\n\n    # Get the label for this row\n    label = tl.load(labels_ptr + row_idx)\n\n    # Compute gradient\n    dloss = tl.load(dloss_ptr + row_idx)\n    grad = probabilities\n    if label != -1:  # Assuming -1 is the ignored_index\n        grad[label] -= 1.0\n    if HAS_SMOOTHING:\n        grad = (1.0 - smoothing) * grad + smoothing / num_classes\n\n    # Store dlogits\n    grad *= dloss\n    tl.store(dlogits_ptr + row_idx * num_classes + col_idx, grad, mask=col_idx < num_classes)\n\n# Wrapper functions\ndef cross_entropy_fwd(logits, labels, smoothing=0.0, logit_scale=1.0, BLOCK_SIZE=128, HAS_SMOOTHING=False, SPLIT=False):\n    num_rows, num_classes = logits.shape\n    loss = torch.empty(num_rows, device=logits.device, dtype=logits.dtype)\n    lse = torch.empty(num_rows, device=logits.device, dtype=logits.dtype)\n    z_loss = torch.empty(num_rows, device=logits.device, dtype=logits.dtype)\n\n    grid = (num_rows,)\n    cross_entropy_fwd_kernel[grid](\n        logits, labels, loss, lse, z_loss, num_classes, num_rows, smoothing, logit_scale,\n        BLOCK_SIZE=BLOCK_SIZE, HAS_SMOOTHING=HAS_SMOOTHING, SPLIT=SPLIT\n    )\n\n    return loss, lse, z_loss\n\ndef cross_entropy_bwd(logits, labels, dloss, smoothing=0.0, logit_scale=1.0, BLOCK_SIZE=128, HAS_SMOOTHING=False, SPLIT=False):\n    num_rows, num_classes = logits.shape\n    dlogits = torch.empty_like(logits)\n\n    grid = (num_rows,)\n    cross_entropy_bwd_kernel[grid](\n        logits, labels, dloss, dlogits, num_classes, num_rows, smoothing, logit_scale,\n        BLOCK_SIZE=BLOCK_SIZE, HAS_SMOOTHING=HAS_SMOOTHING, SPLIT=SPLIT\n    )\n\n    return dlogits\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    loss_ptr,  # data ptrs\n    lse_ptr,\n    z_loss_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    n_rows,\n    logits_row_stride,  # strides\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n            else:\n                loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    dlogits_ptr,  # data ptrs\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    logits_row_stride,  # strides\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, (dloss * logit_scale) * probs, mask=col_offsets < n_cols)\n\ndef cross_entropy_fwd(\n    logits, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, HAS_SMOOTHING, SPLIT\n):\n    n_rows, n_cols = logits.shape\n    loss = torch.empty((n_rows, n_cols), dtype=torch.float32, device=logits.device)\n    lse = torch.empty((n_rows, n_cols), dtype=torch.float32, device=logits.device)\n    z_loss = torch.empty((n_rows, n_cols), dtype=torch.float32, device=logits.device)\n    \n    grid = (n_rows, (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE)\n    \n    # \u8c03\u7528\u524d\u5411\u5185\u6838\uff0c\u4f20\u9012\u76f8\u5173\u53c2\u6570\n    cross_entropy_fwd_kernel[grid](\n        loss, lse, z_loss, logits, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, n_cols, n_rows, logits.stride(0), BLOCK_SIZE, HAS_SMOOTHING, SPLIT\n    )\n    \n    # \u6253\u5370\u635f\u5931\u3001LSE\u548cz_loss\uff0c\u5e2e\u52a9\u8c03\u8bd5\n    print(f\"Forward loss: {loss}\")\n    print(f\"Forward LSE: {lse}\")\n    print(f\"Forward z_loss: {z_loss}\")\n    \n    return loss, lse, z_loss\n\ndef cross_entropy_bwd(\n    dloss, logits, lse, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, BLOCK_SIZE, HAS_SMOOTHING\n):\n    n_rows, n_cols = logits.shape\n    dlogits = torch.empty_like(logits)\n    \n    grid = (n_rows, (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE)\n    \n    # \u8c03\u7528\u53cd\u5411\u5185\u6838\uff0c\u4f20\u9012\u76f8\u5173\u53c2\u6570\n    cross_entropy_bwd_kernel[grid](\n        dlogits, dloss, logits, lse, labels, smoothing, logit_scale, lse_square_scale, ignored_index, total_classes, class_start_idx, n_cols, logits.stride(0), dlogits.stride(0), dloss.stride(0), BLOCK_SIZE, HAS_SMOOTHING\n    )\n    \n    # \u6253\u5370\u53cd\u5411\u68af\u5ea6\uff0c\u5e2e\u52a9\u8c03\u8bd5\n    print(f\"Backward dlogits: {dlogits}\")\n    \n    return dlogits\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel function, `_fwd_kernel_int8kv`, implements the forward pass for an attention mechanism optimized for int8 key and value matrices. It computes the attention scores between a query (Q) and key (K) matrix, applies a softmax scaling, and multiplies the result by a value (V) matrix to produce the output (Out). The kernel uses several input parameters, such as strides and batch information, to optimize memory access patterns. The function `context_attention_fwd_ppl_int8kv` acts as a wrapper, setting up the grid and launching the kernel with appropriate parameters.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_int8kv(\n    Q, K, V, Out,\n    stride_qz, stride_qh, stride_qm,\n    stride_kz, stride_kh, stride_kn,\n    stride_vz, stride_vh, stride_vn,\n    stride_oz, stride_oh, stride_om,\n    batch_size, num_heads, seq_len_q, seq_len_kv, head_dim\n):\n    # Define the program ID, which identifies the current block of threads\n    pid = tl.program_id(0)\n\n    # Calculate the batch and head indices from the program ID\n    batch_id = pid // num_heads\n    head_id = pid % num_heads\n\n    # Define the block size for loading Q, K, and V\n    BLOCK_M = 128  # Number of queries processed by each block\n    BLOCK_N = 128  # Number of keys processed by each block\n    BLOCK_D = 64   # Head dimension\n\n    # Compute the starting index for this block\n    q_start = batch_id * stride_qz + head_id * stride_qh\n    k_start = batch_id * stride_kz + head_id * stride_kh\n    v_start = batch_id * stride_vz + head_id * stride_vh\n    o_start = batch_id * stride_oz + head_id * stride_oh\n\n    # Load the query vectors for this block\n    q_idx = tl.arange(0, BLOCK_M) + tl.arange(0, BLOCK_D) * stride_qm\n    q = tl.load(Q + q_start + q_idx, mask=q_idx < seq_len_q * head_dim, other=0.0)\n\n    # Initialize the output matrix for this block\n    out = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n\n    # Iterate over the keys and values in blocks\n    for nk in range(0, seq_len_kv, BLOCK_N):\n        # Load the key vectors for this block\n        k_idx = tl.arange(0, BLOCK_N) + tl.arange(0, BLOCK_D) * stride_kn\n        k = tl.load(K + k_start + nk * stride_kn + k_idx, mask=k_idx < seq_len_kv * head_dim, other=0.0)\n\n        # Compute attention scores\n        scores = tl.dot(q, k, trans_b=True)\n\n        # Apply softmax to attention scores\n        max_score = tl.max(scores, axis=1)\n        scores = scores - max_score[:, None]\n        exp_scores = tl.exp(scores)\n        sum_exp_scores = tl.sum(exp_scores, axis=1)\n        softmax_scores = exp_scores / sum_exp_scores[:, None]\n\n        # Load the value vectors for this block\n        v_idx = tl.arange(0, BLOCK_N) + tl.arange(0, BLOCK_D) * stride_vn\n        v = tl.load(V + v_start + nk * stride_vn + v_idx, mask=v_idx < seq_len_kv * head_dim, other=0.0)\n\n        # Compute the weighted sum of values\n        out += tl.dot(softmax_scores, v)\n\n    # Store the output\n    out_idx = tl.arange(0, BLOCK_M) + tl.arange(0, BLOCK_D) * stride_om\n    tl.store(Out + o_start + out_idx, out, mask=out_idx < seq_len_q * head_dim)\n\ndef context_attention_fwd_ppl_int8kv(Q, K, V, batch_size, num_heads, seq_len_q, seq_len_kv, head_dim):\n    # Create output tensor\n    Out = torch.empty((batch_size, num_heads, seq_len_q, head_dim), dtype=torch.float32, device=Q.device)\n\n    # Calculate strides\n    stride_qz, stride_qh, stride_qm = Q.stride()\n    stride_kz, stride_kh, stride_kn = K.stride()\n    stride_vz, stride_vh, stride_vn = V.stride()\n    stride_oz, stride_oh, stride_om = Out.stride()\n\n    # Launch kernel\n    grid = (batch_size * num_heads,)\n    _fwd_kernel_int8kv[grid](\n        Q, K, V, Out,\n        stride_qz, stride_qh, stride_qm,\n        stride_kz, stride_kh, stride_kn,\n        stride_vz, stride_vh, stride_vn,\n        stride_oz, stride_oh, stride_om,\n        batch_size, num_heads, seq_len_q, seq_len_kv, head_dim\n    )\n\n    return Out\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\nTESLA = \"Tesla\" in torch.cuda.get_device_name(0)\n\n@triton.jit\ndef _fwd_kernel_int8kv(\n    Q,\n    K,\n    V,\n    sm_scale,\n    Out,\n    B_Start_Loc,\n    B_Seqlen,\n    b_prompt_cache_len,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kb,\n    stride_kh,\n    stride_ks,\n    stride_kd,\n    stride_vb,\n    stride_vh,\n    stride_vs,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    H: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n\n    cur_kv_head = cur_head // kv_group_num\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n\n    block_start_loc = BLOCK_M * start_m\n\n    # initialize offsets\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs\n        + cur_head * stride_qh\n        + offs_d[None, :] * stride_qd\n    )\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len, cur_batch_seq_len + prompt_cache_len)\n    # causal mask\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        # k = tl.load(\n        #     k_ptrs + (start_n + offs_n[None, :]) * stride_ks,\n        #     mask=(start_n + offs_n[None, :]) < block_end_loc,\n        #     other=0,\n        # )\n        off_k = (\n            cur_batch * stride_kb\n            + (start_n + offs_n[None, :]) * stride_ks\n            + cur_kv_head * stride_kh\n            + offs_d[:, None] * stride_kd\n        )\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :]) < block_end_loc, other=0.0)\n\n        qk = tl.dot(q, k)\n        mask = (offs_m[:, None] + prompt_cache_len) >= (start_n + offs_n[None, :])\n        qk = tl.where(mask, qk * sm_scale, -1.0e8)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n\n        # -- update m_i and l_i\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        # -- update output accumulator --\n        acc = acc * alpha[:, None]\n        # update acc\n        # v = tl.load(\n        #     v_ptrs + (start_n + offs_n[:, None]) * stride_vs,\n        #     mask=(start_n + offs_n[:, None]) < block_end_loc,\n        #     other=0.0,\n        # )\n        off_v = (\n            cur_batch * stride_vb\n            + (start_n + offs_n[:, None]) * stride_vs\n            + cur_kv_head * stride_vh\n            + offs_d[None, :] * stride_vd\n        )\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None]) < block_end_loc, other=0.0)\n\n        p = p.to(v.dtype)\n        acc = tl.dot(p, v, acc)\n        # update m_i\n        m_i = m_ij\n\n    acc = acc / l_i[:, None]\n    off_o = (\n        (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs\n        + cur_head * stride_oh\n        + offs_d[None, :] * stride_od\n    )\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n\n\n@torch.no_grad()\ndef context_attention_fwd_ppl_int8kv(q, k, v, o, b_start_loc, b_seq_len, max_input_len, b_prompt_cache_len):\n    BLOCK_M = 128 if not TESLA else 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128, 256}\n\n    # \u8ba1\u7b97scale\u7cfb\u6570, \u5e76\u4e58\u4ee5 1/log(2) = 1.4426950408889634,\n    # \u7b97\u5b50\u5185\u90e8\u4f7f\u7528 tl.math.exp2 \u6765\u4f7f\u8ba1\u7b97\u4e0e\u6807\u51c6attention\u7b49\u4ef7\u3002\n    sm_scale = 1.0 / (Lq ** 0.5) * 1.4426950408889634\n    batch, head = b_seq_len.shape[0], q.shape[1]\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    grid = lambda meta: (triton.cdiv(max_input_len, meta[\"BLOCK_M\"]), batch * head, 1)\n    BLOCK_N = BLOCK_M\n    num_warps = 4 if Lk <= 64 else 8\n    num_stages = 1\n\n    _fwd_kernel_int8kv[grid](\n        q,\n        k,\n        v,\n        sm_scale,\n        o,\n        b_start_loc,\n        b_seq_len,\n        b_prompt_cache_len,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        k.stride(3),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        v.stride(3),\n        o.stride(0),\n        o.stride(1),\n        o.stride(2),\n        kv_group_num=kv_group_num,\n        H=head,\n        BLOCK_DMODEL=Lk,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        num_warps=num_warps,\n        num_stages=num_stages,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `masked_add_kernel` and a wrapper function `masked_add`. The kernel performs an element-wise operation on the input tensors, applying a mask to skip certain elements. The `masked_add` function sets up the necessary parameters and calls the kernel. It modifies `grad` by adding `p_data` scaled by `alpha` only where the mask is 0.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef masked_add_kernel(\n    grad_ptr, p_data_ptr, mask_ptr, alpha,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Program ID\n    pid = tl.program_id(axis=0)\n\n    # Block indices\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load data from global memory\n    grad = tl.load(grad_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    p_data = tl.load(p_data_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    mask = tl.load(mask_ptr + offsets, mask=offsets < n_elements, other=1.0)\n\n    # Apply the mask: update grad only where mask is 0\n    update = tl.where(mask == 0, p_data * alpha, 0.0)\n    grad += update\n\n    # Store the result back to global memory\n    tl.store(grad_ptr + offsets, grad, mask=offsets < n_elements)\n\ndef masked_add(grad, p_data, mask, alpha, block_size=1024):\n    # Ensure the input tensors are contiguous\n    assert grad.is_contiguous()\n    assert p_data.is_contiguous()\n    assert mask.is_contiguous()\n\n    # Number of elements\n    n_elements = grad.numel()\n\n    # Launch the kernel\n    grid = (triton.cdiv(n_elements, block_size),)\n    masked_add_kernel[grid](\n        grad, p_data, mask, alpha,\n        n_elements,\n        BLOCK_SIZE=block_size\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef masked_add_kernel(grad_ptr,\n                      p_ptr,\n                      p_mask_ptr,\n                      n_elements,\n                      alpha,\n                      BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    p_mask = tl.load(p_mask_ptr + offsets, mask=mask).to(tl.int1)\n    mask = mask & ~p_mask\n    p = tl.load(p_ptr + offsets, mask=mask)\n    grad = tl.load(grad_ptr + offsets, mask=mask)\n    grad += p * alpha\n    tl.store(grad_ptr + offsets, grad, mask=mask)\n\ndef masked_add(grad: torch.Tensor, p_data: torch.Tensor, p_mask: torch.Tensor, alpha: float = 0):\n    '''\n    equivalent to\n    grad.add_(p.data * (1 - p.mask), alpha=decay)\n    '''\n    assert grad.is_cuda and p_data.is_cuda and p_mask.is_cuda\n    assert (grad.layout, p_data.layout, p_mask.layout) == (torch.strided, torch.strided, torch.strided)\n    assert grad.stride() == p_data.stride() == p_mask.stride()\n    n_elements = grad.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    masked_add_kernel[grid](grad, p_data, p_mask, n_elements, alpha, BLOCK_SIZE=1024)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel function '_triton_rope' performs a rotational position encoding operation (rope) on query (q) and key (k) matrices. It applies sine and cosine transformations, controlled by the 'cos' and 'sin' inputs, on these matrices. The function 'rope_forward' orchestrates this by preparing the data and invoking the kernel. It rearranges the dimensions of input tensors and calculates padding sizes to fit hardware constraints. The function returns transformed q, k, cos, and sin tensors.\n", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr, k_ptr, cos_ptr, sin_ptr, \n    q_out_ptr, k_out_ptr,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_km, stride_kk,\n    stride_cos, stride_sin,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Define block indices\n    pid = tl.program_id(0)\n    \n    # Compute offsets for each dimension\n    q_offset = pid * BLOCK_SIZE\n    k_offset = pid * BLOCK_SIZE\n    \n    # Load q and k blocks\n    q = tl.load(q_ptr + q_offset, mask=q_offset < stride_qm)\n    k = tl.load(k_ptr + k_offset, mask=k_offset < stride_km)\n    \n    # Load cos and sin blocks\n    cos = tl.load(cos_ptr + q_offset, mask=q_offset < stride_cos)\n    sin = tl.load(sin_ptr + q_offset, mask=q_offset < stride_sin)\n    \n    # Apply the ROPE transformation\n    q_cos = q * cos - k * sin\n    k_sin = q * sin + k * cos\n    \n    # Store the results\n    tl.store(q_out_ptr + q_offset, q_cos, mask=q_offset < stride_qm)\n    tl.store(k_out_ptr + k_offset, k_sin, mask=k_offset < stride_km)\n\ndef rope_forward(q, k, cos, sin):\n    # Ensure inputs are on the correct device\n    assert q.is_cuda and k.is_cuda and cos.is_cuda and sin.is_cuda, \"All inputs must be CUDA tensors\"\n    \n    # Get shapes and ensure they are compatible\n    assert q.shape == k.shape, \"q and k must have the same shape\"\n    assert cos.shape == sin.shape, \"cos and sin must have the same shape\"\n    \n    # Prepare output tensors\n    q_out = torch.empty_like(q)\n    k_out = torch.empty_like(k)\n    \n    # Get the dimensions and calculate strides\n    B, H, M, K = q.shape\n    stride_qz, stride_qh, stride_qm, stride_qk = q.stride()\n    stride_kz, stride_kh, stride_km, stride_kk = k.stride()\n    stride_cos = cos.stride(0)\n    stride_sin = sin.stride(0)\n    \n    # Define block size\n    BLOCK_SIZE = 128  # This can be tuned for performance\n    \n    # Launch the Triton kernel\n    num_blocks = (M * K + BLOCK_SIZE - 1) // BLOCK_SIZE\n    _triton_rope[num_blocks](\n        q, k, cos, sin, q_out, k_out,\n        stride_qz, stride_qh, stride_qm, stride_qk,\n        stride_kz, stride_kh, stride_km, stride_kk,\n        stride_cos, stride_sin,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return q_out, k_out, cos, sin\n\n# Example usage:\n# q, k, cos, sin = torch.randn(...), torch.randn(...), torch.randn(...), torch.randn(...)\n# q_transformed, k_transformed, cos, sin = rope_forward(q, k, cos, sin)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr,\n    q_row_stride,\n    k_ptr,\n    k_row_stride,\n    cos,\n    cos_row_stride,\n    sin,\n    sin_row_stride,\n    sl,\n    bs: tl.constexpr,\n    n_qh: tl.constexpr,\n    n_kh: tl.constexpr,\n    hd: tl.constexpr,\n    pad_n_qh: tl.constexpr,\n    pad_n_kh: tl.constexpr,\n    pad_hd: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr = False,\n):\n    pid = tl.program_id(0)\n\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n\n    cos_row_idx = pid % (sl)\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0).to(sin_row.dtype)\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0).to(sin_row.dtype)\n\n    second_half_q_offsets = first_half_q_offsets + (hd // 2)\n    second_half_k_offsets = first_half_k_offsets + (hd // 2)\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask, other=0).to(sin_row.dtype)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask, other=0).to(sin_row.dtype)\n\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n\n\ndef rope_forward(q, k, cos, sin):\n    q = q.transpose(1, 2)\n    k = k.transpose(1, 2)\n\n    batch_size, seq_len, n_q_head, head_dim = q.shape\n    n_kv_head = k.shape[2]\n    pad_hd = triton.next_power_of_2(head_dim)\n    pad_n_q_head = triton.next_power_of_2(n_q_head)\n    pad_n_kv_head = triton.next_power_of_2(n_kv_head)\n    BLOCK_SIZE = max(pad_n_q_head, pad_n_kv_head)\n\n    n_row = batch_size * seq_len\n\n    q = q.contiguous()\n    k = k.contiguous()\n    cos = cos.contiguous()\n    sin = sin.contiguous()\n\n    _triton_rope[(n_row,)](\n        q,\n        q.stride(1),\n        k,\n        k.stride(1),\n        cos,\n        cos.stride(-2),\n        sin,\n        sin.stride(-2),\n        seq_len,\n        batch_size,\n        n_q_head,\n        n_kv_head,\n        head_dim,\n        pad_n_q_head,\n        pad_n_kv_head,\n        pad_hd,\n        BLOCK_SIZE=BLOCK_SIZE,\n        BACKWARD_PASS=False,\n    )\n    return q.transpose(1, 2), k.transpose(1, 2), cos, sin\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs a backward operation for a batched matrix multiplication. The main function is _bmm_chunk_bwd_kernel, which computes the gradient with respect to the matrix 'a' and optionally adds a residual term. The function uses loop tiling for efficient computation and leverages block sizes for both M, N, and CS dimensions.\n\n            The kernel takes several pointers for data input/output (a_ptr, dout_ptr, db_ptr, res_ptr), along with various dimensions and strides for indexing. It has a configurable BLOCK_SIZE for efficient data loading and processing. The kernel uses tl.dot for matrix multiplication and optionally adds a residual stored at res_ptr if HAS_RESIDUAL is True. \n\n            The function _bmm_chunk_bwd is a Python wrapper that handles the preparation of input tensors and their metadata, like strides and shapes, then launches the Triton kernel with an appropriate grid of block dimensions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef _bmm_chunk_bwd_kernel(a_ptr, dout_ptr, db_ptr, res_ptr,\n                          M, N, K, CS,\n                          stride_am, stride_ak,\n                          stride_do_n, stride_do_k,\n                          stride_dbm, stride_dbn,\n                          stride_resm, stride_resn,\n                          HAS_RESIDUAL: tl.constexpr):\n    # Program ID for indexing\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute block indices\n    m_block_start = pid_m * BLOCK_SIZE_M\n    n_block_start = pid_n * BLOCK_SIZE_N\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over the K dimension in chunks\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load a chunk of matrix 'a'\n        a_offset = m_block_start * stride_am + k * stride_ak\n        a = tl.load(a_ptr + a_offset + tl.arange(0, BLOCK_SIZE_M)[:, None] * stride_am + tl.arange(0, BLOCK_SIZE_K)[None, :])\n\n        # Load a chunk of matrix 'dout'\n        dout_offset = n_block_start * stride_do_n + k * stride_do_k\n        dout = tl.load(dout_ptr + dout_offset + tl.arange(0, BLOCK_SIZE_N)[:, None] * stride_do_n + tl.arange(0, BLOCK_SIZE_K)[None, :])\n\n        # Perform dot product and accumulate\n        acc += tl.dot(a, dout)\n\n    # Store result in db\n    db_offset = m_block_start * stride_dbm + n_block_start * stride_dbn\n    tl.store(db_ptr + db_offset + tl.arange(0, BLOCK_SIZE_M)[:, None] * stride_dbm + tl.arange(0, BLOCK_SIZE_N)[None, :], acc)\n\n    # Optionally add residual\n    if HAS_RESIDUAL:\n        res_offset = m_block_start * stride_resm + n_block_start * stride_resn\n        res = tl.load(res_ptr + res_offset + tl.arange(0, BLOCK_SIZE_M)[:, None] * stride_resm + tl.arange(0, BLOCK_SIZE_N)[None, :])\n        acc += res\n        tl.store(db_ptr + db_offset + tl.arange(0, BLOCK_SIZE_M)[:, None] * stride_dbm + tl.arange(0, BLOCK_SIZE_N)[None, :], acc)\n\ndef _bmm_chunk_bwd(a, dout, db, res=None, has_residual=False):\n    # Get shapes and strides\n    M, K = a.shape\n    _, N = dout.shape\n\n    stride_am, stride_ak = a.stride()\n    stride_do_n, stride_do_k = dout.stride()\n    stride_dbm, stride_dbn = db.stride()\n\n    if has_residual:\n        stride_resm, stride_resn = res.stride()\n    else:\n        stride_resm, stride_resn = 0, 0\n\n    # Define grid size\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n    # Launch the Triton kernel\n    _bmm_chunk_bwd_kernel[grid](\n        a, dout, db, res,\n        M, N, K, 0,\n        stride_am, stride_ak,\n        stride_do_n, stride_do_k,\n        stride_dbm, stride_dbn,\n        stride_resm, stride_resn,\n        HAS_RESIDUAL=has_residual\n    )\n", "label": "import math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2),\n    ],\n    key=['chunk_size', 'K'],\n)\n@triton.jit\ndef _bmm_chunk_bwd_kernel(\n    a_ptr, dout_ptr, db_ptr, res_ptr,\n    seqlen, chunk_size, K, ngroups,\n    stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak,\n    stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_csize_m, stride_dout_csize_n,\n    stride_db_batch, stride_db_seqlen, stride_db_head, stride_db_k,\n    stride_res_batch, stride_res_seqlen, stride_res_head, stride_res_k,\n    dot_dtype: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_CS: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n\n    a_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\n    dout_ptr += pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m)\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS), other=0.0).to(dot_dtype)\n        a = tl.load(a_ptrs, mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS) & (offs_n[None, :] < K), other=0.0).to(dot_dtype)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += pid_b * stride_res_batch + pid_c * chunk_size * stride_res_seqlen + pid_h * stride_res_head\n        res_ptrs = res_ptr + (offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k)\n        res = tl.load(res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)).to(tl.float32)\n        acc += res\n    db = acc.to(db_ptr.dtype.element_ty)\n\n    db_ptr += pid_b * stride_db_batch + pid_c * chunk_size * stride_db_seqlen + pid_h * stride_db_head\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k)\n    tl.store(db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K))\n\ndef _bmm_chunk_bwd(a, dout, residual=None, out=None):\n    has_groups = a.dim() == 4\n    if not has_groups:\n        batch, seqlen, k = a.shape\n    else:\n        batch, seqlen, ngroups, k = a.shape\n    nchunks, chunk_size = dout.shape[1], dout.shape[-1]\n    if a.stride(-1) != 1 and a.stride(-2) != 1:\n        a = a.contiguous()\n    if dout.stride(-1) != 1 and dout.stride(-2) != 1:\n        dout = dout.contiguous()\n    if residual is not None:\n        assert residual.shape == (batch, seqlen, k) if not has_groups else (batch, seqlen, ngroups, k)\n        if residual.stride(-1) != 1 and residual.stride(1) != 1:\n            residual = residual.contiguous()\n    if out is not None:\n        assert out.shape == a.shape\n        assert out.stride(-1) == 1 or out.stride(1) == 1\n    else:\n        out = torch.empty_like(a)\n    dot_dtype = (tl.bfloat16 if a.dtype == torch.bfloat16 or dout.dtype == torch.bfloat16 else\n                 (tl.float16 if a.dtype == torch.float16 or dout.dtype == torch.float16 else tl.float32))\n    grid = lambda META: (triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(k, META['BLOCK_SIZE_N']), batch,\n                    nchunks if not has_groups else nchunks * ngroups)\n    residual_strides = ((residual.stride(0), residual.stride(1), 0 if not has_groups else residual.stride(2),\n                         residual.stride(-1))\n                        if residual is not None else (0, 0, 0, 0))\n    with torch.cuda.device(a.device.index):\n        _bmm_chunk_bwd_kernel[grid](\n            a, dout, out, residual,\n            int(seqlen), int(chunk_size), int(k), int(ngroups if has_groups else 1),\n            a.stride(0), a.stride(1), 0 if not has_groups else a.stride(2), a.stride(-1),\n            dout.stride(0), dout.stride(1), 0 if not has_groups else dout.stride(2), dout.stride(-2), dout.stride(-1),\n            out.stride(0), out.stride(1), 0 if not has_groups else out.stride(2), out.stride(-1),\n            residual_strides[0], residual_strides[1], residual_strides[2], residual_strides[3],\n            dot_dtype,\n            HAS_RESIDUAL=residual is not None,\n        )\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton operator computes the Kullback-Leibler divergence (KL divergence) between predicted log-probabilities and ground truth distributions. It includes both forward and backward kernels. The `kldiv_forward_triton` function handles the forward pass, while `kldiv_backward_triton` takes care of the backward pass.\n\n    The main function `kldiv_forward_triton` accepts the following inputs:\n    - `y_pred`: Predicted log-probabilities tensor with shape [BT, V].\n    - `y_true`: Ground truth probabilities tensor with the same shape [BT, V].\n    - `log_target`: A boolean indicating if the ground truth is in log-space.\n    - `reduction`: A string specifying the reduction method (\"none\", \"sum\", \"mean\", \"batchmean\").\n    - `eps`: A small value to ensure numerical stability in logarithms.\n\n    The kernel `_kldiv_kernel_forward` calculates KL divergence either directly or in log-space depending on `log_target`. If `reduction` is set to \"none\", results are stored directly; otherwise, a sum is computed.\n\n    The function `kldiv_backward_triton` computes the gradient of the loss with respect to the input. It requires:\n    - `target`: The ground truth tensor [BT, V].\n    - `grad_output`: Gradient tensor from subsequent layers.\n    - `new_grads`: Tensor to store the computed gradients.\n    - `log_target`: Boolean to indicate log-space ground truth.\n\n    The backward kernel `_kldiv_kernel_backward` performs differentiation considering whether the target is in log-space.\n\n    The `BLOCK_SIZE` and `num_warps` control the parallel computation's granularity and efficiency.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define constants for block size and number of warps\nBLOCK_SIZE = 128\nNUM_WARPS = 4\n\n@triton.jit\ndef _kldiv_kernel_forward(y_pred_ptr, y_true_ptr, output_ptr, BT, V, log_target, eps, reduction, **meta):\n    pid = tl.program_id(0)\n    # Compute offsets for the block\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < BT * V\n\n    # Load data\n    y_pred = tl.load(y_pred_ptr + offsets, mask=mask)\n    y_true = tl.load(y_true_ptr + offsets, mask=mask)\n\n    # Compute KL divergence\n    if log_target:\n        kl_div = y_true - y_pred + tl.exp(y_pred - y_true)\n    else:\n        kl_div = y_true * (tl.log(y_true + eps) - y_pred)\n\n    # Apply reduction\n    if reduction == \"none\":\n        tl.store(output_ptr + offsets, kl_div, mask=mask)\n    elif reduction in [\"sum\", \"mean\", \"batchmean\"]:\n        kl_div = tl.sum(kl_div, axis=0)\n        if reduction == \"mean\":\n            kl_div /= BT * V\n        elif reduction == \"batchmean\":\n            kl_div /= BT\n        if pid == 0:  # Only one thread writes the result\n            tl.store(output_ptr, kl_div)\n\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction, eps):\n    BT, V = y_pred.shape\n    output = torch.empty_like(y_pred)\n    grid = (triton.cdiv(BT * V, BLOCK_SIZE),)\n    _kldiv_kernel_forward[grid](y_pred, y_true, output, BT, V, log_target, eps, reduction, num_warps=NUM_WARPS)\n    return output\n\n@triton.jit\ndef _kldiv_kernel_backward(y_pred_ptr, y_true_ptr, grad_output_ptr, new_grads_ptr, BT, V, log_target, **meta):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < BT * V\n\n    # Load data\n    y_pred = tl.load(y_pred_ptr + offsets, mask=mask)\n    y_true = tl.load(y_true_ptr + offsets, mask=mask)\n    grad_output = tl.load(grad_output_ptr + offsets, mask=mask)\n\n    # Compute gradient\n    if log_target:\n        grad = -tl.exp(y_pred - y_true) + 1\n    else:\n        grad = -y_true / (tl.exp(y_pred) + eps)\n\n    # Apply gradient and store\n    grad *= grad_output\n    tl.store(new_grads_ptr + offsets, grad, mask=mask)\n\ndef kldiv_backward_triton(y_pred, y_true, grad_output, log_target):\n    BT, V = y_pred.shape\n    new_grads = torch.empty_like(y_pred)\n    grid = (triton.cdiv(BT * V, BLOCK_SIZE),)\n    _kldiv_kernel_backward[grid](y_pred, y_true, grad_output, new_grads, BT, V, log_target, num_warps=NUM_WARPS)\n    return new_grads\n", "label": "from typing import Literal\nimport torch\nimport triton\nimport triton.language as tl\n\n\nMAX_FUSED_SIZE = 65536 // 4  # 65536 // 4 or 8 works the best\nREDUCTION_LITERAL = Literal[\"none\", \"sum\", \"mean\", \"batchmean\"]\n_REDUCTION_MODE_BATCHMEAN = tl.constexpr(3)\n\n@triton.jit\ndef _kldiv_kernel_forward(\n    y_ptr,  # [B, S], prediction ptr, the kernel expects the prediction in log-space\n    y_stride,  # int, prediction stride\n    gt_ptr,  # [B, S], ground truth ptr\n    gt_stride,  # int, ground truth stride\n    loss_ptr,  # [B] or [B, S] if reduction == _REDUCTION_MODE_NONE, output ptr\n    loss_stride,  # int, output stride\n    n_cols,  # int, number of columns in the input tensor\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n    reduction: tl.constexpr = _REDUCTION_MODE_BATCHMEAN,\n):\n    pid = tl.program_id(0).to(tl.int64)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n\n    loss_sum = 0.0\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n\n        # KL(y_true || y) = y_true * (log(y_true) - log(y))\n        # We compute KL(y_true || y) with y in the log-space\n        if not log_target:\n            loss = y_true * (tl.log(tl.maximum(y_true, eps)) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n\n        if reduction == 0:  # _REDUCTION_MODE_NONE\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss_sum += tl.sum(loss, axis=0)\n\n    if reduction != 0:\n        tl.store(loss_ptr, loss_sum)\n\n\n@triton.jit\ndef _kldiv_kernel_backward(\n    target_ptr,\n    target_stride,\n    new_grads_ptr,\n    new_grads_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n):\n    pid = tl.program_id(0).to(tl.int64)\n\n    target_ptr += pid * target_stride\n    new_grads_ptr += pid * new_grads_stride\n\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n\n        tl.store(new_grads_ptr + offsets, res, mask=mask)\n\n\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction, eps):  # [BT, V]\n    BT, V = y_pred.shape\n\n    BLOCK_SIZE = min(16384, triton.next_power_of_2(V))\n    num_warps = 4 if BLOCK_SIZE < 2048 else 8 if BLOCK_SIZE < 8192 else 16 if BLOCK_SIZE < 32768 else 32\n\n    grid = (BT,)\n    reduction = {\"none\": 0, \"sum\": 1, \"mean\": 2, \"batchmean\": 3}[reduction]\n\n    out_size = (BT, V) if reduction == 0 else (BT,)\n    output_tensor = torch.zeros(out_size, device=y_pred.device, dtype=torch.float32)\n\n    _kldiv_kernel_forward[grid](\n        y_pred,\n        y_pred.stride(0),\n        y_true,\n        y_true.stride(0),\n        output_tensor,\n        output_tensor.stride(0),\n        V,\n        eps=eps,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n        reduction=reduction,\n    )\n\n    if reduction == 3:  # _REDUCTION_MODE_BATCHMEAN\n        return output_tensor.sum() / BT\n    elif reduction == 1:  # _REDUCTION_MODE_SUM\n        return output_tensor.sum(dim=0)\n    elif reduction == 2:  # _REDUCTION_MODE_MEAN\n        return output_tensor.sum() / (BT * V)\n    else:\n        return output_tensor\n\n\ndef kldiv_backward_triton(target, grad_output, new_grads, log_target):\n    BT, V = target.shape\n\n    BLOCK_SIZE = min(16384, triton.next_power_of_2(V))\n    num_warps = 4 if BLOCK_SIZE < 2048 else 8 if BLOCK_SIZE < 8192 else 16 if BLOCK_SIZE < 32768 else 32\n\n    grid = (BT,)\n\n    _kldiv_kernel_backward[grid](\n        target,\n        target.stride(0),\n        new_grads,\n        new_grads.stride(0),\n        V,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n    )\n\n    if torch.equal(grad_output, torch.tensor(1.0, device=grad_output.device)):\n        return new_grads\n\n    return new_grads * grad_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton-based softmax operator for GPU execution, including a kernel function `softmax_kernel` and a wrapper function `softmax`.\n        The `softmax_kernel` computes the softmax function for each row of a 2D input tensor. It parallelizes across rows with one Triton program instance per row.\n        It loads row data, applies the softmax transformation by subtracting the maximum for numerical stability, and writes the result back to output memory.\n        The wrapper function `softmax` determines grid and block configuration based on input dimensions and manages memory, invoking the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    input_ptr, output_ptr, row_stride, n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Program ID for parallelization across rows\n    row_idx = tl.program_id(0)\n\n    # Calculate the start of the row\n    row_start = row_idx * row_stride\n\n    # Create a range of indices for the row\n    offsets = row_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load the row data\n    row_data = tl.load(input_ptr + offsets, mask=offsets < n_cols, other=-float('inf'))\n\n    # Compute the maximum value for numerical stability\n    row_max = tl.max(row_data, axis=0)\n\n    # Subtract the max and exponentiate\n    row_data = tl.exp(row_data - row_max)\n\n    # Compute the sum of the exponentiated values\n    row_sum = tl.sum(row_data, axis=0)\n\n    # Normalize to get softmax probabilities\n    softmax_output = row_data / row_sum\n\n    # Store the result\n    tl.store(output_ptr + offsets, softmax_output, mask=offsets < n_cols)\n\ndef softmax(input_tensor):\n    # Get the dimensions of the input tensor\n    n_rows, n_cols = input_tensor.shape\n\n    # Define the block size\n    BLOCK_SIZE = 1024  # Adjust based on your GPU's capability\n\n    # Allocate output tensor\n    output_tensor = torch.empty_like(input_tensor)\n\n    # Define the grid configuration\n    grid = (n_rows,)\n\n    # Launch the kernel\n    softmax_kernel[grid](\n        input_tensor,\n        output_tensor,\n        input_tensor.stride(0),\n        n_cols,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return output_tensor\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # The rows of the softmax are independent, so we parallelize across those\n    row_idx = tl.program_id(0)\n    # The stride represents how much we need to increase the pointer to advance 1 row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    # The block size is the next power of two greater than n_cols, so we can fit each\n    # row in a single block\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    # Subtract maximum for numerical stability\n    row_minus_max = row - tl.max(row, axis=0)\n    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    # Write back output to DRAM\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\ndef softmax(x):\n    n_rows, n_cols = x.shape\n    # The block size is the smallest power of two greater than the number of columns in `x`\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    y = torch.empty_like(x)\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row of the input matrix\n    softmax_kernel[(n_rows, )](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_cols,\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator implements a scalar-tensor power operation. The main function is `pow_func_scalar_tensor_kernel_rank_1`, which performs element-wise exponentiation of a tensor by a scalar. The kernel function is launched by the wrapper `pow_func_scalar_tensor_wrapper_rank_1`. It takes a scalar `val0`, a tensor `in0` (or a `StridedBuffer`), and outputs to `out0`. The operation is partitioned into tiles for efficient parallel processing. The wrapper also uses heuristics to determine optimal tile sizes and the number of warps.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef pow_func_scalar_tensor_kernel_rank_1(\n    val0, in0_ptr, out0_ptr, n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Calculate the index of the current thread\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Create a mask to handle out-of-bounds\n    mask = offsets < n_elements\n\n    # Load input tensor elements\n    in0 = tl.load(in0_ptr + offsets, mask=mask)\n\n    # Perform element-wise exponentiation\n    out0 = tl.math.pow(val0, in0)\n\n    # Store the result\n    tl.store(out0_ptr + offsets, out0, mask=mask)\n\ndef pow_func_scalar_tensor_wrapper_rank_1(val0, in0, out0):\n    # Determine the number of elements\n    n_elements = in0.size\n\n    # Define block size heuristically\n    BLOCK_SIZE = 1024  # This can be tuned based on the hardware and input size\n\n    # Calculate the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    pow_func_scalar_tensor_kernel_rank_1[grid](\n        val0, in0, out0, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage:\n# import torch\n# val0 = 2.0  # Scalar value\n# in0 = torch.tensor([1.0, 2.0, 3.0, 4.0], device='cuda')  # Input tensor\n# out0 = torch.empty_like(in0)  # Output tensor\n# pow_func_scalar_tensor_wrapper_rank_1(val0, in0, out0)\n# print(out0)  # Should print tensor([2.0, 4.0, 8.0, 16.0], device='cuda')\n", "label": "import math\nfrom typing import Union\nimport torch\nimport triton\nfrom triton import language as tl\n\ntry:\n    from triton.language.extra.cuda.libdevice import pow as _pow\nexcept ImportError:\n    try:\n        from triton.language.math import pow as _pow\n    except ImportError:\n        from triton.language.libdevice import pow as _pow\n\ndef heuristics_for_tile_size(max_tile_size, *sizes):\n    ndim = len(sizes)\n    tile_sizes = [0 for _ in range(ndim)]\n    for i in range(ndim):\n        size = sizes[ndim - 1 - i]\n        tile_size = min(max_tile_size, triton.next_power_of_2(size))\n        tile_sizes[ndim - 1 - i] = tile_size\n        max_tile_size = max(1, max_tile_size // tile_size)\n    return tuple(tile_sizes)\n\ndef heuristics_for_num_warps(tile_size):\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n    \n\nclass StridedBuffer:\n    \"\"\"A drop-in replacement of torch.Tensor that can be used in wrapper generated by\n    PointwiseDynamicFunction. It allows us to use a different shape, stride, data\n    pointer that that of the base tensor.\n\n    It is a kind of reinterpretation of the base tensor. We make this class since we\n    cannot get a Tensor view with negative strides via torch APIs, while we need this\n    to implement flip op.\n\n    Although generated code can accept torch.Tensor & StridedBuffer, but StridedBuffer\n    may not have all the methods as torch.Tensors do. We add some attributes & methods\n    with the same name as torch.Tensor, which are used in the generated code. But we\n    may not cover all the methods, add one if what you need is missing here.\n\n    And can also be used in triton kernels since it also has dtype & data_ptr().\n    \"\"\"\n\n    def __init__(\n        self, base: torch.Tensor, shape=None, strides=None, dtype=None, offset=0\n    ):\n        self._base = base\n        self.dtype = dtype or base.dtype\n        if offset == 0:\n            self._data_ptr = self._base.data_ptr()\n        else:\n            offset = self.dtype.itemsize * offset\n            self._data_ptr = self._base.data_ptr() + offset\n        self.shape = tuple(shape if shape is not None else self._base.shape)\n        self._strides = tuple(strides if strides is not None else self._base.stride())\n        self.device = self._base.device\n        self.ndim = len(self.shape)\n\n    def stride(self):\n        return self._strides\n\n    def size(self):\n        return self.shape\n\n    def element_size(self):\n        return self.dtype.itemsize\n\n    def numel(self):\n        return math.prod(self.shape)\n\n    def dim(self):\n        return self.ndim\n\n    def unwrap(self):\n        return self._base\n\n    def data_ptr(self):\n        return self._data_ptr\n\n\ndef pow_func_scalar_tensor_wrapper_rank_1(val0, in0: Union[torch.Tensor, StridedBuffer], /, *, out0: Union[torch.Tensor, StridedBuffer]): \n    \"\"\"Generated wrapper function with Pointwise: scalar, StridedBuffer, StridedBuffer(a1!) -> StridedBuffer(a1!)\"\"\"\n    assert in0.shape == out0.shape, 'operand shapes mismatch'\n    # task partitioning\n    shape = out0.shape\n    num_tasks = out0.numel()\n    tile_sizes = heuristics_for_tile_size(512, *shape)\n    tile_size = math.prod(tile_sizes)\n    num_tiles = math.prod(triton.cdiv(size, tile_size) for size, tile_size in zip(shape, tile_sizes))\n    num_ctas = min(65536, num_tiles)\n    tiles_per_cta = triton.cdiv(num_tiles, num_ctas)\n    num_warps = heuristics_for_num_warps(tile_size)\n    one_tile_per_cta = tiles_per_cta==1\n    grid = (num_ctas, 1, 1)\n    # kernel launch\n    in0_strides = in0.stride()\n    in0_stride_order = (0,)\n    out0_strides = out0.stride()\n    out0_stride_order = (0,)\n    with torch.cuda._DeviceGuard(in0.device.index):\n        pow_func_scalar_tensor_kernel_rank_1[grid](\n            val0, in0, out0,\n            in0_strides[0], # stride for in0\n            in0_stride_order[0], # stride order for in0\n            out0_strides[0], # stride for out0\n            out0_stride_order[0], # stride orderfor out0\n            shape[0], # task indexing space\n            num_tasks, # num tasks\n            tiles_per_cta=tiles_per_cta, # tiles_per_cta\n            tile_size0=tile_sizes[0],\n            one_tile_per_cta=one_tile_per_cta,\n            num_warps=num_warps,\n        )\n    return out0\n\n@triton.jit\ndef pow_func_scalar_tensor(x, exponent):\n    return _pow(x.to(tl.float32), exponent)\n\n@triton.jit(do_not_specialize=['val0'])\ndef pow_func_scalar_tensor_kernel_rank_1(\n    val0,\n    in0_ptr: tl.tensor, # of tl.pointer_type\n    out0_ptr: tl.tensor, # of tl.pointer_type\n    in0_stride0: int, # strides for in0\n    in0_stride_order0: tl.constexpr, # stride order for in0\n    out0_stride0: int, # strides for out0\n    out0_stride_order0: tl.constexpr, # stride order for out0\n    s0: int, # task_space\n    num_tasks: int,\n    tiles_per_cta: int,\n    tile_size0: tl.constexpr,\n    one_tile_per_cta: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_tiles0 = tl.cdiv(s0, tile_size0)\n    if one_tile_per_cta: # monolitic kernel style\n        tile_id = pid\n        # pid multi index recontruction: we use c ordering, right axes changes fastest\n        tile_id0 = tile_id\n\n        # tile offsets\n        offset0 = tile_id0 * tile_size0\n        # loads\n        in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n        in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n        # compute\n        out0 = pow_func_scalar_tensor(val0, in0)\n\n        # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n        out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n        tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n    else: # grid-stride-loop style kernel\n        num_ctas = tl.num_programs(0)\n        for j in range(0, tiles_per_cta):\n            tile_id = pid + j * num_ctas\n            # pid multi index recontruction: we use c ordering, right axes changes fastest\n            tile_id0 = tile_id\n\n            # tile offsets\n            offset0 = tile_id0 * tile_size0\n            # loads\n            in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n            in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n            # compute\n            out0 = pow_func_scalar_tensor(val0, in0)\n\n            # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n            out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n            tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines two functions, `_l2_norm_fwd` and `_l2_norm_bwd`, utilizing Triton for parallel computation of L2 normalization and its backward pass on 2D input tensors.\n\n            1. `_l2_norm_fwd_1pass_kernel`: This Triton kernel is called in `_l2_norm_fwd`. It computes the L2 normalization for each row of the input tensor `X`. It loads data, computes variance, uses it to normalize input, and stores the result in output tensor `Y`.\n\n            2. `_l2_norm_bwd_kernel`: This Triton kernel is used in `_l2_norm_bwd`. It computes the backward pass for L2 normalization. It loads inputs `X` and gradient `DY`, calculates gradients w.r.t input, and stores results in `DX`.\n\n            Both kernels leverage parallel processing to efficiently compute row-wise L2 normalization. The functions `_l2_norm_fwd` and `_l2_norm_bwd` reshape the input tensors for compatibility and invoke these kernels with appropriate arguments.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(X_ptr, Y_ptr, stride_x, stride_y, N, BLOCK_SIZE: tl.constexpr):\n    # Pointers to input and output rows\n    row_idx = tl.program_id(0)\n    X_row_ptr = X_ptr + row_idx * stride_x\n    Y_row_ptr = Y_ptr + row_idx * stride_y\n\n    # Load data from the row\n    offsets = tl.arange(0, BLOCK_SIZE)\n    X_row = tl.load(X_row_ptr + offsets, mask=offsets < N, other=0.0)\n\n    # Compute the squared norm of the row\n    X_squared = X_row * X_row\n    sum_X_squared = tl.sum(X_squared, axis=0)\n\n    # Compute L2 norm\n    norm = tl.sqrt(sum_X_squared)\n\n    # Normalize the row\n    Y_row = X_row / norm\n\n    # Store the result\n    tl.store(Y_row_ptr + offsets, Y_row, mask=offsets < N)\n\n@triton.jit\ndef _l2_norm_bwd_kernel(X_ptr, DY_ptr, DX_ptr, stride_x, stride_dy, stride_dx, N, BLOCK_SIZE: tl.constexpr):\n    # Pointers to input, gradient, and output rows\n    row_idx = tl.program_id(0)\n    X_row_ptr = X_ptr + row_idx * stride_x\n    DY_row_ptr = DY_ptr + row_idx * stride_dy\n    DX_row_ptr = DX_ptr + row_idx * stride_dx\n\n    # Load data from the row\n    offsets = tl.arange(0, BLOCK_SIZE)\n    X_row = tl.load(X_row_ptr + offsets, mask=offsets < N, other=0.0)\n    DY_row = tl.load(DY_row_ptr + offsets, mask=offsets < N, other=0.0)\n\n    # Compute the squared norm of the row\n    X_squared = X_row * X_row\n    sum_X_squared = tl.sum(X_squared, axis=0)\n\n    # Compute L2 norm\n    norm = tl.sqrt(sum_X_squared)\n\n    # Compute gradient w.r.t input\n    X_norm = X_row / norm\n    sum_XDY = tl.sum(X_norm * DY_row, axis=0)\n    DX_row = (DY_row - X_norm * sum_XDY) / norm\n\n    # Store the result\n    tl.store(DX_row_ptr + offsets, DX_row, mask=offsets < N)\n\ndef _l2_norm_fwd(X):\n    # Ensure X is a 2D tensor\n    assert X.ndim == 2\n    M, N = X.shape\n    Y = torch.empty_like(X)\n\n    # Launch Triton kernel\n    BLOCK_SIZE = 1024  # Adjust block size as needed\n    grid = (M,)\n    _l2_norm_fwd_1pass_kernel[grid](\n        X, Y,\n        X.stride(0), Y.stride(0),\n        N,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return Y\n\ndef _l2_norm_bwd(X, DY):\n    # Ensure X and DY are 2D tensors\n    assert X.ndim == 2 and DY.ndim == 2\n    M, N = X.shape\n    DX = torch.empty_like(X)\n\n    # Launch Triton kernel\n    BLOCK_SIZE = 1024  # Adjust block size as needed\n    grid = (M,)\n    _l2_norm_bwd_kernel[grid](\n        X, DY, DX,\n        X.stride(0), DY.stride(0), DX.stride(0),\n        N,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    return DX\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    # Compute mean and variance\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) \n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    mask = cols < N\n    y = x * rstd\n    # Write output\n    tl.store(Y + cols, y, mask=mask)\n\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X,  # pointer to the input\n    DY,  # pointer to the output gradient\n    DX,  # pointer to the input gradient\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x) \n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0).to(tl.float32)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var+eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n\ndef _l2_norm_fwd(\n    x, eps=1e-6\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, x.shape[-1])\n    if x.stride(-1) != 1:\n        x = x.contiguous()\n    assert x.stride(-1) == 1 \n    # allocate output\n    y = torch.empty_like(x)\n    assert y.stride(-1) == 1\n    N = x.shape[-1]\n    M = x.shape[0]\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    # heuristics for number of warps\n    with torch.cuda.device(x.device.index):\n        _l2_norm_fwd_1pass_kernel[(M,)](\n            x,\n            y,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return y.reshape(x_shape_og)\n\ndef _l2_norm_bwd(\n    x, dy, eps=1e-5,\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, dy.shape[-1])\n    dy = dy.reshape(-1, dy.shape[-1])\n    if dy.stride(-1) != 1:\n        dy = dy.contiguous()\n    assert dy.shape == x.shape\n    # allocate output\n    dx = torch.empty_like(x)\n    N = x.shape[-1]\n    M = x.shape[0]\n    assert x.stride(-1) == 1\n    assert dy.stride(-1) == 1\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    # heuristics for number of warps\n    with torch.cuda.device(x.device.index):\n        _l2_norm_bwd_kernel[(M,)](\n            x,\n            dy,\n            dx,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return dx.reshape(x_shape_og)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel performs layer normalization and its backward pass efficiently on CUDA. The `Fast_Layernorm` class is a PyTorch custom autograd function that uses two Triton kernels: `layernorm_forward` and `layernorm_backward`. These functions are responsible for the forward and backward computations of layer normalization. The `calculate_settings` function determines the block size and number of warps based on the number of columns to optimize for parallel execution. The `fast_layernorm` function is a utility to apply this custom layer normalization on an input tensor `X` using the provided layernorm module weights and biases.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Kernel for forward pass of layer normalization\n@triton.jit\ndef layernorm_forward(X_ptr, gamma_ptr, beta_ptr, Y_ptr, mean_ptr, rstd_ptr, N, BLOCK_SIZE: tl.constexpr):\n    # Get the program id\n    pid = tl.program_id(0)\n    # Create a block of indices\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    # Mask to prevent out-of-bounds memory access\n    mask = offsets < N\n    # Load input, gamma, and beta\n    X = tl.load(X_ptr + offsets, mask=mask, other=0.0)\n    gamma = tl.load(gamma_ptr + offsets, mask=mask, other=1.0)\n    beta = tl.load(beta_ptr + offsets, mask=mask, other=0.0)\n    # Compute mean\n    mean = tl.sum(X, axis=0) / N\n    tl.store(mean_ptr + pid, mean)\n    # Compute variance and rstd (reciprocal of standard deviation)\n    var = tl.sum((X - mean) * (X - mean), axis=0) / N\n    rstd = 1.0 / tl.sqrt(var + 1e-5)\n    tl.store(rstd_ptr + pid, rstd)\n    # Normalize and apply gamma and beta\n    Y = (X - mean) * rstd * gamma + beta\n    tl.store(Y_ptr + offsets, Y, mask=mask)\n\n# Kernel for backward pass of layer normalization\n@triton.jit\ndef layernorm_backward(dY_ptr, X_ptr, mean_ptr, rstd_ptr, gamma_ptr, dX_ptr, dgamma_ptr, dbeta_ptr, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    # Load data\n    dY = tl.load(dY_ptr + offsets, mask=mask, other=0.0)\n    X = tl.load(X_ptr + offsets, mask=mask, other=0.0)\n    mean = tl.load(mean_ptr + pid)\n    rstd = tl.load(rstd_ptr + pid)\n    gamma = tl.load(gamma_ptr + offsets, mask=mask, other=1.0)\n    # Compute gradients\n    X_hat = (X - mean) * rstd\n    dX_hat = dY * gamma\n    dvar = tl.sum(dX_hat * (X - mean) * (-0.5) * rstd**3, axis=0)\n    dmean = tl.sum(dX_hat * (-rstd), axis=0) + dvar * tl.sum(-2.0 * (X - mean), axis=0) / N\n    dX = dX_hat * rstd + dvar * 2.0 * (X - mean) / N + dmean / N\n    dgamma = tl.sum(dY * X_hat, axis=0)\n    dbeta = tl.sum(dY, axis=0)\n    # Store gradients\n    tl.store(dX_ptr + offsets, dX, mask=mask)\n    tl.store(dgamma_ptr + offsets, dgamma, mask=mask)\n    tl.store(dbeta_ptr + offsets, dbeta, mask=mask)\n\n# Function to calculate block size and number of warps\ndef calculate_settings(num_columns):\n    BLOCK_SIZE = 128\n    num_warps = 4\n    if num_columns <= 128:\n        BLOCK_SIZE = 128\n        num_warps = 4\n    elif num_columns <= 256:\n        BLOCK_SIZE = 256\n        num_warps = 8\n    else:\n        BLOCK_SIZE = 512\n        num_warps = 16\n    return BLOCK_SIZE, num_warps\n\n# PyTorch custom autograd function\nclass Fast_Layernorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, gamma, beta):\n        # Determine settings\n        N = X.shape[1]\n        BLOCK_SIZE, num_warps = calculate_settings(N)\n        # Allocate output tensors\n        Y = torch.empty_like(X)\n        mean = torch.empty(X.shape[0], device=X.device)\n        rstd = torch.empty(X.shape[0], device=X.device)\n        # Launch forward kernel\n        layernorm_forward[(X.shape[0],)](X, gamma, beta, Y, mean, rstd, N, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n        # Save context for backward pass\n        ctx.save_for_backward(X, gamma, mean, rstd)\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        return Y\n\n    @staticmethod\n    def backward(ctx, dY):\n        X, gamma, mean, rstd = ctx.saved_tensors\n        BLOCK_SIZE = ctx.BLOCK_SIZE\n        N = X.shape[1]\n        # Allocate gradients\n        dX = torch.empty_like(X)\n        dgamma = torch.empty_like(gamma)\n        dbeta = torch.empty_like(gamma)\n        # Launch backward kernel\n        layernorm_backward[(X.shape[0],)](dY, X, mean, rstd, gamma, dX, dgamma, dbeta, N, BLOCK_SIZE=BLOCK_SIZE)\n        return dX, dgamma, dbeta\n\n# Utility function to apply fast layer normalization\ndef fast_layernorm(X, gamma, beta):\n    return Fast_Layernorm.apply(X, gamma, beta)\n\n# Example usage\nif __name__ == \"__main__\":\n    X = torch.randn(64, 256, device='cuda')\n    gamma = torch.ones(256, device='cuda')\n    beta = torch.zeros(256, device='cuda')\n    Y = fast_layernorm(X, gamma, beta)\n    print(Y)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\nnext_power_of_2 = triton.next_power_of_2\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int,):\n    BLOCK_SIZE : int = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef layernorm_forward(\n    Y, Y_row_stride,\n    X, X_row_stride,\n    W,\n    b,\n    r,\n    mu,\n    n_cols, eps,\n    BLOCK_SIZE : tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y  += row_idx * Y_row_stride\n    X  += row_idx * X_row_stride\n    r  += row_idx\n    mu += row_idx\n\n    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask = mask, other = 0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask = mask, other = 0).to(tl.float32)\n\n    mean_X  = tl.sum(X_row,   axis = 0) / n_cols\n    XX      = X_row - mean_X\n    row_var = tl.sum(XX * XX, axis = 0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store (r, inv_var)\n    tl.store (mu, mean_X)\n    output = (XX * inv_var) * W_row + b_row\n    tl.store(Y + col_offsets, output, mask = mask)\n\n@triton.jit\ndef layernorm_backward(\n    dY, dY_row_stride,\n    X,   X_row_stride,\n    W,\n    b,\n    r,\n    mu,\n    n_cols, eps,\n    BLOCK_SIZE : tl.constexpr\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dY += row_idx * dY_row_stride\n    X  += row_idx *  X_row_stride\n    r  += row_idx\n    mu += row_idx\n\n    dY_row = tl.load(dY + col_offsets, mask = mask, other = 0).to(tl.float32)\n    X_row  = tl.load(X  + col_offsets, mask = mask, other = 0).to(tl.float32)\n    W_row  = tl.load(W  + col_offsets, mask = mask, other = 0).to(tl.float32)\n    b_row  = tl.load(b  + col_offsets, mask = mask, other = 0).to(tl.float32)\n\n    inv_var = tl.load(r) .to(tl.float32)\n    mean    = tl.load(mu).to(tl.float32)\n    normed  = (X_row - mean) * inv_var\n    dY_W = dY_row * W_row\n    dX_row = dY_W - tl.sum(dY_W, axis = 0) / n_cols - normed * tl.sum(dY_W * normed, axis = 0) / n_cols\n    dX_row = dX_row * inv_var\n    tl.store(dY + col_offsets, dX_row, mask = mask)\n\nclass Fast_Layernorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, b, eps):\n        shape = X.shape\n        dim = shape[-1]\n        X = X.view(-1, dim)\n        n_rows, n_cols = X.shape\n        BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n        Y  = torch.empty((n_rows, n_cols), dtype = X.dtype, device = \"cuda:0\")\n        r  = torch.empty(n_rows, dtype = torch.float32, device = \"cuda:0\")\n        mu = torch.empty(n_rows, dtype = torch.float32, device = \"cuda:0\")\n\n        layernorm_forward[(n_rows,)](\n            Y, Y.stride(0),\n            X, X.stride(0),\n            W,\n            b,\n            r,\n            mu,\n            n_cols, eps,\n            BLOCK_SIZE = BLOCK_SIZE,\n            num_warps  = num_warps,\n        )\n        ctx.eps = eps\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps  = num_warps\n        ctx.save_for_backward(X, W, b, r, mu)\n        return Y.view(*shape)\n    \n    @staticmethod\n    def backward(ctx, dY):\n        shape = dY.shape\n        dim = shape[-1]\n        dY = dY.view(-1, dim)\n        X, W, b, r, mu = ctx.saved_tensors\n        n_rows, n_cols = dY.shape\n\n        layernorm_backward[(n_rows,)](\n            dY, dY.stride(0),\n            X,  X .stride(0),\n            W,\n            b,\n            r,\n            mu,\n            n_cols, ctx.eps,\n            BLOCK_SIZE = ctx.BLOCK_SIZE,\n            num_warps  = ctx.num_warps,\n        )\n        dX = dY.view(*shape)\n        return dX, None, None, None, None\n    \ndef fast_layernorm(layernorm, X):\n    assert(layernorm.elementwise_affine is True)\n    W    = layernorm.weight\n    bias = layernorm.bias\n    eps = layernorm.variance_epsilon if \\\n        hasattr(layernorm, \"variance_epsilon\") \\\n        else layernorm.eps\n    out = Fast_Layernorm.apply(X, W, bias, eps)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel calculates the mean of a tensor along specified dimensions. The `mean_dim_kernel` is the main function, which operates on a 2D tensor `X` and computes the mean across a dimension specified by `M` and `N`, using block sizes `BLOCK_M` and `BLOCK_N`. `pid` calculates the program ID for distributed computation. `dim_compress` reorders tensor dimensions for efficient processing. The `mean_dim` function orchestrates the process, handling input transformation and kernel invocation.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Kernel to compute the mean along specified dimensions\n@triton.jit\ndef mean_dim_kernel(X_ptr, mean_ptr, M, N, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Program ID for distributed computation\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Offsets for the current block\n    offsets_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offsets_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    # Mask to handle out-of-bounds access\n    mask_m = offsets_m < M\n    mask_n = offsets_n < N\n\n    # Load the block of data\n    X_block = tl.load(X_ptr + offsets_m[:, None] * N + offsets_n[None, :], mask=mask_m[:, None] & mask_n[None, :])\n\n    # Compute the mean along the N dimension\n    mean_value = tl.sum(X_block, axis=1) / N\n\n    # Store the result\n    tl.store(mean_ptr + offsets_m, mean_value, mask=mask_m)\n\n# Function to compute mean along specified dimension\ndef mean_dim(X, dim):\n    assert X.ndim == 2, \"Input tensor must be 2D\"\n    \n    # Reorder dimensions if needed\n    if dim == 1:\n        X = X.T\n\n    M, N = X.shape\n    BLOCK_M = 128  # Block size for M dimension\n    BLOCK_N = 128  # Block size for N dimension\n\n    # Output tensor for mean values\n    mean = torch.empty(M, dtype=X.dtype, device=X.device)\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n    mean_dim_kernel[grid](X, mean, M, N, BLOCK_M, BLOCK_N)\n\n    return mean\n\n# Example usage\nimport torch\n\n# Create a 2D tensor\nX = torch.randn(256, 256, device='cuda')\n\n# Compute mean along dimension 1\nmean_result = mean_dim(X, dim=1)\nprint(mean_result)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef mean_dim_kernel(X, Mean, M, N, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Map the program id to the row of X it should compute.\n    pid = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)[:, None] \n    X = X + pid * N\n    Mean = Mean + pid\n    row_mask = pid < M\n\n    # Compute mean\n    _mean = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    for off in range(0, N, BLOCK_N):\n        cols = off + tl.arange(0, BLOCK_N)[None, :]\n        col_mask = cols < N\n        mask = row_mask and col_mask\n\n        a = tl.load(X + cols, mask, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=1) / N\n    mean = mean[:, None]\n    tl.store(Mean, mean, row_mask)\n\ndef dim_compress(inp: torch.Tensor, dims):\n    if isinstance(dims, int):\n        dims = [dims]\n    dim = inp.ndim\n    stride = inp.stride()\n    batch_dim = [i for i in range(dim) if i not in dims]\n    sorted_reduction_dim = sorted(dims, key=lambda x: stride[x], reverse=True)\n    order = batch_dim + sorted_reduction_dim\n    return inp.permute(order).contiguous()\n\ndef mean_dim(x, dim, keepdim=False, *, dtype=None):\n  if dtype is None:\n    dtype = x.dtype\n  \n  shape = list(x.shape)\n  if isinstance(dim, int):\n     dim = [dim]\n  dim = [d % x.ndim for d in dim]\n  x = dim_compress(x, dim)\n  N = 1\n  for i in dim:\n    N *= shape[i]\n    shape[i] = 1\n  M = x.numel() // N\n  out = torch.empty(shape, dtype=dtype, device=x.device)\n  grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]),)\n\n  with torch.cuda.device(x.device):\n    mean_dim_kernel[grid](x, out, M, N, BLOCK_M=8, BLOCK_N=8)\n  if not keepdim:\n    out = out.squeeze(dim)\n  return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton operator code provides a function called `fill_kv_cache`. This function is responsible for filling key/value states into a cache, intended for use in paged attention mechanisms. The function takes several tensor arguments including `k_states`, `v_states`, `k_caches`, `v_caches`, `q_start_loc`, `q_seq_length`, `kv_seq_length`, and `block_offsets`, as well as optional `k_scales_zeros` and `v_scales_zeros`. The `quant_policy` determines the quantization method: 0 for no quantization, 4 for int4 quantization, and 8 for int8 quantization. The operator utilizes two Triton JIT-decorated functions: `_fill_kv_cache_kernel` and `_fill_kv_cache_quant_kernel`. The former is used when no quantization is applied, while the latter applies either int4 or int8 quantization depending on `quant_policy`. Both kernels are executed over a grid, organized by `batch_size` and `max_num_blocks`, and employ pre-computed strides for efficient memory access.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fill_kv_cache_kernel(\n    k_states_ptr, v_states_ptr, k_caches_ptr, v_caches_ptr,\n    q_start_loc, q_seq_length, kv_seq_length, block_offsets,\n    BLOCK_SIZE: tl.constexpr\n):\n    batch_id = tl.program_id(0)\n    block_id = tl.program_id(1)\n\n    # Calculate the start index in the cache\n    start_idx = q_start_loc[batch_id] + block_offsets[batch_id, block_id] * BLOCK_SIZE\n\n    # Load data from k_states and v_states\n    k_data = tl.load(k_states_ptr + start_idx)\n    v_data = tl.load(v_states_ptr + start_idx)\n\n    # Store data into k_caches and v_caches\n    tl.store(k_caches_ptr + start_idx, k_data)\n    tl.store(v_caches_ptr + start_idx, v_data)\n\n@triton.jit\ndef _fill_kv_cache_quant_kernel(\n    k_states_ptr, v_states_ptr, k_caches_ptr, v_caches_ptr,\n    q_start_loc, q_seq_length, kv_seq_length, block_offsets,\n    k_scales_zeros, v_scales_zeros, quant_policy: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    batch_id = tl.program_id(0)\n    block_id = tl.program_id(1)\n\n    # Calculate the start index in the cache\n    start_idx = q_start_loc[batch_id] + block_offsets[batch_id, block_id] * BLOCK_SIZE\n\n    # Load data from k_states and v_states\n    k_data = tl.load(k_states_ptr + start_idx)\n    v_data = tl.load(v_states_ptr + start_idx)\n\n    # Quantize data based on the quant_policy\n    if quant_policy == 4:\n        # Example for int4 quantization\n        k_data = (k_data - k_scales_zeros[0]) * k_scales_zeros[1]\n        v_data = (v_data - v_scales_zeros[0]) * v_scales_zeros[1]\n    elif quant_policy == 8:\n        # Example for int8 quantization\n        k_data = (k_data - k_scales_zeros[0]) * k_scales_zeros[1]\n        v_data = (v_data - v_scales_zeros[0]) * v_scales_zeros[1]\n\n    # Store data into k_caches and v_caches\n    tl.store(k_caches_ptr + start_idx, k_data)\n    tl.store(v_caches_ptr + start_idx, v_data)\n\ndef fill_kv_cache(\n    k_states, v_states, k_caches, v_caches,\n    q_start_loc, q_seq_length, kv_seq_length, block_offsets,\n    k_scales_zeros=None, v_scales_zeros=None, quant_policy=0\n):\n    batch_size, max_num_blocks = k_states.shape[0], block_offsets.shape[1]\n    BLOCK_SIZE = 128  # Define a suitable block size\n\n    grid = (batch_size, max_num_blocks)\n\n    if quant_policy == 0:\n        _fill_kv_cache_kernel[grid](\n            k_states, v_states, k_caches, v_caches,\n            q_start_loc, q_seq_length, kv_seq_length, block_offsets,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n    else:\n        _fill_kv_cache_quant_kernel[grid](\n            k_states, v_states, k_caches, v_caches,\n            q_start_loc, q_seq_length, kv_seq_length, block_offsets,\n            k_scales_zeros, v_scales_zeros, quant_policy=quant_policy,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n", "label": "from typing import Literal\nimport torch\nimport triton\nimport triton.language as tl\n\n\nKERNEL_META = dict()\n\ndef get_kernel_meta(tensor: torch.Tensor):\n    \"\"\"kernel meta.\"\"\"\n    return KERNEL_META\n    \n@triton.jit\ndef _div_up(val, other):\n    return (val + other - 1) // other\n\n\n@triton.jit\ndef _quant_int8(val):\n    val_min = tl.min(val, 1)\n    val_max = tl.max(val, 1)\n    scales = (val_max - val_min) / 255\n    zeros = -val_min / scales\n    q_val = (val / scales[:, None] + zeros[:, None] + 0.5).to(tl.uint8)\n    return q_val, scales, zeros\n\n\n@triton.jit\ndef _quant_int4(val1, val2):\n    val1 = val1.to(tl.float32)\n    val2 = val2.to(tl.float32)\n    val_min = tl.min(tl.minimum(val1, val2), 1)\n    val_max = tl.max(tl.maximum(val1, val2), 1)\n    scales = (val_max - val_min) / 15\n    zeros = -val_min / scales\n    q_val1 = (val1 / scales[:, None] + zeros[:, None] + 0.5).to(tl.uint8)\n    q_val2 = (val2 / scales[:, None] + zeros[:, None] + 0.5).to(tl.uint8)\n    q_val = q_val1 + q_val2 * 16\n    return q_val, scales, zeros\n\n\n@triton.jit\ndef _fill_kv_cache_kernel(\n    KStates,\n    VStates,\n    KCaches,\n    VCaches,\n    QStartLoc,\n    QSeqLens,\n    KVSeqLens,\n    BlockOffsets,\n    num_heads: tl.constexpr,\n    head_dim: tl.constexpr,\n    head_dim_v: tl.constexpr,\n    stride_kss,\n    stride_ksh,\n    stride_ksd,\n    stride_vss,\n    stride_vsh,\n    stride_vsd,\n    stride_kcn: tl.constexpr,\n    stride_kcb: tl.constexpr,\n    stride_kch: tl.constexpr,\n    stride_kcd: tl.constexpr,\n    stride_vcn: tl.constexpr,\n    stride_vcb: tl.constexpr,\n    stride_vch: tl.constexpr,\n    stride_vcd: tl.constexpr,\n    stride_boff,\n    BLOCK: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_DV: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    \"\"\"fill kv cache kernel.\"\"\"\n    batch_id = tl.program_id(0)\n    block_id = tl.program_id(1)\n\n    # initialize\n    h_off = tl.arange(0, BLOCK_H)\n    d_off = tl.arange(0, BLOCK_D)\n\n    q_startloc = tl.load(QStartLoc + batch_id)\n    q_seqlen = tl.load(QSeqLens + batch_id)\n    kv_seqlen = tl.load(KVSeqLens + batch_id)\n    history_seqlen = kv_seqlen - q_seqlen\n\n    block0_first_tokenloc = history_seqlen % BLOCK\n\n    state_token_offset = tl.maximum(block_id * BLOCK - block0_first_tokenloc,\n                                    0)\n    kv_block_id = _div_up(history_seqlen + 1, BLOCK) - 1 + block_id\n    kv_block_id = min(kv_block_id, stride_boff - 1)\n    block_off = tl.load(BlockOffsets + batch_id * stride_boff + kv_block_id)\n\n    cur_startloc = q_startloc + state_token_offset\n    ks_ptr = KStates + cur_startloc * stride_kss\n    vs_ptr = VStates + cur_startloc * stride_vss\n\n    kc_ptr = KCaches + block_off * stride_kcn\n    vc_ptr = VCaches + block_off * stride_vcn\n\n    c_first_tokenloc = block0_first_tokenloc\n    if block_id != 0:\n        c_first_tokenloc *= 0\n    c_last_tokenloc = tl.minimum(\n        BLOCK, q_seqlen + block0_first_tokenloc - block_id * BLOCK)\n\n    for bidx in range(c_first_tokenloc, c_last_tokenloc):\n        sidx = bidx - c_first_tokenloc\n        mask = (h_off[:, None] < num_heads) & (d_off[None, :] < head_dim)\n        k = tl.load(ks_ptr + sidx * stride_kss + h_off[:, None] * stride_ksh +\n                    d_off[None, :] * stride_ksd,\n                    mask=mask)\n        tl.store(kc_ptr + bidx * stride_kcb + h_off[:, None] * stride_kch +\n                 d_off[None, :] * stride_kcd,\n                 k,\n                 mask=mask)\n\n        if BLOCK_DV > 0:\n            dv_off = tl.arange(0, BLOCK_DV)\n            maskv = (h_off[:, None] < num_heads) & (dv_off[None, :] <\n                                                    head_dim_v)\n            v = tl.load(vs_ptr + sidx * stride_vss +\n                        h_off[:, None] * stride_vsh +\n                        dv_off[None, :] * stride_vsd,\n                        mask=maskv)\n            tl.store(vc_ptr + bidx * stride_vcb + h_off[:, None] * stride_vch +\n                     dv_off[None, :] * stride_vcd,\n                     v,\n                     mask=maskv)\n\n\n@triton.jit\ndef _fill_kv_cache_quant_kernel(\n    KStates,\n    VStates,\n    KCaches,\n    VCaches,\n    KScalesZeros,\n    VScalesZeros,\n    QStartLoc,\n    QSeqLens,\n    KVSeqLens,\n    BlockOffsets,\n    num_heads: tl.constexpr,\n    head_dim: tl.constexpr,\n    head_dim_v: tl.constexpr,\n    stride_kss,\n    stride_ksh,\n    stride_ksd,\n    stride_vss,\n    stride_vsh,\n    stride_vsd,\n    stride_kcn: tl.constexpr,\n    stride_kcb: tl.constexpr,\n    stride_kch: tl.constexpr,\n    stride_kcd: tl.constexpr,\n    stride_vcn: tl.constexpr,\n    stride_vcb: tl.constexpr,\n    stride_vch: tl.constexpr,\n    stride_vcd: tl.constexpr,\n    stride_kszn: tl.constexpr,\n    stride_kszb: tl.constexpr,\n    stride_kszh: tl.constexpr,\n    stride_kszd: tl.constexpr,\n    stride_vszn: tl.constexpr,\n    stride_vszb: tl.constexpr,\n    stride_vszh: tl.constexpr,\n    stride_vszd: tl.constexpr,\n    quant_policy: tl.constexpr,\n    stride_boff,\n    BLOCK: tl.constexpr,\n    BLOCK_D: tl.constexpr,\n    BLOCK_DV: tl.constexpr,\n    BLOCK_H: tl.constexpr,\n):\n    \"\"\"fill kv cache kernel with int4 and int8 quant fused.\n\n    Args:\n        stride_xss: stride of sequence length dim of key or value states\n        stride_xsh: stride of head_num dim of key or value states\n        stride_xsh: stride of head_size dim of key or value states\n        stride_xn: stride of page num dim\n        stride_xb: stride of block size dim\n        stride_xh: stride of head_num dim\n        stride_xd: stride of head_size dim\n    \"\"\"\n    batch_id = tl.program_id(0)\n    block_id = tl.program_id(1)\n    d_off = tl.arange(0, BLOCK_D)\n\n    # initialize\n    h_off = tl.arange(0, BLOCK_H)\n    szd_off = tl.arange(0, 2)\n\n    q_startloc = tl.load(QStartLoc + batch_id)\n    q_seqlen = tl.load(QSeqLens + batch_id)\n    kv_seqlen = tl.load(KVSeqLens + batch_id)\n    history_seqlen = kv_seqlen - q_seqlen\n\n    block0_first_tokenloc = history_seqlen % BLOCK\n\n    state_token_offset = tl.maximum(block_id * BLOCK - block0_first_tokenloc,\n                                    0)\n    kv_block_id = _div_up(history_seqlen + 1, BLOCK) - 1 + block_id\n    kv_block_id = min(kv_block_id, stride_boff - 1)\n    block_off = tl.load(BlockOffsets + batch_id * stride_boff + kv_block_id)\n\n    cur_startloc = q_startloc + state_token_offset\n    ks_ptr = KStates + cur_startloc * stride_kss\n    vs_ptr = VStates + cur_startloc * stride_vss\n\n    kc_ptr = KCaches + block_off * stride_kcn\n    vc_ptr = VCaches + block_off * stride_vcn\n\n    ksz_ptr = KScalesZeros + block_off * stride_kszn\n    vsz_ptr = VScalesZeros + block_off * stride_vszn\n\n    c_first_tokenloc = block0_first_tokenloc\n    if block_id != 0:\n        c_first_tokenloc *= 0\n    c_last_tokenloc = tl.minimum(\n        BLOCK, q_seqlen + block0_first_tokenloc - block_id * BLOCK)\n\n    for bidx in range(c_first_tokenloc, c_last_tokenloc):\n        sidx = bidx - c_first_tokenloc\n        mask = (h_off[:, None] < num_heads) & (d_off[None, :] < head_dim)\n        if quant_policy == 4:\n            k1 = tl.load(ks_ptr + sidx * stride_kss +\n                         h_off[:, None] * stride_ksh +\n                         d_off[None, :] * stride_ksd,\n                         mask=mask)\n            k2 = tl.load(ks_ptr + sidx * stride_kss +\n                         h_off[:, None] * stride_ksh +\n                         d_off[None, :] * stride_ksd + head_dim * stride_ksd,\n                         mask=mask)\n            q_k, k_scales, k_zeros = _quant_int4(k1, k2)\n        else:\n            k = tl.load(ks_ptr + sidx * stride_kss +\n                        h_off[:, None] * stride_ksh +\n                        d_off[None, :] * stride_ksd,\n                        mask=mask)\n            q_k, k_scales, k_zeros = _quant_int8(k)\n        tl.store(kc_ptr + bidx * stride_kcb + h_off[:, None] * stride_kch +\n                 d_off[None, :] * stride_kcd,\n                 q_k,\n                 mask=mask)\n        tl.store(ksz_ptr + bidx * stride_kszb + h_off[:, None] * stride_kszh +\n                 szd_off[None, :] * stride_kszd,\n                 k_scales[:, None],\n                 mask=(h_off[:, None] < num_heads) & (szd_off[None, :] < 1))\n        tl.store(ksz_ptr + bidx * stride_kszb + h_off[:, None] * stride_kszh +\n                 szd_off[None, :] * stride_kszd,\n                 k_zeros[:, None],\n                 mask=(h_off[:, None] < num_heads) & (szd_off[None, :] == 1))\n\n        if BLOCK_DV > 0:\n            if quant_policy == 4:\n                dv_off = tl.arange(0, BLOCK_DV //\n                                   2)  # int4 pack, half the head_dim\n                maskv = (h_off[:, None] < num_heads) & (dv_off[None, :] <\n                                                        head_dim_v // 2)\n                v1 = tl.load(vs_ptr + sidx * stride_vss +\n                             h_off[:, None] * stride_vsh +\n                             dv_off[None, :] * stride_vsd,\n                             mask=maskv)\n                v2 = tl.load(vs_ptr + sidx * stride_vss +\n                             h_off[:, None] * stride_vsh +\n                             dv_off[None, :] * stride_vsd +\n                             head_dim_v // 2 * stride_vsd,\n                             mask=maskv)\n                q_v, v_scales, v_zeros = _quant_int4(v1, v2)\n            else:\n                dv_off = tl.arange(0, BLOCK_DV)\n                maskv = (h_off[:, None] < num_heads) & (dv_off[None, :] <\n                                                        head_dim_v)\n                v = tl.load(vs_ptr + sidx * stride_vss +\n                            h_off[:, None] * stride_vsh +\n                            dv_off[None, :] * stride_vsd,\n                            mask=maskv)\n                q_v, v_scales, v_zeros = _quant_int8(v)\n            tl.store(vc_ptr + bidx * stride_vcb + h_off[:, None] * stride_vch +\n                     dv_off[None, :] * stride_vcd,\n                     q_v,\n                     mask=maskv)\n            tl.store(\n                vsz_ptr + bidx * stride_vszb + h_off[:, None] * stride_vszh +\n                szd_off[None, :] * stride_vszd,\n                v_scales[:, None],\n                mask=(h_off[:, None] < num_heads) & (szd_off[None, :] < 1))\n            tl.store(\n                vsz_ptr + bidx * stride_vszb + h_off[:, None] * stride_vszh +\n                szd_off[None, :] * stride_vszd,\n                v_zeros[:, None],\n                mask=(h_off[:, None] < num_heads) & (szd_off[None, :] == 1))\n\n\ndef fill_kv_cache(k_states: torch.Tensor,\n                  v_states: torch.Tensor,\n                  k_caches: torch.Tensor,\n                  v_caches: torch.Tensor,\n                  q_start_loc: torch.Tensor,\n                  q_seq_length: torch.Tensor,\n                  kv_seq_length: torch.Tensor,\n                  max_q_seq_length: int,\n                  block_offsets: torch.Tensor,\n                  k_scales_zeros: torch.Tensor = None,\n                  v_scales_zeros: torch.Tensor = None,\n                  quant_policy: Literal[0, 4, 8] = 0):\n    \"\"\"fill key/value state to cache for paged attention.\"\"\"\n\n    block_offsets = block_offsets.contiguous()\n    batch_size = block_offsets.size(0)\n    block_size, num_heads, head_dim = k_caches.size()[1:]\n    head_dim_v = v_states.size(-1)\n    max_num_blocks = triton.cdiv(max_q_seq_length, block_size) + 1\n\n    BLOCK = block_size\n    BLOCK_H = triton.next_power_of_2(num_heads)\n    BLOCK_D = triton.next_power_of_2(head_dim)\n    BLOCK_DV = triton.next_power_of_2(head_dim_v)\n    grid = [batch_size, max_num_blocks]\n    kernel_meta = get_kernel_meta(k_states)\n    if quant_policy == 0:\n        _fill_kv_cache_kernel[grid](\n            k_states,\n            v_states,\n            k_caches,\n            v_caches,\n            q_start_loc,\n            q_seq_length,\n            kv_seq_length,\n            block_offsets,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            head_dim_v=head_dim_v,\n            stride_kss=k_states.stride(-3),\n            stride_ksh=k_states.stride(-2),\n            stride_ksd=k_states.stride(-1),\n            stride_vss=v_states.stride(-3),\n            stride_vsh=v_states.stride(-2),\n            stride_vsd=v_states.stride(-1),\n            stride_kcn=k_caches.stride(0),\n            stride_kcb=k_caches.stride(1),\n            stride_kch=k_caches.stride(2),\n            stride_kcd=k_caches.stride(3),\n            stride_vcn=v_caches.stride(0),\n            stride_vcb=v_caches.stride(1),\n            stride_vch=v_caches.stride(2),\n            stride_vcd=v_caches.stride(3),\n            stride_boff=block_offsets.stride(0),\n            BLOCK=BLOCK,\n            BLOCK_D=BLOCK_D,\n            BLOCK_DV=BLOCK_DV,\n            BLOCK_H=BLOCK_H,\n            num_warps=4,\n            num_stages=3,\n            **kernel_meta,\n        )\n    else:\n        _fill_kv_cache_quant_kernel[grid](\n            k_states,\n            v_states,\n            k_caches,\n            v_caches,\n            k_scales_zeros,\n            v_scales_zeros,\n            q_start_loc,\n            q_seq_length,\n            kv_seq_length,\n            block_offsets,\n            num_heads=num_heads,\n            head_dim=head_dim,\n            head_dim_v=head_dim_v,\n            stride_kss=k_states.stride(-3),\n            stride_ksh=k_states.stride(-2),\n            stride_ksd=k_states.stride(-1),\n            stride_vss=v_states.stride(-3),\n            stride_vsh=v_states.stride(-2),\n            stride_vsd=v_states.stride(-1),\n            stride_kcn=k_caches.stride(0),\n            stride_kcb=k_caches.stride(1),\n            stride_kch=k_caches.stride(2),\n            stride_kcd=k_caches.stride(3),\n            stride_vcn=v_caches.stride(0),\n            stride_vcb=v_caches.stride(1),\n            stride_vch=v_caches.stride(2),\n            stride_vcd=v_caches.stride(3),\n            stride_kszn=k_scales_zeros.stride(0),\n            stride_kszb=k_scales_zeros.stride(1),\n            stride_kszh=k_scales_zeros.stride(2),\n            stride_kszd=k_scales_zeros.stride(3),\n            stride_vszn=v_scales_zeros.stride(0),\n            stride_vszb=v_scales_zeros.stride(1),\n            stride_vszh=v_scales_zeros.stride(2),\n            stride_vszd=v_scales_zeros.stride(3),\n            quant_policy=quant_policy,\n            stride_boff=block_offsets.stride(0),\n            BLOCK=BLOCK,\n            BLOCK_D=BLOCK_D,\n            BLOCK_DV=BLOCK_DV,\n            BLOCK_H=BLOCK_H,\n            num_warps=4,\n            num_stages=3,\n            **kernel_meta,\n        )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs the ReLU (Rectified Linear Unit) operation on an input tensor. The `relu_kernel` function is the main computation unit. It reads elements from the input tensor, applies the ReLU operation, and writes the result back to the output tensor. The input tensor is pointed to by `x_ptr`, and the output tensor is pointed to by `out_ptr`. The kernel operates in parallel across multiple blocks, with each block processing a segment of the tensor. The block size is set to 1024 elements. The ReLU operation sets all negative elements to zero, keeping positive elements unchanged. \n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef relu_kernel(x_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Program ID determines which block of data this kernel is responsible for\n    pid = tl.program_id(0)\n    \n    # Calculate the starting index for this block\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load data from the input tensor\n    x = tl.load(x_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    \n    # Apply ReLU operation\n    relu_x = tl.where(x > 0, x, 0.0)\n    \n    # Store the result in the output tensor\n    tl.store(out_ptr + offsets, relu_x, mask=offsets < n_elements)\n\ndef relu(x):\n    # Number of elements in the input tensor\n    n_elements = x.numel()\n    \n    # Define block size\n    BLOCK_SIZE = 1024\n    \n    # Allocate output tensor\n    out = torch.empty_like(x)\n    \n    # Launch the kernel\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    relu_kernel[grid](x, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return out\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef relu_kernel(x_ptr, out_ptr, N: tl.constexpr, block_size: tl.constexpr):\n    # Get the index of the current thread\n    pid = tl.program_id(0)\n    block_start = pid * block_size\n    offsets = block_start + tl.arange(0, block_size)\n    mask = offsets < N\n\n    # Load elements from global memory\n    x = tl.load(x_ptr + offsets, mask=mask)\n\n    # Compute linear layer\n    result = tl.where(x >= 0, x, 0.0)\n\n    # Write result to global memory\n    if pid == 0:\n        tl.store(out_ptr + offsets, result, mask=mask)\n\ndef relu(x):\n    # Prepare output tensor\n    out = torch.empty_like(x, dtype=torch.float32, device=x.device)\n    N = out.numel()\n\n    BLOCK_SIZE = 1024\n    num_blocks = (N + BLOCK_SIZE - 1) // BLOCK_SIZE  # Calculate the number of blocks needed\n\n    # Launch Triton kernel\n    grid = lambda meta: (triton.cdiv(N, BLOCK_SIZE), )\n\n    relu_kernel[grid](x, out, N, BLOCK_SIZE)\n\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code defines a series of GPU kernels designed to perform maximum value calculations on input tensors. It includes functions `max`, `max_dim`, and three Triton kernels: `max_kernel_1`, `max_kernel_2`, and `max_kernel`. The main purpose of these kernels is to compute the maximum values and their indices across different dimensions of the input tensor using parallel processing capabilities of GPUs.\n            - `max_kernel_1` processes a large input tensor in blocks to compute the maximum values for each block. It loads segments of the input, applies a mask for boundary handling, computes the max, and stores intermediate results in a mid buffer.\n            - `max_kernel_2` takes the results from `max_kernel_1` to compute the final maximum value for the entire tensor.\n            - `max_kernel` is used in `max_dim` to find maximum values along a specified dimension. It uses 2D grid dimensions to handle multi-dimensional data, computing the max value and its index along the specified axis.\n            - The `max` function acts as a wrapper to execute `max_kernel_1` and `max_kernel_2` sequentially.\n            - The `max_dim` function is an extension that allows finding max values along a specified dimension and returning both the values and their indices.\n            The kernels leverage Triton's just-in-time compilation to optimize for different hardware configurations.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Kernel 1: Compute block-wise maximum values\n@triton.jit\ndef max_kernel_1(X, mid, BLOCK_SIZE: tl.constexpr, N: tl.constexpr):\n    pid = tl.program_id(0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < N\n    x = tl.load(X + offset, mask=mask, other=-float('inf'))\n    max_val = tl.max(x, axis=0)\n    tl.store(mid + pid, max_val)\n\n# Kernel 2: Compute final maximum value from block-wise results\n@triton.jit\ndef max_kernel_2(mid, result, N: tl.constexpr):\n    x = tl.load(mid + tl.arange(0, N))\n    max_val = tl.max(x, axis=0)\n    tl.store(result, max_val)\n\n# Kernel for max along a specific dimension\n@triton.jit\ndef max_kernel(X, max_vals, max_indices, stride, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, M: tl.constexpr, N: tl.constexpr):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    offsets_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    mask_m = offsets_m < M\n    mask_n = offsets_n < N\n    x = tl.load(X + offsets_m[:, None] * stride + offsets_n[None, :], mask=mask_m[:, None] & mask_n[None, :], other=-float('inf'))\n    max_val = tl.max(x, axis=1)\n    max_idx = tl.argmax(x, axis=1)\n    tl.store(max_vals + offsets_m, max_val, mask=mask_m)\n    tl.store(max_indices + offsets_m, max_idx, mask=mask_m)\n\n# Wrapper for computing max over entire tensor\ndef max(X, BLOCK_SIZE=1024):\n    N = X.size\n    mid = triton.empty([N // BLOCK_SIZE], dtype=X.dtype)\n    result = triton.empty([1], dtype=X.dtype)\n    grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n    max_kernel_1[grid](X, mid, BLOCK_SIZE, N)\n    max_kernel_2[1](mid, result, grid)\n    return result[0]\n\n# Wrapper for computing max along a specified dimension\ndef max_dim(X, dim, BLOCK_SIZE_M=128, BLOCK_SIZE_N=128):\n    shape = X.shape\n    M, N = shape[dim], shape[1 - dim]\n    max_vals = triton.empty([M], dtype=X.dtype)\n    max_indices = triton.empty([M], dtype=tl.int32)\n    stride = shape[1] if dim == 0 else 1\n    grid = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M, (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N\n    max_kernel[grid](X, max_vals, max_indices, stride, BLOCK_SIZE_M, BLOCK_SIZE_N, M, N)\n    return max_vals, max_indices\n", "label": "import logging\nimport math\nfrom collections import namedtuple\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef max_kernel_1(\n    inp,\n    mid,\n    M,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    inp_ptrs = inp + offset\n    mask = offset < M\n    inp_val = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    max_val = tl.max(inp_val)\n    mid_ptr = mid + pid\n    tl.store(mid_ptr, max_val)\n\n\n@triton.jit\ndef max_kernel_2(mid, out, mid_size, BLOCK_MID: tl.constexpr):\n    offset = tl.arange(0, BLOCK_MID)\n    mid_ptrs = mid + offset\n    mask = offset < mid_size\n    mid_val = tl.load(mid_ptrs, mask=mask, other=-float(\"inf\"))\n    max_val = tl.max(mid_val)\n    tl.store(out, max_val)\n\n\ndef heur_block_n(args):\n    return triton.next_power_of_2(args[\"N\"])\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 8}, num_warps=8),\n        triton.Config({\"BLOCK_M\": 16}, num_warps=8),\n        triton.Config({\"BLOCK_M\": 32}, num_warps=8),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"BLOCK_N\": heur_block_n,\n    }\n)\n@triton.jit\ndef max_kernel(\n    inp,\n    out_value,\n    out_index,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    # set offset\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    offset_index = m_offset * K + pid_k\n    # set mask\n    mask1 = m_offset < M\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    inp_ptrs = inp + offset\n    inp_vals = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    result_value, result_index = tl.max(inp_vals, axis=1, return_indices=True)\n\n    out_value_ptrs = out_value + offset_index\n    out_index_ptrs = out_index + offset_index\n\n    tl.store(out_value_ptrs, result_value, mask=mask1)\n    tl.store(out_index_ptrs, result_index, mask=mask1)\n\n\ndef max(inp):\n    logging.debug(\"GEMS MAX\")\n    M = inp.numel()\n    block_size = triton.next_power_of_2(math.ceil(math.sqrt(M)))\n    mid_size = triton.cdiv(M, block_size)\n    block_mid = triton.next_power_of_2(mid_size)\n\n    dtype = inp.dtype\n    mid = torch.empty((mid_size,), dtype=dtype, device=inp.device)\n    out = torch.empty([], dtype=dtype, device=inp.device)\n\n    with torch.cuda.device(inp.device):\n        max_kernel_1[(mid_size, 1, 1)](inp, mid, M, block_size)\n        max_kernel_2[(1, 1, 1)](mid, out, mid_size, block_mid)\n    return out\n\n\ndef max_dim(inp, dim=None, keepdim=False):\n    logging.debug(\"GEMS MAX DIM\")\n    assert dim >= -inp.ndim and dim < inp.ndim, \"Invalid dim\"\n    shape = inp.shape\n    dim = dim % inp.ndim\n    N = shape[dim]\n    M = math.prod(shape[:dim])\n    K = inp.numel() // M // N\n\n    inp = inp.contiguous()\n\n    shape_list = list(shape)\n    shape_list[dim] = 1\n    out_value = torch.empty(shape_list, dtype=inp.dtype, device=inp.device)\n    out_index = torch.empty(shape_list, dtype=torch.int64, device=inp.device)\n\n    if not keepdim:\n        out_value = torch.squeeze(out_value, dim)\n        out_index = torch.squeeze(out_index, dim)\n\n    grid = lambda meta: (\n        triton.cdiv(M, meta[\"BLOCK_M\"]),\n        K,\n    )\n    with torch.cuda.device(inp.device):\n        max_kernel[grid](inp, out_value, out_index, M, N, K)\n    Max_out = namedtuple(\"max\", [\"values\", \"indices\"])\n    out = Max_out(values=out_value, indices=out_index)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implements a matrix multiplication operator with optimizations for memory hierarchy and parallel execution. The main functions are `swizzle_tile`, `linear_tile`, `mac_loop`, `first_wave`, and `full_tiles`.\n\n            - `swizzle_tile` and `linear_tile` calculate tile coordinates (pid_m, pid_n) based on the tile ID. Swizzling helps improve cache performance.\n            - `mac_loop` is the core computation loop, performing matrix multiplication for a range of iterations and writing results to the output matrix C.\n            - `first_wave` and `full_tiles` manage the scheduling and execution of the computation across available tiles, using a two-wave strategy for better resource utilization.\n            - The `matmul` class provides a high-level interface to execute the kernel and manage parameters like block sizes and the number of stages/warps.\n\n            Inputs and outputs:\n            - Inputs include matrices A, B, output matrix C, dimensions M, N, K, stride information for accessing matrices, and kernel parameters like block sizes and warp/stage numbers.\n            - Output is the product matrix C.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(A_ptr, B_ptr, C_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M, BLOCK_N, BLOCK_K, num_warps, num_stages):\n    pid = tl.program_id(axis=0)\n    \n    # Swizzle and linear tile computation\n    def swizzle_tile(pid, num_tiles):\n        pid_m = pid // num_tiles\n        pid_n = pid % num_tiles\n        return pid_m, pid_n\n    \n    def linear_tile(pid, num_tiles):\n        pid_m = pid // num_tiles\n        pid_n = pid % num_tiles\n        return pid_m, pid_n\n    \n    # Compute tile coordinates\n    num_tiles_m = (M + BLOCK_M - 1) // BLOCK_M\n    num_tiles_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m, pid_n = swizzle_tile(pid, num_tiles_n)\n    \n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    \n    # Main computation loop\n    def mac_loop(start_k, end_k):\n        for k in range(start_k, end_k, BLOCK_K):\n            a = tl.load(A_ptr + (pid_m * BLOCK_M) * stride_am + k * stride_ak)\n            b = tl.load(B_ptr + k * stride_bk + (pid_n * BLOCK_N) * stride_bn)\n            acc += tl.dot(a, b)\n    \n    # First wave: Handle initial set of tiles\n    def first_wave():\n        mac_loop(0, K)\n    \n    # Full tiles: Process remaining tiles\n    def full_tiles():\n        mac_loop(0, K)\n    \n    # Execute computation\n    first_wave()\n    full_tiles()\n    \n    # Write results to output matrix C\n    tl.store(C_ptr + (pid_m * BLOCK_M) * stride_cm + (pid_n * BLOCK_N) * stride_cn, acc)\n\nclass matmul:\n    def __init__(self, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, num_warps=4, num_stages=2):\n        self.BLOCK_M = BLOCK_M\n        self.BLOCK_N = BLOCK_N\n        self.BLOCK_K = BLOCK_K\n        self.num_warps = num_warps\n        self.num_stages = num_stages\n    \n    def __call__(self, A, B, C, M, N, K):\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n        matmul_kernel[grid](\n            A, B, C, M, N, K,\n            A.stride(0), A.stride(1),\n            B.stride(0), B.stride(1),\n            C.stride(0), C.stride(1),\n            BLOCK_M=self.BLOCK_M, BLOCK_N=self.BLOCK_N, BLOCK_K=self.BLOCK_K,\n            num_warps=self.num_warps, num_stages=self.num_stages\n        )\n\n# Example usage:\n# A, B, and C are Triton tensors with appropriate strides and dimensions.\n# M, N, K are the dimensions of the matrices for multiplication.\n# matmul_op = matmul()\n# matmul_op(A, B, C, M, N, K)\n", "label": "import torch\nimport triton\nfrom triton import language as tl\n\n@triton.jit()\ndef swizzle_tile(tile_id,\n                 M, N, K,\n                 BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                 GROUP_M: tl.constexpr\n                 ):\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_id = tile_id // width\n    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (tile_id % group_size)\n    pid_n = (tile_id % width) // group_size\n    return pid_m, pid_n\n\n\n@triton.jit()\ndef linear_tile(tile_id,\n                M, N, K,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                GROUP_M: tl.constexpr\n                ):\n    pid_m = tile_id // tl.cdiv(N, BLOCK_N)\n    pid_n = tile_id % tl.cdiv(N, BLOCK_N)\n    return pid_m, pid_n\n\n\n@triton.jit()\ndef mac_loop(A, B, C,\n             M, N, K,\n             locks,\n             stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n             iters_per_tile,\n             start_iter, end_iter,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr):\n\n    # where are we in the grid\n    tile_id = start_iter // iters_per_tile\n    if GROUP_M  > 0:\n        pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n    else:\n        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak) + BLOCK_K * stride_ak * (start_iter % iters_per_tile)\n    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn) + BLOCK_K * stride_bk * (start_iter % iters_per_tile)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n\n    for current_iter in range(start_iter, end_iter):\n        a = tl.load(A)\n        b = tl.load(B)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n\n    if end_iter % iters_per_tile == 0:  # last iteration of the tile always happens before its start on another SM\n        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)  # compute inside the if/else to avoid spilling!\n        tl.store(C_, acc)\n        if start_iter % iters_per_tile != 0:  # only if tile has been partially processed\n            tl.atomic_xchg(locks + tile_id, 1)\n    else:\n        while tl.atomic_cas(locks + tile_id, 1, 1) != 1:\n            pass\n        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)  # compute inside the if/else to avoid spilling!\n        tl.atomic_add(C_, acc)\n\n\n@triton.jit()\ndef first_wave(\n        A, B, C,\n        M, N, K,\n        locks,\n        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n        total_full_tiles_streamk, total_partial_tiles_streamk, iters_per_tile,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr,\n        GROUP_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    start_iter = pid * total_full_tiles_streamk + tl.minimum(pid, total_partial_tiles_streamk)\n    last_iter = (pid + 1) * total_full_tiles_streamk + tl.minimum(pid + 1, total_partial_tiles_streamk)\n\n    while start_iter < last_iter:\n        end_iter = tl.minimum(start_iter + (iters_per_tile - start_iter % iters_per_tile), last_iter)\n        mac_loop(A, B, C,\n                 M, N, K,\n                 locks,\n                 stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n                 iters_per_tile,\n                 start_iter, end_iter,\n                 BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE,\n                 GROUP_M,\n                 )\n\n        start_iter = end_iter\n\n\n@triton.jit()\ndef full_tiles(\n        A, B, C,\n        M, N, K,\n        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n        total_tiles_streamk,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr,\n        GROUP_M: tl.constexpr,\n):\n    # first wave has done more tiles than there are SMs, we adjust pid\n    tile_id = tl.program_id(0) + total_tiles_streamk\n    if GROUP_M > 0:\n        pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n    else:\n        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n\n    # do matrix multiplication\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    # pointers\n    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        a = tl.load(A)\n        b = tl.load(B)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    acc = acc.to(tl.float16)  # restore C.dtype.element_ty\n    # rematerialize rm and rn to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    tl.store(C, acc)\n\n\nclass matmul(torch.autograd.Function):\n\n    _debug = False\n\n    @staticmethod\n    def set_debug(debug: bool):\n        matmul._debug = debug\n\n    @staticmethod\n    def _call(a: torch.Tensor, b: torch.Tensor, total_programs_streamk: int, BLK_M: int, BLK_N: int, BLK_K: int, two_tiles: bool, num_stages: int, num_warps: int):\n        device = a.device\n\n        assert a.is_contiguous() and b.is_contiguous(), \"non-contiguous inputs are not supported\"\n        # checks constraints\n        assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n        M, K = a.shape\n        _, N = b.shape\n        # accumulator types\n        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n        # compute grid (work to do per SM on the first wave)\n        total_blocks_M = triton.cdiv(M, BLK_M)\n        total_blocks_N = triton.cdiv(N, BLK_N)\n        iters_per_tile = triton.cdiv(K, BLK_K)\n        GROUP_M = 8  # 0 to disable swizzling\n        total_tiles = total_blocks_M * total_blocks_N\n\n        if total_programs_streamk > 0:  # Stream-K\n            # last wave may occupy less than total_programs_streamk SMs\n            total_tiles_streamk = total_tiles % total_programs_streamk\n            # for two-tile Stream-K + data-parallel from original paper\n            if two_tiles and total_tiles - total_tiles_streamk > total_programs_streamk:\n                total_tiles_streamk += total_programs_streamk\n            # remaining tiles are computed using classical blocking\n            total_blocking_tiles = total_tiles - total_tiles_streamk\n            total_iters_streamk = total_tiles_streamk * iters_per_tile\n            # iterations related to full waves\n            total_full_tiles_streamk = total_iters_streamk // total_programs_streamk\n            # iterations related to last (partial) wave\n            total_partial_tiles_streamk = total_iters_streamk % total_programs_streamk\n\n        else:  # all tiles are computed using classical blocking\n            total_blocking_tiles = total_tiles\n            total_tiles_streamk = 0\n            total_full_tiles_streamk = 0\n            total_partial_tiles_streamk = 0\n            total_iters_streamk = 0\n\n        if matmul._debug:\n            print(f\"M,N,K={M},{N},{K} ; BLK_M,N,K={BLK_M},{BLK_N},{BLK_K}\")\n            print(f\"{total_blocks_M=} x {total_blocks_N=} = {total_tiles=}\")\n            print(f\"{total_tiles_streamk=} + {total_blocking_tiles=} = {total_tiles=}\")\n            print(f\"{total_programs_streamk=}\")\n            print(f\"{total_blocking_tiles=}\")\n            print(f\"{iters_per_tile=}\")\n            print(f\"{total_iters_streamk=}\")\n\n        # allocates output\n        c = torch.empty((M, N), device=device, dtype=a.dtype)\n        # allocates locks to sync work accross SMs\n        locks = torch.zeros((total_tiles_streamk,), device=device, dtype=torch.int32)\n        k1 = first_wave[(total_programs_streamk,)](\n            a,\n            b,\n            c,\n            M,\n            N,\n            K,\n            locks,\n            a.stride(0),\n            a.stride(1),\n            b.stride(0),\n            b.stride(1),\n            c.stride(0),\n            c.stride(1),\n            total_full_tiles_streamk=total_full_tiles_streamk,\n            total_partial_tiles_streamk=total_partial_tiles_streamk,\n            iters_per_tile=iters_per_tile,\n            BLOCK_M=BLK_M,\n            BLOCK_N=BLK_N,\n            BLOCK_K=BLK_K,\n            ACC_TYPE=ACC_TYPE,\n            GROUP_M=GROUP_M,\n            num_stages=num_stages,\n            num_warps=num_warps,\n        )\n        if matmul._debug:\n            print(f\"{k1.n_regs} registers used, {k1.n_spills} spills\")\n        k2 = full_tiles[(total_blocking_tiles,)](\n            a,\n            b,\n            c,\n            M,\n            N,\n            K,\n            a.stride(0),\n            a.stride(1),\n            b.stride(0),\n            b.stride(1),\n            c.stride(0),\n            c.stride(1),\n            total_tiles_streamk=total_tiles_streamk,\n            BLOCK_M=BLK_M,\n            BLOCK_N=BLK_N,\n            BLOCK_K=BLK_K,\n            ACC_TYPE=ACC_TYPE,\n            GROUP_M=GROUP_M,\n            num_stages=num_stages,\n            num_warps=num_warps,\n        )\n        if matmul._debug:\n            print(f\"{k2.n_regs} registers used, {k2.n_spills} spills\")\n        return c\n\n    @staticmethod\n    def forward(ctx, a: torch.Tensor, b: torch.Tensor, grid: int, BLK_M=128, BLK_N=128, BLK_K=32, two_tiles=True, num_stages=3, num_warps=4):\n        return matmul._call(a=a, b=b, total_programs_streamk=grid, BLK_M=BLK_M, BLK_N=BLK_N, BLK_K=BLK_K, two_tiles=two_tiles, num_warps=num_warps, num_stages=num_stages)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel is designed for recurrent computations, specifically with chunked gating mechanisms. It contains both a forward pass function `_fwd_recurrence` and a backward pass function `_bwd_recurrence`, as well as a wrapper class `ChunkGateRecurrent`.\n\n        `_fwd_recurrence`: \n        - Inputs: `S`, `d`, `O`, `NUM_HEAD`, `NUM_BLOCK`, `D_MODEL_K`, `D_MODEL_V`, `BLOCK_MODEL_K`, `BLOCK_MODEL_V`, `last_kv`\n        - Outputs: Updated `O`\n        - Logic: Computes recurrent transformations for chunks of key/value pairs, optionally using the last key/value (`last_kv`). It iteratively applies transformations across `NUM_BLOCK` blocks using decay coefficients from `d`.\n\n        `_bwd_recurrence`: \n        - Inputs: `S`, `d`, `DI`, `DG`, `DL`, `DS`, `NUM_HEAD`, `NUM_BLOCK`, `D_MODEL_K`, `D_MODEL_V`, `BLOCK_MODEL_K`, `BLOCK_MODEL_V`\n        - Outputs: Updated `DI`, `DG`, and `DL`\n        - Logic: Performs the backward pass of the recurrent transformation. Computes gradients with respect to inputs by reversing the recurrent operation and accumulating gradients.\n\n        `ChunkGateRecurrent`: \n        - Wraps the forward and backward functions, managing input/output tensors and executing them on GPU.\n        - Methods: `forward` sets up input tensors and launches `_fwd_recurrence`. `backward` sets up gradient tensors and launches `_bwd_recurrence`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_recurrence(S, d, O, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V, last_kv, stride_s, stride_d, stride_o, stride_last_kv, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    head_idx = pid // NUM_BLOCK\n    block_idx = pid % NUM_BLOCK\n\n    # Load the decay coefficient for this block\n    decay = tl.load(d + block_idx)\n\n    # Iterate over blocks of keys/values\n    for i in range(0, BLOCK_SIZE, BLOCK_MODEL_K):\n        # Compute the indices for the block\n        block_start = block_idx * BLOCK_MODEL_K + i\n\n        # Load the current key/value chunk\n        kv_chunk = tl.load(S + head_idx * stride_s + block_start, mask=block_start < D_MODEL_K)\n\n        # Optionally use the last key/value\n        if last_kv is not None:\n            kv_chunk = kv_chunk + tl.load(last_kv + head_idx * stride_last_kv + block_start, mask=block_start < D_MODEL_K)\n\n        # Apply the decay transformation\n        kv_chunk = kv_chunk * decay\n\n        # Store the result in the output tensor\n        tl.store(O + head_idx * stride_o + block_start, kv_chunk, mask=block_start < D_MODEL_K)\n\n@triton.jit\ndef _bwd_recurrence(S, d, DI, DG, DL, DS, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V, stride_s, stride_d, stride_di, stride_dg, stride_dl, stride_ds, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    head_idx = pid // NUM_BLOCK\n    block_idx = pid % NUM_BLOCK\n\n    # Load the decay coefficient for this block\n    decay = tl.load(d + block_idx)\n\n    # Iterate over blocks in reverse order for backpropagation\n    for i in range(BLOCK_SIZE - BLOCK_MODEL_K, -1, -BLOCK_MODEL_K):\n        block_start = block_idx * BLOCK_MODEL_K + i\n\n        # Load the current key/value chunk\n        kv_chunk = tl.load(S + head_idx * stride_s + block_start, mask=block_start < D_MODEL_K)\n\n        # Compute gradients\n        grad_kv_chunk = tl.load(DI + head_idx * stride_di + block_start, mask=block_start < D_MODEL_K)\n        grad_kv_chunk = grad_kv_chunk * decay\n\n        # Accumulate gradients\n        tl.atomic_add(DG + block_idx, grad_kv_chunk)\n\n        # Store the result in the gradient tensor\n        tl.store(DS + head_idx * stride_ds + block_start, grad_kv_chunk, mask=block_start < D_MODEL_K)\n\nclass ChunkGateRecurrent:\n    def __init__(self, NUM_HEAD, NUM_BLOCK, D_MODEL_K, D_MODEL_V, BLOCK_MODEL_K, BLOCK_MODEL_V):\n        self.NUM_HEAD = NUM_HEAD\n        self.NUM_BLOCK = NUM_BLOCK\n        self.D_MODEL_K = D_MODEL_K\n        self.D_MODEL_V = D_MODEL_V\n        self.BLOCK_MODEL_K = BLOCK_MODEL_K\n        self.BLOCK_MODEL_V = BLOCK_MODEL_V\n\n    def forward(self, S, d, last_kv=None):\n        O = torch.empty_like(S)\n        grid = (self.NUM_HEAD * self.NUM_BLOCK,)\n        _fwd_recurrence[grid](S, d, O, self.NUM_HEAD, self.NUM_BLOCK, self.D_MODEL_K, self.D_MODEL_V, self.BLOCK_MODEL_K, self.BLOCK_MODEL_V, last_kv)\n        return O\n\n    def backward(self, S, d, DI, DG, DL):\n        DS = torch.empty_like(S)\n        grid = (self.NUM_HEAD * self.NUM_BLOCK,)\n        _bwd_recurrence[grid](S, d, DI, DG, DL, DS, self.NUM_HEAD, self.NUM_BLOCK, self.D_MODEL_K, self.D_MODEL_V, self.BLOCK_MODEL_K, self.BLOCK_MODEL_V)\n        return DS\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\ntorch.backends.cudnn.allow_tf32 = True\n\n@triton.jit\ndef _fwd_recurrence(\n    S, d, \n    O,\n    NUM_HEAD, NUM_BLOCK, \n    D_MODEL_K: tl.constexpr, D_MODEL_V: tl.constexpr,\n    BLOCK_MODEL_K: tl.constexpr, BLOCK_MODEL_V: tl.constexpr,\n    last_kv: tl.tensor  # \u4e0d\u518d\u4f7f\u7528 Optional\n):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)    \n\n    S = S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K + tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n    O = O + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n\n    if last_kv is not None:\n        last_kv = last_kv + offset_bh * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n        acc = tl.load(last_kv).to(tl.float32)\n    else:\n        acc = tl.zeros([BLOCK_MODEL_K, BLOCK_MODEL_V], dtype=tl.float32)\n\n    tl.store(O, acc.to(O.dtype.element_ty))\n    O += D_MODEL_K * D_MODEL_V\n    d = d + offset_bh * NUM_BLOCK\n    for i in range(NUM_BLOCK-1):\n        d_i = tl.load(d)\n        S_i = tl.load(S) \n        acc = acc * d_i + S_i\n        tl.store(O, acc.to(O.dtype.element_ty))\n        d += 1\n        S += D_MODEL_K * D_MODEL_V\n        O += D_MODEL_K * D_MODEL_V\n     \n\n## NUM_SPLIT_K/V. K/V dimension split into NUM_SPLIT_K/V parts with equal size BLOCK_MODEL\n@triton.jit\ndef _bwd_recurrence(\n    S, d, \n    DI, DG, DL, DS, \n    NUM_HEAD, NUM_BLOCK,\n    D_MODEL_K: tl.constexpr, D_MODEL_V: tl.constexpr,\n    BLOCK_MODEL_K: tl.constexpr, BLOCK_MODEL_V: tl.constexpr,\n    \n ):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)    \n\n    # offset_h = offset_bh % NUM_HEAD\n    NUM_K = D_MODEL_K // BLOCK_MODEL_K\n    NUM_V = D_MODEL_V // BLOCK_MODEL_V\n    # skip the last chunk because it is never used\n    S = S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K + tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :] + (NUM_BLOCK - 2) * D_MODEL_K * D_MODEL_V\n\n    DI = DI + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K + tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :] + (NUM_BLOCK - 2) * D_MODEL_K * D_MODEL_V\n\n    # start from the last chunk  \n    DS = DS + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :] + (NUM_BLOCK - 1) * D_MODEL_K * D_MODEL_V\n\n    DG = DG + offset_bh * NUM_BLOCK * NUM_K * NUM_V + offset_d * NUM_V + offset_s + (NUM_BLOCK - 2) * NUM_K * NUM_V\n\n    d = d + offset_bh * NUM_BLOCK + (NUM_BLOCK - 1)\n\n    Dacc = tl.zeros([BLOCK_MODEL_K, BLOCK_MODEL_V], dtype=tl.float32) \n\n    # ignore the first chunk\n    for i in range(NUM_BLOCK - 1):\n        S_i = tl.load(S)\n        DS_i = tl.load(DS)\n        d_i = tl.load(d)\n        Dacc = Dacc * d_i + DS_i\n        DG_i = tl.sum(Dacc * S_i.to(tl.float32))\n\n        tl.store(DG, DG_i.to(DG.dtype.element_ty))\n        tl.store(DI, Dacc.to(DI.dtype.element_ty))    \n\n        S -= D_MODEL_K * D_MODEL_V\n        DI -= D_MODEL_K * D_MODEL_V \n        DS -= D_MODEL_K * D_MODEL_V\n        DG -= NUM_K * NUM_V\n        d -= 1\n    \n    DL = DL + offset_bh * D_MODEL_K * D_MODEL_V + offset_d * D_MODEL_V * BLOCK_MODEL_K  +  tl.arange(0, BLOCK_MODEL_K)[:, None] * D_MODEL_V + offset_s * BLOCK_MODEL_V + tl.arange(0, BLOCK_MODEL_V)[None, :]\n    DS_i = tl.load(DS)\n    d_i = tl.load(d)\n    Dacc = Dacc * d_i + DS_i\n    tl.store(DL, Dacc.to(DL.dtype.element_ty))  \n\nclass ChunkGateRecurrent(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, kv, cross_decay, last_kv=None):\n        cross_decay = cross_decay.contiguous()\n        kv = kv.contiguous()\n\n        B, H, N, D_k, D_v = kv.shape \n        output = torch.empty_like(kv)        \n        BLOCK_MODEL_K = 64\n        BLOCK_MODEL_V = 16\n    \n        assert D_k % BLOCK_MODEL_K == 0\n        assert D_v % BLOCK_MODEL_V == 0\n\n        grid = (B*H, D_k//BLOCK_MODEL_K, D_v//BLOCK_MODEL_V)\n        ctx.grid = grid\n        ctx.have_last_kv = last_kv is not None\n        ctx.BLOCK_MODEL_K = BLOCK_MODEL_K\n        ctx.BLOCK_MODEL_V = BLOCK_MODEL_V\n\n        _fwd_recurrence[grid](\n            kv,\n            cross_decay,\n            output,\n            D_MODEL_K=D_k, D_MODEL_V=D_v,\n            NUM_BLOCK=N, NUM_HEAD=H,\n            BLOCK_MODEL_K=BLOCK_MODEL_K,\n            BLOCK_MODEL_V=BLOCK_MODEL_V,\n            last_kv=last_kv\n        )\n\n        ctx.save_for_backward(output, cross_decay)        \n        return output\n\n    @staticmethod\n    def backward(ctx, DO):\n        DO = DO.contiguous()\n\n        output, cross_decay = ctx.saved_tensors \n\n        B, H, N, D_k, D_v = output.shape \n        \n        BLOCK_MODEL_K = 64\n        BLOCK_MODEL_V = 16\n\n        grid = (B*H, D_k//BLOCK_MODEL_K, D_v//BLOCK_MODEL_V)\n\n        DI = torch.empty_like(DO)\n        DG = torch.empty(B*H, N, D_k//BLOCK_MODEL_K, D_v//BLOCK_MODEL_V, device=cross_decay.device, dtype=cross_decay.dtype)\n        DL = torch.empty(B, H, D_k, D_v, device=output.device, dtype=output.dtype)\n        _bwd_recurrence[grid](\n            output, cross_decay,\n            DI, DG, DL, DO, \n            NUM_HEAD=H, NUM_BLOCK = N, \n            D_MODEL_K = D_k,\n            D_MODEL_V = D_v, \n            BLOCK_MODEL_K=BLOCK_MODEL_K,\n            BLOCK_MODEL_V=BLOCK_MODEL_V,\n        )\n\n        DI[:, :, -1] = 0\n        DG[:, -1] = 0\n        DG = DG.view(B, H, N, -1).sum(dim=-1)\n        return DI, DG, DL if ctx.have_last_kv else None\n\nchunk_gate_recurrent = ChunkGateRecurrent.apply\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided Triton kernel code includes two main kernel functions: \n    `chunk_gated_abc_fwd_kernel_cum` and `chunk_gated_abc_fwd_kernel_h`. \n    These are used in a forward pass computation commonly found in attention-like mechanisms. \n\n    `chunk_gated_abc_fwd_kernel_cum` handles the computation of a cumulative matrix. It takes inputs: \n    `s` (source), `o` (output), along with stride parameters and block sizes (`T`, `S`, `BT`, `BS`). \n    It calculates a mask `m_s` that ensures only upper triangular parts of the matrix are involved in \n    the computation. The results are stored in the output `o`.\n\n    `chunk_gated_abc_fwd_kernel_h` is designed to handle a gated accumulation operation over key, value, \n    and gating tensors. Inputs include `k` (key), `v` (value), `g` (gate), `h` (output state), optional \n    initial `h0`, and final `ht` states, along with stride and block size parameters. The kernel iteratively \n    accumulates the product of `k` and `v`, adjusted by `g`, into `h`.\n\n    The `fwd_pre` function sets up the grid for `chunk_gated_abc_fwd_kernel_cum`, applying a cumulative sum \n    operation on the input tensor `g`, reshaping, and storing the result.\n\n    The `fwd_inner` function prepares and launches `chunk_gated_abc_fwd_kernel_h`, iterating over blocks \n    of data to compute the gated attention mechanism with options for initial and final states.\n\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_cum(s_ptr, o_ptr, stride_s, stride_o, T, S, BT, BS, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    bid = tl.program_id(1)\n    \n    # Calculate block start positions\n    row_start = pid * BLOCK_SIZE\n    col_start = bid * BLOCK_SIZE\n\n    # Load source data\n    s = tl.load(s_ptr + row_start * stride_s + col_start, mask=(row_start < T) & (col_start < S))\n    \n    # Initialize output with zeros\n    o = tl.zeros([BLOCK_SIZE, BLOCK_SIZE], dtype=tl.float32)\n\n    # Upper triangular mask\n    m_s = tl.arange(0, BLOCK_SIZE)[:, None] <= tl.arange(0, BLOCK_SIZE)\n\n    # Cumulative computation\n    for i in range(BLOCK_SIZE):\n        if m_s[i, i]:\n            o[i, :] = tl.sum(s[i, :], axis=0)\n    \n    # Store the result\n    tl.store(o_ptr + row_start * stride_o + col_start, o, mask=(row_start < T) & (col_start < S))\n\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_h(k_ptr, v_ptr, g_ptr, h_ptr, stride_k, stride_v, stride_g, stride_h, T, S, BT, BS, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    bid = tl.program_id(1)\n    \n    # Calculate block start positions\n    row_start = pid * BLOCK_SIZE\n    col_start = bid * BLOCK_SIZE\n\n    # Load key, value, and gate data\n    k = tl.load(k_ptr + row_start * stride_k + col_start, mask=(row_start < T) & (col_start < S))\n    v = tl.load(v_ptr + row_start * stride_v + col_start, mask=(row_start < T) & (col_start < S))\n    g = tl.load(g_ptr + row_start * stride_g + col_start, mask=(row_start < T) & (col_start < S))\n\n    # Initialize output\n    h = tl.zeros([BLOCK_SIZE, BLOCK_SIZE], dtype=tl.float32)\n\n    # Gated accumulation\n    for i in range(BLOCK_SIZE):\n        h[i, :] += g[i, :] * (k[i, :] @ v[i, :])\n\n    # Store the result\n    tl.store(h_ptr + row_start * stride_h + col_start, h, mask=(row_start < T) & (col_start < S))\n\ndef fwd_pre(s, T, S, BT, BS):\n    # Set up grid for chunk_gated_abc_fwd_kernel_cum\n    grid = (triton.cdiv(T, BS), triton.cdiv(S, BS))\n    o = torch.empty_like(s)\n    \n    # Launch the kernel\n    chunk_gated_abc_fwd_kernel_cum[grid](s, o, s.stride(0), o.stride(0), T, S, BT, BS, BLOCK_SIZE=BS)\n    \n    return o\n\ndef fwd_inner(k, v, g, h, T, S, BT, BS):\n    # Set up grid for chunk_gated_abc_fwd_kernel_h\n    grid = (triton.cdiv(T, BS), triton.cdiv(S, BS))\n    h_out = torch.empty_like(h)\n    \n    # Launch the kernel\n    chunk_gated_abc_fwd_kernel_h[grid](k, v, g, h_out, k.stride(0), v.stride(0), g.stride(0), h_out.stride(0), T, S, BT, BS, BLOCK_SIZE=BS)\n    \n    return h_out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BS': 16}, num_warps=2),\n        triton.Config({'BS': 16}, num_warps=4),\n        triton.Config({'BS': 16}, num_warps=8),\n        triton.Config({'BS': 32}, num_warps=2),\n        triton.Config({'BS': 32}, num_warps=4),\n        triton.Config({'BS': 32}, num_warps=8),\n        triton.Config({'BS': 64}, num_warps=2),\n        triton.Config({'BS': 64}, num_warps=4),\n        triton.Config({'BS': 64}, num_warps=8),\n    ],\n    key=['S']\n)\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_cum(\n    s,\n    o,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr,\n):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1., 0.).to(tl.float32)\n\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    # [BT, BS]\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_gated_abc_fwd_kernel_h(\n    k,\n    v,\n    g,\n    h,\n    h0,\n    ht,\n    s_k_h,\n    s_k_t,\n    s_k_d,\n    s_v_h,\n    s_v_t,\n    s_v_d,\n    s_h_h,\n    s_h_t,\n    s_h_d,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    GATEK: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if GATEK:\n            p_g = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            # [BK,]\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            # [BK, BV]\n            b_h *= tl.exp(b_gn)[:, None]\n            # [BK, BT]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_k = (b_k * tl.exp(b_gn[:, None] - b_g)).to(b_k.dtype)\n        else:\n            p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            # [BV,]\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            # [BK, BV]\n            b_h *= tl.exp(b_gn)[None, :]\n            # [BT, BV]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_v = (b_v * tl.exp(b_gn[None, :] - b_g)).to(b_v.dtype)\n        # [BK, BV]\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n\n\ndef fwd_pre(g, B, H, T, S, BT):\n    NT = triton.cdiv(T, BT)\n    g_org, g = g, torch.empty_like(g, dtype=torch.float)\n    def grid(meta): return (triton.cdiv(meta['S'], meta['BS']), NT, B * H)\n    # keep cummulative normalizer in fp32\n    # this kernel is equivalent to\n    # g = g.view(B, H, NT, BT, -1).cumsum(-2).view(B, H, T, -1)\n    chunk_gated_abc_fwd_kernel_cum[grid](\n        g_org, g,\n        g.stride(1), g.stride(2), g.stride(3),\n        T=T, S=S, BT=BT\n    )\n    return g\n\n\ndef fwd_inner(q, k, v, g, B, H, T, K, V, BT, BK, BV, gatek=False, h0=None, ht=None):\n    NT = triton.cdiv(T, BT)\n    NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n    num_warps = 4 if BK == 64 else 2\n    num_stages = 1\n\n    h = q.new_empty(B, H, NT * K, V)\n    grid = (NV, NK, B * H)\n    chunk_gated_abc_fwd_kernel_h[grid](\n        k, v, g, h, h0, ht,\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2), h.stride(3),\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n        GATEK=gatek,\n        USE_INITIAL_STATE=h0 is not None,\n        STORE_FINAL_STATE=ht is not None,\n        num_warps=num_warps,\n        num_stages=num_stages\n    )\n    return h\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code implements a custom operation named SWiGLU using the Triton language for high-performance computations on GPUs. \n            The `swiglu_forward` function accepts two input tensors, `a` and `b`, computes their element-wise product after applying the SiLU (Sigmoid Linear Unit) activation to `a`, and returns the result in `c`. \n            The forward pass relies on the `_swiglu_forward_kernel` Triton kernel function, which is launched with the grid size corresponding to the number of rows in the reshaped input.\n            The `swiglu_backward` function calculates the gradients with respect to the input tensors `a` and `b`, given the gradient `dc` of the output, using the `_swiglu_backward_kernel`. Both kernel functions operate over blocks of data, specified by the `BLOCK_SIZE`, and use `num_warps` for optimizing parallel execution. \n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nBLOCK_SIZE = 1024  # You can adjust this based on your GPU and problem size\nNUM_WARPS = 4      # Number of warps for parallel execution\n\n@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the indices for this program\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load input elements\n    a = tl.load(a_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    b = tl.load(b_ptr + offsets, mask=offsets < n_elements, other=0.0)\n\n    # Compute SiLU activation\n    silu_a = a * tl.sigmoid(a)\n\n    # Compute the element-wise product\n    c = silu_a * b\n\n    # Store the result\n    tl.store(c_ptr + offsets, c, mask=offsets < n_elements)\n\ndef swiglu_forward(a, b):\n    assert a.shape == b.shape, \"Input tensors must have the same shape\"\n    n_elements = a.numel()\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    c = torch.empty_like(a)\n\n    _swiglu_forward_kernel[grid](\n        a_ptr=a, b_ptr=b, c_ptr=c,\n        n_elements=n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=NUM_WARPS\n    )\n\n    return c\n\n@triton.jit\ndef _swiglu_backward_kernel(a_ptr, b_ptr, dc_ptr, da_ptr, db_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the indices for this program\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load input elements and output gradient\n    a = tl.load(a_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    b = tl.load(b_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    dc = tl.load(dc_ptr + offsets, mask=offsets < n_elements, other=0.0)\n\n    # Compute SiLU and its derivative\n    sigmoid_a = tl.sigmoid(a)\n    silu_a = a * sigmoid_a\n    d_silu_a = sigmoid_a * (1 + a * (1 - sigmoid_a))\n\n    # Compute gradients\n    da = dc * b * d_silu_a\n    db = dc * silu_a\n\n    # Store the gradients\n    tl.store(da_ptr + offsets, da, mask=offsets < n_elements)\n    tl.store(db_ptr + offsets, db, mask=offsets < n_elements)\n\ndef swiglu_backward(a, b, dc):\n    assert a.shape == b.shape == dc.shape, \"All tensors must have the same shape\"\n    n_elements = a.numel()\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    da = torch.empty_like(a)\n    db = torch.empty_like(b)\n\n    _swiglu_backward_kernel[grid](\n        a_ptr=a, b_ptr=b, dc_ptr=dc, da_ptr=da, db_ptr=db,\n        n_elements=n_elements,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=NUM_WARPS\n    )\n\n    return da, db\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport functools\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\ndef calculate_settings(n):\n    MAX_FUSED_SIZE = 65536\n    BLOCK_SIZE = triton.next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(\n            f\"Cannot launch Triton kernel since n = {n} exceeds \"\n            f\"the recommended Triton blocksize = {MAX_FUSED_SIZE}.\"\n        )\n\n    num_warps = 4\n    if BLOCK_SIZE >= 32768:\n        num_warps = 32 if not is_hip() else 16\n    elif BLOCK_SIZE >= 8192:\n        num_warps = 16\n    elif BLOCK_SIZE >= 2048:\n        num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n\n@triton.jit\ndef _swiglu_forward_kernel(\n    a_ptr, b_ptr, c_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    # sigmoid requires type float32\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n\n@triton.jit\ndef _swiglu_backward_kernel(\n    dc_ptr, a_ptr, b_ptr, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    # sigmoid requires type float32\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n\n    # recomputation to save memory\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n\ndef swiglu_forward(a, b):\n    ori_shape = a.shape\n\n    n_cols = ori_shape[-1]\n    a = a.view(-1, n_cols)\n    b = b.view(-1, n_cols)\n    c = torch.empty_like(a)\n    n_rows = a.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _swiglu_forward_kernel[(n_rows,)](\n        a,\n        b,\n        c,\n        c.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return a, b, c.view(*ori_shape)\n\ndef swiglu_backward(a, b, dc):\n    ori_shape = dc.shape\n    n_cols = ori_shape[-1]\n    dc = dc.view(-1, n_cols)\n    n_rows = dc.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _swiglu_backward_kernel[(n_rows,)](\n        dc,\n        a,\n        b,\n        dc.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return a.view(*ori_shape), b.view(*ori_shape)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe provided Triton code defines a custom addition operation on two input PyTorch tensors, `a` and `b`. The main computation is performed in the `_add_kernel` Triton kernel. The function `custom_add` wraps this kernel to apply it over the input tensors.\n\nFunction `_add_kernel(A, B, C, size, BLOCK)`:\n- It is a Triton kernel function, decorated with `@triton.jit`, which indicates that this function is just-in-time compiled by Triton.\n- Inputs: Pointers to input tensors `A` and `B`, a pointer to the output tensor `C`, the size of the tensors, and a constant parameter `BLOCK` that indicates the block size of computation.\n- Each program instance computes a block of the tensor addition. The index of the program is obtained by `tl.program_id(0)`.\n- The offsets for the block are calculated using `prog_id` and `tl.arange(0, BLOCK)`.\n- `tl.load` and `tl.store` functions are used to read from and write to tensors with boundary checks using a mask `offs < size`.\n\nFunction `custom_add(a, b)`:\n- It is a wrapper around the `_add_kernel` to add two PyTorch tensors `a` and `b`.\n- It creates an empty tensor `c` of the same shape as `a` to store the result.\n- `grid` is determined by dividing `size` by `BLOCK`, ensuring each block is processed by a program instance.\n- The Triton kernel is launched with `_add_kernel[grid]`, passing necessary arguments.\n- Returns the result tensor `c`.\n\nThe BLOCK constant defines how many elements each instance processes, here set to 16. The kernel ensures boundary conditions are respected using the mask.\n", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the Triton kernel for element-wise addition\n@triton.jit\ndef _add_kernel(A, B, C, size, BLOCK: tl.constexpr):\n    # Get the program ID for this instance\n    prog_id = tl.program_id(0)\n    \n    # Calculate the start offset for this block\n    offsets = prog_id * BLOCK + tl.arange(0, BLOCK)\n    \n    # Create a mask to handle boundary conditions\n    mask = offsets < size\n    \n    # Load data from input tensors A and B with boundary check\n    a_vals = tl.load(A + offsets, mask=mask, other=0.0)\n    b_vals = tl.load(B + offsets, mask=mask, other=0.0)\n    \n    # Perform element-wise addition\n    c_vals = a_vals + b_vals\n    \n    # Store the result in output tensor C\n    tl.store(C + offsets, c_vals, mask=mask)\n\n# Define the wrapper function for the Triton kernel\ndef custom_add(a, b):\n    # Ensure input tensors are of the same size\n    assert a.size() == b.size(), \"Input tensors must have the same size\"\n    \n    # Determine the size of the tensors\n    size = a.numel()\n    \n    # Create an output tensor of the same size\n    c = torch.empty_like(a)\n    \n    # Define the block size for the kernel\n    BLOCK = 16\n    \n    # Calculate the grid size needed to cover the entire tensor\n    grid = (size + BLOCK - 1) // BLOCK  # This ensures we cover the entire tensor\n    \n    # Launch the Triton kernel\n    _add_kernel[grid](a, b, c, size, BLOCK)\n    \n    # Return the result tensor\n    return c\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create example tensors\n    a = torch.randn(128, device='cuda')\n    b = torch.randn(128, device='cuda')\n    \n    # Perform custom addition\n    c = custom_add(a, b)\n    \n    # Print the result\n    print(c)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _add_kernel(A, B, C, size, BLOCK: tl.constexpr):\n    \"\"\"add kernel.\"\"\"\n    prog_id = tl.program_id(0)\n    offs = prog_id * BLOCK + tl.arange(0, BLOCK)\n    a = tl.load(A + offs, mask=offs < size)\n    b = tl.load(B + offs, mask=offs < size)\n    tl.store(C + offs, a + b, mask=offs < size)\n\ndef custom_add(a, b):\n    \"\"\"custom add one.\"\"\"\n    c = torch.empty_like(a)\n    size = c.size(0)\n    BLOCK = 16\n\n    grid = (triton.cdiv(size, BLOCK), )\n    _add_kernel[grid](a, b, c, size, BLOCK=BLOCK)\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs a 2D convolution operation on input data using a set of weights. \n            The kernel is called `conv2d_forward_kernel` and is launched with specific grid dimensions based on input size, output size, and the number of groups.\n            The kernel takes pointers to the input, weight, and output tensors along with their respective dimensions and strides. \n            It calculates the output feature map by iterating over the kernel dimensions and computing matrix products between input patches and weight matrices.\n            The function `conv2d_forward` serves as a wrapper for this Triton kernel, taking PyTorch tensors as input and calculating the necessary strides and dimensions.\n            It initializes an output tensor and defines block sizes to partition work across Triton threads.\n            Finally, the Triton kernel is launched with the prepared inputs and block configurations, and the resulting output tensor is returned.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef conv2d_forward_kernel(\n    input_ptr, weight_ptr, output_ptr,\n    input_height, input_width, input_channels,\n    output_height, output_width, output_channels,\n    kernel_height, kernel_width, stride_height, stride_width,\n    input_stride_n, input_stride_c, input_stride_h, input_stride_w,\n    weight_stride_o, weight_stride_i, weight_stride_h, weight_stride_w,\n    output_stride_n, output_stride_c, output_stride_h, output_stride_w,\n    BLOCK_SIZE_N, BLOCK_SIZE_C, BLOCK_SIZE_H, BLOCK_SIZE_W\n):\n    # Define the block indices\n    n_idx = tl.program_id(0)\n    c_out_idx = tl.program_id(1)\n    h_out_idx = tl.program_id(2)\n    w_out_idx = tl.program_id(3)\n\n    # Compute the starting point of the block\n    n_start = n_idx * BLOCK_SIZE_N\n    c_out_start = c_out_idx * BLOCK_SIZE_C\n    h_out_start = h_out_idx * BLOCK_SIZE_H\n    w_out_start = w_out_idx * BLOCK_SIZE_W\n\n    # Initialize accumulators\n    acc = tl.zeros([BLOCK_SIZE_N, BLOCK_SIZE_C, BLOCK_SIZE_H, BLOCK_SIZE_W], dtype=tl.float32)\n\n    # Iterate over the input channels and kernel dimensions\n    for c_in in range(input_channels):\n        for kh in range(kernel_height):\n            for kw in range(kernel_width):\n                # Compute input indices\n                h_in = h_out_start * stride_height + kh\n                w_in = w_out_start * stride_width + kw\n\n                # Load input and weight values\n                input_val = tl.load(input_ptr + n_start * input_stride_n + c_in * input_stride_c + h_in * input_stride_h + w_in * input_stride_w)\n                weight_val = tl.load(weight_ptr + c_out_start * weight_stride_o + c_in * weight_stride_i + kh * weight_stride_h + kw * weight_stride_w)\n\n                # Perform the convolution\n                acc += input_val * weight_val\n\n    # Store the result\n    tl.store(output_ptr + n_start * output_stride_n + c_out_start * output_stride_c + h_out_start * output_stride_h + w_out_start * output_stride_w, acc)\n\ndef conv2d_forward(input_tensor, weight_tensor, stride=(1, 1)):\n    # Extract dimensions\n    n, c_in, h_in, w_in = input_tensor.shape\n    c_out, _, kh, kw = weight_tensor.shape\n\n    # Calculate output dimensions\n    h_out = (h_in - kh) // stride[0] + 1\n    w_out = (w_in - kw) // stride[1] + 1\n\n    # Prepare output tensor\n    output_tensor = torch.empty((n, c_out, h_out, w_out), device=input_tensor.device, dtype=input_tensor.dtype)\n\n    # Define block sizes\n    BLOCK_SIZE_N = 1\n    BLOCK_SIZE_C = 1\n    BLOCK_SIZE_H = 8\n    BLOCK_SIZE_W = 8\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(n, BLOCK_SIZE_N), triton.cdiv(c_out, BLOCK_SIZE_C), triton.cdiv(h_out, BLOCK_SIZE_H), triton.cdiv(w_out, BLOCK_SIZE_W))\n    conv2d_forward_kernel[grid](\n        input_tensor, weight_tensor, output_tensor,\n        h_in, w_in, c_in,\n        h_out, w_out, c_out,\n        kh, kw, stride[0], stride[1],\n        *input_tensor.stride(), *weight_tensor.stride(), *output_tensor.stride(),\n        BLOCK_SIZE_N, BLOCK_SIZE_C, BLOCK_SIZE_H, BLOCK_SIZE_W\n    )\n\n    return output_tensor\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef conv2d_forward_kernel(\n    input_pointer, weight_pointer, output_pointer,\n    batch_dim, in_feat_dim, in_height, in_width,\n    out_feat_dim, out_height, out_width,\n    input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride,\n    weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride,\n    output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride,\n    kernel_height: tl.constexpr, kernel_width: tl.constexpr,\n    stride_height: tl.constexpr, stride_width: tl.constexpr,\n    padding_height: tl.constexpr, padding_width: tl.constexpr,\n    groups: tl.constexpr, fp16: tl.constexpr, tf32: tl.constexpr,\n    BLOCK_SIZE_BATCH_HEIGHT_WIDTH: tl.constexpr, BLOCK_SIZE_IN_FEAT: tl.constexpr,\n    BLOCK_SIZE_OUT_FEAT: tl.constexpr,\n    ):\n    \"\"\"\n    2D-convolves over the input using weights.\n\n    Args:\n        input_pointer: Pointer to the input to convolve over.\n            The input must be of shape [batch_dim, in_feat_dim, in_height, in_width].\n        weight_pointer: Pointer to the weights input is convolved over by.\n            The weights must be of shape [out_feat_dim, in_feat_dim, kernel_height, kernel_width].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, out_feat_dim, out_height, out_width].\n        batch_dim: Batch dimension of the input and output.\n        in_feat_dim: Dimensionality of the input features.\n        in_height: Input height.\n        in_width: Input width.\n        out_feat_dim: Dimensionality of the output features.\n        out_height: Output height.\n        out_width: Output width.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_in_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_height_stride: Stride necessary to jump one element along the\n            input's height dimension.\n        input_width_stride: Stride necessary to jump one element along the\n            input's width dimension.\n        weight_out_feat_stride: Stride necessary to jump one element along the\n            weights' output feature dimension.\n        weight_in_feat_stride: Stride necessary to jump one element along the\n            weights' input feature dimension.\n        weight_height_stride: Stride necessary to jump one element along the\n            weights' height dimension.\n        weight_width_stride: Stride necessary to jump one element along the\n            weights' width dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output's batch dimension.\n        output_out_feat_stride: Stride necessary to jump one element along the\n            output's feature dimension.\n        output_height_stride: Stride necessary to jump one element along the\n            output's height dimension.\n        output_width_stride: Stride necessary to jump one element along the\n            output's width dimension.\n        kernel_height: Kernel height.\n        kernel_width: Kernel width.\n        stride_height: Stride of kernel across the height dimension.\n        stride_width: Stride of kernel across the width dimension.\n        padding_height: Padding applied to the input across the height dimension.\n        padding_width: Padding applied to the input across the width dimension.\n        groups: Number of groups for the convolution.\n        fp16: Flag for loading the input and weights in FP16.\n        tf32: Flag for performing matrix products in TF32.\n        BLOCK_SIZE_BATCH_HEIGHT_WIDTH: Block size across the batch, height, and\n            width dimensions.\n        BLOCK_SIZE_IN_FEAT: Block size across the input feature dimension.\n        BLOCK_SIZE_OUT_FEAT: Block size across the output feature dimension.\n    \"\"\"\n    batch_height_width_pid = tl.program_id(0)\n    out_feat_pid = tl.program_id(1)\n    group_pid = tl.program_id(2)\n\n    in_group_dim = in_feat_dim // groups\n    out_group_dim = out_feat_dim // groups\n\n    batch_height_width_offset = (batch_height_width_pid * BLOCK_SIZE_BATCH_HEIGHT_WIDTH +\n                                 tl.arange(0, BLOCK_SIZE_BATCH_HEIGHT_WIDTH))\n    batch_height_offset = batch_height_width_offset // out_width\n    batch_offset = batch_height_offset // out_height\n\n    output_feat_offset = (out_feat_pid * BLOCK_SIZE_OUT_FEAT +\n                          tl.arange(0, BLOCK_SIZE_OUT_FEAT))\n    output_height_offset = batch_height_offset % out_height\n    output_width_offset = batch_height_width_offset % out_width\n\n    input_pointer += (input_batch_stride * batch_offset +\n                      input_in_feat_stride * group_pid * in_group_dim)[:, None]\n    weight_pointer += (weight_out_feat_stride * output_feat_offset +\n                       weight_out_feat_stride * group_pid * out_group_dim)[None, :]\n\n    accum = tl.zeros((BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_OUT_FEAT),\n                     dtype=tl.float32)\n\n    for h in range(kernel_height):\n        for w in range(kernel_width):\n            for c in range(0, in_group_dim, BLOCK_SIZE_IN_FEAT):\n                input_feat_offset = c + tl.arange(0, BLOCK_SIZE_IN_FEAT)\n                input_height_offset = (h - padding_height +\n                                       stride_height * output_height_offset)\n                input_width_offset = (w - padding_width +\n                                      stride_width * output_width_offset)\n\n                curr_input_pointer = (input_pointer +\n                                     (input_in_feat_stride * input_feat_offset)[None, :] +\n                                     (input_height_stride * input_height_offset)[:, None] +\n                                     (input_width_stride * input_width_offset)[:, None])\n                curr_weight_pointer = (weight_pointer +\n                                      (weight_in_feat_stride * input_feat_offset)[:, None] +\n                                      (weight_height_stride * h) +\n                                      (weight_width_stride * w))\n\n                input_mask = ((batch_offset < batch_dim)[:, None] &\n                              (input_feat_offset < in_group_dim)[None, :] &\n                              (0 <= input_height_offset)[:, None] &\n                              (input_height_offset < in_height)[:, None] &\n                              (0 <= input_width_offset)[:, None] &\n                              (input_width_offset < in_width)[:, None])\n                weight_mask = ((input_feat_offset < in_group_dim)[:, None] &\n                               (output_feat_offset < out_group_dim)[None, :])\n\n                input_block = tl.load(curr_input_pointer, mask=input_mask)\n                weight_block = tl.load(curr_weight_pointer, mask=weight_mask)\n\n                if fp16:\n                    input_block = input_block.to(tl.float16)\n                    weight_block = weight_block.to(tl.float16)\n\n                accum += tl.dot(input_block, weight_block, allow_tf32=tf32)\n\n    output_pointer += ((output_batch_stride * batch_offset)[:, None] +\n                       (output_out_feat_stride * (group_pid * out_group_dim + output_feat_offset))[None, :] +\n                       (output_height_stride * output_height_offset)[:, None] +\n                       (output_width_stride * output_width_offset)[:, None])\n    output_mask = ((batch_offset < batch_dim)[:, None] &\n                   (output_feat_offset < out_group_dim)[None, :] &\n                   (output_height_offset < out_height)[:, None] &\n                   (output_width_offset < out_width)[:, None])\n\n    tl.store(output_pointer, accum, mask=output_mask)\n\n\ndef conv2d_forward(input_tensor, weight_tensor, kernel_height, kernel_width, stride_height, stride_width, padding_height, padding_width, groups, fp16=False, tf32=False):\n    # Extract dimensions\n    batch_dim, in_feat_dim, in_height, in_width = input_tensor.shape\n    out_feat_dim, _, _, _ = weight_tensor.shape\n    out_height = (in_height + 2 * padding_height - kernel_height) // stride_height + 1\n    out_width = (in_width + 2 * padding_width - kernel_width) // stride_width + 1\n\n    # Calculate strides\n    input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride = input_tensor.stride()\n    weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride = weight_tensor.stride()\n    output_tensor = torch.empty((batch_dim, out_feat_dim, out_height, out_width), device=input_tensor.device, dtype=torch.float32)\n    output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride = output_tensor.stride()\n\n    # Define block sizes\n    BLOCK_SIZE_BATCH_HEIGHT_WIDTH = 128\n    BLOCK_SIZE_IN_FEAT = 32\n    BLOCK_SIZE_OUT_FEAT = 32\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(batch_dim * out_height * out_width, BLOCK_SIZE_BATCH_HEIGHT_WIDTH),\n            triton.cdiv(out_feat_dim, BLOCK_SIZE_OUT_FEAT),\n            groups)\n    \n    conv2d_forward_kernel[grid](\n        input_tensor, weight_tensor, output_tensor,\n        batch_dim, in_feat_dim, in_height, in_width,\n        out_feat_dim, out_height, out_width,\n        input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride,\n        weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride,\n        output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride,\n        kernel_height, kernel_width,\n        stride_height, stride_width,\n        padding_height, padding_width,\n        groups, fp16, tf32,\n        BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_IN_FEAT, BLOCK_SIZE_OUT_FEAT\n    )\n    \n    return output_tensor\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines a Triton-based function to determine if each element in a tensor is finite. The `isfinite_func_wrapper_rank_1` function is a Python wrapper that takes an input tensor and an output tensor. It calculates optimal tile sizes and warps for execution, then invokes the `isfinite_func_kernel_rank_1` kernel. The kernel iterates over the tensor, loads data using Triton block pointers, applies the `isfinite_func` operation, and writes results back. The kernel supports float32 and float64 types, utilizing Triton's library functions.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef isfinite_func_kernel_rank_1(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the program index\n    pid = tl.program_id(0)\n    \n    # Compute the block start and end indices\n    block_start = pid * BLOCK_SIZE\n    block_end = block_start + BLOCK_SIZE\n    \n    # Clamp the block end to the number of elements\n    block_end = tl.min(block_end, n_elements)\n    \n    # Iterate over the block of elements\n    for i in range(block_start, block_end):\n        # Load the input element\n        input_element = tl.load(input_ptr + i)\n        \n        # Check if the element is finite\n        is_finite = tl.isfinite(input_element)\n        \n        # Store the result\n        tl.store(output_ptr + i, is_finite)\n\ndef isfinite_func_wrapper_rank_1(input_tensor, output_tensor):\n    # Ensure the input and output tensors are on the GPU\n    assert input_tensor.is_cuda and output_tensor.is_cuda\n    \n    # Get the number of elements in the input tensor\n    n_elements = input_tensor.numel()\n    \n    # Define the block size\n    BLOCK_SIZE = 1024\n    \n    # Calculate the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the kernel\n    isfinite_func_kernel_rank_1[grid](\n        input_tensor.data_ptr(),\n        output_tensor.data_ptr(),\n        n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage\ninput_tensor = torch.tensor([1.0, float('inf'), 3.0, float('nan')], dtype=torch.float32, device='cuda')\noutput_tensor = torch.empty_like(input_tensor, dtype=torch.bool, device='cuda')\n\nisfinite_func_wrapper_rank_1(input_tensor, output_tensor)\n\nprint(output_tensor.cpu())  # Should print tensor([ True, False,  True, False])\n", "label": "import math\nfrom typing import Union\nimport torch\nimport triton\nfrom triton import language as tl\n\ntry:\n    from triton.language.extra.cuda.libdevice import isfinited as _isfinited\nexcept ImportError:\n    try:\n        from triton.language.math import isfinited as _isfinited\n    except ImportError:\n        from triton.language.libdevice import isfinited as _isfinited\n\ntry:\n    from triton.language.extra.cuda.libdevice import finitef as _finitef\nexcept ImportError:\n    try:\n        from triton.language.math import finitef as _finitef\n    except ImportError:\n        from triton.language.libdevice import finitef as _finitef\n\ndef heuristics_for_tile_size(max_tile_size, *sizes):\n    ndim = len(sizes)\n    tile_sizes = [0 for _ in range(ndim)]\n    for i in range(ndim):\n        size = sizes[ndim - 1 - i]\n        tile_size = min(max_tile_size, triton.next_power_of_2(size))\n        tile_sizes[ndim - 1 - i] = tile_size\n        max_tile_size = max(1, max_tile_size // tile_size)\n    return tuple(tile_sizes)\n\ndef heuristics_for_num_warps(tile_size):\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n    \n\nclass StridedBuffer:\n    \"\"\"A drop-in replacement of torch.Tensor that can be used in wrapper generated by\n    PointwiseDynamicFunction. It allows us to use a different shape, stride, data\n    pointer that that of the base tensor.\n\n    It is a kind of reinterpretation of the base tensor. We make this class since we\n    cannot get a Tensor view with negative strides via torch APIs, while we need this\n    to implement flip op.\n\n    Although generated code can accept torch.Tensor & StridedBuffer, but StridedBuffer\n    may not have all the methods as torch.Tensors do. We add some attributes & methods\n    with the same name as torch.Tensor, which are used in the generated code. But we\n    may not cover all the methods, add one if what you need is missing here.\n\n    And can also be used in triton kernels since it also has dtype & data_ptr().\n    \"\"\"\n\n    def __init__(\n        self, base: torch.Tensor, shape=None, strides=None, dtype=None, offset=0\n    ):\n        self._base = base\n        self.dtype = dtype or base.dtype\n        if offset == 0:\n            self._data_ptr = self._base.data_ptr()\n        else:\n            offset = self.dtype.itemsize * offset\n            self._data_ptr = self._base.data_ptr() + offset\n        self.shape = tuple(shape if shape is not None else self._base.shape)\n        self._strides = tuple(strides if strides is not None else self._base.stride())\n        self.device = self._base.device\n        self.ndim = len(self.shape)\n\n    def stride(self):\n        return self._strides\n\n    def size(self):\n        return self.shape\n\n    def element_size(self):\n        return self.dtype.itemsize\n\n    def numel(self):\n        return math.prod(self.shape)\n\n    def dim(self):\n        return self.ndim\n\n    def unwrap(self):\n        return self._base\n\n    def data_ptr(self):\n        return self._data_ptr\n\n\ndef isfinite_func_wrapper_rank_1(in0: Union[torch.Tensor, StridedBuffer], /, *, out0: Union[torch.Tensor, StridedBuffer]): \n    \"\"\"Generated wrapper function with Pointwise: StridedBuffer, StridedBuffer(a1!) -> StridedBuffer(a1!)\"\"\"\n    assert in0.shape == out0.shape, 'operand shapes mismatch'\n    # task partitioning\n    shape = out0.shape\n    num_tasks = out0.numel()\n    tile_sizes = heuristics_for_tile_size(512, *shape)\n    tile_size = math.prod(tile_sizes)\n    num_tiles = math.prod(triton.cdiv(size, tile_size) for size, tile_size in zip(shape, tile_sizes))\n    num_ctas = min(65536, num_tiles)\n    tiles_per_cta = triton.cdiv(num_tiles, num_ctas)\n    num_warps = heuristics_for_num_warps(tile_size)\n    one_tile_per_cta = tiles_per_cta==1\n    grid = (num_ctas, 1, 1)\n    # kernel launch\n    in0_strides = in0.stride()\n    in0_stride_order = (0,)\n    out0_strides = out0.stride()\n    out0_stride_order = (0,)\n    with torch.cuda._DeviceGuard(in0.device.index):\n        isfinite_func_kernel_rank_1[grid](\n            in0, out0,\n            in0_strides[0], # stride for in0\n            in0_stride_order[0], # stride order for in0\n            out0_strides[0], # stride for out0\n            out0_stride_order[0], # stride orderfor out0\n            shape[0], # task indexing space\n            num_tasks, # num tasks\n            tiles_per_cta=tiles_per_cta, # tiles_per_cta\n            tile_size0=tile_sizes[0],\n            one_tile_per_cta=one_tile_per_cta,\n            num_warps=num_warps,\n        )\n    return out0\n\n@triton.jit\ndef isfinite_func(x):\n    return _isfinited(x) if x.dtype.is_fp64() else _finitef(x.to(tl.float32))\n\n@triton.jit\ndef isfinite_func_kernel_rank_1(\n    in0_ptr: tl.tensor, # of tl.pointer_type\n    out0_ptr: tl.tensor, # of tl.pointer_type\n    in0_stride0: int, # strides for in0\n    in0_stride_order0: tl.constexpr, # stride order for in0\n    out0_stride0: int, # strides for out0\n    out0_stride_order0: tl.constexpr, # stride order for out0\n    s0: int, # task_space\n    num_tasks: int,\n    tiles_per_cta: int,\n    tile_size0: tl.constexpr,\n    one_tile_per_cta: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_tiles0 = tl.cdiv(s0, tile_size0)\n    if one_tile_per_cta: # monolitic kernel style\n        tile_id = pid\n        # pid multi index recontruction: we use c ordering, right axes changes fastest\n        tile_id0 = tile_id\n\n        # tile offsets\n        offset0 = tile_id0 * tile_size0\n        # loads\n        in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n        in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n        # compute\n        out0 = isfinite_func(in0)\n\n        # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n        out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n        tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n    else: # grid-stride-loop style kernel\n        num_ctas = tl.num_programs(0)\n        for j in range(0, tiles_per_cta):\n            tile_id = pid + j * num_ctas\n            # pid multi index recontruction: we use c ordering, right axes changes fastest\n            tile_id0 = tile_id\n\n            # tile offsets\n            offset0 = tile_id0 * tile_size0\n            # loads\n            in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n            in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n            # compute\n            out0 = isfinite_func(in0)\n\n            # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n            out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n            tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton operator code consists of a kernel function, `triton_f4_to_bf16_kernel`, and a helper function, `_fp4_packed_to_bf16`. The main purpose of this code is to convert packed FP4 values to BF16 values efficiently on CUDA devices using Triton.\n\n        1. `_fp4_packed_to_bf16`: This function unpacks the FP4 values and converts them to BF16 format. It handles special cases like zero and the denormal value (0.5). It extracts the sign, exponent, and mantissa of the FP4 representation and maps them to a format compatible with BF16.\n\n        2. `triton_f4_to_bf16_kernel`: This is the main Triton kernel that utilizes the `_fp4_packed_to_bf16` helper function. It takes as input pointers to the packed FP4 data (`x_ptr`) and outputs BF16 data (`output_ptr`). The kernel divides the data into blocks and processes each block in parallel.\n\n        The input to the Triton kernel includes various parameters required for the conversion process, such as bit masks and biases. It operates block-wise to take advantage of Triton's parallelism on CUDA GPUs. The number of elements to process and block size are calculated based on the input tensor size.\n\n        The `triton_f4_to_bf16` function is a Python wrapper around the Triton kernel. It ensures that the input tensor is properly shaped and allocated on the CUDA device and computes the grid size for launching the kernel. Finally, it calls the kernel with the necessary parameters to perform the conversion.\n\n        Important Constants:\n        - `SIGN_MASK_F4`, `MANTISSA_MASK_F4`: Masks to extract the sign and mantissa from FP4 values.\n        - `ZERO_BITS_F32`, `ZERO_POINT_FIVE_BITS_F32`: Representations of zero and 0.5 in FP32 format.\n        - `F4_E2M1_EXP_BIAS`, `F32_EXP_BIAS`: Exponent biases for FP4 and FP32.\n\n        Overall, this code efficiently converts packed FP4 data to BF16 on GPUs using the Triton language by leveraging the parallel computing capabilities of CUDA.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nSIGN_MASK_F4 = 0x8  # Example value for FP4 sign mask\nMANTISSA_MASK_F4 = 0x3  # Example value for FP4 mantissa mask\nZERO_BITS_F32 = 0x00000000  # FP32 representation of zero\nZERO_POINT_FIVE_BITS_F32 = 0x3F000000  # FP32 representation of 0.5\nF4_E2M1_EXP_BIAS = 7  # Example exponent bias for FP4\nF32_EXP_BIAS = 127  # Exponent bias for FP32\n\n@triton.jit\ndef triton_f4_to_bf16_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load FP4 packed data\n    fp4_packed = tl.load(x_ptr + offsets, mask=offsets < n_elements, other=0)\n\n    # Convert FP4 to BF16\n    bf16_values = _fp4_packed_to_bf16(fp4_packed)\n\n    # Store the result\n    tl.store(output_ptr + offsets, bf16_values, mask=offsets < n_elements)\n\n@triton.jit\ndef _fp4_packed_to_bf16(fp4_packed):\n    # Extract sign, exponent, and mantissa\n    sign = (fp4_packed & SIGN_MASK_F4) << 28  # Shift sign to BF16 position\n    exponent = ((fp4_packed >> 1) & 0x3)  # Extract exponent bits\n    mantissa = (fp4_packed & MANTISSA_MASK_F4) << 23  # Align mantissa\n\n    # Handle special cases\n    is_zero = exponent == 0\n    is_denormal = exponent == 1\n\n    # Convert to BF16 format\n    exponent = (exponent + F32_EXP_BIAS - F4_E2M1_EXP_BIAS) << 23\n    bf16_value = sign | exponent | mantissa\n\n    # Handle zero and denormal\n    bf16_value = tl.where(is_zero, ZERO_BITS_F32, bf16_value)\n    bf16_value = tl.where(is_denormal, ZERO_POINT_FIVE_BITS_F32, bf16_value)\n\n    return bf16_value\n\ndef triton_f4_to_bf16(x, device):\n    # Ensure input is on the correct device\n    x = x.to(device)\n\n    # Allocate output tensor\n    output = torch.empty_like(x, dtype=torch.bfloat16, device=device)\n\n    # Determine grid and block sizes\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024  # Example block size\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the Triton kernel\n    triton_f4_to_bf16_kernel[grid](x, output, n_elements, BLOCK_SIZE)\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fp4_packed_to_bf16(\n    x_packed,\n    sign_mask_f4,\n    mantissa_mask_f4,\n    mbits_f4_e2m1,\n    ebits_f4_e2m1,\n    f4_e2m1_exp_bias,\n    mbits_f32,\n    ebits_f32,\n    f32_exp_bias,\n    zero_bits_f32,\n    zero_point_five_bits_f32,\n):\n    \"\"\"\n    Input: a tensor of packed fp4 values\n    Output: a tensor of bfloat16 values\n    \"\"\"\n\n    # low-bits: original location 0:3\n    # high-bits: original location 4:7\n    x_low_bits = x_packed >> 4\n    x_high_bits = x_packed & 0xF\n    x = tl.interleave(x_low_bits, x_high_bits)\n\n    # cast logic below\n    # output = x_unpacked.to(tl.float32)\n\n    # save the sign\n    sign_f4 = x & sign_mask_f4\n\n    # set everything to positive, will add sign back at the end\n    x_pos = x ^ sign_f4\n\n    # Special case zero\n    zero_mask = x_pos == 0\n\n    # There is only one denormal value in fp4: s001, which is 0.5 in f32\n    # Special case it.\n    # TODO(later): will it be faster to repeat this for all 8 positive\n    # values instead of the bit manipulations?\n    denormal_mask = x_pos == 1\n\n    # calculate the new exponent and shift it to bits 2:9 of the result\n    exp_biased_f4 = x_pos >> mbits_f4_e2m1\n    exp_biased_f32 = exp_biased_f4 - f4_e2m1_exp_bias + f32_exp_bias\n    exp_biased_f32 = exp_biased_f32.to(tl.int32) << mbits_f32\n\n    # shift the mantissa to bits 10:32 of the result\n    mantissa_f4 = x_pos & mantissa_mask_f4\n    mantissa_f32 = mantissa_f4.to(tl.int32) << (mbits_f32 - mbits_f4_e2m1)\n    output = mantissa_f32\n\n    # combine the pieces\n    result = exp_biased_f32 | mantissa_f32\n    # result[zero_mask] = ZERO_BITS_F32\n    result = tl.where(zero_mask, zero_bits_f32, result)\n    # result[denormal_mask] = ZERO_POINT_FIVE_BITS_F32\n    result = tl.where(denormal_mask, zero_point_five_bits_f32, result)\n\n    # add sign back\n    sign_f32 = sign_f4.to(tl.int32) << (\n        mbits_f32 - mbits_f4_e2m1 + ebits_f32 - ebits_f4_e2m1\n    )\n    result = result | sign_f32\n\n    # The bit shifting above is for float32, so for now we\n    # bitcast to float32 and then regular cast to bfloat16\n    # TODO(later): it should be pretty easy to cast directly to bf16, just\n    # need to adjust the mbits/ebits/special values. Perf impact is likely\n    # to be small as we would not be chaning memory access patterns.\n    output = result.to(tl.float32, bitcast=True)\n    output = output.to(tl.bfloat16)\n    return output\n\n@triton.jit\ndef triton_f4_to_bf16_kernel(\n    x_ptr,\n    output_ptr,\n    n_elements_in,\n    sign_mask_f4: tl.constexpr,\n    mantissa_mask_f4: tl.constexpr,\n    mbits_f4_e2m1: tl.constexpr,\n    ebits_f4_e2m1: tl.constexpr,\n    f4_e2m1_exp_bias: tl.constexpr,\n    mbits_f32: tl.constexpr,\n    ebits_f32: tl.constexpr,\n    f32_exp_bias: tl.constexpr,\n    zero_bits_f32: tl.constexpr,\n    zero_point_five_bits_f32: tl.constexpr,\n    BLOCK_SIZE_IN: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n\n    mask_in = offsets_in < n_elements_in\n\n    # packed uint8\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(\n        x_packed,\n        sign_mask_f4,\n        mantissa_mask_f4,\n        mbits_f4_e2m1,\n        ebits_f4_e2m1,\n        f4_e2m1_exp_bias,\n        mbits_f32,\n        ebits_f32,\n        f32_exp_bias,\n        zero_bits_f32,\n        zero_point_five_bits_f32,\n    )\n\n    # set up output offsets\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)\n\nEBITS_F32, MBITS_F32 = 8, 23\nEBITS_F4_E2M1, MBITS_F4_E2M1 = 2, 1\nEBITS_F6_E2M3, MBITS_F6_E2M3 = 2, 3\nEBITS_F6_E3M2, MBITS_F6_E3M2 = 3, 2\n\nSIGN_MASK_F4 = 0x8  # 1000\nMANTISSA_MASK_F4 = 0x1  # 0001\n\nZERO_BITS_F32 = 0x0\nZERO_POINT_FIVE_BITS_F32 = 0x3F000000\nF4_E2M1_EXP_BIAS = 1\nF32_EXP_BIAS = 127\n\ndef triton_f4_to_bf16(x: torch.Tensor):\n    \"\"\"\n    Input: a tensor of packed fp4 values\n    Output: a tensor of bfloat16 values\n\n    Note: this function is only used in testing, so we can test\n      the numerical correctness of the cast without the scaling.\n    \"\"\"\n    new_shape = (*x.shape[:-1], x.shape[-1] * 2)\n    output = torch.empty(*new_shape, device=x.device, dtype=torch.bfloat16)\n    assert x.is_contiguous()\n    assert x.is_cuda and output.is_cuda\n    n_elements_in = x.numel()\n    grid = lambda meta: (  # noqa: E731\n        triton.cdiv(n_elements_in, meta[\"BLOCK_SIZE_IN\"]),\n    )  # noqa: E731,E501\n    triton_f4_to_bf16_kernel[grid](\n        x,\n        output,\n        n_elements_in,\n        sign_mask_f4=SIGN_MASK_F4,\n        mantissa_mask_f4=MANTISSA_MASK_F4,\n        mbits_f4_e2m1=MBITS_F4_E2M1,\n        ebits_f4_e2m1=EBITS_F4_E2M1,\n        f4_e2m1_exp_bias=F4_E2M1_EXP_BIAS,\n        mbits_f32=MBITS_F32,\n        ebits_f32=EBITS_F32,\n        f32_exp_bias=F32_EXP_BIAS,\n        zero_bits_f32=ZERO_BITS_F32,\n        zero_point_five_bits_f32=ZERO_POINT_FIVE_BITS_F32,\n        BLOCK_SIZE_IN=512,\n    )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `_triton_rope` performs a rotation-based positional encoding transformation on query (`q_ptr`) and key (`k_ptr`) matrices using cosine and sine values. It is part of a mechanism for rotational position embeddings, commonly used in transformer architectures. The operation is configurable for forward or backward passes through the `BACKWARD_PASS` parameter. The input matrices are divided into \"tiles\" that are rotated by applying trigonometric transformations. The `rope_backward` function sets up this kernel for a backward pass, transposing inputs and handling memory strides. It returns the transformed query and key gradients in their original shapes.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr, k_ptr, cos_ptr, sin_ptr, \n    output_q_ptr, output_k_ptr,\n    stride_qm, stride_ql, stride_km, stride_kl,\n    stride_om, stride_ol, stride_okm, stride_okl,\n    N, M, D, BACKWARD_PASS: tl.constexpr\n):\n    # Define block size\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_D = 64\n    \n    # Calculate block indices\n    m_block = tl.program_id(0)\n    d_block = tl.program_id(1)\n    \n    # Calculate offsets\n    m_offset = m_block * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    d_offset = d_block * BLOCK_SIZE_D + tl.arange(0, BLOCK_SIZE_D)\n    \n    # Mask to handle out-of-bounds accesses\n    mask_m = m_offset < M\n    mask_d = d_offset < D\n    \n    # Load data from global memory\n    q = tl.load(q_ptr + m_offset[:, None] * stride_qm + d_offset[None, :] * stride_ql, mask=mask_m[:, None] & mask_d[None, :], other=0.0)\n    k = tl.load(k_ptr + m_offset[:, None] * stride_km + d_offset[None, :] * stride_kl, mask=mask_m[:, None] & mask_d[None, :], other=0.0)\n    \n    # Load cosine and sine values\n    cos = tl.load(cos_ptr + d_offset, mask=mask_d, other=1.0)\n    sin = tl.load(sin_ptr + d_offset, mask=mask_d, other=0.0)\n    \n    # Apply rotation-based positional encoding\n    if BACKWARD_PASS:\n        q_rot = q * cos[None, :] - k * sin[None, :]\n        k_rot = k * cos[None, :] + q * sin[None, :]\n    else:\n        q_rot = q * cos[None, :] + k * sin[None, :]\n        k_rot = k * cos[None, :] - q * sin[None, :]\n    \n    # Store results back to global memory\n    tl.store(output_q_ptr + m_offset[:, None] * stride_om + d_offset[None, :] * stride_ol, q_rot, mask=mask_m[:, None] & mask_d[None, :])\n    tl.store(output_k_ptr + m_offset[:, None] * stride_okm + d_offset[None, :] * stride_okl, k_rot, mask=mask_m[:, None] & mask_d[None, :])\n\ndef rope_backward(q, k, cos, sin, stride_q, stride_k, stride_cos, stride_sin):\n    # Get dimensions\n    M, D = q.shape\n    N = k.shape[0]\n    \n    # Prepare output buffers\n    output_q = torch.empty_like(q)\n    output_k = torch.empty_like(k)\n    \n    # Launch Triton kernel\n    grid = (triton.cdiv(M, 128), triton.cdiv(D, 64))\n    _triton_rope[grid](\n        q, k, cos, sin,\n        output_q, output_k,\n        stride_q[0], stride_q[1],\n        stride_k[0], stride_k[1],\n        output_q.stride(0), output_q.stride(1),\n        output_k.stride(0), output_k.stride(1),\n        N, M, D, BACKWARD_PASS=True\n    )\n    \n    return output_q, output_k\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_rope(\n    q_ptr,\n    q_row_stride,\n    k_ptr,\n    k_row_stride,\n    cos,\n    cos_row_stride,\n    sin,\n    sin_row_stride,\n    sl,\n    bs: tl.constexpr,\n    n_qh: tl.constexpr,\n    n_kh: tl.constexpr,\n    hd: tl.constexpr,\n    pad_n_qh: tl.constexpr,\n    pad_n_kh: tl.constexpr,\n    pad_hd: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    BACKWARD_PASS: tl.constexpr = False,\n):\n    pid = tl.program_id(0)\n\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n\n    cos_row_idx = pid % (sl)\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0, pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0).to(sin_row.dtype)\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0).to(sin_row.dtype)\n\n    second_half_q_offsets = first_half_q_offsets + (hd // 2)\n    second_half_k_offsets = first_half_k_offsets + (hd // 2)\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask, other=0).to(sin_row.dtype)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask, other=0).to(sin_row.dtype)\n\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=second_q_mask)\n\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=second_k_mask)\n\ndef rope_backward(dq, dk, cos, sin):\n    dq = dq.transpose(1, 2)\n    dk = dk.transpose(1, 2)\n\n    batch_size, seq_len, n_q_head, head_dim = dq.shape\n    n_kv_head = dk.shape[2]\n    pad_hd = triton.next_power_of_2(head_dim)\n    pad_n_q_head = triton.next_power_of_2(n_q_head)\n    pad_n_kv_head = triton.next_power_of_2(n_kv_head)\n    BLOCK_SIZE = max(pad_n_q_head, pad_n_kv_head)\n\n    n_row = batch_size * seq_len\n\n    dq = dq.contiguous()\n    dk = dk.contiguous()\n\n    _triton_rope[(n_row,)](\n        dq,\n        dq.stride(1),\n        dk,\n        dk.stride(1),\n        cos,\n        cos.stride(-2),\n        sin,\n        sin.stride(-2),\n        seq_len,\n        batch_size,\n        n_q_head,\n        n_kv_head,\n        head_dim,\n        pad_n_q_head,\n        pad_n_kv_head,\n        pad_hd,\n        BLOCK_SIZE=BLOCK_SIZE,\n        BACKWARD_PASS=True,\n    )\n    return dq.transpose(1, 2), dk.transpose(1, 2)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `_sgmv_expand_slice_kernel` and a wrapper function `_sgmv_expand_slice`. \n            The kernel performs a batched sparse matrix-vector multiplication (SGMV) with some additional features. \n            It uses input tensors and LoRA weights to compute an output tensor. \n            The kernel uses Triton's parallelism capabilities by launching a grid of programs, each processing a block of the input tensor.\n            The `_sgmv_expand_slice` function prepares the data, determines block sizes, and launches the Triton kernel with the appropriate parameters.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _sgmv_expand_slice_kernel(\n    # Pointers to data\n    data_ptr, indices_ptr, indptr_ptr, lora_weights_ptr, output_ptr,\n    # Sizes\n    batch_size, num_rows, num_cols,\n    # LoRA configuration\n    lora_rank,\n    # Block sizes\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    # Program ID for parallel execution\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the start of the block for this program\n    row_start = pid_m * BLOCK_SIZE_M\n    col_start = pid_n * BLOCK_SIZE_N\n\n    # Loop over the batch dimension\n    for batch in range(batch_size):\n        # Loop over rows in the block\n        for row in range(row_start, min(row_start + BLOCK_SIZE_M, num_rows)):\n            # Initialize the output for this row\n            output_val = 0.0\n\n            # Get the range of non-zero elements for this row\n            start_idx = tl.load(indptr_ptr + row)\n            end_idx = tl.load(indptr_ptr + row + 1)\n\n            # Loop over non-zero elements in this row\n            for idx in range(start_idx, end_idx):\n                col = tl.load(indices_ptr + idx)\n                if col_start <= col < col_start + BLOCK_SIZE_N:\n                    # Load the value and the corresponding vector element\n                    value = tl.load(data_ptr + idx)\n                    vec_val = tl.load(lora_weights_ptr + batch * num_cols + col)\n\n                    # Perform the multiplication and accumulate\n                    output_val += value * vec_val\n\n            # Store the result in the output tensor\n            tl.store(output_ptr + batch * num_rows + row, output_val)\n\n### Wrapper Function\n\ndef _sgmv_expand_slice(data, indices, indptr, lora_weights, batch_size, num_rows, num_cols, lora_rank):\n    # Allocate output tensor\n    output = torch.zeros((batch_size, num_rows), device=data.device, dtype=data.dtype)\n\n    # Define block sizes\n    BLOCK_SIZE_M = 128  # Adjust based on hardware capabilities\n    BLOCK_SIZE_N = 128  # Adjust based on hardware capabilities\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(num_rows, BLOCK_SIZE_M), triton.cdiv(num_cols, BLOCK_SIZE_N))\n    _sgmv_expand_slice_kernel[grid](\n        data_ptr=data,\n        indices_ptr=indices,\n        indptr_ptr=indptr,\n        lora_weights_ptr=lora_weights,\n        output_ptr=output,\n        batch_size=batch_size,\n        num_rows=num_rows,\n        num_cols=num_cols,\n        lora_rank=lora_rank,\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N\n    )\n\n    return output\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _sgmv_expand_slice_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    b_seq_start_loc,\n    seq_lens,\n    lora_indices,\n    xm_stride,\n    xk_stride,  # 1\n    l0_stride,  # hidden_size*max_rank\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    slice_offset,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ADD_INPUTS: tl.constexpr,\n    CAST_TYPE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    cta_n_num = tl.cdiv(N, BLOCK_N)\n    pid_m = pid // cta_n_num\n    pid_n = pid % cta_n_num\n    M = tl.load(seq_lens + cur_batch)\n    if pid_m * BLOCK_M > M:\n        return\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n    cur_seq_start = tl.load(b_seq_start_loc + cur_batch)\n    offset_m = tl.arange(0, BLOCK_M) + pid_m * BLOCK_M\n    offset_n = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N\n    offset_k = tl.arange(0, BLOCK_K)\n    ram = tl.max_contiguous(tl.multiple_of(offset_m % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(offset_n % N, BLOCK_N), BLOCK_N)\n\n    a_ptr = (input_ptr + cur_seq_start * xm_stride + ram[:, None] * xm_stride +\n             offset_k[None, :] * xk_stride, )\n    b_ptr = (lora_ptr + l0_stride * lora_index +\n             offset_k[:, None] * lora_n_stride + rbn[None, :] * lora_k_stride)\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(tl.cdiv(K, BLOCK_K)):\n        if EVEN_K:\n            tiled_a = tl.load(a_ptr)\n            tiled_b = tl.load(b_ptr)\n        else:\n            tiled_a = tl.load(a_ptr,\n                              mask=offset_k[None, :] < K - k * BLOCK_K,\n                              other=0)\n            tiled_b = tl.load(b_ptr,\n                              mask=offset_k[:, None] < K - k * BLOCK_K,\n                              other=0)\n        if CAST_TYPE:\n            tiled_a = tiled_a.to(lora_ptr.dtype.element_ty)\n        accumulator += tl.dot(\n            tiled_a,\n            tiled_b,\n        )\n        a_ptr += BLOCK_K * xk_stride\n        b_ptr += BLOCK_K * lora_n_stride\n    tiled_c = accumulator.to(lora_ptr.dtype.element_ty)\n    offset_cm = cur_seq_start + tl.arange(0, BLOCK_M) + pid_m * BLOCK_M\n    offset_cn = tl.arange(0, BLOCK_N) + pid_n * BLOCK_N + slice_offset\n    c_ptr = (out_ptr + offset_cm[:, None] * cm_stride +\n             offset_cn[None, :] * cn_stride)\n    M = tl.load(seq_lens + cur_batch)\n    c_mask = (offset_cm[:, None] < (cur_seq_start + M)) & (offset_cn[None, :] <\n                                                           (slice_offset + N))\n    if ADD_INPUTS:\n        tiled_out = tl.load(c_ptr, mask=c_mask)\n        tiled_c += tiled_out\n    tl.store(c_ptr, tiled_c, mask=c_mask)\n\n\n@torch.inference_mode()\ndef _sgmv_expand_slice(\n    inputs: torch.Tensor,\n    lora_b_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    b_seq_start_loc: torch.Tensor,\n    seq_len_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    batches: int,\n    max_seq_length: int,\n    token_nums: int,\n    slice_offset: int,\n    slice_size: int,\n    add_inputs: bool = False,\n) -> None:\n\n    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]\n    assert lora_b_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(0) == token_nums\n    assert inputs.size(1) == lora_b_weights.size(-1)\n    assert b_seq_start_loc.size(0) == batches\n    assert lora_indices_tensor.size(0) == batches\n    assert slice_size == lora_b_weights.size(-2)\n    assert inputs.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    if lora_b_weights.ndim == 4:  # shape:(lora_num,1,size,rank)\n        assert lora_b_weights.size(1) == 1\n        lora_b_weights = lora_b_weights.squeeze(dim=1)\n    else:\n        assert lora_b_weights.ndim == 3  # shape:(lora_num,size,rank)\n\n    assert lora_b_weights.is_contiguous()\n\n    N, K = lora_b_weights.shape[-2:]\n\n    BLOCK_M = 32\n    BLOCK_N = 32\n    BLOCK_K = 16\n    EVEN_K = K % BLOCK_K == 0\n    ADD_INPUTS = add_inputs\n    CAST_TYPE = False\n    if inputs.dtype == torch.float32 and lora_b_weights.dtype in [\n            torch.float16,\n            torch.bfloat16,\n    ]:\n        CAST_TYPE = True\n    grid = (\n        triton.cdiv(max_seq_length, BLOCK_M) * triton.cdiv(N, BLOCK_N),\n        batches,\n    )\n    _sgmv_expand_slice_kernel[grid](\n        inputs,\n        lora_b_weights,\n        output_tensor,\n        N,\n        K,\n        b_seq_start_loc,\n        seq_len_tensor,\n        lora_indices_tensor,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_b_weights.stride(0),\n        lora_b_weights.stride(1),\n        lora_b_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        slice_offset,\n        BLOCK_M,\n        BLOCK_N,\n        BLOCK_K,\n        EVEN_K,\n        ADD_INPUTS,\n        CAST_TYPE,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton function embedding_kernel is designed to efficiently compute embeddings by loading values from the embedding weight matrix based on input token IDs. It takes input arguments including weight, input_ids, and various dimensions and constants to control the computation, and stores the resulting vectors in the 'out' tensor. The kernel iterates over a sequence of input IDs, computes valid token masks, and loads relevant vectors from the embedding weights, storing them in the output tensor based on defined strides. The embedding function orchestrates the kernel launch, defining grid dimensions and constants based on input tensor shapes and characteristics.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight_ptr, input_ids_ptr, out_ptr,\n    num_embeddings, embedding_dim,\n    stride_w, stride_o, num_ids,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Block index\n    pid = tl.program_id(0)\n\n    # Calculate the start position for this block\n    start = pid * BLOCK_SIZE\n\n    # Loop over each token ID in the block\n    for i in range(BLOCK_SIZE):\n        # Calculate the index for this particular thread\n        idx = start + i\n\n        # Check if this index is within the range of input IDs\n        if idx < num_ids:\n            # Load the token ID\n            token_id = tl.load(input_ids_ptr + idx)\n\n            # Check if the token ID is valid\n            if token_id < num_embeddings:\n                # Calculate the position in the weight matrix\n                offset_w = token_id * stride_w\n\n                # Calculate the position in the output matrix\n                offset_o = idx * stride_o\n\n                # Load the embedding vector and store it in the output\n                for j in range(embedding_dim):\n                    embedding_val = tl.load(weight_ptr + offset_w + j)\n                    tl.store(out_ptr + offset_o + j, embedding_val)\n\ndef embedding(weight, input_ids, out, BLOCK_SIZE=128):\n    # Get dimensions and strides\n    num_ids = input_ids.shape[0]\n    num_embeddings, embedding_dim = weight.shape\n    stride_w = weight.stride(0)\n    stride_o = out.stride(0)\n\n    # Define grid size\n    grid = (triton.cdiv(num_ids, BLOCK_SIZE),)\n\n    # Launch the kernel\n    embedding_kernel[grid](\n        weight, input_ids, out,\n        num_embeddings, embedding_dim,\n        stride_w, stride_o, num_ids,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage\n# weight = torch.randn(num_embeddings, embedding_dim, device='cuda')\n# input_ids = torch.randint(0, num_embeddings, (num_ids,), device='cuda')\n# out = torch.empty(num_ids, embedding_dim, device='cuda')\n# embedding(weight, input_ids, out)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    start_n = tl.program_id(0) * BLOCK_N\n\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq + offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :], vecs, mask=store_mask)\n\n@torch.no_grad()\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    BLOCK_N = 64\n    BLOCK_NN = 1\n    BLOCK_DMODEL = triton.next_power_of_2(weight.shape[1])\n    n_ctx = input_ids.shape[0]\n\n    grid = (triton.cdiv(n_ctx, BLOCK_N), 1, 1)\n\n    embedding_kernel[grid](\n        weight,\n        input_ids,\n        out,\n        vob_start_id,\n        vob_end_id,\n        weight.stride(0),\n        out.stride(0),\n        n_ctx=n_ctx,\n        hiden_size=weight.shape[1],\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK_N,\n        BLOCK_NN=BLOCK_NN,\n        num_warps=1,\n        num_stages=1,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel `uniform_kernel` generates uniform random numbers within a specified range `[from_, to)`. It uses a Philox pseudo-random number generator to create random numbers efficiently on GPU. The input arguments are `out_ptr` (output pointer), `N` (number of elements), `philox_seed` and `philox_offset` (for random number generation), `from_` and `to` (defining the range), and `BLOCK` (block size for execution). The kernel computes random numbers in batches of four and writes them to memory in a loop, ensuring proper range conversion and memory alignment.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef uniform_kernel(out_ptr, N, philox_seed, philox_offset, from_, to, BLOCK: tl.constexpr):\n    # Calculate the number of blocks\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK * 4  # Processing in batches of 4\n    offsets = block_start + tl.arange(0, BLOCK * 4)\n\n    # Ensure we do not exceed N\n    mask = offsets < N\n\n    # Initialize Philox RNG state\n    key = philox_seed\n    counter = philox_offset + offsets // 4  # Each counter generates 4 random numbers\n\n    # Generate random numbers using Philox\n    random_values = tl.philox_4x32(counter, key)\n\n    # Normalize and scale to the desired range\n    scale = (to - from_) / (1 << 32)\n    offset = from_\n\n    # Compute the uniform random numbers\n    uniform_randoms = random_values.to(tl.float32) * scale + offset\n\n    # Write the results to memory\n    tl.store(out_ptr + offsets, uniform_randoms, mask=mask)\n\ndef generate_uniform_randoms(out_ptr, N, philox_seed, philox_offset, from_, to, BLOCK=256):\n    # Calculate the number of blocks needed\n    num_blocks = (N + BLOCK * 4 - 1) // (BLOCK * 4)\n\n    # Launch the kernel\n    uniform_kernel[(num_blocks,)](out_ptr, N, philox_seed, philox_offset, from_, to, BLOCK)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\nimport functools\nimport operator\nfrom typing import Tuple\n\nShape = Tuple[int]\n\n\ntry:\n    uint_to_uniform_float = tl.uint_to_uniform_float\nexcept AttributeError:\n    # Copied from triton.language package for compatibility\n    @triton.jit\n    def uint_to_uniform_float(x):\n        \"\"\"\n        Numerically stable function to convert a random uint into a random float uniformly sampled in [0, 1).\n        \"\"\"\n        # TODO: fix frontend issues and cleanup\n        # conditions can be simplified\n        # scale is ((2**23 - 1) / 2**23) * 2**(N_BITS - 1)\n        if tl.constexpr(x.dtype == tl.uint32) or tl.constexpr(x.dtype == tl.int32):\n            # maximum value such that `MAX_INT * scale < 1.0` (with float rounding)\n            x = x.to(tl.int32, bitcast=True)\n            scale = 4.6566127342e-10\n        else:\n            tl.static_assert(\n                tl.constexpr(x.dtype == tl.uint64) or tl.constexpr(x.dtype == tl.int64)\n            )\n            x = x.to(tl.int64, bitcast=True)\n            scale = 1.0842020432385337e-19\n        x = tl.where(x < 0, -x - 1, x)\n        return x * scale\n\n\ndef philox_cuda_seed_offset(increment, device=None):\n    device = device or torch.cuda.current_device()\n    gen = torch.cuda.default_generators[device]\n    state_copy = gen.get_state()\n    c0, c1 = state_copy.view(torch.int64)\n    seed, offset = int(c0), int(c1)\n    increment = (increment + 3) // 4 * 4\n    c1 += increment\n    # get_state returns a new tensor, so it needs set_state to update the actual generator state.\n    gen.set_state(state_copy)\n    return seed, offset\n\n\ndef heur_block(args):\n    if args[\"N\"] <= 512:\n        return 512\n    else:\n        return 1024\n\n\ndef heur_num_warps(args):\n    if args[\"N\"] <= 512:\n        return 4\n    elif args[\"N\"] <= 1024:\n        return 8\n    else:\n        return 16\n\n\n@triton.heuristics(\n    {\n        \"BLOCK\": heur_block,\n        \"num_warps\": heur_num_warps,\n    }\n)\n@triton.jit(do_not_specialize=[\"philox_seed\", \"philox_offset\"])\ndef uniform_kernel(\n    out_ptr,\n    N,\n    philox_seed,\n    philox_offset,\n    from_,\n    to,\n    BLOCK: tl.constexpr,\n):\n    philox_seed = philox_seed.to(tl.int64)\n    philox_offset = philox_offset.to(tl.int64)\n    c0 = (philox_offset & 0xFFFFFFFF).to(tl.uint32)\n    c1 = ((philox_offset >> 32) & 0xFFFFFFFF).to(tl.uint32)\n    i4 = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n    c0 += i4\n    _O = c0 * 0\n    r0, r1, r2, r3 = tl.philox(philox_seed, c0, c1, _O, _O)\n    r0 = uint_to_uniform_float(r0) * (to - from_) + from_\n    r1 = uint_to_uniform_float(r1) * (to - from_) + from_\n    r2 = uint_to_uniform_float(r2) * (to - from_) + from_\n    r3 = uint_to_uniform_float(r3) * (to - from_) + from_\n    off_0 = tl.program_id(0) * BLOCK * 4 + tl.arange(0, BLOCK)\n    off_1 = off_0 + BLOCK\n    off_2 = off_1 + BLOCK\n    off_3 = off_2 + BLOCK\n    tl.store(out_ptr + off_0, r0, mask=off_0 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_1, r1, mask=off_1 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_2, r2, mask=off_2 < N, eviction_policy=\"evict_first\")\n    tl.store(out_ptr + off_3, r3, mask=off_3 < N, eviction_policy=\"evict_first\")\n\n\n\ndef volume(shape: Shape) -> int:\n    return functools.reduce(operator.mul, shape, 1)\n\n\nUNROLL = 4\n\n\ndef uniform_(self, from_=0.0, to=1.0, *, generator=None):\n    logging.debug(\"GEMS UNIFORM\")\n    N = volume(self.shape)\n    grid_fn = lambda meta: (triton.cdiv(N, meta[\"BLOCK\"] * UNROLL),)\n\n    increment = triton.cdiv(N, UNROLL)\n    philox_seed, philox_offset = philox_cuda_seed_offset(increment)\n    with torch.cuda.device(self.device):\n        uniform_kernel[grid_fn](self, N, philox_seed, philox_offset, from_, to)\n    return self\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel, `_bgmv_expand_slice_kernel`, performs a batched Generalized Matrix-Vector Multiply (GroupGEMV) operation, which is optimized for large hidden sizes by splitting the operation across multiple blocks. The kernel reads from an input tensor and a LoRA (Low-Rank Adaptation) weight tensor, performs matrix-vector multiplication, and writes the result to an output tensor. It supports type casting and accumulation with existing outputs. The main function, `_bgmv_expand_slice`, is a wrapper that sets up the kernel with parameters like input and output tensors, LoRA weight tensors, and configuration options.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_slice_kernel(\n    input_ptr,       # Pointer to the input tensor\n    weight_ptr,      # Pointer to the LoRA weight tensor\n    output_ptr,      # Pointer to the output tensor\n    batch_size,      # Number of batches\n    hidden_size,     # Size of the hidden layer\n    output_size,     # Size of the output layer\n    BLOCK_SIZE: tl.constexpr,  # Block size for tiling\n    ACCUMULATE: tl.constexpr   # Whether to accumulate with existing output\n):\n    # Calculate the program index\n    pid = tl.program_id(axis=0)\n\n    # Determine the block start for the batch and hidden dimensions\n    batch_start = pid // (hidden_size // BLOCK_SIZE)\n    hidden_start = (pid % (hidden_size // BLOCK_SIZE)) * BLOCK_SIZE\n\n    # Create pointers for the input and weight blocks\n    input_block_ptr = input_ptr + batch_start * hidden_size + hidden_start\n    weight_block_ptr = weight_ptr + hidden_start * output_size\n\n    # Create a pointer for the output block\n    output_block_ptr = output_ptr + batch_start * output_size\n\n    # Load input and weight blocks\n    input_block = tl.load(input_block_ptr, mask=hidden_start + tl.arange(0, BLOCK_SIZE) < hidden_size)\n    weight_block = tl.load(weight_block_ptr)\n\n    # Perform matrix-vector multiplication\n    result = tl.dot(input_block, weight_block)\n\n    # Accumulate with existing output if needed\n    if ACCUMULATE:\n        existing_output = tl.load(output_block_ptr)\n        result += existing_output\n\n    # Store the result\n    tl.store(output_block_ptr, result)\n\n### Wrapper Function: `_bgmv_expand_slice`\n\nThe wrapper function sets up the kernel with the appropriate parameters and launches it.\n\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_slice_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    lora_indices,\n    xm_stride,\n    xk_stride,\n    l0_stride,\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    slice_offset,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ADD_INPUTS: tl.constexpr,\n    CAST_TYPE: tl.constexpr,\n):\n    \"\"\"\n    GroupGEMV, introducing SPLIT_N can improve large hidden_size's performance\n    \"\"\"\n    pid_sn = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n    offset_k = tl.arange(0, BLOCK_K)\n    offset_n = tl.arange(0, BLOCK_N)\n    if EVEN_K:\n        tiled_a = tl.load(input_ptr + cur_batch * xm_stride +\n                          offset_k * xk_stride)\n    else:\n        tiled_a = tl.load(\n            input_ptr + cur_batch * xm_stride + offset_k * xk_stride,\n            mask=offset_k < K,\n            other=0,\n        )\n    split_n_length = tl.cdiv(N, SPLIT_N)\n    if CAST_TYPE:\n        tiled_a = tiled_a.to(lora_ptr.dtype.element_ty)\n    b_ptr = (lora_ptr + l0_stride * lora_index +\n             pid_sn * split_n_length * lora_k_stride)\n    c_ptr = (out_ptr + cur_batch * cm_stride + pid_sn * split_n_length +\n             slice_offset * cn_stride)\n\n    for n in range(0, split_n_length, BLOCK_N):\n        current_n = n + offset_n\n        b_ptr_mask = (current_n[:, None] < split_n_length) & (offset_k[None, :]\n                                                              < K)\n        c_mask = current_n < split_n_length\n        tiled_b = tl.load(\n            b_ptr + current_n[:, None] * lora_k_stride +\n            offset_k[None, :] * lora_n_stride,\n            mask=b_ptr_mask,\n            other=0.0,\n        )\n        if ADD_INPUTS:\n            tiled_out = tl.load(c_ptr + current_n * cn_stride, mask=c_mask)\n            accumulator = tl.sum(tiled_a * tiled_b, 1) + tiled_out\n        else:\n            accumulator = tl.sum(tiled_a * tiled_b, 1)\n        tl.store(c_ptr + current_n * cn_stride, accumulator, mask=c_mask)\n\n@torch.inference_mode()\ndef _bgmv_expand_slice(\n    inputs: torch.Tensor,\n    lora_b_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    slice_offset: int,\n    slice_size: int,\n    add_inputs: bool = True,\n) -> None:\n    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]\n    assert lora_b_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(1) == lora_b_weights.size(-1)\n\n    assert slice_size == lora_b_weights.size(-2)\n    assert inputs.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    if lora_b_weights.ndim == 4:\n        assert lora_b_weights.size(1) == 1\n        lora_b_weights = lora_b_weights.squeeze(dim=1)\n    else:\n        assert lora_b_weights.ndim == 3\n\n    assert lora_b_weights.is_contiguous()\n\n    N, K = lora_b_weights.shape[-2:]\n    BLOCK_K = triton.next_power_of_2(K)\n    EVEN_K = K % BLOCK_K == 0\n    ADD_INPUTS = add_inputs\n    CAST_TYPE = False\n    if inputs.dtype == torch.float32 and lora_b_weights.dtype in [\n            torch.float16,\n            torch.bfloat16,\n    ]:\n        CAST_TYPE = True\n\n    batches = lora_indices_tensor.size(0)\n\n    # config = get_lora_op_configs(\"expand\", batches, N)\n\n    grid = lambda META: (\n        META[\"SPLIT_N\"],\n        batches,\n    )\n    _bgmv_expand_slice_kernel[grid](\n        inputs,\n        lora_b_weights,\n        output_tensor,\n        N,\n        K,\n        lora_indices_tensor,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_b_weights.stride(0),\n        lora_b_weights.stride(1),\n        lora_b_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        slice_offset,\n        BLOCK_K=BLOCK_K,\n        BLOCK_N=256,\n        SPLIT_N=lora_b_weights.shape[-2:][0],\n        EVEN_K=EVEN_K,\n        ADD_INPUTS=ADD_INPUTS,\n        CAST_TYPE=CAST_TYPE,\n        # **config,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton-based kernel implements a fused recurrent neural network operation. The forward kernel 'fused_recurrent_fwd_kernel' takes inputs like query (q), key (k), value (v) tensors, and computes an output tensor using recurrent operations. The backward kernel 'fused_recurrent_bwd_kernel' computes the gradients with respect to the inputs. The main parameters include q, k, v, beta, scale, and optionally initial state tensors. The operations involve element-wise multiplication and summation across specified dimensions using Triton's parallelism. Functions FusedRecurrentFunction.forward and FusedRecurrentFunction.backward wrap the kernels for use in PyTorch autograd. The auxiliary function fused_recurrent_delta_rule serves as a user-facing API to apply the kernels with error checking and default parameter handling.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef fused_recurrent_fwd_kernel(\n    q_ptr, k_ptr, v_ptr, out_ptr, beta, scale,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    # Load q, k, v\n    q = tl.load(q_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    k = tl.load(k_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    v = tl.load(v_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    \n    # Element-wise operations\n    product = q * k\n    weighted_sum = product * v * beta * scale\n    \n    # Write to output\n    tl.store(out_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE), weighted_sum)\n\n@triton.jit\ndef fused_recurrent_bwd_kernel(\n    q_ptr, k_ptr, v_ptr, grad_out_ptr, grad_q_ptr, grad_k_ptr, grad_v_ptr, beta, scale,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    # Load inputs and gradients\n    q = tl.load(q_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    k = tl.load(k_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    v = tl.load(v_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    grad_out = tl.load(grad_out_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE))\n    \n    # Compute gradients\n    grad_q = grad_out * k * v * beta * scale\n    grad_k = grad_out * q * v * beta * scale\n    grad_v = grad_out * q * k * beta * scale\n    \n    # Write gradients to output\n    tl.store(grad_q_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE), grad_q)\n    tl.store(grad_k_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE), grad_k)\n    tl.store(grad_v_ptr + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE), grad_v)\n\nclass FusedRecurrentFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, beta, scale):\n        BLOCK_SIZE = 128  # Example block size\n        out = torch.empty_like(q)\n        \n        # Launch forward kernel\n        grid = (q.numel() // BLOCK_SIZE,)\n        fused_recurrent_fwd_kernel[grid](q, k, v, out, beta, scale, BLOCK_SIZE=BLOCK_SIZE)\n        \n        # Save for backward\n        ctx.save_for_backward(q, k, v, torch.tensor(beta), torch.tensor(scale))\n        \n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        q, k, v, beta, scale = ctx.saved_tensors\n        BLOCK_SIZE = 128  # Example block size\n        \n        # Prepare gradients\n        grad_q = torch.empty_like(q)\n        grad_k = torch.empty_like(k)\n        grad_v = torch.empty_like(v)\n        \n        # Launch backward kernel\n        grid = (q.numel() // BLOCK_SIZE,)\n        fused_recurrent_bwd_kernel[grid](\n            q, k, v, grad_out, grad_q, grad_k, grad_v, beta.item(), scale.item(), BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return grad_q, grad_k, grad_v, None, None\n\ndef fused_recurrent_delta_rule(q, k, v, beta=1.0, scale=1.0):\n    if not all(isinstance(t, torch.Tensor) for t in [q, k, v]):\n        raise ValueError(\"All inputs must be PyTorch tensors\")\n    \n    return FusedRecurrentFunction.apply(q, k, v, beta, scale)\n\nq = torch.randn(256, device='cuda')\nk = torch.randn(256, device='cuda')\nv = torch.randn(256, device='cuda')\n\noutput = fused_recurrent_delta_rule(q, k, v, beta=0.9, scale=0.1)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.jit\ndef fused_recurrent_fwd_kernel(\n    q, k, v, beta, o, h0, ht, s_qk_h, s_vo_h, scale, B, H, T, K: tl.constexpr, V: tl.constexpr, \n    BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, \n    IS_HEADWISE_BETA: tl.constexpr\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    if IS_HEADWISE_BETA:\n        p_beta = beta + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    else:\n        p_beta = beta + i_bh * T\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < K\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        _v_minus = tl.sum(h * b_k[None, :], axis=1)\n        b_v -= _v_minus\n        if IS_HEADWISE_BETA:\n            b_beta = tl.load(p_beta, mask=mask_bv, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        tl.store(p_v, b_v.to(p_v.dtype.element_ty), mask=mask_bv)\n        b_v *= b_beta\n        h += b_k[None, :] * b_v[:, None]\n        _o = h * b_q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o.to(p_o.dtype.element_ty), mask=mask_bv)\n\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n        p_beta += V if IS_HEADWISE_BETA else 1\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, h.to(p_ht.dtype.element_ty), mask=mask_kv)\n\n@triton.jit\ndef fused_recurrent_bwd_kernel(\n    q, k, v, beta, dht, dh0, do, dq, dk, dv, dbeta, h0, s_qk_h, s_vo_h, NK, scale, B, H, T, \n    K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, \n    IS_HEADWISE_BETA: tl.constexpr, USE_DH0: tl.constexpr, USE_DHT: tl.constexpr\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    if IS_HEADWISE_BETA:\n        p_beta = beta + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    else:\n        p_beta = beta + i_bh * T + T - 1\n\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    if IS_HEADWISE_BETA:\n        p_dbeta = dbeta + (i_bh + i_k * B * H + i_v * B * H * NK) * s_vo_h + tl.arange(0, BV) + (T - 1) * V\n    else:\n        p_dbeta = dbeta + (i_bh + i_v * B * H) * T + T - 1\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        d_h += tl.load(p_ht, mask=mask_bk[:, None] & mask_bv[None, :], other=0).to(tl.float32)\n\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        if IS_HEADWISE_BETA:\n            b_beta = tl.load(p_beta, mask=mask_bv, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * (b_v * b_beta)[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n\n        d_beta = d_v * b_v if IS_HEADWISE_BETA else tl.sum(d_v * b_v)\n        d_v = d_v * b_beta\n\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n        if IS_HEADWISE_BETA:\n            tl.store(p_dbeta, d_beta.to(p_dbeta.dtype.element_ty), mask=mask_bv)\n        else:\n            tl.store(p_dbeta, d_beta.to(p_dbeta.dtype.element_ty))\n\n        d_h -= b_k[:, None] * d_v[None, :]\n\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n        p_dbeta -= V if IS_HEADWISE_BETA else 1\n        p_beta -= V if IS_HEADWISE_BETA else 1\n\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, d_h.to(p_dh0.dtype.element_ty), mask=mask_bk[:, None] & mask_bv[None, :])\n\n    tl.debug_barrier()\n\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    if IS_HEADWISE_BETA:\n        p_beta = beta + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    else:\n        p_beta = beta + i_bh * T\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    for i in range(0, T):\n        d_k = tl.load(p_dk, mask=mask_bk, other=0).to(tl.float32)\n        d_v = tl.load(p_dv, mask=mask_bv, other=0).to(tl.float32)\n        d_k -= tl.sum(d_v[None, :] * h, axis=1)\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        if IS_HEADWISE_BETA:\n            b_beta = tl.load(p_beta, mask=mask_bv, other=0).to(tl.float32)\n        else:\n            b_beta = tl.load(p_beta).to(tl.float32)\n        b_v *= b_beta\n\n        h += b_k[:, None] * b_v[None, :]\n        _d_q = h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dk += K\n        p_dv += V\n        p_dq += K\n        p_beta += V if IS_HEADWISE_BETA else 1\n\nclass FusedRecurrentFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, beta, scale=None, initial_state=None, output_final_state=False):\n        B, H, T, K, V = *q.shape, v.shape[-1]\n\n        BK, BV = triton.next_power_of_2(K), min(triton.next_power_of_2(V), 8)\n        NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 1\n        assert NK == 1, \"NK > 1 is not supported yet\"\n        o = q.new_empty(NK, B, H, T, V)\n\n        if output_final_state:\n            final_state = q.new_empty(B, H, K, V, dtype=torch.float32)\n        else:\n            final_state = None\n\n        grid = (NV, NK, B * H)\n        fused_recurrent_fwd_kernel[grid](\n            q, k, v, beta, o, initial_state, final_state,\n            q.stride(1),\n            v.stride(1),\n            scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BK=BK, BV=BV,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None,\n            IS_HEADWISE_BETA=beta.ndim == v.ndim,\n            num_warps=num_warps,\n            num_stages=num_stages,\n        )\n        o = o.squeeze(0)\n        ctx.save_for_backward(q, k, v, beta, initial_state)\n        ctx.scale = scale\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, dht):\n        q, k, v, beta, initial_state = ctx.saved_tensors\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        scale = ctx.scale\n        BK, BV = triton.next_power_of_2(K), min(triton.next_power_of_2(V), 32)\n        NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n        assert NK == 1, \"NK > 1 is not supported yet\"\n        num_stages = 1\n        num_warps = 2\n\n        beta_vector = beta.ndim == v.ndim\n\n        dq = q.new_empty(NV, B, H, T, K)\n        dk = q.new_empty(NV, B, H, T, K)\n        dv = q.new_empty(NK, B, H, T, V)\n        if beta_vector:\n            dbeta = q.new_empty(NV, NK, B, H, T, V)\n        else:\n            dbeta = q.new_empty(NV, B, H, T)\n        grid = (NV, NK, B * H)\n\n        if initial_state is not None and initial_state.requires_grad:\n            dh0 = torch.empty_like(initial_state, dtype=torch.float32)\n        else:\n            dh0 = None\n\n        fused_recurrent_bwd_kernel[grid](\n            q, k, v, beta, dht, dh0, do, dq, dk, dv, dbeta, initial_state,\n            q.stride(1),\n            v.stride(1),\n            NK, scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BK=BK, BV=BV,\n            USE_INITIAL_STATE=initial_state is not None,\n            USE_DH0=dh0 is not None,\n            USE_DHT=dht is not None,\n            IS_HEADWISE_BETA=beta_vector,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        dq = dq.sum(0)\n        dk = dk.sum(0)\n        dv = dv.sum(0)\n        dbeta = dbeta.sum((0, 1)) if beta_vector else dbeta.sum(0)\n        return dq.to(q), dk.to(k), dv.to(v), dbeta.to(beta), None, dh0, None\n\ndef fused_recurrent_delta_rule(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    beta: torch.Tensor = None,\n    scale: float = None,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if scale is None:\n        scale = q.shape[-1] ** -0.5\n    else:\n        assert scale > 0, \"scale must be positive\"\n    if beta is None:\n        beta = torch.ones_like(q[..., 0])\n    o, final_state = FusedRecurrentFunction.apply(q, k, v, beta, scale, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named `add_kernel` and a wrapper function `add`. The kernel takes pointers to two input vectors and an output vector, and adds corresponding elements together. The `add` function ensures the inputs and output are CUDA tensors, calculates the number of elements, and launches the kernel on a 1D grid. BLOCK_SIZE is set to 1024 for parallel processing.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef add_kernel(\n    x_ptr,  # Pointer to the first input vector\n    y_ptr,  # Pointer to the second input vector\n    output_ptr,  # Pointer to the output vector\n    n_elements,  # Number of elements in the vectors\n    BLOCK_SIZE: tl.constexpr  # Size of each block\n):\n    # Get the program ID for this block\n    pid = tl.program_id(0)\n    \n    # Calculate the starting index for this block\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n    \n    # Load x and y from global memory\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    \n    # Perform the addition\n    result = x + y\n    \n    # Store the result back to global memory\n    tl.store(output_ptr + offsets, result, mask=mask)\n\n# Define the wrapper function\ndef add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    assert x.is_cuda and y.is_cuda, \"Inputs must be CUDA tensors\"\n    assert x.shape == y.shape, \"Input tensors must have the same shape\"\n    \n    # Allocate output tensor\n    output = torch.empty_like(x)\n    \n    # Get the number of elements\n    n_elements = x.numel()\n    \n    # Define block size\n    BLOCK_SIZE = 1024\n    \n    # Calculate the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the kernel\n    add_kernel[grid](\n        x_ptr=x, \n        y_ptr=y, \n        output_ptr=output, \n        n_elements=n_elements, \n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return output\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create CUDA tensors\n    x = torch.randn(10240, device='cuda')\n    y = torch.randn(10240, device='cuda')\n    \n    # Call the add function\n    result = add(x, y)\n    \n    # Print the result\n    print(result)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr,  # *Pointer* to first input vector.\n               y_ptr,  # *Pointer* to second input vector.\n               output_ptr,  # *Pointer* to output vector.\n               n_elements,  # Size of the vector.\n               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n               ):\n    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef add(x: torch.Tensor, y: torch.Tensor):\n    output = torch.empty_like(x)\n    assert x.is_cuda and y.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs Root Mean Square (RMS) normalization on a 2D tensor. The function `rms_norm_kernel` is a JIT-compiled Triton function that normalizes each row of the input matrix `X` using RMS normalization. It multiplies the normalized values by a weight vector `W` and stores the result in an output matrix `Y`. The `RmsNorm` class is an autograd-compatible wrapper around this kernel for PyTorch.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_norm_kernel(X, W, Y, stride_x, stride_y, stride_w, stride_out, N):\n    # Pointers to the start of the row in each tensor\n    row_id = tl.program_id(0)\n    X_row_ptr = X + row_id * stride_x\n    W_row_ptr = W + row_id * stride_w\n    Y_row_ptr = Y + row_id * stride_y\n\n    # Load the row\n    x = tl.load(X_row_ptr + tl.arange(0, N))\n    \n    # Compute RMS\n    mean_sq = tl.sum(x * x, axis=0) / N\n    rms = tl.sqrt(mean_sq)\n\n    # Normalize\n    x_norm = x / rms\n\n    # Scale by weight\n    w = tl.load(W_row_ptr + tl.arange(0, N))\n    y = x_norm * w\n\n    # Store the result\n    tl.store(Y_row_ptr + tl.arange(0, N), y)\n\nclass RmsNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, w):\n        assert x.is_contiguous() and w.is_contiguous()\n        assert x.shape[1] == w.shape[0]\n\n        # Allocate output\n        y = torch.empty_like(x)\n\n        # Grid\n        num_rows = x.shape[0]\n        N = x.shape[1]\n        grid = (num_rows,)\n\n        # Launch kernel\n        rms_norm_kernel[grid](\n            x, w, y,\n            x.stride(0), y.stride(0), w.stride(0), y.stride(0),\n            N\n        )\n\n        # Save for backward\n        ctx.save_for_backward(x, w, y)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retrieve saved tensors\n        x, w, y = ctx.saved_tensors\n        N = x.shape[1]\n\n        # Compute gradients\n        grad_x = torch.empty_like(x)\n        grad_w = torch.empty_like(w)\n\n        # Calculate RMS and normalized x\n        mean_sq = torch.sum(x * x, dim=1, keepdim=True) / N\n        rms = torch.sqrt(mean_sq)\n        x_norm = x / rms\n\n        # Compute grad_w\n        grad_w = torch.sum(grad_output * x_norm, dim=0)\n\n        # Compute grad_x\n        grad_x_norm = grad_output * w\n        grad_rms = -torch.sum(grad_x_norm * x_norm, dim=1, keepdim=True) / rms\n        grad_x = grad_x_norm / rms + (x * grad_rms) / (N * rms)\n\n        return grad_x, grad_w\n\n# Example usage\nx = torch.randn(10, 5, device='cuda', requires_grad=True)\nw = torch.randn(5, device='cuda', requires_grad=True)\ny = RmsNorm.apply(x, w)\n", "label": "import torch\nimport triton\nimport math\nimport triton.language as tl\n\n@triton.jit(do_not_specialize=[\"eps\"])\ndef rms_norm_kernel(\n    Y,  # pointer to the output\n    X,  # pointer to the input\n    W,  # pointer to the weights\n    y_stride_r,\n    y_stride_c,\n    x_stride_r,  # how much to increase the pointer when moving by 1 row\n    x_stride_c,  # how much to increase the pointer when moving by 1 col\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    Y += pid * y_stride_r\n    X += pid * x_stride_r\n\n    mask = tl.arange(0, BLOCK_SIZE) < N\n    cols = tl.arange(0, BLOCK_SIZE)\n    x = tl.load(X + cols * x_stride_c, mask, other=0.0).to(tl.float32)\n\n    var = tl.sum(x * x, axis=0) / N\n    rrms = 1 / tl.sqrt(var + eps)\n\n    w = tl.load(W + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)\n    y = (x * rrms).to(Y.dtype.element_ty) * w\n    tl.store(Y + cols * y_stride_c, y, mask=mask)\n\n\nclass RmsNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, normalized_shape, weight, eps=1e-5):\n        dim = x.ndim - len(normalized_shape)\n        M = math.prod(x.shape[:dim])\n        N = math.prod(normalized_shape)\n\n        BLOCK_SIZE = triton.next_power_of_2(N)\n        x = x.contiguous()\n        weight = weight.contiguous()\n        y = torch.empty_like(x)\n\n        with torch.cuda.device(x.device):\n            rms_norm_kernel[M,](y, x, weight, N, 1, N, 1, N, eps, BLOCK_SIZE)\n        return y\n\n\ndef rms_norm(x, normalized_shape, weight, eps=1e-5):\n    return RmsNorm.apply(x, normalized_shape, weight, eps)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe provided code implements a multinomial sampling operation using Triton, designed to efficiently sample indices from a given 2D tensor of scores. The primary function, `multinomial_sampling`, is a wrapper that configures and launches the Triton kernel `_multinomial_sampling_kernel`. The inputs are `scores`, a tensor of size [batch_size, num_tokens], `seeds`, and `offsets`, both 1D tensors used for random number generation in the sampling process. The function outputs the sampled indices for each batch.\n\nThe kernel divides the workload using a block-based approach, iterating over tokens within each block to calculate cumulative scores and determine valid sampling positions. The result is stored in the `outputs` tensor, which is then returned.\n", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _multinomial_sampling_kernel(scores_ptr, seeds_ptr, offsets_ptr, outputs_ptr,\n                                 batch_size, num_tokens, BLOCK_SIZE: tl.constexpr):\n    # Define the block index and block size\n    batch_idx = tl.program_id(0)\n    block_start = tl.program_id(1) * BLOCK_SIZE\n\n    # Load seeds and offsets\n    seed = tl.load(seeds_ptr + batch_idx)\n    offset = tl.load(offsets_ptr + batch_idx)\n\n    # Initialize cumulative sum and random threshold\n    cumulative_sum = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    rand_threshold = tl.rand(seed, offset) * tl.sum(tl.load(scores_ptr + batch_idx * num_tokens, mask=tl.arange(0, BLOCK_SIZE) < num_tokens))\n\n    # Iterate over the tokens within the block\n    for i in range(BLOCK_SIZE):\n        token_idx = block_start + i\n        if token_idx < num_tokens:\n            score = tl.load(scores_ptr + batch_idx * num_tokens + token_idx)\n            cumulative_sum[i] = score + (cumulative_sum[i - 1] if i > 0 else 0)\n\n            # Check if cumulative sum exceeds the random threshold\n            if cumulative_sum[i] > rand_threshold:\n                tl.store(outputs_ptr + batch_idx, token_idx)\n                break\n\ndef multinomial_sampling(scores, seeds, offsets):\n    # Get dimensions\n    batch_size, num_tokens = scores.shape\n\n    # Allocate output tensor\n    outputs = torch.empty((batch_size,), dtype=torch.int32, device=scores.device)\n\n    # Launch the Triton kernel\n    BLOCK_SIZE = 128  # Define an appropriate block size\n    grid = (batch_size, (num_tokens + BLOCK_SIZE - 1) // BLOCK_SIZE)\n    _multinomial_sampling_kernel[grid](\n        scores_ptr=scores,\n        seeds_ptr=seeds,\n        offsets_ptr=offsets,\n        outputs_ptr=outputs,\n        batch_size=batch_size,\n        num_tokens=num_tokens,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return outputs\n\n# Example usage\nbatch_size = 1024\nnum_tokens = 10000\nscores = torch.rand((batch_size, num_tokens), device='cuda')\nseeds = torch.randint(0, 2**31, (batch_size,), dtype=torch.int32, device='cuda')\noffsets = torch.randint(0, 2**31, (batch_size,), dtype=torch.int32, device='cuda')\n\nsampled_indices = multinomial_sampling(scores, seeds, offsets)\nprint(sampled_indices)\n", "label": "from typing import Dict, Sequence, Union\nimport torch\nimport triton\nimport triton.language as tl\n\n\nKERNEL_META = dict()\n\ndef get_kernel_meta(tensor: torch.Tensor):\n    \"\"\"kernel meta.\"\"\"\n    return KERNEL_META\n\nTypeHintType = Union[Dict[str, type], Sequence[type], None]\n\n\n@triton.jit\ndef _multinomial_sampling_kernel(Scores, Seeds, Offsets, Indices, Outputs,\n                                 stride_sb, stride_st, stride_ib, stride_it,\n                                 num_batchs, num_tokens, BLOCK: tl.constexpr,\n                                 BLOCK_N: tl.constexpr):\n    \"\"\"Kernel.\"\"\"\n    batch_block_id = tl.program_id(0)\n\n    off = batch_block_id * BLOCK + tl.arange(0, BLOCK)\n    n_off = tl.arange(0, BLOCK_N)\n\n    off_mask = off < num_batchs\n    seed = tl.load(Seeds + off, mask=off_mask)\n    offset = tl.load(Offsets + off, mask=off_mask).to(tl.int32)\n\n    samp = tl.rand(seed, offset)[:, None]\n    acc = tl.zeros((BLOCK, ), dtype=tl.float32)\n    output = tl.load(Indices + off * stride_ib, mask=off_mask)\n\n    for b_idx in range(0, num_tokens, BLOCK_N):\n        s_off = b_idx + n_off\n        s_mask = off_mask[:, None] & (s_off[None, :] < num_tokens)\n        scores = tl.load(Scores + off[:, None] * stride_sb +\n                         s_off[None, :] * stride_st,\n                         mask=s_mask,\n                         other=0.0).to(tl.float32)\n        c_scores = tl.cumsum(scores, 1)\n        cum_scores = acc[:, None] + c_scores\n        acc += tl.max(c_scores, 1)\n\n        pre_cum_scores = cum_scores - scores\n        valid_mask = (samp > pre_cum_scores) & (samp <= cum_scores)\n        found_mask = tl.sum(valid_mask, 1) > 0\n\n        valid_pos = b_idx + tl.argmax(valid_mask.to(tl.int32), 1)\n        indices = tl.load(Indices + off * stride_ib + valid_pos * stride_it,\n                          mask=found_mask & off_mask,\n                          other=-1)\n        output = tl.where(found_mask, indices, output)\n\n    tl.store(Outputs + off, output, mask=off_mask)\n\n\ndef multinomial_sampling(scores: torch.Tensor,\n                         seeds: torch.LongTensor,\n                         offsets: torch.LongTensor,\n                         indices: torch.Tensor = None):\n    \"\"\"multinomial sampling.\"\"\"\n\n    assert scores.dim() == 2\n    batch_size, num_tokens = scores.size()\n    device = scores.device\n\n    if num_tokens == 1:\n        return torch.zeros_like(scores, dtype=torch.long)\n\n    if indices is None:\n        indices = torch.arange(num_tokens, device=device)\n        indices = indices.expand_as(scores)\n\n    assert indices.dim() == 2\n    assert indices.size() == scores.size()\n\n    outputs = indices[:, 0].clone()\n\n    BLOCK = 8\n    BLOCK_N = 128\n\n    grid = [triton.cdiv(batch_size, BLOCK)]\n    kernel_meta = get_kernel_meta(scores)\n    _multinomial_sampling_kernel[grid](scores,\n                                       seeds,\n                                       offsets,\n                                       indices,\n                                       outputs,\n                                       stride_sb=scores.stride(0),\n                                       stride_st=scores.stride(1),\n                                       stride_ib=indices.stride(0),\n                                       stride_it=indices.stride(1),\n                                       num_batchs=batch_size,\n                                       num_tokens=num_tokens,\n                                       BLOCK=BLOCK,\n                                       BLOCK_N=BLOCK_N,\n                                       num_warps=8,\n                                       **kernel_meta)\n\n    return outputs\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator performs a mixed sparse attention operation. It processes query, key, and value tensors to compute an output tensor using sparse attention patterns. The function `_triton_mixed_sparse_attn_fwd_kernel` is a Triton kernel that operates on blocks of the input tensors, applying attention based on sparse patterns and causal masking. Inputs include tensors Q, K, V for query, key, and value, sequence lengths, scaling factors, and metadata for sparse patterns. The output tensor is computed using these inputs and the sparse attention logic. The wrapper function `_triton_mixed_sparse_attention` sets up the problem dimensions and launches the kernel.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(\n    Q, K, V, Out,\n    seq_len, scale, sparse_meta,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Calculate block indices\n    batch_idx = tl.program_id(0)\n    seq_idx = tl.program_id(1)\n\n    # Calculate offsets for the current block\n    q_offset = batch_idx * seq_len + seq_idx * BLOCK_SIZE\n    k_offset = batch_idx * seq_len\n    v_offset = batch_idx * seq_len\n\n    # Load Q, K, V blocks\n    q = tl.load(Q + q_offset, mask=q_offset < seq_len)\n    k = tl.load(K + k_offset, mask=k_offset < seq_len)\n    v = tl.load(V + v_offset, mask=v_offset < seq_len)\n\n    # Compute scaled dot-product\n    attn_scores = tl.dot(q, tl.trans(k)) * scale\n\n    # Apply sparse pattern (example, needs adaptation)\n    sparse_mask = tl.load(sparse_meta + seq_idx * BLOCK_SIZE, mask=seq_idx * BLOCK_SIZE < seq_len)\n    attn_scores = tl.where(sparse_mask, attn_scores, float('-inf'))\n\n    # Causal masking (example, needs adaptation)\n    causal_mask = tl.arange(0, BLOCK_SIZE) <= tl.arange(0, BLOCK_SIZE)[:, None]\n    attn_scores = tl.where(causal_mask, attn_scores, float('-inf'))\n\n    # Softmax\n    attn_probs = tl.softmax(attn_scores)\n\n    # Compute output\n    out = tl.dot(attn_probs, v)\n    tl.store(Out + q_offset, out)\n\n\ndef _triton_mixed_sparse_attention(Q, K, V, seq_len, scale, sparse_meta):\n    # Assume Q, K, V are contiguous and of shape (batch_size, seq_len, head_dim)\n    batch_size, _, head_dim = Q.shape\n\n    # Define block size\n    BLOCK_SIZE = 128  # This is an example, choose according to your needs\n\n    # Allocate output tensor\n    Out = torch.empty_like(Q)\n\n    # Launch kernel\n    grid = (batch_size, seq_len // BLOCK_SIZE)\n    _triton_mixed_sparse_attn_fwd_kernel[grid](\n        Q, K, V, Out,\n        seq_len, scale, sparse_meta,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return Out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(\n    Q, K, V, seqlens, sm_scale,\n    block_count, block_offset, column_count, column_index,\n    Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_oz, stride_oh, stride_om, stride_ok,\n    Z, H, N_CTX,\n    NUM_ROWS, NNZ_S, NNZ_V,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    dtype: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    qo_offset = (off_hz // H) * stride_qz + (off_hz % H) * stride_qh\n    kv_offset = (off_hz // H) * stride_kz + (off_hz % H) * stride_kh\n\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :] * stride_ok\n\n    num_blks = tl.load(block_count + off_hz * NUM_ROWS + start_m)\n    blks_ptr = block_offset + (off_hz * NUM_ROWS + start_m) * NNZ_S\n    num_cols = tl.load(column_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = column_index + (off_hz * NUM_ROWS + start_m) * NNZ_V\n\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = (q * qk_scale).to(dtype)\n\n    m_mask = offs_m[:, None] < seqlen\n\n    # \u8bbe\u7f6e\u6700\u5927\u5757\u6570\n    max_num_blks = 8  # \u6839\u636e\u5b9e\u9645\u9700\u6c42\u8c03\u6574\n    for block_index in range(max_num_blks):\n        # \u4f7f\u7528\u6761\u4ef6\u63a9\u7801\u4ee3\u66ff break\n        cond = block_index < num_blks\n        # \u4ec5\u5728 cond \u4e3a True \u65f6\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\n        start_n = tl.load(blks_ptr + block_index, mask=cond)\n        cols = start_n + offs_n\n        n_mask = (cols < seqlen) & cond[:, None]\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = (cols[None, :] <= offs_m[:, None])\n        qk = tl.where(m_mask & causal_mask, qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n    # \u8bbe\u7f6e\u6700\u5927\u5217\u6570\n    max_num_cols = 16  # \u6839\u636e\u5b9e\u9645\u9700\u6c42\u8c03\u6574\n    for start_n in range(0, max_num_cols, BLOCK_N):\n        # \u4f7f\u7528\u6761\u4ef6\u63a9\u7801\u4ee3\u66ff break\n        cond = start_n < num_cols\n        n_mask = (start_n + offs_n < num_cols) & cond\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=cond[:, None], other=0)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(dtype), v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc.to(dtype), mask=m_mask)\n\n\ndef _triton_mixed_sparse_attention(\n    q: torch.Tensor,          # [BATCH, N_HEADS, N_CTX, D_HEAD]\n    k: torch.Tensor,          # [BATCH, N_HEADS, N_CTX, D_HEAD]\n    v: torch.Tensor,          # [BATCH, N_HEADS, N_CTX, D_HEAD]\n    seqlens: torch.Tensor,    # [BATCH, ]\n    block_count: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]\n    block_offset: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_S]\n    column_count: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M)]\n    column_index: torch.Tensor,  # [BATCH, N_HEADS, cdiv(N_CTX, BLOCK_SIZE_M), NNZ_V]\n    sm_scale: float,\n    block_size_M: int = 64,\n    block_size_N: int = 64,\n) -> torch.Tensor:\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n    o = torch.zeros_like(q)\n    grid = (triton.cdiv(q.shape[2], block_size_M), q.shape[0] * q.shape[1], 1)\n    dtype = tl.bfloat16 if q.dtype == torch.bfloat16 else tl.float16\n    _triton_mixed_sparse_attn_fwd_kernel[grid](\n        q, k, v, seqlens, sm_scale,\n        block_count, block_offset, column_count, column_index,\n        o,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n        q.shape[0], q.shape[1], q.shape[2],\n        block_count.shape[-1], block_offset.shape[-1], column_index.shape[-1],\n        BLOCK_M=block_size_M, BLOCK_N=block_size_N,\n        BLOCK_DMODEL=Lk,\n        dtype=dtype,\n        num_warps=4, num_stages=2,\n    )\n\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines several Triton kernels and related functions to perform a chunked retention operation on tensors, including forward and backward passes. This operation processes tensors `q`, `k`, `v` and handles optional initial states, scaling, and gradient calculations.\n\n            Key functions:\n            - `chunk_retention_fwd_kernel_h`: A Triton kernel for the forward pass, updating hidden states.\n            - `chunk_retention_fwd_kernel_o`: A Triton kernel to compute the output by scaling the product of `q`, `k`, `v` tensors.\n            - `chunk_retention_bwd_kernel_dh`: A Triton kernel for backward pass, computing gradients for the hidden state.\n            - `chunk_retention_bwd_kernel_dqkv`: A Triton kernel for backward pass, computing gradients for the input tensors `q`, `k`, and `v`.\n            \n            Core logic involves loading blocks of the input tensors, performing matrix multiplications and element-wise operations, and storing the results. The operations handle scaling and exponential decay factors for softmax-like transformations.\n            \n            The functions `chunk_fwd_h_fn`, `chunk_fwd_o_fn`, `chunk_bwd_dh_fn`, and `chunk_bwd_dqkv_fn` are Python functions that wrap around these Triton kernels, preparing inputs and executing the kernels on specific grid sizes. \n\n            The `ChunkRetentionFunction` class defines a custom PyTorch autograd function that applies the above kernels and supports backpropagation through the defined operations. \n            \n            The `chunk_retention` function is the main entry point, taking tensors `q`, `k`, `v`, an optional initial state, and other parameters to perform the chunk retention operation and return the resulting tensor and optional final state.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Triton Kernel for Forward Pass: Update Hidden States\n@triton.jit\ndef chunk_retention_fwd_kernel_h(q_ptr, k_ptr, v_ptr, h_ptr, out_ptr, scale, stride_q, stride_k, stride_v, stride_h, stride_out, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    # Load blocks of q, k, v\n    q = tl.load(q_ptr + pid * stride_q + tl.arange(0, BLOCK_SIZE))\n    k = tl.load(k_ptr + pid * stride_k + tl.arange(0, BLOCK_SIZE))\n    v = tl.load(v_ptr + pid * stride_v + tl.arange(0, BLOCK_SIZE))\n    # Compute new hidden state\n    h = tl.load(h_ptr + pid * stride_h + tl.arange(0, BLOCK_SIZE))\n    new_h = h * scale + tl.dot(q, k) * v\n    tl.store(h_ptr + pid * stride_h + tl.arange(0, BLOCK_SIZE), new_h)\n    # Write output\n    tl.store(out_ptr + pid * stride_out + tl.arange(0, BLOCK_SIZE), new_h)\n\n# Triton Kernel for Forward Pass: Compute Output\n@triton.jit\ndef chunk_retention_fwd_kernel_o(q_ptr, k_ptr, v_ptr, out_ptr, scale, stride_q, stride_k, stride_v, stride_out, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    # Load blocks of q, k, v\n    q = tl.load(q_ptr + pid * stride_q + tl.arange(0, BLOCK_SIZE))\n    k = tl.load(k_ptr + pid * stride_k + tl.arange(0, BLOCK_SIZE))\n    v = tl.load(v_ptr + pid * stride_v + tl.arange(0, BLOCK_SIZE))\n    # Compute output\n    out = tl.dot(q, k) * v * scale\n    tl.store(out_ptr + pid * stride_out + tl.arange(0, BLOCK_SIZE), out)\n\n# Triton Kernel for Backward Pass: Gradients for Hidden State\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(dout_ptr, dh_ptr, q_ptr, k_ptr, v_ptr, scale, stride_dout, stride_dh, stride_q, stride_k, stride_v, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    # Load blocks\n    dout = tl.load(dout_ptr + pid * stride_dout + tl.arange(0, BLOCK_SIZE))\n    q = tl.load(q_ptr + pid * stride_q + tl.arange(0, BLOCK_SIZE))\n    k = tl.load(k_ptr + pid * stride_k + tl.arange(0, BLOCK_SIZE))\n    v = tl.load(v_ptr + pid * stride_v + tl.arange(0, BLOCK_SIZE))\n    # Compute gradient for hidden state\n    dh = dout * scale * tl.dot(q, k) * v\n    tl.store(dh_ptr + pid * stride_dh + tl.arange(0, BLOCK_SIZE), dh)\n\n# Triton Kernel for Backward Pass: Gradients for Input Tensors\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(dout_ptr, dq_ptr, dk_ptr, dv_ptr, q_ptr, k_ptr, v_ptr, scale, stride_dout, stride_dq, stride_dk, stride_dv, stride_q, stride_k, stride_v, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    # Load blocks\n    dout = tl.load(dout_ptr + pid * stride_dout + tl.arange(0, BLOCK_SIZE))\n    q = tl.load(q_ptr + pid * stride_q + tl.arange(0, BLOCK_SIZE))\n    k = tl.load(k_ptr + pid * stride_k + tl.arange(0, BLOCK_SIZE))\n    v = tl.load(v_ptr + pid * stride_v + tl.arange(0, BLOCK_SIZE))\n    # Compute gradients\n    dq = dout * scale * tl.dot(k, v)\n    dk = dout * scale * tl.dot(q, v)\n    dv = dout * scale * tl.dot(q, k)\n    # Store gradients\n    tl.store(dq_ptr + pid * stride_dq + tl.arange(0, BLOCK_SIZE), dq)\n    tl.store(dk_ptr + pid * stride_dk + tl.arange(0, BLOCK_SIZE), dk)\n    tl.store(dv_ptr + pid * stride_dv + tl.arange(0, BLOCK_SIZE), dv)\n\n# Wrapper Functions\ndef chunk_fwd_h_fn(q, k, v, h, scale, BLOCK_SIZE=128):\n    grid = (q.size(0) // BLOCK_SIZE,)\n    out = torch.empty_like(h)\n    chunk_retention_fwd_kernel_h[grid](q, k, v, h, out, scale, q.stride(0), k.stride(0), v.stride(0), h.stride(0), out.stride(0), BLOCK_SIZE=BLOCK_SIZE)\n    return out\n\ndef chunk_fwd_o_fn(q, k, v, scale, BLOCK_SIZE=128):\n    grid = (q.size(0) // BLOCK_SIZE,)\n    out = torch.empty_like(q)\n    chunk_retention_fwd_kernel_o[grid](q, k, v, out, scale, q.stride(0), k.stride(0), v.stride(0), out.stride(0), BLOCK_SIZE=BLOCK_SIZE)\n    return out\n\ndef chunk_bwd_dh_fn(dout, q, k, v, scale, BLOCK_SIZE=128):\n    grid = (dout.size(0) // BLOCK_SIZE,)\n    dh = torch.empty_like(dout)\n    chunk_retention_bwd_kernel_dh[grid](dout, dh, q, k, v, scale, dout.stride(0), dh.stride(0), q.stride(0), k.stride(0), v.stride(0), BLOCK_SIZE=BLOCK_SIZE)\n    return dh\n\ndef chunk_bwd_dqkv_fn(dout, q, k, v, scale, BLOCK_SIZE=128):\n    grid = (dout.size(0) // BLOCK_SIZE,)\n    dq = torch.empty_like(q)\n    dk = torch.empty_like(k)\n    dv = torch.empty_like(v)\n    chunk_retention_bwd_kernel_dqkv[grid](dout, dq, dk, dv, q, k, v, scale, dout.stride(0), dq.stride(0), dk.stride(0), dv.stride(0), q.stride(0), k.stride(0), v.stride(0), BLOCK_SIZE=BLOCK_SIZE)\n    return dq, dk, dv\n\n# PyTorch Custom Autograd Function\nclass ChunkRetentionFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, initial_h=None, scale=1.0):\n        if initial_h is None:\n            initial_h = torch.zeros_like(q)\n        ctx.save_for_backward(q, k, v, initial_h)\n        ctx.scale = scale\n        h = chunk_fwd_h_fn(q, k, v, initial_h, scale)\n        out = chunk_fwd_o_fn(q, k, v, scale)\n        return out, h\n\n    @staticmethod\n    def backward(ctx, dout, dh):\n        q, k, v, initial_h = ctx.saved_tensors\n        scale = ctx.scale\n        dq, dk, dv = chunk_bwd_dqkv_fn(dout, q, k, v, scale)\n        dh = chunk_bwd_dh_fn(dout, q, k, v, scale)\n        return dq, dk, dv, dh, None\n\n# Main Entry Point\ndef chunk_retention(q, k, v, initial_h=None, scale=1.0):\n    return ChunkRetentionFunction.apply(q, k, v, initial_h, scale)\n\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_fwd_kernel_h(\n    k, v, h, h0, ht, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr\n):\n    # Triton kernel code for forward pass of chunk retention with initial and final state handling\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if i_t == NT - 1 and (T % BT) != 0:\n            d_b = tl.math.exp2((T % BT) * b_b)\n            d_i = tl.math.exp2(((T % BT) - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_i[:, None]).to(b_k.dtype), allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_fwd_kernel_o(\n    q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr\n):\n    # Triton kernel code for forward pass of chunk retention with output scaling\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot((b_q * d_i[:, None]).to(b_q.dtype), b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s *= d_s\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(\n    q, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    # Triton kernel code for backward pass of chunk retention, computing gradients for hidden state\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, (b_do * d_i[:, None]).to(b_q.dtype), allow_tf32=False)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(\n    q, k, v, h, do, dh, dq, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t,\n    scale, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    # Triton kernel code for backward pass of chunk retention, computing gradients for q, k, v\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    d_q = (d_q * scale).to(d_q.dtype)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k*n_bh+i_bh)*s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = (b_ds * d_s).to(b_q.dtype)\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n\ndef chunk_fwd_h_fn(k, v, BT, initial_state, output_final_state):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    final_state = None\n    if output_final_state:\n        final_state = k.new_empty(B, H, K, V, dtype=torch.float32)\n    BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n    NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n    h = k.new_empty(B, H, NT * K, V)\n    grid = (NK, NV, B * H)\n    chunk_retention_fwd_kernel_h[grid](\n        k, v, h, initial_state, final_state,\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2),\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n        USE_INITIAL_STATE=initial_state is not None,\n        STORE_FINAL_STATE=output_final_state\n    )\n    return h, final_state\n\n\ndef chunk_fwd_o_fn(h, q, k, v, BT, scale):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    o = torch.empty_like(v)\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NV = triton.cdiv(V, BV)\n    NT = triton.cdiv(T, BT)\n    grid = (NV, NT, B * H)\n    chunk_retention_fwd_kernel_o[grid](\n        q, k, v, h, o,\n        q.stride(1), q.stride(2), q.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2),\n        scale,\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV\n    )\n    return o\n\n\ndef chunk_bwd_dh_fn(do, q, k, v, BT, scale):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    BT = 64\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n    dh = k.new_empty(B, H, NT * K, V)\n    grid = (NK, NV, B * H)\n    chunk_retention_bwd_kernel_dh[grid](\n        q, do, dh,\n        q.stride(1), q.stride(2), q.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        dh.stride(1), dh.stride(2),\n        scale,\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT\n    )\n    return dh\n\n\ndef chunk_bwd_dqkv_fn(do, q, k, v, h, dh, scale):\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    BT = 64\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NT, NK = triton.cdiv(T, BT), triton.cdiv(K, BK)\n    grid = (NK, NT, B * H)\n    dq = torch.empty_like(q)\n    dk = torch.empty_like(k)\n    dv = v.new_empty(NK, *v.shape)\n    chunk_retention_bwd_kernel_dqkv[grid](\n        q, k, v, h, do, dh, dq, dk, dv,\n        q.stride(1), q.stride(2), q.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        h.stride(1), h.stride(2),\n        scale,\n        H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT\n    )\n    dv = dv.sum(0)\n    return dq, dk, dv\n\n\nclass ChunkRetentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, initial_state, output_final_state, scale, checkpoint_level):\n        BT = 64\n        h, final_state = chunk_fwd_h_fn(k, v, BT, initial_state, output_final_state)\n        o = chunk_fwd_o_fn(h, q, k, v, BT, scale)\n        if checkpoint_level == 1:\n            h = None\n        ctx.save_for_backward(q, k, v, h, initial_state)\n        ctx.BT, ctx.scale = BT, scale\n        return o.to(q.dtype), final_state\n\n    @staticmethod\n    def backward(ctx, do, d_ht=None):\n        BT, scale = ctx.BT, ctx.scale\n        q, k, v, h, initial_state = ctx.saved_tensors\n        if h is None:\n            h, _ = chunk_fwd_h_fn(k, v, BT, initial_state, False)\n        dh = chunk_bwd_dh_fn(do, q, k, v, BT, scale)\n        dq, dk, dv = chunk_bwd_dqkv_fn(do, q, k, v, h, dh, scale)\n        return dq.to(q.dtype), dk.to(k.dtype), dv.to(v.dtype), None, None, None, None\n\n\ndef chunk_retention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False,\n    scale: float = None,\n    checkpoint_level: int = 1\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    assert checkpoint_level in [0, 1], \"checkpoint_level must be 0, 1\"\n    assert q.dim() == k.dim() == v.dim() == 4, \"q, k, v must have 4 dimensions (b, h, l, d)\"\n    assert q.dtype == k.dtype == v.dtype, \"q, k, v must have the same dtype\"\n    if scale is None:\n        scale = q.size(-1) ** -0.5\n    o, final_state = ChunkRetentionFunction.apply(\n        q, k, v, initial_state, output_final_state, scale, checkpoint_level)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided Triton kernel and its wrapper function are designed to compute the element-wise square of a 2D input tensor. The `square_kernel` function is a JIT-compiled Triton kernel that takes input and output pointers along with metadata to perform this computation. It iterates over each row independently using a parallel execution model. The BLOCK_SIZE ensures memory alignment and efficient memory access. The main logic is loading a row, squaring each element, and storing the result. The `square` function initializes the output tensor, determines the block size and number of warps based on input dimensions, and launches the kernel for each row.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef square_kernel(\n    input_ptr,  # Pointer to the input tensor\n    output_ptr,  # Pointer to the output tensor\n    n_cols,  # Number of columns in the 2D tensor\n    BLOCK_SIZE: tl.constexpr  # Size of the block\n):\n    # Get the program ID for the current block\n    row_idx = tl.program_id(0)\n    \n    # Create a block of indices for this row\n    col_indices = tl.arange(0, BLOCK_SIZE)\n    \n    # Compute the offset for this row\n    row_offset = row_idx * n_cols\n    \n    # Compute the memory addresses for the input and output\n    input_ptrs = input_ptr + row_offset + col_indices\n    output_ptrs = output_ptr + row_offset + col_indices\n    \n    # Load a block of data from the input tensor\n    input_vals = tl.load(input_ptrs, mask=col_indices < n_cols)\n    \n    # Compute the square of each element\n    output_vals = input_vals * input_vals\n    \n    # Store the results back to the output tensor\n    tl.store(output_ptrs, output_vals, mask=col_indices < n_cols)\n\n# Define the wrapper function\ndef square(input_tensor):\n    # Ensure the input tensor is a 2D tensor\n    assert input_tensor.ndim == 2, \"Input must be a 2D tensor\"\n    \n    # Get the dimensions of the input tensor\n    n_rows, n_cols = input_tensor.shape\n    \n    # Create an output tensor with the same shape\n    output_tensor = torch.empty_like(input_tensor)\n    \n    # Determine block size and number of warps\n    BLOCK_SIZE = 128  # This can be tuned for optimal performance\n    num_warps = 4  # This can be tuned based on the hardware\n    \n    # Launch the Triton kernel\n    grid = (n_rows,)  # One block per row\n    square_kernel[grid](\n        input_tensor,\n        output_tensor,\n        n_cols,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return output_tensor\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample input tensor\n    input_tensor = torch.randn(1024, 512, device='cuda')\n    \n    # Compute the square using the Triton kernel\n    output_tensor = square(input_tensor)\n    \n    # Print the result\n    print(output_tensor)\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef square_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # The rows of the softmax are independent, so we parallelize across those\n    row_idx = tl.program_id(0)\n    # The stride represents how much we need to increase the pointer to advance 1 row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    # The block size is the next power of two greater than n_cols, so we can fit each\n    # row in a single block\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n\n    square_output = row * row\n    \n    # Write back output to DRAM\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, square_output, mask=col_offsets < n_cols)\n\n\ndef square(x):\n    n_rows, n_cols = x.shape\n    # The block size is the smallest power of two greater than the number of columns in `x`\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    # Another trick we can use is to ask the compiler to use more threads per row by\n    # increasing the number of warps (`num_warps`) over which each row is distributed.\n    # You will see in the next tutorial how to auto-tune this value in a more natural\n    # way so you don't have to come up with manual heuristics yourself.\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    y = torch.empty_like(x)\n    # Enqueue kernel. The 1D launch grid is simple: we have one kernel instance per row o\n    # f the input matrix\n    square_kernel[(n_rows, )](\n        y,\n        x,\n        x.stride(0),\n        y.stride(0),\n        n_cols,\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return y\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code includes several kernels designed for performing computations related to a specific type of block matrix operation. Here's a summary of the main components:\n\n            1. **chunk_gla_fwd_A_kernel_intra_sub_inter**:\n               - Inputs: `q`, `k`, `g`, `A`, and several scalar configurations.\n               - Operation: This kernel processes blocks of matrix `A` based on inputs `q`, `k`, and `g`. It performs a dot product operation on selected blocks of matrices, adjusting them with scaling and other transformations.\n               - Output: It stores the result back into the matrix `A`.\n\n            2. **chunk_gla_fwd_A_kernel_intra_sub_intra**:\n               - Similar to the first kernel but handles intra-sub block computations. It loads and processes matrix sections, performing transformations based on conditions.\n\n            3. **chunk_gla_fwd_A_kernel_intra_sub_intra_split**:\n               - Splits the computation over the dimension K into smaller chunks to handle larger sizes efficiently.\n               - Uses an additional temporary matrix `A_intra` for intermediate computations.\n\n            4. **chunk_gla_fwd_A_kernel_intra_sub_intra_merge**:\n               - Merges results from the split kernel back into the primary matrix `A`.\n               - Iterates over blocks and aggregates them to form the complete result.\n\n            5. **chunk_gla_fwd_kernel_o**:\n               - Handles forward computation to generate output `o` based on matrices `q`, `v`, `g_cumsum`, `A`, and `h`.\n               - Performs a sequence of matrix operations followed by storing the results into `o`.\n\n            Wrapper Functions:\n            - `chunk_fwd_intra_gated_gk_fn`: Manages the execution of the first set of kernels to compute matrix `A`.\n            - `chunk_fwd_o_gated_gk_fn`: Manages the computation and generation of output `o`.\n\n            The kernels make use of Triton-specific operations like `tl.load`, `tl.store`, and `tl.dot` for efficient execution on modern hardware, especially GPUs.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Kernel for processing blocks of matrix A based on inputs q, k, and g\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(q_ptr, k_ptr, g_ptr, A_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Define block indices\n    block_m = pid // (N // BLOCK_SIZE)\n    block_n = pid % (N // BLOCK_SIZE)\n    \n    # Calculate offsets\n    offs_m = block_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = block_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Load blocks of q and k\n    q = tl.load(q_ptr + offs_m[:, None] * K + tl.arange(0, K)[None, :])\n    k = tl.load(k_ptr + tl.arange(0, K)[:, None] * N + offs_n[None, :])\n    \n    # Perform the dot product\n    A_block = tl.dot(q, k)\n    \n    # Store result back to A\n    tl.store(A_ptr + offs_m[:, None] * N + offs_n[None, :], A_block)\n\n# Kernel for intra-sub block computations\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(q_ptr, k_ptr, g_ptr, A_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Similar logic as the previous kernel but adjusted for intra-sub computations\n    block_m = pid // (N // BLOCK_SIZE)\n    block_n = pid % (N // BLOCK_SIZE)\n    \n    offs_m = block_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = block_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    q = tl.load(q_ptr + offs_m[:, None] * K + tl.arange(0, K)[None, :])\n    k = tl.load(k_ptr + tl.arange(0, K)[:, None] * N + offs_n[None, :])\n    \n    A_block = tl.dot(q, k)\n    \n    tl.store(A_ptr + offs_m[:, None] * N + offs_n[None, :], A_block)\n\n# Kernel for handling split computation over dimension K\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(q_ptr, k_ptr, g_ptr, A_ptr, A_intra_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Logic to split the computation into smaller chunks\n    # Intermediate results are stored in A_intra\n    \n    # Example implementation (simplified)\n    block_m = pid // (N // BLOCK_SIZE)\n    block_n = pid % (N // BLOCK_SIZE)\n    \n    offs_m = block_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = block_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    q = tl.load(q_ptr + offs_m[:, None] * K + tl.arange(0, K)[None, :])\n    k = tl.load(k_ptr + tl.arange(0, K)[:, None] * N + offs_n[None, :])\n    \n    A_intra_block = tl.dot(q, k)\n    \n    tl.store(A_intra_ptr + offs_m[:, None] * N + offs_n[None, :], A_intra_block)\n\n# Kernel for merging results back into matrix A\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(A_intra_ptr, A_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Logic to merge the results from A_intra back into A\n    \n    block_m = pid // (N // BLOCK_SIZE)\n    block_n = pid % (N // BLOCK_SIZE)\n    \n    offs_m = block_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = block_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    A_intra_block = tl.load(A_intra_ptr + offs_m[:, None] * N + offs_n[None, :])\n    \n    # Aggregate and store back to A\n    tl.store(A_ptr + offs_m[:, None] * N + offs_n[None, :], A_intra_block)\n\n# Kernel for forward computation to generate output o\n@triton.jit\ndef chunk_gla_fwd_kernel_o(q_ptr, v_ptr, g_cumsum_ptr, A_ptr, h_ptr, o_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Logic to compute output o\n    block_m = pid // (N // BLOCK_SIZE)\n    block_n = pid % (N // BLOCK_SIZE)\n    \n    offs_m = block_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = block_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    q = tl.load(q_ptr + offs_m[:, None] * K + tl.arange(0, K)[None, :])\n    v = tl.load(v_ptr + tl.arange(0, K)[:, None] * N + offs_n[None, :])\n    \n    # Example operation: dot product\n    o_block = tl.dot(q, v)\n    \n    tl.store(o_ptr + offs_m[:, None] * N + offs_n[None, :], o_block)\n\n# Wrapper function to manage the execution of the first set of kernels\ndef chunk_fwd_intra_gated_gk_fn(q, k, g, A, M, N, K, BLOCK_SIZE):\n    # Launch the kernel\n    grid = (M // BLOCK_SIZE) * (N // BLOCK_SIZE)\n    chunk_gla_fwd_A_kernel_intra_sub_inter[grid](q, k, g, A, M, N, K, BLOCK_SIZE)\n\n# Wrapper function to manage the computation and generation of output o\ndef chunk_fwd_o_gated_gk_fn(q, v, g_cumsum, A, h, o, M, N, K, BLOCK_SIZE):\n    # Launch the kernel\n    grid = (M // BLOCK_SIZE) * (N // BLOCK_SIZE)\n    chunk_gla_fwd_kernel_o[grid](q, v, g_cumsum, A, h, o, M, N, K, BLOCK_SIZE)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BC\", \"BK\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_i * BC) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BK\", \"BT\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr\n):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_j * BC\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n\n    p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n    p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T-i_t*BT-i_i*BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BC\", \"BK\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(\n    q,\n    k,\n    g,\n    A,\n    s_k_h,\n    s_k_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    NC: tl.constexpr\n):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    n_bh = tl.num_programs(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BC\n    m_k = o_k < K\n    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T\n\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n    p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK)\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T-i_t*BT-i_i*BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BC\"],\n)\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(\n    A,\n    A2,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    NK: tl.constexpr\n):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(A + (i_bh + i_k*n_bh) * T * BC, (T, BC), (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"BK\", \"BV\", \"BT\"],\n)\n@triton.jit\ndef chunk_gla_fwd_kernel_o(\n    q,\n    v,\n    g,\n    h,\n    o,\n    A,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = (b_q * tl.exp(b_g)).to(b_q.dtype)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\n\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.).to(b_v.dtype)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\ndef chunk_fwd_intra_gated_gk_fn(q, k, g, scale, BT):\n    B, H, T, K = q.shape\n    BC = 16\n    NC = triton.cdiv(BT, BC)\n    NT = triton.cdiv(T, BT)\n\n    BK = min(64, triton.next_power_of_2(K))\n    A = q.new_empty(B, H, T, BT, dtype=torch.float32)\n    grid = (NT, NC * NC, B * H)\n    chunk_gla_fwd_A_kernel_intra_sub_inter[grid](\n        q, k, g, A,\n        k.stride(1), k.stride(2),\n        scale,\n        T=T, K=K, BT=BT, BC=BC, BK=BK, NC=NC\n    )\n    grid = (NT, NC, B * H)\n    if K <= 256:\n        BK = triton.next_power_of_2(K)\n        chunk_gla_fwd_A_kernel_intra_sub_intra[grid](\n            q, k, g, A,\n            k.stride(1), k.stride(2),\n            scale,\n            T=T, K=K, BT=BT, BC=BC, BK=BK\n        )\n    else:\n        BK = 128\n        NK = triton.cdiv(K, BK)\n        A_intra = q.new_empty(NK, B, H, BT, BC, dtype=torch.float32)\n        grid = (NK, NT * NC, B * H)\n        chunk_gla_fwd_A_kernel_intra_sub_intra_split[grid](\n            q, k, g, A_intra,\n            k.stride(1), k.stride(2),\n            scale,\n            T=T, K=K, BT=BT, BC=BC, BK=BK, NC=NC\n        )\n        grid = (NT, NC, B * H)\n        chunk_gla_fwd_A_kernel_intra_sub_intra_merge[grid](\n            A_intra, A,\n            T=T, BT=BT, BC=BC, NK=NK\n        )\n    return A\n\ndef chunk_fwd_o_gated_gk_fn(q, v, g_cumsum, A, h, BT, scale):\n    B, H, T, K, V = *q.shape, v.shape[-1]\n    BK = min(32, triton.next_power_of_2(K))\n    BV = min(32, triton.next_power_of_2(V))\n    NV = triton.cdiv(V, BV)\n    NT = triton.cdiv(T, BT)\n\n    grid = (NV, NT, B * H)\n    o = torch.empty_like(v)\n    chunk_gla_fwd_kernel_o[grid](\n        q, v, g_cumsum, h, o, A,\n        q.stride(1), q.stride(2),\n        v.stride(1), v.stride(2),\n        h.stride(1), h.stride(2),\n        scale,\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV\n    )\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator implements a custom layer normalization for both the forward and backward passes. The forward pass function `_layer_norm_fwd_1pass_kernel` normalizes input tensors and applies weights and biases. It supports features like residual connections and RMS normalization, controlled by input flags. The backward pass `_layer_norm_bwd_kernel` computes gradients for the inputs, weights, and biases, also optionally recomputing the forward output for more accurate gradients. Inputs are typically 2D tensors, and various configurations are supported for optimization via Triton's autotuning.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_stages=2),\n    ],\n    key=['N']\n)\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, W, B, Y, M, V, N,  # Inputs and outputs\n    eps,                  # Epsilon for numerical stability\n    residual, rms_norm,   # Flags for optional features\n    stride_xm, stride_ym, stride_n,  # Strides for input and output tensors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallelization\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Load input and compute mean\n    x = tl.load(X + offsets * stride_n, mask=mask, other=0.0)\n    mean = tl.sum(x, axis=0) / N\n\n    # Compute variance\n    x_centered = x - mean\n    var = tl.sum(x_centered * x_centered, axis=0) / N\n\n    # Normalize\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    x_norm = x_centered * inv_std\n\n    # Apply weights and biases\n    if rms_norm:\n        norm = tl.sqrt(tl.sum(x * x, axis=0) / N)\n        x_norm = x / norm\n\n    y = x_norm * tl.load(W + offsets * stride_n, mask=mask, other=1.0) + tl.load(B + offsets * stride_n, mask=mask, other=0.0)\n\n    if residual:\n        y += x\n\n    # Store output\n    tl.store(Y + offsets * stride_n, y, mask=mask)\n    tl.store(M + offsets * stride_n, mean, mask=mask)\n    tl.store(V + offsets * stride_n, var, mask=mask)\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 256}, num_stages=2),\n        triton.Config({'BLOCK_SIZE': 512}, num_stages=2),\n    ],\n    key=['N']\n)\n@triton.jit\ndef _layer_norm_bwd_kernel(\n    DY, X, W, B, DX, DW, DB, M, V, N,  # Inputs and outputs\n    eps,                              # Epsilon for numerical stability\n    residual, rms_norm,               # Flags for optional features\n    stride_dym, stride_xm, stride_n,  # Strides for input and output tensors\n    BLOCK_SIZE: tl.constexpr  # Block size for parallelization\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n\n    # Load inputs and forward pass results\n    dy = tl.load(DY + offsets * stride_n, mask=mask, other=0.0)\n    x = tl.load(X + offsets * stride_n, mask=mask, other=0.0)\n    mean = tl.load(M + offsets * stride_n, mask=mask, other=0.0)\n    var = tl.load(V + offsets * stride_n, mask=mask, other=0.0)\n\n    # Recompute forward output if needed\n    inv_std = 1.0 / tl.sqrt(var + eps)\n    x_centered = x - mean\n    x_norm = x_centered * inv_std\n\n    if rms_norm:\n        norm = tl.sqrt(tl.sum(x * x, axis=0) / N)\n        x_norm = x / norm\n\n    # Compute gradients\n    dw = dy * x_norm\n    db = dy\n    dx_norm = dy * tl.load(W + offsets * stride_n, mask=mask, other=1.0)\n\n    # Backprop through normalization\n    dvar = tl.sum(dx_norm * x_centered * -0.5 * inv_std**3, axis=0)\n    dmean = tl.sum(dx_norm * -inv_std, axis=0) + dvar * tl.sum(-2.0 * x_centered, axis=0) / N\n    dx = dx_norm * inv_std + dvar * 2.0 * x_centered / N + dmean / N\n\n    if residual:\n        dx += dy\n\n    # Store gradients\n    tl.store(DX + offsets * stride_n, dx, mask=mask)\n    tl.atomic_add(DW + offsets * stride_n, dw, mask=mask)\n    tl.atomic_add(DB + offsets * stride_n, db, mask=mask)\n\ndef layer_norm(x, weight, bias, eps=1e-5, residual=False, rms_norm=False):\n    # Determine the size of the input tensor\n    N, M = x.shape\n\n    # Allocate output tensors\n    y = torch.empty_like(x)\n    mean = torch.empty(N, dtype=x.dtype, device=x.device)\n    var = torch.empty(N, dtype=x.dtype, device=x.device)\n\n    # Launch the forward kernel\n    _layer_norm_fwd_1pass_kernel[(N,)](\n        x, weight, bias, y, mean, var, N,\n        eps, residual, rms_norm,\n        x.stride(0), y.stride(0), x.stride(1)\n    )\n\n    return y, mean, var\n\ndef layer_norm_backward(dy, x, weight, bias, mean, var, eps=1e-5, residual=False, rms_norm=False):\n    # Determine the size of the input tensor\n    N, M = x.shape\n\n    # Allocate gradient tensors\n    dx = torch.empty_like(x)\n    dw = torch.empty_like(weight)\n    db = torch.empty_like(bias)\n\n    # Launch the backward kernel\n    _layer_norm_bwd_kernel[(N,)](\n        dy, x, weight, bias, dx, dw, db, mean, var, N,\n        eps, residual, rms_norm,\n        dy.stride(0), x.stride(0), x.stride(1)\n    )\n\n    return dx, dw, db\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_RESIDUAL\", \"STORE_RESIDUAL_OUT\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean, Rstd, \n    stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, \n    N, eps, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, \n    HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_BIAS: tl.constexpr\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n\n\ndef _layer_norm_fwd(\n    x, weight, bias, eps, residual=None, out_dtype=None, \n    residual_dtype=None, is_rms_norm=False\n):\n    if residual is not None:\n        residual_dtype = residual.dtype\n    M, N = x.shape\n    assert x.stride(-1) == 1\n    if residual is not None:\n        assert residual.stride(-1) == 1\n        assert residual.shape == (M, N)\n    assert weight.shape == (N,)\n    assert weight.stride(-1) == 1\n    if bias is not None:\n        assert bias.stride(-1) == 1\n        assert bias.shape == (N,)\n    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)\n    assert y.stride(-1) == 1\n    if residual is not None or (residual_dtype is not None and residual_dtype != x.dtype):\n        residual_out = torch.empty(M, N, device=x.device, dtype=residual_dtype)\n        assert residual_out.stride(-1) == 1\n    else:\n        residual_out = None\n    mean = torch.empty((M,), dtype=torch.float32, device=\"cuda\") if not is_rms_norm else None\n    rstd = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _layer_norm_fwd_1pass_kernel[(M,)](\n            x, y, weight, bias, residual, residual_out, \n            mean, rstd, x.stride(0), y.stride(0), \n            residual.stride(0) if residual is not None else 0, \n            residual_out.stride(0) if residual_out is not None else 0, \n            N, eps, is_rms_norm, BLOCK_N, residual is not None, \n            residual_out is not None, bias is not None\n        )\n    return y, mean, rstd, residual_out if residual_out is not None else x\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_DRESIDUAL\", \"STORE_DRESIDUAL\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n@triton.heuristics({\"RECOMPUTE_OUTPUT\": lambda args: args[\"Y\"] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(\n    X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, DRESIDUAL_IN, \n    Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row, \n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, \n    N, eps, rows_per_program, IS_RMS_NORM: tl.constexpr, \n    BLOCK_N: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, \n    STORE_DRESIDUAL: tl.constexpr, HAS_BIAS: tl.constexpr, \n    RECOMPUTE_OUTPUT: tl.constexpr\n):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row) if Mean is not None else 0.0  # \u4fee\u6539\u6b64\u884c\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n\n\ndef _layer_norm_bwd(\n    dy, x, weight, bias, eps, mean, rstd, dresidual=None, \n    has_residual=False, is_rms_norm=False, x_dtype=None, recompute_output=False\n):\n    M, N = x.shape\n    assert x.stride(-1) == 1\n    assert dy.stride(-1) == 1\n    assert dy.shape == (M, N)\n    if dresidual is not None:\n        assert dresidual.stride(-1) == 1\n        assert dresidual.shape == (M, N)\n    assert weight.shape == (N,)\n    assert weight.stride(-1) == 1\n    if bias is not None:\n        assert bias.stride(-1) == 1\n        assert bias.shape == (N,)\n    dx = (\n        torch.empty_like(x)\n        if x_dtype is None\n        else torch.empty(M, N, dtype=x_dtype, device=x.device)\n    )\n    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None\n    y = torch.empty(M, N, dtype=dy.dtype, device=dy.device) if recompute_output else None\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    sm_count = torch.cuda.get_device_properties(x.device).multi_processor_count\n    _dw = torch.empty((sm_count, N), dtype=torch.float32, device=weight.device)\n    _db = (\n        torch.empty((sm_count, N), dtype=torch.float32, device=bias.device)\n        if bias is not None\n        else None\n    )\n    rows_per_program = math.ceil(M / sm_count)\n    grid = (sm_count,)\n    with torch.cuda.device(x.device.index):\n        _layer_norm_bwd_kernel[grid](\n            x, weight, bias, y, dy, dx, _dw, _db, dresidual, \n            dresidual_in, mean, rstd, x.stride(0), \n            0 if not recompute_output else y.stride(0), dy.stride(0), \n            dx.stride(0), dresidual.stride(0) if dresidual is not None else 0, \n            dresidual_in.stride(0) if dresidual_in is not None else 0, \n            M, N, eps, rows_per_program, is_rms_norm, BLOCK_N, \n            dresidual is not None, dresidual_in is not None, bias is not None\n        )\n    dw = _dw.sum(0).to(weight.dtype)\n    db = _db.sum(0).to(bias.dtype) if bias is not None else None\n    if has_residual and dx.dtype == x.dtype:\n        dresidual_in = dx\n    return (dx, dw, db, dresidual_in) if not recompute_output else (dx, dw, db, dresidual_in, y)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel and wrapper function are designed to copy keys and values from tensors `K` and `V` to the blocked cache tensors `KCache` and `VCache` during the decoding stage of a model. \n            The kernel function `_copy_to_kvcache_seqlen1_kernel` is defined using the Triton JIT compiler, allowing it to efficiently perform memory operations with different types of cache layouts.\n            The function `copy_kv_to_blocked_cache` acts as a wrapper that prepares input data and parameters for the Triton kernel call.\n            The kernel copies segments of the input `K` and `V` to the respective locations in `KCache` and `VCache`, based on the computed offsets and cache layout.\n            Grid dimensions `(bsz, num_kv_heads)` are used to launch the kernel, where `bsz` is the batch size and `num_kv_heads` is the number of key-value attention heads.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _copy_to_kvcache_seqlen1_kernel(\n    K_ptr, V_ptr, KCache_ptr, VCache_ptr,\n    bsz, num_kv_heads, head_dim,\n    K_stride0, K_stride1, K_stride2,\n    V_stride0, V_stride1, V_stride2,\n    KCache_stride0, KCache_stride1, KCache_stride2,\n    VCache_stride0, VCache_stride1, VCache_stride2,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Define program ID for grid\n    pid0 = tl.program_id(0)  # batch index\n    pid1 = tl.program_id(1)  # head index\n\n    # Compute the offsets for K and V\n    K_offset = pid0 * K_stride0 + pid1 * K_stride1\n    V_offset = pid0 * V_stride0 + pid1 * V_stride1\n\n    # Compute the offsets for KCache and VCache\n    KCache_offset = pid0 * KCache_stride0 + pid1 * KCache_stride1\n    VCache_offset = pid0 * VCache_stride0 + pid1 * VCache_stride1\n\n    # Load K and V values\n    K = tl.load(K_ptr + K_offset + tl.arange(0, BLOCK_SIZE))\n    V = tl.load(V_ptr + V_offset + tl.arange(0, BLOCK_SIZE))\n\n    # Store values into KCache and VCache\n    tl.store(KCache_ptr + KCache_offset + tl.arange(0, BLOCK_SIZE), K)\n    tl.store(VCache_ptr + VCache_offset + tl.arange(0, BLOCK_SIZE), V)\n\n\ndef copy_kv_to_blocked_cache(K, V, KCache, VCache, bsz, num_kv_heads, head_dim):\n    # Define block size for the kernel\n    BLOCK_SIZE = head_dim\n\n    # Compute strides for input and cache tensors\n    K_stride0, K_stride1, K_stride2 = K.stride()\n    V_stride0, V_stride1, V_stride2 = V.stride()\n    KCache_stride0, KCache_stride1, KCache_stride2 = KCache.stride()\n    VCache_stride0, VCache_stride1, VCache_stride2 = VCache.stride()\n\n    # Launch the Triton kernel\n    grid = (bsz, num_kv_heads)\n    _copy_to_kvcache_seqlen1_kernel[grid](\n        K, V, KCache, VCache,\n        bsz, num_kv_heads, head_dim,\n        K_stride0, K_stride1, K_stride2,\n        V_stride0, V_stride1, V_stride2,\n        KCache_stride0, KCache_stride1, KCache_stride2,\n        VCache_stride0, VCache_stride1, VCache_stride2,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n# supports two types of cache layouts\n# 1. [num_blocks, num_kv_heads, block_size, head_dim]\n# 2. [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n@triton.jit\ndef _copy_to_kvcache_seqlen1_kernel(\n    K,\n    V,\n    KCache,\n    VCache,\n    BLOCK_TABLES,\n    context_lengths,\n    stride_kt,\n    stride_kh,\n    stride_kd,\n    stride_vt,\n    stride_vh,\n    stride_vd,\n    stride_kcb,\n    stride_kch,\n    stride_kcsplit_x,\n    stride_kcs,\n    stride_kcd,\n    stride_vcb,\n    stride_vch,\n    stride_vcs,\n    stride_vcd,\n    stride_bts,\n    stride_btb,\n    block_size,\n    HEAD_DIM: tl.constexpr,\n    KCACHE_X: tl.constexpr,\n):\n    cur_seq_idx = tl.program_id(0)\n    cur_kv_head_idx = tl.program_id(1)\n\n    past_kv_seq_len = tl.load(context_lengths + cur_seq_idx) - 1\n    last_bt_block_idx = past_kv_seq_len // block_size\n    block_table_ptr = BLOCK_TABLES + cur_seq_idx * stride_bts\n    block_id = tl.load(block_table_ptr + last_bt_block_idx * stride_btb)\n    offsets_in_last_block = past_kv_seq_len % block_size\n\n    range_x = tl.arange(0, KCACHE_X)\n    offsets_dmodel_x_partition = tl.arange(0, KCACHE_X)\n\n    for split_x in tl.static_range(HEAD_DIM // KCACHE_X):\n        offsets_dmodel_x_partition = tl.arange(split_x * KCACHE_X, (split_x + 1) * KCACHE_X)\n        offsets_k = cur_seq_idx * stride_kt + cur_kv_head_idx * stride_kh + offsets_dmodel_x_partition * stride_kd\n        k = tl.load(K + offsets_k)\n        offsets_v = cur_seq_idx * stride_vt + cur_kv_head_idx * stride_vh + offsets_dmodel_x_partition * stride_vd\n        v = tl.load(V + offsets_v)\n\n        offsets_kcache = (\n            block_id * stride_kcb\n            + cur_kv_head_idx * stride_kch\n            + split_x * stride_kcsplit_x\n            + offsets_in_last_block * stride_kcs\n            + range_x\n        )\n        tl.store(KCache + offsets_kcache, k)\n        offsets_vcache = (\n            block_id * stride_vcb\n            + cur_kv_head_idx * stride_vch\n            + offsets_in_last_block * stride_vcs\n            + offsets_dmodel_x_partition * stride_vcd\n        )\n        tl.store(VCache + offsets_vcache, v)\n    return\n\n\ndef copy_kv_to_blocked_cache(\n    k: torch.Tensor,\n    v: torch.Tensor,\n    k_cache: torch.Tensor,\n    v_cache: torch.Tensor,\n    kv_lengths: torch.Tensor,\n    block_tables: torch.Tensor,\n    use_new_kcache_layout: bool = False,\n):\n    \"\"\"\n    Copy keys or values to the blocked key/value cache during decoding stage.\n\n    Args:\n        k (torch.Tensor): [bsz, 1, num_kv_heads, head_dim]/[bsz, num_kv_heads, head_dim] - Keys during decoding with seq len 1.\n        v (torch.Tensor): [bsz, 1, num_kv_heads, head_dim]/[bsz, num_kv_heads, head_dim] - Values during decoding with seq len 1.\n        k_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim] - Blocked key cache.\n        v_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim] - Blocked value cache.\n        kv_lengths (torch.Tensor): [bsz] - Past key/value sequence lengths plus current sequence length for each sequence.\n        block_tables (torch.Tensor): [bsz, max_blocks_per_sequence] - Block tables for each sequence.\n        use_new_kcache_layout (bool): Whether to use the new layout for kcache. Default to False.\n    \"\"\"\n    k_cache_shape = k_cache.shape\n    v_cache_shape = v_cache.shape\n\n    if use_new_kcache_layout:\n        assert (\n            len(k_cache_shape) == 5\n            and k_cache_shape[1] == v_cache_shape[1]\n            and k_cache_shape[2] * k_cache_shape[4] == v_cache_shape[3]\n        ), f\"Invalid KCache shape {k_cache_shape} and VCache shape {v_cache_shape}\"\n    else:\n        assert k.size(-1) == k_cache_shape[-1], \"Incompatible head dim\"\n        assert (\n            k_cache_shape == v_cache_shape\n        ), f\"Incompatible KCache shape {k_cache_shape} and VCache shape {v_cache_shape}\"\n    assert v.size(-1) == v_cache_shape[-1], \"Incompatible head dim\"\n\n    k = k.squeeze(1) if k.dim() == 4 else k\n    assert k.dim() == 3, f\"Incompatible k dim {k.dim()}\"\n    v = v.squeeze(1) if v.dim() == 4 else v\n    assert v.dim() == 3, f\"Incompatible v dim {v.dim()}\"\n\n    bsz, num_kv_heads, head_dim = k.shape\n    assert kv_lengths.shape[0] == block_tables.shape[0] == bsz, (\n        f\"Got incompatible batch size (number of seqs):\\n\"\n        f\"  Past kv sequence lengths bsz {kv_lengths.shape[0]}; \"\n        f\" block tables bsz {block_tables.shape[0]}, input k batch size {bsz}\"\n    )\n\n    # Modify if the shape of kv cahce is changed.\n    block_size = k_cache.size(-2)\n\n    x = head_dim\n    stride_kcsplit_x, stride_kcs, stride_kcd = 0, k_cache.stride(2), k_cache.stride(3)\n    if use_new_kcache_layout:\n        x = k_cache.size(-1)\n        stride_kcsplit_x, stride_kcs, stride_kcd = k_cache.stride()[2:]\n\n    num_warps = 8 if head_dim > 128 else 4\n    grid = (bsz, num_kv_heads)\n    _copy_to_kvcache_seqlen1_kernel[grid](\n        k,\n        v,\n        k_cache,\n        v_cache,\n        block_tables,\n        kv_lengths,\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        k_cache.stride(0),\n        k_cache.stride(1),\n        stride_kcsplit_x,\n        stride_kcs,\n        stride_kcd,\n        v_cache.stride(0),\n        v_cache.stride(1),\n        v_cache.stride(2),\n        v_cache.stride(3),\n        block_tables.stride(0),\n        block_tables.stride(1),\n        block_size,\n        HEAD_DIM=head_dim,\n        KCACHE_X=x,\n        num_warps=num_warps,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `chunk_simple_gla_fwd_kernel_o` function is a Triton kernel designed to perform a forward pass operation for a chunked computation with queries (`q`), keys (`k`), values (`v`), and an auxiliary tensor (`h`). It outputs tensor `o`. The function takes in dimensions like `BT`, `BK`, and `BV` which represent chunk sizes. The kernel processes input tensors to compute intermediate results using block pointers and dot products, applying exponential transformations and scaling. The `chunk_fwd_o_fn` is a wrapper that prepares parameters and calls this kernel on a grid defined by the dimensions of the inputs.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for forward pass operation\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(q_ptr, k_ptr, v_ptr, h_ptr, o_ptr,\n                                  BT, BK, BV,\n                                  stride_qb, stride_qk,\n                                  stride_kb, stride_kk,\n                                  stride_vb, stride_vk,\n                                  stride_hb, stride_hk,\n                                  stride_ob, stride_oo,\n                                  BLOCK_SIZE: tl.constexpr):\n    # Program ID for parallel execution\n    pid = tl.program_id(0)\n    \n    # Compute block start indices\n    block_start_b = pid // BT\n    block_start_k = (pid % BT) * BLOCK_SIZE\n\n    # Pointers to the current block\n    q_block_ptr = q_ptr + block_start_b * stride_qb + block_start_k * stride_qk\n    k_block_ptr = k_ptr + block_start_b * stride_kb + block_start_k * stride_kk\n    v_block_ptr = v_ptr + block_start_b * stride_vb + block_start_k * stride_vk\n    h_block_ptr = h_ptr + block_start_b * stride_hb + block_start_k * stride_hk\n    o_block_ptr = o_ptr + block_start_b * stride_ob + block_start_k * stride_oo\n\n    # Load q, k, v, h into shared memory\n    q = tl.load(q_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_qk)\n    k = tl.load(k_block_ptr + tl.arange(0, BLOCK_SIZE)[None, :] * stride_kk)\n    v = tl.load(v_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_vk)\n    h = tl.load(h_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_hk)\n\n    # Compute dot product q * k^T\n    qk = tl.dot(q, k)\n\n    # Apply exponential transformation and scaling\n    qk_exp = tl.exp(qk)\n    qk_scaled = qk_exp * h\n\n    # Compute the output\n    o = tl.dot(qk_scaled, v)\n\n    # Store the result\n    tl.store(o_block_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_oo, o)\n\n\n# Wrapper function to prepare parameters and launch the kernel\ndef chunk_fwd_o_fn(q, k, v, h, o, BT, BK, BV):\n    # Get the shape and strides of the input tensors\n    q_shape, q_strides = q.shape, q.strides\n    k_shape, k_strides = k.shape, k.strides\n    v_shape, v_strides = v.shape, v.strides\n    h_shape, h_strides = h.shape, h.strides\n    o_shape, o_strides = o.shape, o.strides\n\n    # Define grid size\n    grid = (BT * (q_shape[1] // BK),)\n\n    # Launch the kernel\n    chunk_simple_gla_fwd_kernel_o[grid](\n        q, k, v, h, o,\n        BT, BK, BV,\n        q_strides[0], q_strides[1],\n        k_strides[0], k_strides[1],\n        v_strides[0], v_strides[1],\n        h_strides[0], h_strides[1],\n        o_strides[0], o_strides[1],\n        BLOCK_SIZE=BK\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    g,\n    o,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    # Kernel implementation\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        # [BT, BK]\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BK, BV]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_g)[:, None]\n    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\ndef chunk_fwd_o_fn(h, q, k, v, g, BT, scale):\n    # Kernel call\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    o = torch.empty_like(v)\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NV = triton.cdiv(V, BV)\n    NT = triton.cdiv(T, BT)\n    grid = (NV, NT, B * H)\n    chunk_simple_gla_fwd_kernel_o[grid](\n        q, k, v, h, g, o,\n        q.stride(1), q.stride(2),\n        v.stride(1), v.stride(2),\n        h.stride(1), h.stride(2),\n        scale,\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV\n    )\n    return o\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code implements a matrix multiplication routine involving quantization for integer 8-bit arithmetic. The main functions are `quantize_int8_perrow_kernel`, `quantize_int8_perrow`, `matmul_kernel`, `matmul_quantize_int8`, `matmul_int8`, and `quantize_int8`. The `quantize_int8_perrow_kernel` function is a Triton kernel that quantizes floating-point matrices to int8 by computing the maximum per row and then scaling elements to fit into int8. `quantize_int8_perrow` is a helper function to perform this operation and return quantized matrices. `matmul_kernel` performs the core matrix multiplication, supporting operations with quantized matrices by using scaling factors to reconstruct the floating-point results. `matmul_quantize_int8` is a wrapper to first quantize the input matrix and then call the matrix multiplication routine. `matmul_int8` performs matrix multiplication on already quantized inputs using Triton kernels. The function `quantize_int8` applies quantization along the specified axis and prepares data for efficient computation. Input parameters are mainly pointers to matrices and their dimensions. The outputs include quantized matrices and results from the matrix multiplication.", "predict": "import triton\nimport triton.language as tl\n\n# Kernel for quantizing a matrix to int8 per row\n@triton.jit\ndef quantize_int8_perrow_kernel(\n    matrix_ptr, quantized_ptr, scale_ptr, \n    M, N, stride_m, stride_n,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    # Compute row index\n    row_idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = row_idx < M\n\n    # Load the row from the matrix\n    row = tl.load(matrix_ptr + row_idx[:, None] * stride_m + tl.arange(0, N) * stride_n, mask=mask[:, None])\n\n    # Find the maximum value per row\n    max_val = tl.max(row, axis=1)\n\n    # Compute the scale factor\n    scale = max_val / 127.0\n    tl.store(scale_ptr + row_idx, scale, mask=mask)\n\n    # Quantize the row\n    quantized_row = (row / scale[:, None]).to(tl.int8)\n    tl.store(quantized_ptr + row_idx[:, None] * stride_m + tl.arange(0, N) * stride_n, quantized_row, mask=mask[:, None])\n\ndef quantize_int8_perrow(matrix, M, N):\n    quantized_matrix = triton.empty((M, N), dtype=tl.int8)\n    scale_factors = triton.empty((M,), dtype=tl.float32)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE']),)\n    quantize_int8_perrow_kernel[grid](\n        matrix, quantized_matrix, scale_factors,\n        M, N, matrix.stride(0), matrix.stride(1),\n        BLOCK_SIZE=128\n    )\n    return quantized_matrix, scale_factors\n\n# Kernel for matrix multiplication with quantized inputs\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr, \n    scale_A_ptr, scale_B_ptr, M, N, K, \n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute start of the block\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    # Create accumulators\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a = tl.load(A_ptr + (start_m + tl.arange(0, BLOCK_SIZE_M))[:, None] * stride_am + (k + tl.arange(0, BLOCK_SIZE_K)) * stride_ak)\n        b = tl.load(B_ptr + (k + tl.arange(0, BLOCK_SIZE_K))[:, None] * stride_bk + (start_n + tl.arange(0, BLOCK_SIZE_N)) * stride_bn)\n\n        # Load scales\n        scale_A = tl.load(scale_A_ptr + start_m + tl.arange(0, BLOCK_SIZE_M))\n        scale_B = tl.load(scale_B_ptr + k + tl.arange(0, BLOCK_SIZE_K))\n\n        # Convert to float32\n        a = a.to(tl.float32) * scale_A[:, None]\n        b = b.to(tl.float32) * scale_B[None, :]\n\n        # Compute matrix multiplication\n        acc += tl.dot(a, b)\n\n    # Store result\n    tl.store(C_ptr + (start_m + tl.arange(0, BLOCK_SIZE_M))[:, None] * stride_cm + (start_n + tl.arange(0, BLOCK_SIZE_N)) * stride_cn, acc)\n\ndef matmul_quantize_int8(A, B, M, N, K):\n    # Quantize matrices A and B\n    A_quantized, scale_A = quantize_int8_perrow(A, M, K)\n    B_quantized, scale_B = quantize_int8_perrow(B, K, N)\n\n    # Prepare output matrix\n    C = triton.empty((M, N), dtype=tl.float32)\n\n    # Define grid\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']), triton.cdiv(N, META['BLOCK_SIZE_N']))\n\n    # Launch matmul kernel\n    matmul_kernel[grid](\n        A_quantized, B_quantized, C, \n        scale_A, scale_B, M, N, K,\n        A.stride(0), A.stride(1), B.stride(0), B.stride(1), C.stride(0), C.stride(1),\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32\n    )\n    return C\n\ndef matmul_int8(A_quantized, B_quantized, scale_A, scale_B, M, N, K):\n    C = triton.empty((M, N), dtype=tl.float32)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']), triton.cdiv(N, META['BLOCK_SIZE_N']))\n    matmul_kernel[grid](\n        A_quantized, B_quantized, C, \n        scale_A, scale_B, M, N, K,\n        A_quantized.stride(0), A_quantized.stride(1), B_quantized.stride(0), B_quantized.stride(1), C.stride(0), C.stride(1),\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32\n    )\n    return C\n\ndef quantize_int8(matrix, axis):\n    if axis == 0:\n        # Quantize per row\n        return quantize_int8_perrow(matrix, *matrix.shape)\n    else:\n        # For simplicity, we only implement per-row quantization here\n        raise NotImplementedError(\"Only per-row quantization is implemented.\")\n", "label": "import time\nimport torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=4),\n        triton.Config({}, num_stages=2, num_warps=2),\n        triton.Config({}, num_stages=2, num_warps=1),\n     ],\n    key=['K'],\n)\n@triton.jit\ndef quantize_int8_perrow_kernel(\n    fpa_ptr, a_ptr, as_ptr,\n    M, K, \n    stride_fpam, stride_fpak,\n    stride_am, stride_ak,\n    stride_asm,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = (a_max / 127.)\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        inta = (fpa / a_scale[:, None]).to(tl.int8)\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n\n\ndef quantize_int8_perrow(fpa):\n    a = torch.empty(fpa.shape, device=fpa.device, dtype=torch.int8)\n    a_scale = torch.empty(fpa.shape[0], device=fpa.device, dtype=torch.float16)\n    M, K = fpa.shape\n    BLOCK_SIZE_M = 1\n    BLOCK_SIZE_K = triton.next_power_of_2(K)\n    grid = (M // BLOCK_SIZE_M,)\n    quantize_int8_perrow_kernel[grid](\n        fpa, a, a_scale,\n        M, K,\n        fpa.stride(0), fpa.stride(1),\n        a.stride(0), a.stride(1),\n        a_scale.stride(0),\n        BLOCK_SIZE_M, BLOCK_SIZE_K,\n    )\n    return a, a_scale\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n    ],\n    key=['M', 'N', 'K'],\n    reset_to_zero=['c_ptr']\n)\n@triton.jit\ndef matmul_kernel(\n    # Pointers to matrices\n    a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr,\n    # Matrix dimensions\n    M, N, K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n    # by to get the element one row down (A has M rows).\n    stride_am, stride_ak,\n    stride_asm,\n    stride_bk, stride_bn,\n    stride_bsn,\n    stride_cm, stride_cn,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr, \n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    # -----------------------------------------------------------\n    # Map program ids `pid` to the block of C it should compute.\n    # This is done in a grouped ordering to promote L2 data reuse.\n    # See above `L2 Cache Optimizations` section for details.\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # ----------------------------------------------------------\n    # Create pointers for the first blocks of A and B.\n    # We will advance this pointer as we move in the K direction\n    # and accumulate\n    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n    # See above `Pointer Arithmetics` section for details\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        # If it is out of bounds, set it to 0.\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n        # We accumulate along the K dimension.\n        accumulator += tl.dot(a, b)\n        # Advance the ptrs to the next K block.\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    # You can fuse arbitrary activation functions here\n    # while the accumulator is still in FP32!\n    c = (accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]).to(tl.float16)\n    # -----------------------------------------------------------\n    # Write back the block of the output matrix C with masks.\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_quantize_int8(fpa, b, b_scale, out=None):\n    a, a_scale = quantize_int8_perrow(fpa)\n    # a, a_scale = quantize_int8(fpa, axis=1)\n    return matmul_int8(a, a_scale, b, b_scale, out)\n\n\ndef matmul_int8(a, a_scale, b, b_scale, out=None):\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n    # Allocates output.\n    if out == None:\n        c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n    else:\n        c = out.fill_(0.)\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n        META['SPLIT_K'],\n    )\n    matmul_kernel[grid](\n        a, a_scale, b, b_scale, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        a_scale.stride(0),\n        b.stride(0), b.stride(1),\n        b_scale.stride(0),\n        c.stride(0), c.stride(1),\n    )\n    return c\n\n\ndef quantize_int8(weight, axis=0):\n    # Weight shape: [H1, H2]\n    # Scale shape: [H2]\n    scale = weight.abs().amax(axis, keepdim=True) / 127.\n    weight = (weight / scale).to(torch.int8)\n    # col major will accelerate i8xi8 kernel.\n    if axis == 0:\n        weight = weight.t().contiguous().t()\n    scale = scale.squeeze(axis)\n    return weight, scale, None\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a Triton kernel named _seeded_dropout and a corresponding Python wrapper function named seeded_dropout. The main function of this code is to perform a dropout operation with a fixed seed for reproducibility. The kernel processes input data in blocks of a specified size and applies a dropout mask generated by a random number, comparing it against a probability threshold. The output is written back with scaling to maintain the expected value. Input parameters are x, the input tensor; p, the dropout probability; seed, the random seed; and BLOCK_SIZE, which specifies the number of elements each block handles. The output is a tensor with the same shape as the input, where elements are either zeroed out or scaled.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _seeded_dropout(x_ptr, output_ptr, p, seed, BLOCK_SIZE, n_elements, **meta):\n    # Compute the index of the block\n    block_start = tl.program_id(0) * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load the data\n    x = tl.load(x_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    \n    # Initialize random state\n    rng_state = tl.program_id(0) + seed\n    # Generate random numbers for dropout mask\n    random_values = tl.rand(rng_state, offsets)\n    \n    # Create dropout mask\n    mask = random_values > p\n    \n    # Apply mask and scale the output\n    output = tl.where(mask, x / (1 - p), 0.0)\n    \n    # Store the result\n    tl.store(output_ptr + offsets, output, mask=offsets < n_elements)\n\ndef seeded_dropout(x, p, seed, BLOCK_SIZE=1024):\n    assert 0 <= p <= 1, \"Dropout probability must be in [0, 1]\"\n    assert x.is_cuda, \"Input tensor must be on CUDA device\"\n    \n    # Get the number of elements in the input tensor\n    n_elements = x.numel()\n    \n    # Allocate output tensor\n    output = torch.empty_like(x)\n    \n    # Launch the Triton kernel\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    _seeded_dropout[grid](\n        x, output, p, seed, BLOCK_SIZE, n_elements,\n        num_warps=4  # Number of warps to use for the kernel\n    )\n    \n    return output\n\n# Example usage\nx = torch.randn(4096, device='cuda')\np = 0.5\nseed = 42\noutput = seeded_dropout(x, p, seed)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _seeded_dropout(\n    x_ptr,\n    output_ptr,\n    n_elements,\n    p,\n    seed,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # compute memory offsets of elements handled by this instance\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    # load data from x\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    # randomly prune it\n    random = tl.rand(seed, offsets)\n    x_keep = random > p\n    # write-back\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef seeded_dropout(x, p, seed):\n    output = torch.empty_like(x)\n    assert x.is_contiguous()\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n    return output\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The function `int8_matmul_rowwise_dequantize` performs matrix multiplication on int8 matrices A and B, while applying row-wise dequantization using scaling factors from `state_x` and `state_w`. The result is stored in a float16 matrix C. The kernel `_int8_matmul_rowwise_dequantize` is optimized for performance using Triton's `autotune` decorator. Inputs: matrices A, B, scaling factors `state_x`, `state_w`, and optional `bias`. Outputs: matrix C, the dequantized product. Key parameters: BLOCK_M, BLOCK_N, BLOCK_K control tiling; SPLIT_K allows partitioning K dimension for parallelism.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_warps=8),\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 2}, num_warps=8),\n        # Add more configurations as needed\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef _int8_matmul_rowwise_dequantize(A_ptr, B_ptr, state_x_ptr, state_w_ptr, C_ptr, bias_ptr, \n                                    M, N, K, stride_am, stride_ak, stride_bn, stride_bk, stride_cm, stride_cn,\n                                    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, SPLIT_K: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_m = (M + BLOCK_M - 1) // BLOCK_M\n    num_pid_n = (N + BLOCK_N - 1) // BLOCK_N\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    A_tile = tl.load(A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak), mask=offs_am[:, None] < M)\n    B_tile = tl.load(B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn), mask=offs_bn[None, :] < N)\n\n    state_x = tl.load(state_x_ptr + offs_am, mask=offs_am < M)\n    state_w = tl.load(state_w_ptr + offs_bn, mask=offs_bn < N)\n\n    C_tile = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        A_sub = A_tile.to(tl.int32)\n        B_sub = B_tile.to(tl.int32)\n        C_tile += tl.dot(A_sub, B_sub)\n\n    C_tile = C_tile * (state_x[:, None].to(tl.float32) * state_w[None, :].to(tl.float32))\n\n    if bias_ptr is not None:\n        bias = tl.load(bias_ptr + offs_bn, mask=offs_bn < N)\n        C_tile += bias[None, :]\n\n    C_tile = C_tile.to(tl.float16)\n    tl.store(C_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn), C_tile, mask=offs_am[:, None] < M)\n\ndef int8_matmul_rowwise_dequantize(A, B, state_x, state_w, bias=None):\n    M, K = A.shape\n    K, N = B.shape\n\n    C = torch.empty((M, N), dtype=torch.float16, device=A.device)\n\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    _int8_matmul_rowwise_dequantize[grid](\n        A, B, state_x, state_w, C, bias,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(1), B.stride(0),\n        C.stride(0), C.stride(1),\n        BLOCK_M=64, BLOCK_N=64, BLOCK_K=32, SPLIT_K=1  # Default values, autotuner will choose best\n    )\n    return C\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time\n\n\ndef get_configs_io_bound():\n    def init_to_zero(name):\n        return lambda nargs: nargs[name].zero_()\n\n    configs = []\n    for num_stages in [2, 3, 4, 5, 6]:\n        for block_m in [16, 32]:\n            for block_k in [32, 64]:\n                for block_n in [32, 64, 128, 256]:\n                    num_warps = 2 if block_n <= 64 else 4\n                    configs.append(\n                        triton.Config(\n                            {\"BLOCK_M\": block_m, \"BLOCK_N\": block_n, \"BLOCK_K\": block_k, \"SPLIT_K\": 1},\n                            num_stages=num_stages,\n                            num_warps=num_warps,\n                        ),\n                    )\n                    # split_k\n                    for split_k in [2, 4, 8, 16]:\n                        configs.append(\n                            triton.Config(\n                                {\"BLOCK_M\": block_m, \"BLOCK_N\": block_n, \"BLOCK_K\": block_k, \"SPLIT_K\": split_k},\n                                num_stages=num_stages,\n                                num_warps=num_warps,\n                                pre_hook=init_to_zero(\"C\"),\n                            ),\n                        )\n    return configs\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n        *get_configs_io_bound(),\n    ],\n    key=[\"M\", \"N\", \"K\"],\n    prune_configs_by={\"early_config_prune\": early_config_prune, \"perf_model\": estimate_matmul_time, \"top_k\": 10},\n)\n@triton.heuristics(\n    {\n        \"EVEN_K\": lambda args: args[\"K\"] % (args[\"BLOCK_K\"] * args[\"SPLIT_K\"]) == 0,\n    },\n)\n@triton.jit\ndef _int8_matmul_rowwise_dequantize(\n    A,\n    B,\n    C,\n    bias,\n    state_x_ptr,\n    state_w_ptr,\n    M,\n    N,\n    K,\n    divfactor,\n    has_bias: tl.constexpr,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    w_factor = tl.load(state_w_ptr + rbn)[None, :]\n    x_factor = tl.load(state_x_ptr + ram)[:, None]\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc = w_factor * (x_factor * (acc * divfactor))\n    acc = acc.to(C.dtype.element_ty)\n    if has_bias:\n        bias = tl.load(bias + rn).to(C.dtype.element_ty)\n        acc = acc + bias[None, :]\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n\ndef int8_matmul_rowwise_dequantize(a, b, state_x, state_w, bias):\n    divfactor = 1.0 / (127.0 * 127.0)\n    has_bias = 0 if bias is None else 1\n    device = a.device\n    if a.stride(0) > 1 and a.stride(1) > 1:\n        a = a.contiguous()\n    if b.stride(0) > 1 and b.stride(1) > 1:\n        b = b.contiguous()\n    assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n    M, K = a.shape\n    _, N = b.shape\n    c = torch.empty((M, N), device=device, dtype=torch.float16)\n    ACC_TYPE = tl.float32\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]), META[\"SPLIT_K\"])\n    _int8_matmul_rowwise_dequantize[grid](\n        a,\n        b,\n        c,\n        bias,\n        state_x,\n        state_w,\n        M,\n        N,\n        K,\n        divfactor,\n        has_bias,\n        a.stride(0),\n        a.stride(1),\n        b.stride(0),\n        b.stride(1),\n        c.stride(0),\n        c.stride(1),\n        GROUP_M=8,\n        ACC_TYPE=ACC_TYPE,\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is designed to perform a combined operation of quantizing and transposing a matrix. The main kernel is `_quantize_global_transpose`, which takes in a float matrix `A`, an inverse maximum absolute value `absmax_inv_ptr`, and produces an int8 matrix `B` that is transposed from `A`. The operation quantizes the elements of `A` based on the scaling factor derived from the maximum absolute value in `A`, then transposes the result into `B`. The input matrix `A` is quantized by multiplying it with the `absmax_inv`, and the result is scaled to the range of int8 values. The kernel uses grid-stride loops to handle matrices larger than the block size specified by `BLOCK_M` and `BLOCK_N`. The helper function `quantize_global_transpose` sets up the necessary configuration and calls the Triton kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the block sizes\nBLOCK_M = 128\nBLOCK_N = 128\n\n@triton.jit\ndef _quantize_global_transpose(A_ptr, absmax_inv_ptr, B_ptr, M, N, stride_am, stride_an, stride_bm, stride_bn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Compute the row and column indices for this block\n    row_idx = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    col_idx = tl.program_id(1) * BLOCK_N + tl.arange(0, BLOCK_N)\n    \n    # Load the inverse of the maximum absolute value\n    absmax_inv = tl.load(absmax_inv_ptr)\n    \n    # Create a mask for the bounds of the matrix\n    mask = (row_idx < M) & (col_idx < N)\n    \n    # Load the block from A\n    A = tl.load(A_ptr + row_idx[:, None] * stride_am + col_idx[None, :] * stride_an, mask=mask, other=0.0)\n    \n    # Quantize the block\n    A_quantized = A * absmax_inv\n    A_quantized = tl.clamp(A_quantized, -128, 127).to(tl.int8)\n    \n    # Transpose the block\n    B = tl.trans(A_quantized)\n    \n    # Store the transposed and quantized block into B\n    tl.store(B_ptr + col_idx[:, None] * stride_bm + row_idx[None, :] * stride_bn, B, mask=mask)\n\ndef quantize_global_transpose(A, absmax_inv, M, N):\n    # Calculate the number of blocks\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n    \n    # Allocate output matrix B\n    B = torch.empty((N, M), dtype=torch.int8, device='cuda')\n    \n    # Launch the Triton kernel\n    _quantize_global_transpose[grid](\n        A, absmax_inv, B,\n        M, N,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N\n    )\n    \n    return B\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# global quantize and transpose\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"GROUP_M\": 8}, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"GROUP_M\": 8}, num_warps=4),\n        # ...\n    ],\n    key=[\"M\", \"N\"],\n)\n@triton.jit\ndef _quantize_global_transpose(\n    A,\n    absmax_inv_ptr,\n    B,\n    stride_am,\n    stride_an,\n    stride_bn,\n    stride_bm,\n    M,\n    N,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // group_size\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A = A + (rm[:, None] * stride_am + rn[None, :] * stride_an)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    a = tl.load(A, mask=mask)\n    absmax_inv = tl.load(absmax_inv_ptr)\n\n    # rematerialize to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    B = B + (rm[:, None] * stride_bm + rn[None, :] * stride_bn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (a * absmax_inv))\n\n    tl.store(B, output, mask=mask)\n\ndef quantize_global_transpose(input):\n    absmax = input.abs().max().unsqueeze(0)\n    absmax_inv = 1.0 / absmax\n    M, N = input.shape\n    out = torch.empty(N, M, device=\"cuda\", dtype=torch.int8)\n\n    assert out.size(0) == N and out.size(1) == M\n    assert input.stride(0) == 1 or input.stride(1) == 1\n    assert out.stride(0) == 1 or out.stride(1) == 1\n\n    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),)\n    _quantize_global_transpose[grid](\n        input,\n        absmax_inv,\n        out,\n        input.stride(0),\n        input.stride(1),\n        out.stride(0),\n        out.stride(1),\n        M,\n        N,\n    )\n    return out, absmax\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel `_fwd_kernel_token_att2` performs attention-like operations on input tensors. The main functionality is to compute the weighted sum of values `V` based on probabilities `Prob` and store the result in `Out`. The kernel processes data in blocks, iterating over each block of the sequence and aggregating results. The function `token_att_fwd2` sets up and launches this Triton kernel with provided input tensors and additional configuration, such as the batch size, head count, and block sizes.\n", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for attention-like operation\n@triton.jit\ndef _fwd_kernel_token_att2(Prob_ptr, V_ptr, Out_ptr, \n                           Batch, Heads, Seq_len, \n                           Block_size_m, Block_size_n, \n                           stride_prob_m, stride_prob_h, stride_prob_s, \n                           stride_v_m, stride_v_h, stride_v_s, \n                           stride_out_m, stride_out_h, stride_out_s,\n                           **meta):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Calculate the start index of each block\n    start_m = pid_m * Block_size_m\n    start_n = pid_n * Block_size_n\n    \n    # Create block indices\n    offsets_m = start_m + tl.arange(0, Block_size_m)\n    offsets_n = start_n + tl.arange(0, Block_size_n)\n    \n    # Initialize output accumulator\n    Out_acc = tl.zeros([Block_size_m, Block_size_n], dtype=tl.float32)\n    \n    # Iterate over the sequence\n    for seq_idx in range(0, Seq_len, Block_size_n):\n        # Load probabilities and values for the current block\n        Prob = tl.load(Prob_ptr + offsets_m[:, None] * stride_prob_m + \n                       offsets_n[None, :] * stride_prob_s + \n                       seq_idx * stride_prob_s, \n                       mask=offsets_m[:, None] < Batch * Heads * Seq_len)\n        \n        V = tl.load(V_ptr + offsets_m[:, None] * stride_v_m + \n                    offsets_n[None, :] * stride_v_s + \n                    seq_idx * stride_v_s, \n                    mask=offsets_m[:, None] < Batch * Heads * Seq_len)\n        \n        # Compute weighted sum\n        Out_acc += Prob @ V\n    \n    # Store the result\n    tl.store(Out_ptr + offsets_m[:, None] * stride_out_m + \n             offsets_n[None, :] * stride_out_s, Out_acc)\n\n# Wrapper function to launch the Triton kernel\ndef token_att_fwd2(Prob, V, Out, Batch, Heads, Seq_len, \n                   Block_size_m=128, Block_size_n=128):\n    assert Prob.shape == (Batch, Heads, Seq_len, Seq_len)\n    assert V.shape == (Batch, Heads, Seq_len, -1)\n    assert Out.shape == (Batch, Heads, Seq_len, -1)\n    \n    # Strides for input tensors\n    stride_prob_m = Prob.stride(0)\n    stride_prob_h = Prob.stride(1)\n    stride_prob_s = Prob.stride(2)\n    \n    stride_v_m = V.stride(0)\n    stride_v_h = V.stride(1)\n    stride_v_s = V.stride(2)\n    \n    stride_out_m = Out.stride(0)\n    stride_out_h = Out.stride(1)\n    stride_out_s = Out.stride(2)\n    \n    # Launch kernel\n    grid = (triton.cdiv(Batch * Heads, Block_size_m), triton.cdiv(Seq_len, Block_size_n))\n    _fwd_kernel_token_att2[grid](\n        Prob, V, Out,\n        Batch, Heads, Seq_len,\n        Block_size_m, Block_size_n,\n        stride_prob_m, stride_prob_h, stride_prob_s,\n        stride_v_m, stride_v_h, stride_v_s,\n        stride_out_m, stride_out_h, stride_out_s\n    )\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_token_att2(\n    Prob,\n    V,\n    Out,\n    Req_to_tokens,\n    B_req_idx,\n    B_Start_Loc,\n    B_Seqlen,\n    stride_req_to_tokens_b,\n    stride_req_to_tokens_s,\n    stride_ph,\n    stride_pbs,\n    stride_vbs,\n    stride_vh,\n    stride_vd,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    kv_group_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    cur_kv_head = cur_head // kv_group_num\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=(start_n + offs_n) < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(\n            Req_to_tokens + v_loc_off + start_n * stride_req_to_tokens_s,\n            mask=(start_n + offs_n) < cur_batch_seq_len,\n            other=0.0,\n        )\n        v_value = tl.load(\n            V + v_offs + v_loc[:, None] * stride_vbs, mask=(start_n + offs_n[:, None]) < cur_batch_seq_len, other=0.0\n        )\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n\n    acc = acc.to(Out.dtype.element_ty)\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_att_fwd2(prob, v, out, Req_to_tokens, B_req_idx, B_Start_Loc, B_Seqlen):\n    BLOCK = 128\n    batch, head = B_req_idx.shape[0], prob.shape[0]\n    grid = (batch, head)\n    num_warps = 4\n    dim = v.shape[-1]\n\n    kv_group_num = prob.shape[0] // v.shape[1]\n\n    _fwd_kernel_token_att2[grid](\n        prob,\n        v,\n        out,\n        Req_to_tokens,\n        B_req_idx,\n        B_Start_Loc,\n        B_Seqlen,\n        Req_to_tokens.stride(0),\n        Req_to_tokens.stride(1),\n        prob.stride(0),\n        prob.stride(1),\n        v.stride(0),\n        v.stride(1),\n        v.stride(2),\n        out.stride(0),\n        out.stride(1),\n        out.stride(2),\n        kv_group_num=kv_group_num,\n        BLOCK_DMODEL=dim,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code implements a LayerNorm operation using the Triton library for efficient execution on GPUs. \n            There are three main Triton kernels: \n            1. `_layer_norm_fwd_fused`: This kernel performs the forward pass of the LayerNorm. It normalizes the input `X`, applies scale `W`, and offset `B`, storing the result in `Y`. It also computes the mean and reciprocal standard deviation (`Rstd`) for each row and stores these for use in the backward pass. \n            2. `_layer_norm_bwd_dx_fused`: This kernel computes the gradient of the input `DX` from the gradient of the output `DY`. It also accumulates partial sums of gradients w.r.t. weights and biases (`DW` and `DB`) using atomic operations to manage concurrent writes. \n            3. `_layer_norm_bwd_dwdb`: This kernel aggregates partial weight and bias gradients from `_layer_norm_bwd_dx_fused` to produce the final gradients `FINAL_DW` and `FINAL_DB`.\n            The `LayerNorm` class wraps these kernels and provides a PyTorch-compatible interface, utilizing the Triton kernels for the forward and backward passes.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Forward pass kernel\n@triton.jit\ndef _layer_norm_fwd_fused(X, W, B, Y, MEAN, RSTD, M, N, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    \n    # Load data\n    x_ptrs = X + row_idx * N + cols\n    x = tl.load(x_ptrs, mask=cols < N, other=0.0)\n    \n    # Compute mean\n    mean = tl.sum(x, axis=0) / N\n    x_centered = x - mean\n    \n    # Compute variance and std\n    var = tl.sum(x_centered * x_centered, axis=0) / N\n    rstd = 1.0 / tl.sqrt(var + 1e-5)\n    \n    # Normalize\n    x_norm = x_centered * rstd\n    \n    # Apply scale and shift\n    w_ptrs = W + cols\n    b_ptrs = B + cols\n    w = tl.load(w_ptrs, mask=cols < N)\n    b = tl.load(b_ptrs, mask=cols < N)\n    \n    y = x_norm * w + b\n    y_ptrs = Y + row_idx * N + cols\n    tl.store(y_ptrs, y, mask=cols < N)\n    \n    # Store mean and rstd for backward pass\n    tl.store(MEAN + row_idx, mean)\n    tl.store(RSTD + row_idx, rstd)\n\n# Backward pass kernel for input gradient\n@triton.jit\ndef _layer_norm_bwd_dx_fused(DY, X, MEAN, RSTD, W, DX, DW, DB, M, N, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    \n    # Load data\n    dy_ptrs = DY + row_idx * N + cols\n    dy = tl.load(dy_ptrs, mask=cols < N, other=0.0)\n    \n    x_ptrs = X + row_idx * N + cols\n    x = tl.load(x_ptrs, mask=cols < N, other=0.0)\n    \n    mean = tl.load(MEAN + row_idx)\n    rstd = tl.load(RSTD + row_idx)\n    \n    w_ptrs = W + cols\n    w = tl.load(w_ptrs, mask=cols < N)\n    \n    # Compute dx\n    x_centered = x - mean\n    x_norm = x_centered * rstd\n    dx_norm = dy * w\n    dx_centered = dx_norm * rstd\n    dx = dx_centered - tl.sum(dx_centered, axis=0) / N - x_norm * tl.sum(dx_centered * x_norm, axis=0) / N\n    dx_ptrs = DX + row_idx * N + cols\n    tl.store(dx_ptrs, dx, mask=cols < N)\n    \n    # Compute partial dw and db\n    dw_partial = dy * x_norm\n    db_partial = dy\n    \n    # Atomic add for dw and db\n    tl.atomic_add(DW + cols, dw_partial, mask=cols < N)\n    tl.atomic_add(DB + cols, db_partial, mask=cols < N)\n\n# Backward pass kernel for weight and bias gradients\n@triton.jit\ndef _layer_norm_bwd_dwdb(DW_PARTIAL, DB_PARTIAL, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE: tl.constexpr):\n    cols = tl.arange(0, BLOCK_SIZE)\n    \n    # Aggregate partial sums\n    dw_partial = tl.load(DW_PARTIAL + cols, mask=cols < N, other=0.0)\n    db_partial = tl.load(DB_PARTIAL + cols, mask=cols < N, other=0.0)\n    \n    # Store final dw and db\n    tl.store(FINAL_DW + cols, dw_partial, mask=cols < N)\n    tl.store(FINAL_DB + cols, db_partial, mask=cols < N)\n\n# Wrapper class for LayerNorm\nclass LayerNorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, B):\n        M, N = X.shape\n        BLOCK_SIZE = 1024  # This can be tuned based on the hardware\n\n        # Allocate output tensors\n        Y = torch.empty_like(X)\n        MEAN = torch.empty(M, device=X.device, dtype=X.dtype)\n        RSTD = torch.empty(M, device=X.device, dtype=X.dtype)\n\n        # Launch forward kernel\n        _layer_norm_fwd_fused[(M,)](X, W, B, Y, MEAN, RSTD, M, N, BLOCK_SIZE=BLOCK_SIZE)\n\n        # Save context for backward\n        ctx.save_for_backward(X, W, MEAN, RSTD)\n        return Y\n\n    @staticmethod\n    def backward(ctx, DY):\n        X, W, MEAN, RSTD = ctx.saved_tensors\n        M, N = X.shape\n        BLOCK_SIZE = 1024\n\n        # Allocate gradient tensors\n        DX = torch.empty_like(X)\n        DW_PARTIAL = torch.zeros_like(W)\n        DB_PARTIAL = torch.zeros_like(B)\n\n        # Launch backward kernel for input gradient\n        _layer_norm_bwd_dx_fused[(M,)](DY, X, MEAN, RSTD, W, DX, DW_PARTIAL, DB_PARTIAL, M, N, BLOCK_SIZE=BLOCK_SIZE)\n\n        # Allocate final gradients\n        FINAL_DW = torch.empty_like(W)\n        FINAL_DB = torch.empty_like(B)\n\n        # Launch backward kernel for weight and bias gradients\n        _layer_norm_bwd_dwdb[(1,)](DW_PARTIAL, DB_PARTIAL, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE=BLOCK_SIZE)\n\n        return DX, FINAL_DW, FINAL_DB\n\n# Usage example\ndef layer_norm(x, w, b):\n    return LayerNorm.apply(x, w, b)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_fwd_fused(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    W,  # pointer to the weights\n    B,  # pointer to the biases\n    Mean,  # pointer to the mean\n    Rstd,  # pointer to the 1/std\n    stride,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n        x = tl.where(cols < N, x - mean, 0.)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n\n@triton.jit\ndef _layer_norm_bwd_dx_fused(DX,  # pointer to the input gradient\n                             DY,  # pointer to the output gradient\n                             DW,  # pointer to the partial sum of weights gradient\n                             DB,  # pointer to the partial sum of biases gradient\n                             X,  # pointer to the input\n                             W,  # pointer to the weights\n                             Mean,  # pointer to the mean\n                             Rstd,  # pointer to the 1/std\n                             Lock,  # pointer to the lock\n                             stride,  # how much to increase the pointer when moving by 1 row\n                             N,  # number of columns in X\n                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.)\n    wdy = tl.where(mask, wdy, 0.)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = (dy * xhat).to(w.dtype)\n    partial_db = (dy).to(w.dtype)\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n\n@triton.jit\ndef _layer_norm_bwd_dwdb(DW,  # pointer to the partial sum of weights gradient\n                         DB,  # pointer to the partial sum of biases gradient\n                         FINAL_DW,  # pointer to the weights gradient\n                         FINAL_DB,  # pointer to the biases gradient\n                         M,  # GROUP_SIZE_M\n                         N,  # number of columns\n                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.)\n        db += tl.load(DB + offs, mask=mask, other=0.)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n\nclass LayerNorm(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, normalized_shape, weight, bias, eps):\n        y = torch.empty_like(x)\n        x_arg = x.reshape(-1, x.shape[-1])\n        M, N = x_arg.shape\n        mean = torch.empty((M, ), dtype=torch.float32, device=x.device)\n        rstd = torch.empty((M, ), dtype=torch.float32, device=x.device)\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n        if N > BLOCK_SIZE:\n            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n        _layer_norm_fwd_fused[(M, )](  #\n            x_arg, y, weight, bias, mean, rstd,  #\n            x_arg.stride(0), N, eps,  #\n            BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_ctas=1)\n        ctx.save_for_backward(x, weight, bias, mean, rstd)\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps = num_warps\n        ctx.eps = eps\n        return y\n\n    @staticmethod\n    def backward(ctx, dy):\n        x, w, b, m, v = ctx.saved_tensors\n        N = w.shape[0]\n        GROUP_SIZE_M = 64\n        if N <= 8192: GROUP_SIZE_M = 96\n        if N <= 4096: GROUP_SIZE_M = 128\n        if N <= 1024: GROUP_SIZE_M = 256\n        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device=w.device)\n        _dw = torch.zeros((GROUP_SIZE_M, N), dtype=x.dtype, device=w.device)\n        _db = torch.zeros((GROUP_SIZE_M, N), dtype=x.dtype, device=w.device)\n        dw = torch.empty((N, ), dtype=w.dtype, device=w.device)\n        db = torch.empty((N, ), dtype=w.dtype, device=w.device)\n        dx = torch.empty_like(dy)\n        x_arg = x.reshape(-1, x.shape[-1])\n        M, N = x_arg.shape\n        _layer_norm_bwd_dx_fused[(M, )](  #\n            dx, dy, _dw, _db, x, w, m, v, locks,  #\n            x_arg.stride(0), N,  #\n            BLOCK_SIZE_N=ctx.BLOCK_SIZE,  #\n            GROUP_SIZE_M=GROUP_SIZE_M,  #\n            num_warps=ctx.num_warps)\n        grid = lambda meta: [triton.cdiv(N, meta['BLOCK_SIZE_N'])]\n        _layer_norm_bwd_dwdb[grid](\n            _dw, _db, dw, db, min(GROUP_SIZE_M, M), N,  #\n            BLOCK_SIZE_M=32,  #\n            BLOCK_SIZE_N=128, num_ctas=1)\n        return dx, None, dw, db, None\n\nlayer_norm = LayerNorm.apply\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided Triton kernel 'load_reduce_kernel' calculates the maximum value along the second dimension of an input matrix and stores it in an output vector. The function 'load_reduce' serves as a test harness to validate the kernel, using random input data. Key parameters include BLOCK_M and BLOCK_N, which define the block size for processing. The kernel utilizes pointers to access data efficiently and operates with specified strides. It loads a block of data from the input matrix, computes the maximum along the second dimension, and writes the result to the output vector.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef load_reduce_kernel(\n    input_ptr,  # Pointer to the input matrix\n    output_ptr,  # Pointer to the output vector\n    stride_am,  # Stride of the input matrix in the first dimension\n    stride_an,  # Stride of the input matrix in the second dimension\n    M,  # Number of rows in the input matrix\n    N,  # Number of columns in the input matrix\n    BLOCK_M: tl.constexpr,  # Block size in the first dimension\n    BLOCK_N: tl.constexpr  # Block size in the second dimension\n):\n    # Program ID for the block in the first dimension\n    pid_m = tl.program_id(0)\n\n    # Compute the start index for the block\n    block_start_m = pid_m * BLOCK_M\n\n    # Define a mask to handle boundaries\n    mask_m = block_start_m + tl.arange(0, BLOCK_M) < M\n\n    # Initialize a max accumulator with a very low value\n    max_vals = tl.full((BLOCK_M,), -float('inf'), dtype=tl.float32)\n\n    # Iterate over the second dimension in blocks\n    for block_start_n in range(0, N, BLOCK_N):\n        # Load a block of data from the input matrix\n        offs_n = block_start_n + tl.arange(0, BLOCK_N)\n        mask_n = offs_n < N\n\n        # Compute the linear indices for the load\n        offs = block_start_m[:, None] * stride_am + offs_n[None, :] * stride_an\n\n        # Load the data with mask\n        data = tl.load(input_ptr + offs, mask=mask_m[:, None] & mask_n[None, :], other=-float('inf'))\n\n        # Reduce along the second dimension to find the maximum\n        max_vals = tl.maximum(max_vals, tl.max(data, axis=1))\n\n    # Store the result in the output vector\n    tl.store(output_ptr + block_start_m, max_vals, mask=mask_m)\n\n# Wrapper function to test the kernel\ndef load_reduce(input_matrix, BLOCK_M=128, BLOCK_N=128):\n    # Get the dimensions of the input matrix\n    M, N = input_matrix.shape\n\n    # Allocate output vector\n    output_vector = torch.empty(M, dtype=input_matrix.dtype, device=input_matrix.device)\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(M, BLOCK_M),)\n    load_reduce_kernel[grid](\n        input_ptr=input_matrix,\n        output_ptr=output_vector,\n        stride_am=input_matrix.stride(0),\n        stride_an=input_matrix.stride(1),\n        M=M,\n        N=N,\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N\n    )\n\n    return output_vector\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input matrix\n    M, N = 1024, 512\n    input_matrix = torch.randn((M, N), device='cuda')\n\n    # Compute the maximum along the second dimension\n    output_vector = load_reduce(input_matrix)\n\n    # Validate the result\n    expected_output = input_matrix.max(dim=1)[0]\n    assert torch.allclose(output_vector, expected_output), \"Mismatch between Triton and PyTorch results!\"\n    print(\"Test passed!\")\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom torch.testing import assert_close\n\n\n@triton.jit\ndef load_reduce_kernel(\n    x_ptr,      # pointer to the input matrix\n    y_ptr,      # pointer to the output vector\n    stride_xm,  # stride of matrix x in leading dimension\n    stride_xn,  # stride of matrix x in the second dimension\n    stride_y,   # stride of output vector y\n    BLOCK_M: tl.constexpr,  # block size in leading dimension\n    BLOCK_N: tl.constexpr,  # block size in second dimension\n):\n    x_ptr = tl.make_block_ptr(\n        base=x_ptr, shape=(BLOCK_M, BLOCK_N), strides=(stride_xm, stride_xn),\n        offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_N), order=(1, 0)\n    )\n    x = tl.load(x_ptr)\n    y = tl.max(x, axis=1)\n    tl.store(y_ptr + tl.arange(0, BLOCK_M), y)\n\n# Test function for load_reduce_kernel\ndef load_reduce(BLOCK_M, BLOCK_N, dtype_str):\n    dtype_mapping = {\n        'float16': torch.float16,\n        'float32': torch.float32,\n    }\n    dtype = dtype_mapping[dtype_str]\n    x = torch.randn((BLOCK_M, BLOCK_N), device='cuda', dtype=dtype)\n    y = torch.empty((BLOCK_M,), device='cuda', dtype=dtype)\n\n    load_reduce_kernel[(1,)](x, y, x.stride(0), x.stride(1), y.stride(0), BLOCK_M, BLOCK_N)\n\n    golden = x.max(dim=1)[0]\n    torch.set_printoptions(profile='full')\n    assert_close(y, golden, rtol=1e-2, atol=1e-3, check_dtype=False)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel implements the forward pass of a layer normalization operation. The function `_layer_norm_fwd_kernel` takes an input tensor `X`, weights `W`, and outputs the normalized tensor `Y`. The normalization is computed over the last dimension of `X`, and `W` is applied as a scaling factor to the normalized values. The function `layernorm_forward` sets up necessary strides and kernel grid, and launches the Triton kernel.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _layer_norm_fwd_kernel(X_ptr, W_ptr, Y_ptr, N, stride_xm, stride_wn, BLOCK_SIZE: tl.constexpr):\n    # Create a program index for each block\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the starting position for this block\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to prevent out-of-bounds access\n    mask = offs < N\n    \n    # Load input values\n    x = tl.load(X_ptr + offs * stride_xm, mask=mask, other=0.0)\n    \n    # Compute mean\n    mean = tl.sum(x, axis=0) / N\n    \n    # Compute variance\n    var = tl.sum((x - mean) * (x - mean), axis=0) / N\n    \n    # Normalize\n    norm_x = (x - mean) / tl.sqrt(var + 1e-5)\n    \n    # Load weights\n    w = tl.load(W_ptr + offs * stride_wn, mask=mask, other=1.0)\n    \n    # Apply weights\n    y = norm_x * w\n    \n    # Store result\n    tl.store(Y_ptr + offs * stride_xm, y, mask=mask)\n\ndef layernorm_forward(X, W):\n    # Get dimensions\n    M, N = X.shape\n    \n    # Allocate output tensor\n    Y = torch.empty_like(X)\n    \n    # Get strides\n    stride_xm = X.stride(0)\n    stride_wn = W.stride(0)\n    \n    # Define block size\n    BLOCK_SIZE = 1024  # This can be tuned for performance\n    \n    # Calculate number of blocks\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    \n    # Launch kernel\n    _layer_norm_fwd_kernel[grid](X, W, Y, N, stride_xm, stride_wn, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return Y\n\n# Example usage\nM, N = 64, 1024  # Example dimensions\nX = torch.randn(M, N, device='cuda')  # Input tensor\nW = torch.ones(N, device='cuda')  # Weight tensor\nY = layernorm_forward(X, W)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# LayerNorm adapted from triton tutorial\n@triton.jit\ndef _layer_norm_fwd_kernel(\n    X,  # pointer to the input\n    W,  # pointer to the weights\n    Y,  # output pointer\n    stride_x_N,\n    stride_x_hn,\n    stride_x_hd,\n    stride_y_N,\n    stride_y_hn,\n    stride_y_hd,\n    stride_w_hn,\n    stride_w_hd,\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    Seq = tl.program_id(0)\n    H = tl.program_id(1)\n\n    X += Seq * stride_x_N + H * stride_x_hn\n    Y += Seq * stride_y_N + H * stride_y_hn\n    W += H * stride_w_hn\n\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w\n\n        tl.store(Y + cols, y.to(X.dtype.element_ty), mask=mask)\n\n\ndef layernorm_forward(\n    X,  # pointer to the input\n    W,  # pointer to the weights\n    eps,  # epsilon to avoid division by zero\n):\n    assert len(X.shape) == 3\n    assert len(W.shape) == 2\n    assert X.shape[-1] == W.shape[-1]\n    assert X.shape[-2] == W.shape[-2]\n\n    y = torch.empty_like(X)\n\n    stride_x_N = X.stride(0)\n    stride_x_hn = X.stride(1)\n    stride_x_hd = X.stride(2)\n\n    stride_y_N = y.stride(0)\n    stride_y_hn = y.stride(1)\n    stride_y_hd = y.stride(2)\n\n    stride_w_hn = W.stride(0)\n    stride_w_hd = W.stride(1)\n\n    N = X.shape[-1]\n    BLOCK_SIZE = 128\n\n    grid = (X.shape[0], X.shape[1])\n    _layer_norm_fwd_kernel[grid](\n        X,\n        W,\n        y,\n        stride_x_N,\n        stride_x_hn,\n        stride_x_hd,\n        stride_y_N,\n        stride_y_hn,\n        stride_y_hd,\n        stride_w_hn,\n        stride_w_hd,\n        N,\n        eps,\n        BLOCK_SIZE,\n    )\n\n    return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel named _fwd_kernel_aligned is designed to perform scaled dot-product attention with relative position biases. This kernel processes queries (Q), keys (K), and values (V) along with a bias matrix (B0) to compute the output (Out). It operates in blocks defined by BLOCK_M, BLOCK_N, and BLOCK_DMODEL, facilitating efficient memory access and computation. The kernel uses a grid to parallelize computations across the third dimension of Q, and the batch dimension. Input tensors are divided into blocks, loaded, and processed in loops to accumulate results. It performs scaled dot-products using tl.dot, applies biases, computes softmax using tl.math.exp2 for efficiency, and updates accumulators for the result. The final output is normalized and stored back in memory.\n\n        The wrapper function _attention_rel_h_rel_w_kernel_aligned_device configures the kernel for specific input sizes and launches it on a grid of (q.shape[2] / BLOCK_M, q.shape[0] * q.shape[1]). It validates input shapes, ensures data types are compatible, and calculates grid dimensions. It then invokes the Triton kernel with appropriate parameters, including strides, shapes, block sizes, and other constants needed for execution.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef _fwd_kernel_aligned(Q_ptr, K_ptr, V_ptr, B0_ptr, Out_ptr,\n                        stride_qz, stride_qh, stride_qm, stride_qd,\n                        stride_kz, stride_kh, stride_kn, stride_kd,\n                        stride_vz, stride_vh, stride_vn, stride_vd,\n                        stride_b0z, stride_b0h, stride_b0m, stride_b0n,\n                        stride_oz, stride_oh, stride_om, stride_od,\n                        scale, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    \n    pid_m = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n\n    batch_z = pid_batch // stride_qh\n    batch_h = pid_batch % stride_qh\n\n    q_offset = batch_z * stride_qz + batch_h * stride_qh + pid_m * BLOCK_M * stride_qm\n    k_offset = batch_z * stride_kz + batch_h * stride_kh\n    v_offset = batch_z * stride_vz + batch_h * stride_vh\n    b0_offset = batch_z * stride_b0z + batch_h * stride_b0h + pid_m * BLOCK_M * stride_b0m\n    out_offset = batch_z * stride_oz + batch_h * stride_oh + pid_m * BLOCK_M * stride_om\n\n    Q = tl.load(Q_ptr + q_offset + tl.arange(0, BLOCK_M)[:, None] * stride_qm + tl.arange(0, BLOCK_DMODEL)[None, :] * stride_qd)\n    K = tl.load(K_ptr + k_offset + tl.arange(0, BLOCK_N)[:, None] * stride_kn + tl.arange(0, BLOCK_DMODEL)[None, :] * stride_kd)\n    V = tl.load(V_ptr + v_offset + tl.arange(0, BLOCK_N)[:, None] * stride_vn + tl.arange(0, BLOCK_DMODEL)[None, :] * stride_vd)\n    B0 = tl.load(B0_ptr + b0_offset + tl.arange(0, BLOCK_M)[:, None] * stride_b0m + tl.arange(0, BLOCK_N)[None, :] * stride_b0n)\n\n    # Compute QK^T\n    QK = tl.dot(Q, K, trans_b=True) * scale\n    QK += B0\n\n    # Softmax\n    QK_max = tl.max(QK, axis=1)\n    QK_exp = tl.math.exp2(QK - QK_max[:, None])\n    QK_sum = tl.sum(QK_exp, axis=1)\n    QK_softmax = QK_exp / QK_sum[:, None]\n\n    # Compute output\n    Out = tl.dot(QK_softmax, V)\n    tl.store(Out_ptr + out_offset + tl.arange(0, BLOCK_M)[:, None] * stride_om + tl.arange(0, BLOCK_DMODEL)[None, :] * stride_od, Out)\n\n\n# Define the wrapper function\ndef _attention_rel_h_rel_w_kernel_aligned_device(q, k, v, b0, out, scale):\n    BLOCK_M = 128  # Example block size\n    BLOCK_N = 128\n    BLOCK_DMODEL = 64\n\n    assert q.shape[2] % BLOCK_M == 0, \"Q's third dimension must be divisible by BLOCK_M\"\n    assert q.shape[3] == BLOCK_DMODEL, \"Q's fourth dimension must match BLOCK_DMODEL\"\n    assert k.shape[3] == BLOCK_DMODEL, \"K's fourth dimension must match BLOCK_DMODEL\"\n    assert v.shape[3] == BLOCK_DMODEL, \"V's fourth dimension must match BLOCK_DMODEL\"\n    \n    grid = (q.shape[2] // BLOCK_M, q.shape[0] * q.shape[1])\n\n    triton.kernel(_fwd_kernel_aligned, grid=grid)(\n        q, k, v, b0, out,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        b0.stride(0), b0.stride(1), b0.stride(2), b0.stride(3),\n        out.stride(0), out.stride(1), out.stride(2), out.stride(3),\n        scale, BLOCK_M, BLOCK_N, BLOCK_DMODEL\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_aligned(\n    Q, K, V, B0, sm_scale,\n    Out,\n    stride_qh, stride_qm, stride_qk,\n    stride_kh, stride_kn, stride_kk,\n    stride_vh, stride_vk, stride_vn,\n    stride_oh, stride_om, stride_on,\n    stride_b0h, stride_b0m,\n    Z,\n    H,\n    N_CTX,\n    P_SEQ,\n    OUT_DTYPE: tl.constexpr,\n    BIAS_LAST_SIZE: tl.constexpr,\n    B0_NUMEL: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + kv_offset,\n        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1)\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + kv_offset,\n        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n\n    # initialize offsets\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    # scale sm_scale by log_2(e) and use\n    # 2^x instead of exp in the loop because CSE and LICM\n    # don't work as expected with `exp` in the loop\n    qk_scale = sm_scale * 1.44269504\n    # load q: it will stay in SRAM throughout\n    q = tl.load(Q_block_ptr)  # , boundary_check=(1, 0), padding_option=\"zero\")\n    q = (q * qk_scale).to(OUT_DTYPE)\n    # loop over k, v and update accumulator\n    lo = 0\n    hi = N_CTX + P_SEQ\n\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = (tl.arange(0, BLOCK_N) %\n                         BIAS_LAST_SIZE) + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                 * stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        # -- load k, v --\n        # , boundary_check=(0, 1), padding_option=\"zero\")\n        k = tl.load(K_block_ptr)\n        # , boundary_check=(1, 0), padding_option=\"zero\")\n        v = tl.load(V_block_ptr)\n        # -- compute qk ---\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k) #, out_dtype=OUT_DTYPE)\n\n        # -- compute rel_h[:, None] + rel_w[None, :] bias ---\n\n        # Bias\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m)\n                     * stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += ((b0 + b1) * 1.44269504)\n\n        # -- compute scaling constant ---\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        # -- scale and update acc --\n        acc *= alpha[:, None]\n        acc += tl.dot(p.to(OUT_DTYPE), v)\n        # -- update m_i and l_i --\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        # update pointers\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n\n    # write back l and m\n    acc = acc / l_i[:, None]\n\n    # write back O\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0)\n    )\n    tl.store(O_block_ptr, acc.to(OUT_DTYPE))\n\n\ndef _attention_rel_h_rel_w_kernel_aligned_device(q, k, v, rel_h_w, sm_scale, o,\n                                                 BLOCK_M,\n                                                 BLOCK_N,\n                                                 num_warps,\n                                                 num_stages):\n    _, Lk, _ = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert q.size() == k.size()\n    assert q.size() == v.size()\n    assert q.size(-2) == rel_h_w.size(-2)\n    assert (q.dtype == torch.bfloat16 or q.dtype == torch.float16)\n    assert k.dtype == q.dtype\n    assert v.dtype == k.dtype\n    assert o.dtype == v.dtype\n    assert rel_h_w.dtype == q.dtype\n    assert rel_h_w.size(-1) == 128\n    # assert rel_h_w.size(-1) == 2 * BLOCK_N\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    # print(\"q.shape[0] * q.shape[1]: \", q.shape[0] * q.shape[1])\n    P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n    assert P_SEQ == 0\n    assert rel_h_w.is_contiguous(), str(rel_h_w.stride())\n    OUT_DTYPE = tl.float16 if q.dtype == torch.float16 else tl.bfloat16\n    _fwd_kernel_aligned[grid](\n        q, k, v,\n        rel_h_w,\n        sm_scale,\n        o,\n        q.stride(1), q.stride(2), q.stride(3),\n        k.stride(1), k.stride(2), k.stride(3),\n        v.stride(1), v.stride(2), v.stride(3),\n        o.stride(1), o.stride(2), o.stride(3),\n        rel_h_w.stride(1), rel_h_w.stride(2),\n        q.shape[0],\n        q.shape[1],\n        q.shape[2],\n        P_SEQ,\n        OUT_DTYPE=OUT_DTYPE,\n        BIAS_LAST_SIZE=(rel_h_w.size(-1) // 2),\n        B0_NUMEL=rel_h_w.size(-1),\n        BLOCK_M=BLOCK_M,\n        BLOCK_N=BLOCK_N,\n        BLOCK_DMODEL=Lk,\n        num_warps=num_warps,\n        num_stages=num_stages)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is named `_fwd_kernel_flash_decode_stage2`. Its main function is to process input tensors and compute a weighted sum for each batch and head combination. The weighted sum is then stored in the output tensor. The key inputs are:\n            - `B_Seqlen`: Tensor containing the sequence lengths for each batch.\n            - `Mid_O`, `Mid_O_LogExpSum`: Intermediate tensors needed for calculations.\n            - `O`: Output tensor where results are stored.\n            - Various stride values: Used for addressing tensor elements properly.\n            - `BLOCK_SEQ`, `BLOCK_DMODEL`: Constants determining the size of blocks used in computation.\n            The kernel utilizes loops and vector operations to optimize computation across dimensions of `Mid_O`, updating an accumulator that is normalized and stored in `O`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants defining the block sizes\nBLOCK_SEQ = 128\nBLOCK_DMODEL = 64\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, O,\n                                    stride_b_seqlen, stride_mid_o, stride_mid_o_logexpsum, stride_o,\n                                    n_batch, n_head, d_model):\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the batch and head indices from the program ID\n    batch_id = pid // n_head\n    head_id = pid % n_head\n    \n    # Load the sequence length for this batch\n    seq_len = tl.load(B_Seqlen + batch_id * stride_b_seqlen)\n    \n    # Initialize an accumulator for the weighted sum\n    acc = tl.zeros((BLOCK_DMODEL,), dtype=tl.float32)\n    \n    # Iterate over sequence blocks\n    for seq_start in range(0, seq_len, BLOCK_SEQ):\n        # Calculate the effective sequence length for the current block\n        seq_block_len = min(BLOCK_SEQ, seq_len - seq_start)\n        \n        # Load the intermediate results and their log-exp sums\n        mid_o_ptr = Mid_O + (batch_id * n_head + head_id) * stride_mid_o + seq_start * d_model\n        mid_o_logexpsum_ptr = Mid_O_LogExpSum + (batch_id * n_head + head_id) * stride_mid_o_logexpsum + seq_start\n        \n        # Load the current block of Mid_O and Mid_O_LogExpSum\n        mid_o = tl.load(mid_o_ptr + tl.arange(0, seq_block_len)[:, None] * d_model + tl.arange(0, BLOCK_DMODEL)[None, :])\n        mid_o_logexpsum = tl.load(mid_o_logexpsum_ptr + tl.arange(0, seq_block_len))\n        \n        # Compute the weighted sum\n        weights = tl.exp(mid_o_logexpsum[:, None] - mid_o)\n        acc += tl.sum(weights * mid_o, axis=0)\n    \n    # Normalize the accumulator and store the result in the output tensor\n    o_ptr = O + (batch_id * n_head + head_id) * stride_o\n    tl.store(o_ptr + tl.arange(0, BLOCK_DMODEL), acc / seq_len)\n\ndef flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, O, n_batch, n_head, d_model):\n    # Calculate the strides for each tensor\n    stride_b_seqlen = B_Seqlen.stride(0)\n    stride_mid_o = Mid_O.stride(0)\n    stride_mid_o_logexpsum = Mid_O_LogExpSum.stride(0)\n    stride_o = O.stride(0)\n    \n    # Launch the Triton kernel\n    grid = (n_batch * n_head,)\n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen, Mid_O, Mid_O_LogExpSum, O,\n        stride_b_seqlen, stride_mid_o, stride_mid_o_logexpsum, stride_o,\n        n_batch, n_head, d_model\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    B_Seqlen,\n    Mid_O,  # [batch, head, seq_block_num, head_dim]\n    Mid_O_LogExpSum,  # [batch, head, seq_block_num]\n    O,  # [batch, head, head_dim]\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es,\n    stride_obs, stride_oh, stride_od,\n    BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ\n\n    sum_exp = 0.0\n    max_logic = -float(\"inf\")\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        \n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    \n    tl.store(O + cur_batch * stride_obs + cur_head * stride_oh + offs_d, acc / sum_exp)\n    return\n\n@torch.no_grad()\ndef flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq):\n    Lk = mid_out.shape[-1]\n    assert Lk in {16, 32, 64, 128}\n    batch, head_num = mid_out.shape[0], mid_out.shape[1]\n    grid = (batch, head_num)\n    \n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen, mid_out, mid_out_logexpsum, O,\n        mid_out.stride(0), mid_out.stride(1), mid_out.stride(2), mid_out.stride(3),\n        mid_out_logexpsum.stride(0), mid_out_logexpsum.stride(1), mid_out_logexpsum.stride(2),\n        O.stride(0), O.stride(1), O.stride(2),\n        BLOCK_SEQ=block_seq,\n        BLOCK_DMODEL=Lk,\n        num_warps=4,\n        num_stages=2,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton kernel function and a wrapper in PyTorch to perform a backward pass operation of a concatenated index select operation. The main function `index_select_cat_bwd` takes in `grad_source`, `index`, and `grad_output` as inputs, which are 2D CUDA tensors. The function checks the shape and stride compatibility of these inputs, and then invokes the Triton kernel `index_select_cat_bwd_kernel`. This kernel calculates the gradient by adding selected indices from the `grad_output` to `grad_source` according to `index`. The operation is performed in a block-wise parallel manner using Triton's grid-based execution.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef index_select_cat_bwd_kernel(\n    grad_source_ptr, grad_output_ptr, index_ptr,\n    grad_source_stride0, grad_source_stride1,\n    grad_output_stride0, grad_output_stride1,\n    index_stride0, index_stride1,\n    num_indices, BLOCK_SIZE: tl.constexpr\n):\n    # Define the block indices\n    block_idx = tl.program_id(0)\n    \n    # Compute the row start for this block\n    row_start = block_idx * BLOCK_SIZE\n    \n    # Iterate over the block size\n    for i in range(BLOCK_SIZE):\n        # Calculate the current row\n        row = row_start + i\n        \n        # Check if within bounds\n        if row < num_indices:\n            # Load the index\n            index = tl.load(index_ptr + row * index_stride0)\n            \n            # Load the grad_output value\n            grad_output_val = tl.load(grad_output_ptr + row * grad_output_stride0)\n            \n            # Compute the position in grad_source to update\n            grad_source_pos = index * grad_source_stride0\n            \n            # Load the current value in grad_source\n            grad_source_val = tl.load(grad_source_ptr + grad_source_pos)\n            \n            # Update grad_source with the value from grad_output\n            tl.store(grad_source_ptr + grad_source_pos, grad_source_val + grad_output_val)\n\ndef index_select_cat_bwd(grad_source, index, grad_output):\n    # Ensure inputs are on CUDA\n    assert grad_source.is_cuda and index.is_cuda and grad_output.is_cuda, \"All inputs must be CUDA tensors\"\n    \n    # Check dimensions and strides\n    assert grad_source.dim() == 2 and index.dim() == 2 and grad_output.dim() == 2, \"All inputs must be 2D tensors\"\n    assert grad_source.stride(0) == grad_output.stride(0), \"grad_source and grad_output must have the same stride for dim 0\"\n    \n    # Get tensor strides\n    grad_source_stride0, grad_source_stride1 = grad_source.stride()\n    grad_output_stride0, grad_output_stride1 = grad_output.stride()\n    index_stride0, index_stride1 = index.stride()\n    \n    # Number of indices\n    num_indices = index.size(0)\n    \n    # Define block size\n    BLOCK_SIZE = 128  # Can be tuned based on the GPU architecture\n    \n    # Launch the Triton kernel\n    grid = (triton.cdiv(num_indices, BLOCK_SIZE),)\n    index_select_cat_bwd_kernel[grid](\n        grad_source, grad_output, index,\n        grad_source_stride0, grad_source_stride1,\n        grad_output_stride0, grad_output_stride1,\n        index_stride0, index_stride1,\n        num_indices, BLOCK_SIZE\n    )\n\n# Example usage:\ngrad_source = torch.zeros((10, 10), device='cuda')\nindex = torch.tensor([[0], [1], [2]], device='cuda', dtype=torch.int32)\ngrad_output = torch.ones((3, 10), device='cuda')\n\nindex_select_cat_bwd(grad_source, index, grad_output)\nprint(grad_source)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef index_select_cat_bwd_kernel(\n    grad_source_ptr,  # *Pointer* to grad_source tensor.\n    index_ptr,  # *Pointer* to index tensor.\n    grad_output_ptr,  # *Pointer* to grad_output tensor.\n    num_rows,\n    num_indices,\n    num_cols,\n    stride0,  # Stride information of input and source tensor.\n    stride1,\n    BLOCK_SIZE_INDEX: tl.constexpr,  # Number of indices each program should process.\n    BLOCK_SIZE_COL: tl.constexpr,  # Number of cols each program should process.\n):\n    pid0 = tl.program_id(axis=0)  # We use 3D launch grid\n    pid1 = tl.program_id(axis=1)\n\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n\n    # load grad_output\n    grad_output_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    grad_output_offsets = (\n        grad_output_ptr\n        + grad_output_indices[:, None] * stride0\n        + cols[None, :] * stride1\n    )\n    grad_output_mask = (grad_output_indices[:, None] < num_indices) & (\n        cols[None, :] < num_cols\n    )\n    grad_output = tl.load(grad_output_offsets, mask=grad_output_mask).to(tl.float32)\n\n    # select indices from grad_source\n    grad_source_indices = tl.load(\n        index_ptr + grad_output_indices, mask=(grad_output_indices < num_indices)\n    )\n    grad_source_offsets = (\n        grad_source_ptr\n        + grad_source_indices[:, None] * stride0\n        + cols[None, :] * stride1\n    )\n\n    # compute scaled index add and save\n    tl.store(grad_source_offsets, grad_output, mask=grad_output_mask)\n\n\ndef index_select_cat_bwd(\n    grad_source: torch.Tensor,\n    index: torch.Tensor,\n    grad_output: torch.Tensor,\n):\n    if not (grad_source.is_cuda and grad_output.is_cuda):\n        raise ValueError(\"The grad_source and grad_output tensor must be of type CUDA!\")\n\n    if not (grad_source.ndim == 2 and grad_output.ndim == 2):\n        raise ValueError(\n            f\"The grad_source and grad_output must be three-dimensional \"\n            f\"(got {grad_source.ndim} and {grad_output.ndim})!\"\n        )\n    if not grad_source.shape[1] == grad_output.shape[1]:\n        raise ValueError(\n            f\"The number of elements along dimension 1 of grad_source and grad_output must be the same \"\n            f\"(got {grad_source.shape[1]} and {grad_output.shape[1]})\"\n        )\n\n    num_rows, num_cols = grad_source.shape\n    num_indices, num_cols = grad_output.shape\n    if not num_rows >= num_indices:\n        raise ValueError(\n            f\"The number of elements along dimension 0 of grad_source must be larger than that of grad_output \"\n            f\"(got {num_rows} and {num_indices})!\"\n        )\n    if not index.shape[0] == num_indices:\n        raise ValueError(\n            f\"The number of indices and the number of elements along dimension 0 of grad_output must match \"\n            f\"(got {index.shape[0]} and {num_indices})!\"\n        )\n\n    stride0, stride1 = grad_source.stride(0), grad_source.stride(1)\n    if not (grad_output.stride(0) == stride0 and grad_output.stride(1) == stride1):\n        raise ValueError(\n            f\"The strides of the grad_source and grad_output tensors must match \"\n            f\"(got {stride0} vs. {grad_output.stride(0)}, {stride1} vs. {grad_output.stride(1)})!\"\n        )\n\n    def grid(meta):\n        return (\n            triton.cdiv(num_indices, meta[\"BLOCK_SIZE_INDEX\"]),\n            triton.cdiv(num_cols, meta[\"BLOCK_SIZE_COL\"]),\n        )\n\n    index_select_cat_bwd_kernel[grid](\n        grad_source,\n        index,\n        grad_output,\n        num_rows,\n        num_indices,\n        num_cols,\n        grad_source.stride(0),\n        grad_source.stride(1),\n        BLOCK_SIZE_INDEX=1,\n        BLOCK_SIZE_COL=512,\n    )\n\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThis Triton-based operator computes the Kullback-Leibler divergence (KL divergence) in the forward and backward passes. The main functions are `kldiv_forward_triton` and `kldiv_backward_triton`, utilizing kernels `_kldiv_kernel_forward` and `_kldiv_kernel_backward` respectively. \n\n- `kldiv_forward_triton`: Computes the forward pass of the KL divergence. It takes predictions `y_pred`, ground truth `y_true`, a boolean `log_target` indicating if the target is in log-space, and `reduction` type ('none', 'sum', 'mean', 'batchmean'). The result is a tensor with the specified reduction applied. The function uses `BLOCK_SIZE` and `num_warps` for parallel execution configuration.\n\n- `_kldiv_kernel_forward`: Implements the forward computation in Triton, iterating over prediction and target values and calculating the KL divergence for each element or batch depending on the reduction mode.\n\n- `kldiv_backward_triton`: Computes the gradient of the KL divergence with respect to predictions. The function takes `input`, `target`, `grad_output`, and a boolean `log_target`. It configures and invokes `_kldiv_kernel_backward` to compute gradients.\n\n- `_kldiv_kernel_backward`: Implements the backward computation in Triton, calculating gradients based on the target values and handling log-space conditions.\n\nCommon constants include `_REDUCTION_MODE_*` for managing reduction modes, and the function `get_num_warps` helps determine the optimal number of warps based on `BLOCK_SIZE`.\n", "predict": "import triton\nimport triton.language as tl\n\n# Reduction modes\n_REDUCTION_MODE_NONE = 0\n_REDUCTION_MODE_SUM = 1\n_REDUCTION_MODE_MEAN = 2\n_REDUCTION_MODE_BATCHMEAN = 3\n\ndef get_num_warps(block_size):\n    # Simple heuristic to choose the number of warps based on block size\n    if block_size <= 64:\n        return 1\n    elif block_size <= 128:\n        return 2\n    elif block_size <= 256:\n        return 4\n    else:\n        return 8\n\n@triton.jit\ndef _kldiv_kernel_forward(y_pred_ptr, y_true_ptr, output_ptr, n_elements, log_target, reduction_mode, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load predictions and targets\n    y_pred = tl.load(y_pred_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    y_true = tl.load(y_true_ptr + offsets, mask=offsets < n_elements, other=0.0)\n\n    # Compute KL divergence\n    if log_target:\n        kl_div = y_true * (tl.log(y_true) - y_pred)\n    else:\n        kl_div = y_true * (tl.log(y_true) - tl.log(y_pred))\n\n    # Store the result\n    tl.store(output_ptr + offsets, kl_div, mask=offsets < n_elements)\n\ndef kldiv_forward_triton(y_pred, y_true, log_target=False, reduction='none', BLOCK_SIZE=256):\n    # Convert reduction string to mode\n    reduction_mode = {\n        'none': _REDUCTION_MODE_NONE,\n        'sum': _REDUCTION_MODE_SUM,\n        'mean': _REDUCTION_MODE_MEAN,\n        'batchmean': _REDUCTION_MODE_BATCHMEAN\n    }[reduction]\n\n    # Determine the number of elements and configure execution\n    n_elements = y_pred.numel()\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    # Allocate output\n    output = torch.empty_like(y_pred)\n\n    # Launch the kernel\n    grid = lambda meta: (triton.cdiv(n_elements, BLOCK_SIZE),)\n    _kldiv_kernel_forward[grid](y_pred, y_true, output, n_elements, log_target, reduction_mode, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Apply reduction\n    if reduction_mode == _REDUCTION_MODE_SUM:\n        return output.sum()\n    elif reduction_mode == _REDUCTION_MODE_MEAN:\n        return output.mean()\n    elif reduction_mode == _REDUCTION_MODE_BATCHMEAN:\n        return output.sum() / y_pred.size(0)\n    else:\n        return output\n\n@triton.jit\ndef _kldiv_kernel_backward(y_pred_ptr, y_true_ptr, grad_output_ptr, grad_input_ptr, n_elements, log_target, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load predictions, targets, and grad_output\n    y_pred = tl.load(y_pred_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    y_true = tl.load(y_true_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    grad_output = tl.load(grad_output_ptr + offsets, mask=offsets < n_elements, other=0.0)\n\n    # Compute gradient\n    if log_target:\n        grad_input = -y_true * grad_output\n    else:\n        grad_input = -y_true / y_pred * grad_output\n\n    # Store the gradient\n    tl.store(grad_input_ptr + offsets, grad_input, mask=offsets < n_elements)\n\ndef kldiv_backward_triton(y_pred, y_true, grad_output, log_target=False, BLOCK_SIZE=256):\n    # Determine the number of elements and configure execution\n    n_elements = y_pred.numel()\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    # Allocate gradient input\n    grad_input = torch.empty_like(y_pred)\n\n    # Launch the kernel\n    grid = lambda meta: (triton.cdiv(n_elements, BLOCK_SIZE),)\n    _kldiv_kernel_backward[grid](y_pred, y_true, grad_output, grad_input, n_elements, log_target, BLOCK_SIZE=BLOCK_SIZE)\n\n    return grad_input\n", "label": "import torch\nimport triton\nimport triton.language as tl\nimport numpy as np\n\ndef get_num_warps(BLOCK_SIZE):\n    num_warps = 4\n    if BLOCK_SIZE >= 32768:\n        num_warps = 32\n    elif BLOCK_SIZE >= 8192:\n        num_warps = 16\n    elif BLOCK_SIZE >= 2048:\n        num_warps = 8\n\n    return num_warps\n\n\nMAX_FUSED_SIZE = 65536 // 4\n\n_REDUCTION_MODE_NONE = tl.constexpr(0)\n_REDUCTION_MODE_SUM = tl.constexpr(1)\n_REDUCTION_MODE_MEAN = tl.constexpr(2)\n_REDUCTION_MODE_BATCHMEAN = tl.constexpr(3)\n\n_str_to_reduction_mode = {\n    \"none\": _REDUCTION_MODE_NONE.value,\n    \"sum\": _REDUCTION_MODE_SUM.value,\n    \"mean\": _REDUCTION_MODE_MEAN.value,\n    \"batchmean\": _REDUCTION_MODE_BATCHMEAN.value,\n}\n\n@triton.jit\ndef _kldiv_kernel_forward(\n    y_ptr,  # [B, S], prediction ptr, the kernel expects the prediction in log-space\n    y_stride,  # int, prediction stride\n    gt_ptr,  # [B, S], ground truth ptr\n    gt_stride,  # int, ground truth stride\n    loss_ptr,  # [B] or [B, S] if reduction == _REDUCTION_MODE_NONE, output ptr\n    loss_stride,  # int, output stride\n    n_cols,  # int, number of columns in the input tensor\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n    reduction: tl.constexpr = _REDUCTION_MODE_BATCHMEAN,\n):\n    pid = tl.program_id(0).to(tl.int64)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n\n        # KL(y_true || y) = y_true * (log(y_true) - log(y))\n        # We compute KL(y_true || y) with y in the log-space\n        if not log_target:\n            loss = y_true * (tl.log(y_true) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n\n        if reduction == _REDUCTION_MODE_NONE:\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss = tl.sum(loss, axis=0)\n            tl.store(loss_ptr, loss)\n            loss_ptr += 1\n\n@triton.jit\ndef _kldiv_kernel_backward(\n    input_ptr,\n    input_stride,\n    target_ptr,\n    target_stride,\n    n_cols,\n    BLOCK_SIZE: tl.constexpr,\n    log_target: tl.constexpr = False,\n):\n    pid = tl.program_id(0).to(tl.int64)\n\n    input_ptr += pid * input_stride\n    target_ptr += pid * target_stride\n\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n\n        tl.store(input_ptr + offsets, res, mask=mask)\n\ndef kldiv_forward_triton(y_pred, y_true, log_target, reduction):  # [B, S]  # [B, S]\n    B, S = y_pred.shape\n\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(S))\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    grid = (B,)\n    reduction = _str_to_reduction_mode[reduction]\n\n    out_size = (B, S) if reduction == _REDUCTION_MODE_NONE.value else (B,)\n    output_tensor = torch.zeros(\n        out_size,\n        dtype=torch.float32,\n        device=\"cuda\",\n    )\n\n    _kldiv_kernel_forward[grid](\n        y_pred,\n        y_pred.stride(0),\n        y_true,\n        y_true.stride(0),\n        output_tensor,\n        output_tensor.stride(0),\n        S,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n        reduction=reduction,\n    )\n\n    if reduction == _REDUCTION_MODE_BATCHMEAN.value:\n        return output_tensor.sum() / B\n    elif reduction == _REDUCTION_MODE_SUM.value:\n        return output_tensor.sum(axis=0)\n    elif reduction == _REDUCTION_MODE_MEAN.value:\n        return output_tensor.mean(axis=0)\n    else:\n        return output_tensor\n\ndef kldiv_backward_triton(input, target, grad_output, log_target):\n    B, S = input.shape\n\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(S))\n    num_warps = get_num_warps(BLOCK_SIZE)\n\n    grid = (B,)\n\n    _kldiv_kernel_backward[grid](\n        input,\n        input.stride(0),\n        target,\n        target.stride(0),\n        S,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n        log_target=log_target,\n    )\n\n    if torch.equal(\n        grad_output,\n        torch.tensor(\n                1.0,\n            dtype=grad_output.dtype,\n            device=\"cuda\",\n        ),\n    ):\n        return input\n\n    return input * grad_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code defines several matrix multiplication and dequantization kernels for handling 4-bit quantized weights. The main functions are:\n\n            1. `matmul4_kernel`: This kernel computes the matrix product of a matrix `A` (float16) and a quantized matrix `B` (int32) which is encoded in 4-bits per value. The quantized values are dequantized using `scales` and `zeros`, producing an output matrix `C` (float16). It uses block-level parallelism to efficiently perform the operations in parallel.\n\n            2. `matmul_dequantize_int4_gptq`: This function wraps around `matmul4_kernel` and sets up the Triton grid and meta-parameters required to run the kernel. It accepts matrices `x`, `qweight`, `scales`, and `qzeros` and an optional `output` tensor. It determines the grid size based on `M`, `N`, and `K` dimensions and invokes the kernel to compute the matrix product.\n\n            3. `matmul_kernel`: Similar to `matmul4_kernel`, but supports a configurable `SPLIT_K` parameter for splitting the K dimension into multiple parts. This enables processing large matrices by dividing the workload.\n\n            4. `matmul_dequantize_int4_s2`: This is a wrapper function for `matmul_kernel`. It computes the matrix product and dequantizes the result from 4-bit integers. It calculates grid dimensions based on input sizes and invokes the kernel to perform the computation.\n\n            5. `dequantize_kernel`: This kernel dequantizes a quantized weight matrix `b` into a full precision matrix `fpb_ptr`. It applies scales and zero-point correction to convert 4-bit values back into float16 values.\n\n            6. `dequantize_int4`: This function calls the `dequantize_kernel` to convert a 4-bit integer matrix into a float16 matrix. It sets up the grid and other meta-parameters for the kernel.\n\n            7. `matmul_dequantize_int4_s1`: This function dequantizes the weight matrix before performing a standard `torch.mm` matrix multiplication. It is used when the weight matrix is constant and the overhead of dequantization can be amortized over multiple operations.\n\n            Each function and kernel is carefully optimized using Triton's autotuning and is designed to efficiently handle operations on quantized matrices with parallel computation.\n\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul4_kernel(\n    A, B, C, scales, zeros,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    # Compute block indices\n    pid = tl.program_id(axis=0)\n    num_blocks_m = M // BLOCK_M\n    num_blocks_n = N // BLOCK_N\n    block_m = pid // num_blocks_n\n    block_n = pid % num_blocks_n\n\n    # Define offsets\n    offs_am = block_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = block_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # Pointers to the blocks\n    A_block_ptr = A + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    B_block_ptr = B + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n\n    # Loop over K dimension\n    for k in range(0, K, BLOCK_K):\n        # Load A and B blocks\n        A_block = tl.load(A_block_ptr)\n        B_block = tl.load(B_block_ptr)\n\n        # Dequantize B\n        B_block = (B_block.to(tl.float16) - zeros) * scales\n\n        # Matrix multiplication\n        acc += tl.dot(A_block, B_block)\n\n        # Update pointers\n        A_block_ptr += BLOCK_K * stride_ak\n        B_block_ptr += BLOCK_K * stride_bk\n\n    # Store the result\n    C_block_ptr = C + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn\n    tl.store(C_block_ptr, acc)\n\n\ndef matmul_dequantize_int4_gptq(x, qweight, scales, qzeros, output=None):\n    M, K = x.shape\n    _, N = qweight.shape\n    BLOCK_M = 128\n    BLOCK_N = 128\n    BLOCK_K = 32\n\n    # Initialize output if not provided\n    if output is None:\n        output = torch.empty((M, N), dtype=torch.float16, device=x.device)\n\n    # Calculate grid size\n    grid = (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n\n    # Launch kernel\n    matmul4_kernel[grid](\n        x, qweight, output, scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n    )\n\n    return output\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        # Additional configurations are omitted for brevity...\n    ],\n    key=['M', 'N', 'K', 'NO_GROUPS'],\n)\n@triton.jit\ndef matmul4_kernel(\n    a_ptr, b_ptr, c_ptr,\n    scales_ptr, zeros_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n,\n    stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n   # (BLOCK_SIZE_N,)\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)   # (BLOCK_SIZE_N,)\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32    \n        zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)   # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated    \n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)  # (BLOCK_SIZE_N,)\n            ptr = zeros_ptrs + g_id * stride_zeros_g   # (BLOCK_SIZE_N,)\n            zeros = tl.load(ptr)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32    \n            zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n            zeros = (zeros) * scales  # (BLOCK_SIZE_N,) float16    \n        b = (b >> shifter[:, None]) & 0xF  # Extract the 4-bit values\n        b = b * scales[None, :] - zeros[None, :]  # Scale and shift\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk  \n    c = accumulator.to(tl.float16)  \n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n\ndef matmul_dequantize_int4_gptq(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size, output=None) -> torch.FloatTensor:\n    \"\"\"\n    Compute the matrix multiplication C = A x B + bias.\n    Where B is quantized using GPTQ and groupsize = -1 into 4-bit values.\n\n    A is of shape (..., K) float16\n    qweight is of shape (K//8, N) int32\n    scales is of shape (G, N) float16\n    qzeros is of shape (G, N//8) int32\n    bias is of shape (1, N) float16\n\n    groupsize is the number of infeatures in each group.\n    G = K // groupsize\n\n    Returns C of shape (..., N) float16\n    \"\"\"\n    assert x.shape[-1] == (qweight.shape[0] * 8), \"A must be a multiple of 8 in the last dimension\"\n    assert x.is_contiguous(), \"A must be contiguous\"\n\n    M, K = x.shape\n    N = qweight.shape[1]\n\n    if output is None:\n        inplace = False\n        output = torch.empty((M, N), device=x.device, dtype=torch.float16)\n    else:\n        inplace = True\n\n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n    matmul4_kernel[grid](\n        x, qweight, output,\n        scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        group_size, group_size == K,\n    )\n    if not inplace:\n        return output\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        # Additional configurations are omitted for brevity...\n    ],\n    key=['M', 'N', 'K'],\n    reset_to_zero=['c_ptr']\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    bs_ptr, bzp_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    group_size,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr\n    ):\n    \"\"\"\n    Matrix multiplication kernel with dequantization for 4-bit integers\n    A: [M, K], B: [K//8, N], C: [M, N]\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        bs_ptrs = bs_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bsk \\\n            + offs_bn[None, :] * stride_bsn\n        bzp_ptrs = bzp_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bzpk \\\n            + (offs_bn[None, :] // 8) * stride_bzpn\n        b_shift_bits = (offs_k[:, None] % 8) * 4 \n        bzp_shift_bits = (offs_bn[None, :] % 8) * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n\n        int_b = (b >> b_shift_bits) & 0xF\n        int_bzp = (bzp >> bzp_shift_bits) & 0xF\n        b = ((int_b - int_bzp) * bs).to(tl.float16)\n        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16))\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K * SPLIT_K * stride_bk // 8)\n\n    c = accumulator.to(tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_dequantize_int4_s2(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size: int = 128, output=None) -> torch.FloatTensor:\n    \"\"\"\n    Perform matrix multiplication and dequantize the result from 4-bit integers.\n    \"\"\"\n    assert x.is_contiguous(), \"A must be contiguous\"\n    assert qweight.is_contiguous(), \"B must be contiguous\"  \n    M, K = x.shape\n    N = scales.shape[1]\n    if output is None:\n        output = torch.zeros((M, N), device=x.device, dtype=torch.float16)  \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n        META['SPLIT_K'],\n    )\n    matmul_kernel[grid](\n        x, qweight, output,\n        scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        group_size,\n    )\n    return output\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n        # Additional configurations are omitted for brevity...\n    ],\n    key=['K', 'N'],\n)\n@triton.jit\ndef dequantize_kernel(\n    # Pointers to matrices\n    b_ptr, b_scale_ptr, b_zp_ptr, fpb_ptr,\n    # Matrix dimensions\n    K, N, group_size,\n    stride_bk, stride_bn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    stride_fpbk, stride_fpbn,\n    # Meta-parameters\n    BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"Dequantize tile [BLOCK_SIZE_K, BLOCK_SIZE_N] in full precision.\n    We should assert BLOCK_SIZE_N % 8 == 0.\n    weight[K // 8, N], scale[K // group_size, N], zp[K // group_size, N // group_size]\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (offs_n[None, :] // 8) * stride_bzpn\n    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[None, :] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = (offs_k[:, None] % 8) * 4\n    bzp_shift = (offs_n[None, :] % 8) * 4\n    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)\n\n\ndef dequantize_int4(b, b_scale, b_zero_point, device, dtype, group_size):\n    Kw, N = b.shape\n    K = Kw * 8\n    fp_b = torch.ones((K, N), device=device, dtype=dtype)\n    grid = lambda META: (\n        triton.cdiv(K, META['BLOCK_SIZE_K']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']), \n    )\n    dequantize_kernel[grid](\n        b, b_scale, b_zero_point, fp_b,\n        K, N, group_size,\n        b.stride(0), b.stride(1),\n        b_scale.stride(0), b_scale.stride(1),\n        b_zero_point.stride(0), b_zero_point.stride(1),\n        fp_b.stride(0), fp_b.stride(1)\n    )\n    return fp_b\n\n\ndef matmul_dequantize_int4_s1(a, b, b_scale, b_zero_point, group_size=128, out=None):\n    \"\"\"\n    Matmul dequantize int4 s1 dequantize weight to `fp_b` and do fp16 torch.mm,\n    this is for `prefill` stage, since weight size is fixed so is dequantize overhead,\n    perfill stage have more tokens to amortize dequant cost.\n    \"\"\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    Kw, N = b.shape\n    if out is None:\n        out = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    fp_b = dequantize_int4(b, b_scale, b_zero_point, a.device, a.dtype, group_size)\n    torch.mm(a, fp_b, out=out)\n    fp_b = None\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel performs row-wise quantization of a 2D input tensor. The kernel, named _quantize_rowwise, operates on each row independently. For each row, it calculates the maximum absolute value of the elements, scales each element by this max value to fit within an int8 range, and stores the results in an output tensor. The corresponding max values for each row are stored in a separate output tensor. The inputs to the kernel include pointers to the input tensor, output tensor, and max values tensor, along with the number of elements and two block size-related constants (BLOCK_SIZE and P2). The main function, quantize_rowwise, prepares the tensors and configurations required to launch the kernel, and returns the quantized tensor and the max values.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Triton kernel for row-wise quantization\n@triton.jit\ndef _quantize_rowwise(\n    input_ptr, output_ptr, max_values_ptr,\n    num_elements, BLOCK_SIZE: tl.constexpr, P2: tl.constexpr\n):\n    # Get the row index\n    row_idx = tl.program_id(0)\n    \n    # Compute the starting index of the row in the flattened array\n    row_start = row_idx * BLOCK_SIZE\n    \n    # Load the row into shared memory\n    offsets = row_start + tl.arange(0, BLOCK_SIZE)\n    row = tl.load(input_ptr + offsets, mask=offsets < num_elements, other=0.0)\n    \n    # Compute the maximum absolute value in the row\n    max_abs_value = tl.abs(row).max()\n    \n    # Store the max value for this row\n    tl.store(max_values_ptr + row_idx, max_abs_value)\n    \n    # Scale the row elements to fit in int8 range [-128, 127]\n    scale = 127.0 / max_abs_value\n    quantized_row = tl.cast(row * scale, tl.int8)\n    \n    # Store the quantized row\n    tl.store(output_ptr + offsets, quantized_row, mask=offsets < num_elements)\n\n# Wrapper function to prepare data and launch the kernel\ndef quantize_rowwise(input_tensor, BLOCK_SIZE=1024, P2=0):\n    # Ensure input is a 2D tensor\n    assert input_tensor.ndim == 2, \"Input tensor must be 2D\"\n    \n    # Get the dimensions of the input tensor\n    num_rows, num_cols = input_tensor.shape\n    \n    # Allocate output tensors\n    output_tensor = torch.empty_like(input_tensor, dtype=torch.int8)\n    max_values = torch.empty(num_rows, dtype=torch.float32)\n    \n    # Launch the kernel\n    grid = (num_rows,)\n    _quantize_rowwise[grid](\n        input_tensor, output_tensor, max_values,\n        num_cols, BLOCK_SIZE, P2\n    )\n    \n    return output_tensor, max_values\n\n# Example usage\ninput_tensor = torch.randn(10, 1024, dtype=torch.float32, device='cuda')\nquantized_tensor, max_values = quantize_rowwise(input_tensor)\nprint(\"Quantized Tensor:\", quantized_tensor)\nprint(\"Max Values:\", max_values)\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n\n# TODO: autotune this better.\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_stages=1, num_warps=8),\n        triton.Config({}, num_stages=2, num_warps=8),\n        triton.Config({}, num_stages=4, num_warps=8),\n        triton.Config({}, num_stages=8, num_warps=8),\n        triton.Config({}, num_stages=1),\n        triton.Config({}, num_stages=2),\n        triton.Config({}, num_stages=4),\n        triton.Config({}, num_stages=8),\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef _quantize_rowwise(\n    x_ptr,\n    output_ptr,\n    output_maxs,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n    P2: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    arange = tl.arange(0, P2)\n    offsets = block_start + arange\n    row_mask = arange < BLOCK_SIZE\n    x = tl.load(x_ptr + offsets, mask=row_mask)\n\n    abs_x = tl.abs(x)\n    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (x / max_val))\n    tl.store(output_ptr + offsets, output, mask=row_mask)\n    tl.store(output_maxs + pid, max_val)\n\ndef quantize_rowwise(x: torch.Tensor):\n    output = torch.empty(*x.shape, device=x.device, dtype=torch.int8)\n    output_maxs = torch.empty(x.shape[0], device=x.device, dtype=torch.float16)\n\n    P2 = int(2 ** (math.ceil(math.log2(x.shape[1]))))\n\n    assert x.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (x.shape[0],)\n    _quantize_rowwise[grid](x, output, output_maxs, n_elements, BLOCK_SIZE=x.shape[1], P2=P2)\n    return output, output_maxs\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is called '_fwd_kernel'. It computes a forward pass of a token softmax reduction on input matrices for attention mechanisms in deep learning models. The kernel takes as input tensors: 'Logics', 'V', and output 'Out', along with auxiliary tensors like 'B_Loc', 'B_Start_Loc', 'B_Seqlen', and scalar values for dimensions. The kernel operates across batch and head dimensions, determined by 'program_id'. It processes blocks of data using two loop variables 'start_n' and 'offs_n' for efficient computation. Inside the loop, it computes softmax normalization over input logits 'Logics' and updates output tensor 'Out'. Memory access patterns and stride operations are used to efficiently access elements in 'V', 'Logics', and 'Out'. The function 'token_softmax_reducev_fwd' sets up the parameters and executes the kernel across a 2D grid, determined by the batch and head size. ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(\n    Logics_ptr, V_ptr, Out_ptr, \n    B_Loc_ptr, B_Start_Loc_ptr, B_Seqlen_ptr,\n    batch_size, head_size, seq_len, d_model,\n    stride_logics_batch, stride_logics_head, stride_logics_seq,\n    stride_v_batch, stride_v_head, stride_v_seq,\n    stride_out_batch, stride_out_head, stride_out_seq,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Program ID determines which batch and head we're processing\n    pid = tl.program_id(axis=0)\n    batch_id = pid // head_size\n    head_id = pid % head_size\n\n    # Compute the starting location for this batch and head\n    start_loc = tl.load(B_Start_Loc_ptr + batch_id)\n    seq_len = tl.load(B_Seqlen_ptr + batch_id)\n\n    # Loop over sequence length in blocks\n    for start_n in range(0, seq_len, BLOCK_SIZE):\n        offs_n = tl.arange(0, BLOCK_SIZE)\n        mask = offs_n < seq_len\n\n        # Compute offsets for Logics, V, and Out\n        logics_offset = (\n            batch_id * stride_logics_batch +\n            head_id * stride_logics_head +\n            (start_loc + start_n + offs_n) * stride_logics_seq\n        )\n        v_offset = (\n            batch_id * stride_v_batch +\n            head_id * stride_v_head +\n            (start_loc + start_n + offs_n) * stride_v_seq\n        )\n        out_offset = (\n            batch_id * stride_out_batch +\n            head_id * stride_out_head +\n            (start_loc + start_n + offs_n) * stride_out_seq\n        )\n\n        # Load data from Logics and V\n        logics = tl.load(Logics_ptr + logics_offset, mask=mask, other=-float('inf'))\n        v = tl.load(V_ptr + v_offset, mask=mask)\n\n        # Compute softmax normalization\n        max_logics = tl.max(logics, axis=0)\n        logics = logics - max_logics\n        exp_logics = tl.exp(logics)\n        sum_exp_logics = tl.sum(exp_logics, axis=0)\n        softmax = exp_logics / sum_exp_logics\n\n        # Update output tensor\n        out = tl.load(Out_ptr + out_offset, mask=mask)\n        out += softmax * v\n        tl.store(Out_ptr + out_offset, out, mask=mask)\n\ndef token_softmax_reducev_fwd(Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen, batch_size, head_size, seq_len, d_model):\n    BLOCK_SIZE = 128  # Example block size, adjust as needed\n\n    # Define grid dimensions\n    grid = (batch_size * head_size,)\n\n    # Launch the Triton kernel\n    _fwd_kernel[grid](\n        Logics, V, Out, \n        B_Loc, B_Start_Loc, B_Seqlen,\n        batch_size, head_size, seq_len, d_model,\n        Logics.stride(0), Logics.stride(1), Logics.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n\n@triton.jit\ndef _fwd_kernel(\n    Logics, V, Out,\n    B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs,\n    stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od,\n    stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, # Avoid reading NaN data\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    off_v = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len) * stride_b_loc_s\n\n    v_ptrs = V + off_v\n\n    e_max = float(\"-inf\")\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) * stride_b_loc_s, mask=(start_n + offs_n) < cur_batch_seq_len, other=other_kv_index)\n\n        qk = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, \n                     mask=start_n + offs_n < cur_batch_seq_len, other=float(\"-inf\"))\n    \n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n\n\n@torch.no_grad()\ndef token_softmax_reducev_fwd(logics, v, o, b_loc, b_start_loc, b_seq_len, max_input_len, other_kv_index):\n    BLOCK = 64\n    batch, head = b_seq_len.shape[0], logics.shape[0]\n    grid = (batch, head)\n    num_warps = 1\n    _fwd_kernel[grid](\n        logics, v, o, b_loc, b_start_loc, b_seq_len, max_input_len,\n        logics.stride(0), logics.stride(1),\n        v.stride(0), v.stride(1), v.stride(2),\n        o.stride(0), o.stride(1), o.stride(2),\n        b_loc.stride(0), b_loc.stride(1),\n        other_kv_index,\n        BLOCK_DMODEL=v.shape[-1],\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=3\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided code defines a Triton kernel for matrix multiplication with quantized INT4 weights and a set of functions to perform quantization and dequantization. \n        The kernel function 'matmul_kernel' computes a block of the output matrix by iterating over the K dimension, and the function 'matmul_dequantize_int4_s2' sets up the inputs and launches this kernel for computation.\n        'quantize_int4' function quantizes a floating-point weight matrix into INT4 format while 'unpack_int4' is used to verify correctness by unpacking the INT4 weights back to floating-point. \n        These functions take advantage of Triton's ability to utilize GPUs for efficient computation.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Quantize a floating-point weight matrix into INT4 format\ndef quantize_int4(weights, scale):\n    # Scale the weights and clip to the INT4 range\n    quantized = torch.clamp(torch.round(weights / scale), -8, 7).to(torch.int8)\n    return quantized\n\n# Dequantize INT4 weights back to floating-point\ndef unpack_int4(quantized, scale):\n    return quantized.to(torch.float32) * scale\n\n# Triton kernel for matrix multiplication with INT4 weights\n@triton.jit\ndef matmul_kernel(\n    A, B, C, M, N, K, scale, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Define the block indices\n    pid = tl.program_id(0)\n    block_m = pid // (N // BLOCK_SIZE)\n    block_n = pid % (N // BLOCK_SIZE)\n\n    # Define the offsets for A, B, and C\n    offs_am = block_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_bn = block_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_k = tl.arange(0, BLOCK_SIZE)\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE):\n        # Load A and B tiles\n        a = tl.load(A + offs_am[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak)\n        b = tl.load(B + (k + offs_k[:, None]) * stride_bk + offs_bn[None, :] * stride_bn)\n        \n        # Dequantize B (INT4 to float)\n        b = b.to(tl.float32) * scale\n        \n        # Perform matrix multiplication\n        acc += tl.dot(a, b)\n\n    # Store the result\n    tl.store(C + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn, acc)\n\n# Wrapper function to set up and launch the Triton kernel\ndef matmul_dequantize_int4_s2(A, B_quantized, scale, M, N, K):\n    # Allocate output matrix\n    C = torch.empty((M, N), dtype=torch.float32, device='cuda')\n\n    # Launch the Triton kernel\n    BLOCK_SIZE = 16\n    grid = (M // BLOCK_SIZE) * (N // BLOCK_SIZE)\n    matmul_kernel[grid](\n        A, B_quantized, C, M, N, K, scale,\n        A.stride(0), A.stride(1),\n        B_quantized.stride(0), B_quantized.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return C\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define matrix dimensions\n    M, N, K = 128, 128, 128\n\n    # Create input matrices\n    A = torch.randn((M, K), dtype=torch.float32, device='cuda')\n    B = torch.randn((K, N), dtype=torch.float32, device='cuda')\n\n    # Quantize B\n    scale = 0.1  # Example scale factor\n    B_quantized = quantize_int4(B, scale)\n\n    # Perform matrix multiplication\n    C = matmul_dequantize_int4_s2(A, B_quantized, scale, M, N, K)\n\n    # Print the result\n    print(C)\n", "label": "import time\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n\tconfigs=[\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t    \n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n\t\t\n ],\n\tkey=['M', 'N', 'K'],\n    reset_to_zero=['c_ptr']\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    bs_ptr, bzp_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    group_size,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr\n    ):\n    \"\"\"\n    assert K % (BLOCK_SIZE_K * SPLIT_K) == 0\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\n    # [BLOCK_M, BLOCK_K]\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    # [BLOCK_K, BLOCK_N] but repeated 8 times in N\n    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn\n    # tl.static_print(\"shape\", a_ptrs, b_ptrs, bs_ptrs, bzp_ptrs)\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        # Load the next block of A and B.\n        # [BLOCK_K, BLOCK_N] but repeated group_size times in K \n        bs_ptrs = bs_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bsk \\\n            + offs_bn[None, :] * stride_bsn\n        # [BLOCK_K, BLOCK_N] but repeated in K and N\n        bzp_ptrs = bzp_ptr + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size) * stride_bzpk \\\n            + (offs_bn[None, :] // 8) * stride_bzpn\n        b_shift_bits = (offs_k[:, None] % 8) * 4 # assert BLOCK_SIZE_K % 8 == 0\n        bzp_shift_bits = (offs_bn[None, :] % 8) * 4\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        bs = tl.load(bs_ptrs)\n        bzp = tl.load(bzp_ptrs)\n        # We accumulate along the K dimension.\n        int_b = (b >> b_shift_bits) & 0xF\n        int_bzp = (bzp >> bzp_shift_bits) & 0xF\n        b = ((int_b - int_bzp) * bs).to(a.dtype)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        # Advance the ptrs to the next K block.\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K * SPLIT_K * stride_bk // 8)  # assert BLOCK_SIZE_K % 8 == 0\n    # You can fuse arbitrary activation functions here\n    # while the accumulator is still in FP32!\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    # -----------------------------------------------------------\n    # Write back the block of the output matrix C with masks.\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n\n\ndef matmul_dequantize_int4_s2(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size: int = 128, output=None) -> torch.FloatTensor:\n    \"\"\"\n    \"\"\"\n    assert x.is_contiguous(), \"A must be contiguous\"\n    assert qweight.is_contiguous(), \"B must be contiguous\"  \n    M, K = x.shape\n    N = scales.shape[1]\n    if output is None:\n        output = torch.zeros((M, N), device=x.device, dtype=x.dtype)  \n    grid = lambda META: (\n        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n        META['SPLIT_K'],\n    )\n    matmul_kernel[grid](\n        x, qweight, output,\n        scales, qzeros,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        qweight.stride(0), qweight.stride(1),\n        output.stride(0), output.stride(1),\n        scales.stride(0), scales.stride(1),\n        qzeros.stride(0), qzeros.stride(1),\n        group_size,\n    )\n    return output\n\ndef quantize_int4(weight, group_size=128, tp_rank=0):\n    # Weight shape: [H1 // 8, H2]\n    # Scale shape: [H1 // group_size, H2]\n    # zero_pint shape: [H1 // group_size, H2 // 8]\n\n    weight = weight.transpose(1, 0)\n    h1, h2 = weight.shape\n    assert h1 % 8 == 0 and h2 % 8 == 0, \"H1 {} H2 {}\".format(h1, h2)\n    assert h2 % group_size == 0, \"H1 {} H2 {}\".format(h1, h2)\n    weight = weight.contiguous().view(-1, group_size).cuda(tp_rank)\n    weight_max = weight.amax(-1, keepdim=True)\n    weight_max = torch.where(weight_max < 0, 0, weight_max)\n    weight_min = weight.amin(-1, keepdim=True)\n    weight_min = torch.where(weight_min > 0, 0, weight_min)\n    weight_range = weight_max - weight_min \n    scale = weight_range / (2 ** 4 - 1)\n    zero_point = (-weight_min / scale).round().clamp(0, 15).to(torch.int32)\n    weight = (weight / scale + zero_point).round().clamp(0, 15).to(torch.int32).view(h1, h2)\n    int_weight = torch.empty(h1, h2 // 8).to(torch.int32).to(weight.device)\n    int_zero_point = torch.zeros(h1 // 8, h2 // group_size).to(torch.int32).to(weight.device)\n    zero_point = zero_point.view(h1, -1)\n    scale = scale.view(h1, -1)\n    # pack 8 int4 in an int32 number.\n    # Weight pack in row.\n    for pack in range(0, h2, 8):\n        for i in range(8):\n            int_weight[:, pack // 8] += weight[:, pack + i] << (i * 4)\n    # zero point pack in col.\n    for pack in range(0, h1, 8):\n        for i in range(8):\n            int_zero_point[pack // 8, :] += zero_point[pack + i, :] << (i * 4)\n    '''\n    fp_weight = torch.zeros(h1, h2).half().to(weight.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_weight[pack * 8 + i, :] = \\\n                ((int_weight[pack, :] << (28 - i * 4) >> 28) + 16) % 16\n    print((fp_weight - weight).abs().sum())\n\n    fp_zp = torch.zeros(zero_point.shape).half().to(zero_point.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zp[pack * 8 + i, :] = \\\n                (int_zero_point[pack, :] >> (i * 4)) & 15\n\n    print((fp_zp - zero_point).abs().sum())\n    '''\n    weight = None\n    return int_weight.transpose(1, 0).contiguous(), scale.transpose(1, 0).contiguous(), int_zero_point.transpose(1, 0).contiguous(), group_size\n\n\ndef unpack_int4(weight, scale, zp):\n    \"\"\"\n    Test function to verify quantize int4 is correct.\n    Will not be used in model inference.\n    \"\"\"\n    weight = weight.transpose(1, 0)\n    scale = scale.transpose(1, 0)\n    zp = zp.transpose(1, 0)\n    h1, h2 = weight.shape\n    group_size = h2 * 8 // scale.shape[1]\n    group_num = scale.shape[1]\n    fp_weight = torch.zeros(h1, h2 * 8).half().to(weight.device)\n    fp_zero_point = torch.zeros(h1, group_num).to(weight.device)\n    for pack in range(0, h2):\n        for i in range(8):\n            fp_weight[:, pack * 8 + i] = (weight[:, pack] >> (i * 4)) & 0xF\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zero_point[pack * 8 + i, :] = (zp[pack, :] >> (i * 4)) & 0xF\n    for g in range(group_num):\n        fp_weight[:, g * group_size:(g + 1) * group_size] = (fp_weight[:, g * group_size:(g + 1) * group_size] - \\\n                                                             fp_zero_point[:, g].unsqueeze(1)) * scale[:, g].unsqueeze(1)\n    return fp_weight.transpose(1, 0)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton code implements a simple kernel function called 'puzzle1_kernel' that is designed to add a constant value to each element of an input tensor 'x'. The function 'puzzle1' serves as a wrapper to call this kernel on a given input tensor 'x', producing an output tensor. The primary goal is to perform an element-wise addition of a value (10 in this case) to the input tensor when using CUDA. It operates over a 1D grid of threads with a block size defined as a constant expression (BLOCK_SIZE). The output is stored in a separate output tensor.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 1024  # You can adjust this value based on your GPU architecture and problem size\n\n@triton.jit\ndef puzzle1_kernel(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Calculate the program index\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the start and end index for this block\n    start = pid * BLOCK_SIZE\n    end = tl.min(start + BLOCK_SIZE, n_elements)\n    \n    # Create a range of indices for the current block\n    offsets = start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we do not go out of bounds\n    mask = offsets < n_elements\n    \n    # Load x from global memory\n    x = tl.load(x_ptr + offsets, mask=mask)\n    \n    # Add the constant value (10) to each element\n    y = x + 10\n    \n    # Store the result in the output tensor\n    tl.store(y_ptr + offsets, y, mask=mask)\n\ndef puzzle1(x):\n    # Get the number of elements in the input tensor\n    n_elements = x.numel()\n    \n    # Allocate output tensor\n    y = torch.empty_like(x)\n    \n    # Calculate the number of blocks needed\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    \n    # Launch the kernel\n    puzzle1_kernel[grid](x, y, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return y\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n# Triton kernel\n@triton.jit\ndef puzzle1_kernel(x_ptr, output_ptr, N, BLOCK_SIZE: tl.constexpr, value):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = x + value\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# Wrapper function to call the kernel\ndef puzzle1(x: torch.Tensor):\n    output = torch.empty_like(x)\n    assert x.is_cuda and output.is_cuda\n    N = output.numel()\n    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n    puzzle1_kernel[grid](x, output, N, BLOCK_SIZE=1024, value=10)\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code provides two Triton kernels and corresponding Python functions for type conversion between float8 and float16. The function `f8_to_f16` converts int8 (representing float8 values) to float16, and the function `f16_to_f8` converts float16 or float32 values to int8 (representing float8 values). Both kernels work with a block-based approach, using a fixed BLOCK_SIZE of 1024. \n            The main operations involve loading data, applying optional masking for out-of-bounds accesses, and storing the result. \n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n# Kernel to convert int8 (representing float8) to float16\n@triton.jit\ndef kernel_f8_to_f16(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Define the block index\n    pid = tl.program_id(0)\n    \n    # Compute the start index for this block\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range of offsets within the block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n    \n    # Load input data, applying the mask\n    input_data = tl.load(input_ptr + offsets, mask=mask, other=0)\n    \n    # Convert int8 to float16 (assuming input_data is int8 representing float8)\n    # Placeholder conversion, as Triton does not natively support float8\n    output_data = input_data.to(tl.float16)\n    \n    # Store the result\n    tl.store(output_ptr + offsets, output_data, mask=mask)\n\n# Kernel to convert float16 (or float32) to int8 (representing float8)\n@triton.jit\ndef kernel_f16_to_f8(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Define the block index\n    pid = tl.program_id(0)\n    \n    # Compute the start index for this block\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range of offsets within the block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n    \n    # Load input data, applying the mask\n    input_data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    \n    # Convert float16 (or float32) to int8 (assuming output is int8 representing float8)\n    # Placeholder conversion, as Triton does not natively support float8\n    output_data = input_data.to(tl.int8)\n    \n    # Store the result\n    tl.store(output_ptr + offsets, output_data, mask=mask)\n\n# Python wrapper for f8 to f16 conversion\ndef f8_to_f16(input_tensor):\n    assert input_tensor.dtype == torch.int8, \"Input tensor must be of type int8\"\n    n_elements = input_tensor.numel()\n    output_tensor = torch.empty(n_elements, dtype=torch.float16, device=input_tensor.device)\n    \n    # Launch the kernel\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    kernel_f8_to_f16[grid](input_tensor, output_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return output_tensor\n\n# Python wrapper for f16 to f8 conversion\ndef f16_to_f8(input_tensor):\n    assert input_tensor.dtype in [torch.float16, torch.float32], \"Input tensor must be of type float16 or float32\"\n    n_elements = input_tensor.numel()\n    output_tensor = torch.empty(n_elements, dtype=torch.int8, device=input_tensor.device)\n    \n    # Launch the kernel\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    kernel_f16_to_f8[grid](input_tensor, output_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return output_tensor\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input tensor for f8_to_f16\n    input_f8 = torch.randint(-128, 127, (2048,), dtype=torch.int8, device='cuda')\n    output_f16 = f8_to_f16(input_f8)\n    print(output_f16)\n\n    # Example input tensor for f16_to_f8\n    input_f16 = torch.randn(2048, dtype=torch.float16, device='cuda')\n    output_f8 = f16_to_f8(input_f16)\n    print(output_f8)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef kernel_f8_to_f16(Y, X, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n    x = tl.load(X + offs, mask=mask)\n    tl.store(Y + offs, x, mask=mask)\n    tl.store(Y + offs, x, mask=mask)\n\ndef f8_to_f16(x, dtypes=tl.float8e5) -> torch.Tensor:\n    assert x.dtype == torch.int8, f\"torch.int8 expected but got {x.dtype}\"\n    assert \"cuda\" in str(x.device), f\"CUDA tensors only but got {x.device}\"\n    ret = torch.empty_like(x, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n    numel = ret.untyped_storage().size() // ret.element_size()\n    kernel_f8_to_f16[grid](ret, triton.reinterpret(x, dtypes), numel, BLOCK_SIZE=1024)\n    return ret\n\n@triton.jit\ndef kernel_f16_to_f8(Y, X, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n    x = tl.load(X + offs, mask=mask)\n    tl.store(Y + offs, x, mask=mask)\n\ndef f16_to_f8(x: torch.Tensor, dtypes=tl.float8e5) -> torch.Tensor:\n    assert x.dtype in [torch.float16, torch.float32]\n    assert \"cuda\" in str(x.device), f\"CUDA tensors only but got {x.device}\"\n    ret = torch.empty_like(x, dtype=torch.int8)\n    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n    numel = x.untyped_storage().size() // x.element_size()\n    kernel_f16_to_f8[grid](triton.reinterpret(ret, dtypes), x, numel, BLOCK_SIZE=1024)\n    return ret\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton kernel called `matmul_kernel`, which performs matrix multiplication. The kernel takes pointers to matrices `a`, `b`, and `c`, along with block sizes `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`. Inside the kernel, each thread computes a block of the output matrix `c` by loading sub-blocks of `a` and `b`, performing a dot product, and storing the results in `c`. The `matmul` function is a wrapper that launches the kernel for specified matrix dimensions `M`, `N`, and `K`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    # Program ID is used to determine which block of the output matrix this instance of the kernel computes\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the starting indices of the block in the output matrix\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create pointers for the input matrices a and b\n    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n\n    # Initialize accumulator for the output block\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of a and b\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n\n        # Compute matrix multiplication for the block\n        acc += tl.dot(a, b)\n\n        # Update pointers for the next block in K dimension\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    # Compute the starting pointer for the block in the output matrix\n    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n\n    # Store the computed block in the output matrix\n    tl.store(c_ptrs, acc)\n\ndef matmul(a, b, c, M, N, K, BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32):\n    # Ensure that dimensions are compatible for matrix multiplication\n    assert a.shape == (M, K)\n    assert b.shape == (K, N)\n    assert c.shape == (M, N)\n\n    # Define the grid size\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n    # Launch the Triton kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef matmul_kernel(c_ptr, a_ptr, b_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    M, N, K = 4096, 4096, 4096\n    stride_am = 4096\n    stride_ak = 1\n    stride_bk = 4096\n    stride_bn = 1\n    stride_cm = 4096\n    stride_cn = 1\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = tl.cast(accumulator, tl.float16)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    tl.store(c_ptrs, c)\n\ndef matmul(c, a, b, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n    matmul_kernel[triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N)](\n        c, a, b, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K\n    )\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a Triton-based kernel and its autograd Function for a fused recurrent retention operation. This operation handles the computation of output tensor 'o' and an optional 'final_state' using input tensors 'q', 'k', 'v', and optionally 'initial_state'. The operation scales the query 'q', multiplies it with the key-value product 'h' which is updated iteratively across a temporal dimension 'T'. The kernel uses block dimensions BK, BV for keys and values, respectively, and can conditionally use initial and store final states. The backward kernel computes gradients for 'q', 'k', 'v' using 'do' and backpropagates through temporal steps in reverse.\n        The main function 'fused_recurrent_retention' takes input tensors, optionally an 'initial_state', and returns the computed output tensor 'o' and optionally 'final_state'.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Forward kernel\n@triton.jit\ndef recurrent_retention_forward_kernel(Q, K, V, O, T, BK, BV, Lq, Lk, Lv, Lo, initial_state_ptr=None, final_state_ptr=None):\n    pid = tl.program_id(0)\n    # Load blocks of Q, K, V\n    q = tl.load(Q + pid * BK, mask=pid < Lq, other=0.0)\n    k = tl.load(K + pid * BK, mask=pid < Lk, other=0.0)\n    v = tl.load(V + pid * BV, mask=pid < Lv, other=0.0)\n\n    # Scale query\n    q = q * tl.sqrt(1.0 / BK)\n\n    # Initialize state\n    if initial_state_ptr is not None:\n        state = tl.load(initial_state_ptr + pid * BK, mask=pid < Lq, other=0.0)\n    else:\n        state = tl.zeros([BK], dtype=tl.float32)\n\n    # Iterate over time steps\n    for t in range(T):\n        # Compute key-value product\n        h = tl.dot(k, v)\n        # Update state\n        state = state + h\n        # Compute output\n        o = tl.dot(q, state)\n        tl.store(O + pid * BV + t * BV, o, mask=pid < Lo)\n\n    # Store final state if needed\n    if final_state_ptr is not None:\n        tl.store(final_state_ptr + pid * BK, state, mask=pid < Lq)\n\n# Backward kernel\n@triton.jit\ndef recurrent_retention_backward_kernel(dO, Q, K, V, dQ, dK, dV, T, BK, BV, Lq, Lk, Lv, Lo):\n    pid = tl.program_id(0)\n    # Load blocks of dO, Q, K, V\n    do = tl.load(dO + pid * BV, mask=pid < Lo, other=0.0)\n    q = tl.load(Q + pid * BK, mask=pid < Lq, other=0.0)\n    k = tl.load(K + pid * BK, mask=pid < Lk, other=0.0)\n    v = tl.load(V + pid * BV, mask=pid < Lv, other=0.0)\n\n    # Initialize gradients\n    dq = tl.zeros([BK], dtype=tl.float32)\n    dk = tl.zeros([BK], dtype=tl.float32)\n    dv = tl.zeros([BV], dtype=tl.float32)\n\n    # Iterate over time steps in reverse\n    for t in reversed(range(T)):\n        # Compute gradients\n        dq += tl.dot(do, k)\n        dk += tl.dot(q, do)\n        dv += tl.dot(k, do)\n\n    # Store gradients\n    tl.store(dQ + pid * BK, dq, mask=pid < Lq)\n    tl.store(dK + pid * BK, dk, mask=pid < Lk)\n    tl.store(dV + pid * BV, dv, mask=pid < Lv)\n\n# Python wrapper\nclass FusedRecurrentRetentionFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, initial_state=None):\n        # Determine block sizes and launch grid\n        BK = 128  # Example block size for keys\n        BV = 128  # Example block size for values\n        T = q.shape[0]  # Temporal dimension\n        Lq, Lk, Lv, Lo = q.shape[1], k.shape[1], v.shape[1], q.shape[1]\n\n        # Allocate output tensors\n        o = torch.empty_like(q)\n        final_state = torch.empty_like(q) if initial_state is not None else None\n\n        # Launch forward kernel\n        recurrent_retention_forward_kernel[(1,)](q, k, v, o, T, BK, BV, Lq, Lk, Lv, Lo, initial_state, final_state)\n\n        # Save for backward\n        ctx.save_for_backward(q, k, v, o, initial_state)\n\n        return (o, final_state) if final_state is not None else o\n\n    @staticmethod\n    def backward(ctx, do, d_final_state=None):\n        q, k, v, o, initial_state = ctx.saved_tensors\n        BK = 128  # Example block size for keys\n        BV = 128  # Example block size for values\n        T = q.shape[0]  # Temporal dimension\n        Lq, Lk, Lv, Lo = q.shape[1], k.shape[1], v.shape[1], q.shape[1]\n\n        # Allocate gradient tensors\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n\n        # Launch backward kernel\n        recurrent_retention_backward_kernel[(1,)](do, q, k, v, dq, dk, dv, T, BK, BV, Lq, Lk, Lv, Lo)\n\n        return dq, dk, dv, None\n\n# Main function\ndef fused_recurrent_retention(q, k, v, initial_state=None):\n    return FusedRecurrentRetentionFunction.apply(q, k, v, initial_state)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.jit\ndef fused_recurrent_retention_fwd_kernel(\n    q, k, v, o, initial_state, final_state, \n    s_qk_h, s_qk_t, s_qk_d, \n    s_vo_h, s_vo_t, s_vo_d, \n    B, H, T, scale, \n    BK: tl.constexpr, BV: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, \n    USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr\n):\n    # Kernel logic\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = (1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < DK\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + \\\n            (i_k * BK + tl.arange(0, BK)[None, :]) * \\\n            DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0).to(tl.float32)\n\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        _v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        _q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n\n        h = b_b * h + _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o.to(p_o.dtype.element_ty), mask=mask_bv)\n\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + \\\n            (i_k * BK + tl.arange(0, BK)[None, :]) * \\\n            DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h.to(p_final_s.dtype.element_ty), mask=mask_kv)\n\n@triton.jit\ndef fused_recurrent_retention_bwd_kernel(\n    q, k, v, do, dq, dk, dv, initial_state, \n    s_qk_h, s_qk_t, s_qk_d, \n    s_vo_h, s_vo_t, s_vo_d, \n    B, H, T, scale, \n    BK: tl.constexpr, BV: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, \n    USE_INITIAL_STATE: tl.constexpr\n):\n    # Kernel logic\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + \\\n            (i_k * BK + tl.arange(0, BK)[:, None]) * \\\n            DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0).to(tl.float32)\n\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        _v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        _do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n\n        h = b_b * h + _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n\n    tl.debug_barrier()\n\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * \\\n        BK + tl.arange(0, BK) + (T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * \\\n        BV + tl.arange(0, BV) + (T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n        _q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        _v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n\n        d_h *= b_b\n        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n\nclass FusedRecurrentRetentionFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, initial_state=None, output_final_state=False):\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n\n        scale = d_head_qk ** -0.5\n        BK, BV = min(d_head_qk, 32), min(d_head_v, 32)\n        NK, NV = triton.cdiv(d_head_qk, BK), triton.cdiv(d_head_v, BV)\n        num_stages = 1\n        num_warps = 1\n\n        o = q.new_empty(NK, batch_size, n_heads, seq_len, d_head_v)\n\n        if output_final_state:\n            final_state = q.new_empty(batch_size, n_heads, d_head_qk, d_head_v)\n        else:\n            final_state = None\n\n        grid = (NV, NK, batch_size * n_heads)\n        fused_recurrent_retention_fwd_kernel[grid](\n            q, k, v, o, initial_state, final_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            DK=d_head_qk, DV=d_head_v, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None\n        )\n\n        o = o.sum(0)\n        ctx.save_for_backward(q, k, v, initial_state)\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, d_final_state=None):\n        q, k, v, initial_state = ctx.saved_tensors\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n        scale = d_head_qk ** -0.5\n\n        BK, BV = min(d_head_qk, 32), min(d_head_v, 32)\n        NK, NV = triton.cdiv(d_head_qk, BK), triton.cdiv(d_head_v, BV)\n        num_stages = 1\n        num_warps = 1\n\n        dq = q.new_empty(NV, batch_size, n_heads, seq_len, d_head_qk)\n        dk = q.new_empty(NV, batch_size, n_heads, seq_len, d_head_qk)\n        dv = q.new_empty(NK, batch_size, n_heads, seq_len, d_head_v)\n        grid = (NV, NK, batch_size * n_heads)\n\n        fused_recurrent_retention_bwd_kernel[grid](\n            q, k, v, do, dq, dk, dv, initial_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            DK=d_head_qk, DV=d_head_v, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages,\n            USE_INITIAL_STATE=initial_state is not None\n        )\n        dq = dq.sum(0)\n        dk = dk.sum(0)\n        dv = dv.sum(0)\n        return dq, dk, dv, None, None\n\ndef fused_recurrent_retention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if initial_state is not None:\n        initial_state = initial_state.detach()\n    o, final_state = FusedRecurrentRetentionFunction.apply(q, k, v, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The function 'rmsnorm_forward' applies RMS normalization on a given input tensor. It utilizes a Triton kernel '_rms_norm_fwd_fused' to perform the operation. The main inputs are 'x', the input tensor, and 'weight', the normalization weight. The output is 'y', a tensor of the same shape as 'x', after applying RMS normalization. The Triton kernel calculates the variance of each row of the input, computes the reciprocal of the square root of the variance plus a small epsilon for numerical stability, and applies this normalization along with the weight vector on the input data.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _rms_norm_fwd_fused(x_ptr, weight_ptr, y_ptr, n_cols, eps, BLOCK_SIZE: tl.constexpr):\n    # Program ID corresponds to the row index\n    row_idx = tl.program_id(0)\n    \n    # Offsets for the row\n    offsets = row_idx * n_cols + tl.arange(0, BLOCK_SIZE)\n    \n    # Load input data for the current row\n    x = tl.load(x_ptr + offsets, mask=offsets < n_cols, other=0.0)\n    \n    # Compute the squared values\n    x_squared = x * x\n    \n    # Compute the sum of squares for the row\n    sum_squares = tl.sum(x_squared, axis=0)\n    \n    # Compute the mean square\n    mean_square = sum_squares / n_cols\n    \n    # Compute the inverse of the RMS (root mean square)\n    inv_rms = 1.0 / tl.sqrt(mean_square + eps)\n    \n    # Load the weight vector\n    weight = tl.load(weight_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < n_cols, other=0.0)\n    \n    # Normalize and apply the weight\n    y = x * inv_rms * weight\n    \n    # Store the result\n    tl.store(y_ptr + offsets, y, mask=offsets < n_cols)\n\ndef rmsnorm_forward(x, weight, eps=1e-5):\n    # Get the shape of the input tensor\n    n_rows, n_cols = x.shape\n    \n    # Allocate output tensor\n    y = torch.empty_like(x)\n    \n    # Launch Triton kernel\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    grid = (n_rows,)\n    \n    _rms_norm_fwd_fused[grid](\n        x_ptr=x,\n        weight_ptr=weight,\n        y_ptr=y,\n        n_cols=n_cols,\n        eps=eps,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return y\n\n# Example usage\nx = torch.randn(1024, 512, device='cuda')\nweight = torch.ones(512, device='cuda')  # Example weight vector\ny = rmsnorm_forward(x, weight)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _rms_norm_fwd_fused(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    W,  # pointer to the weights\n    stride,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    # Compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask).to(tl.float32)\n        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        # Write output\n        tl.store(Y + cols, y.to(tl.float16), mask=mask)\n\ndef rmsnorm_forward(x, weight, eps):\n    # allocate output\n    y = torch.empty_like(x)\n    # reshape input data into 2D tensor\n    x_arg = x.view(-1, x.shape[-1])\n    M, N = x_arg.shape\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_SIZE:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    # heuristics for number of warps\n    num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n    BLOCK_SIZE = 128 * 2 * 2 * 2 * 2 * 2 * 2 * 2\n    num_warps = 8\n    # enqueue kernel\n    _rms_norm_fwd_fused[(M,)](x_arg, y, weight,\n                              x_arg.stride(0), N, eps,\n                              BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n    return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel and a wrapper function to copy values from a source tensor K to a destination tensor Out at specific indices provided by DestLoc. The kernel, _fwd_kernel_destindex_copy_kv, is responsible for loading data from the specified positions in K and storing it in the corresponding locations in Out. The input K is a 3D tensor with dimensions for batch, head, and depth. DestLoc provides the target indices in the batch dimension. The head and depth dimensions are divided into blocks for efficient parallel processing, controlled by BLOCK_HEAD and BLOCK_DMODEL, which are powers of two. The function destindex_copy_kv initializes the grid and block dimensions and calls the kernel for each sequence in the batch.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the kernel\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Out, DestLoc, BLOCK_HEAD: tl.constexpr, BLOCK_DMODEL: tl.constexpr):\n    # Get the program id\n    pid = tl.program_id(0)\n    \n    # Compute the head and depth offsets for the current block\n    head_offset = pid // BLOCK_DMODEL\n    depth_offset = pid % BLOCK_DMODEL\n    \n    # Load the destination index for the current block\n    dest_index = tl.load(DestLoc + head_offset)\n    \n    # Compute the base pointers for K and Out\n    K_ptrs = K + dest_index * K.stride(0) + head_offset * K.stride(1) + depth_offset\n    Out_ptrs = Out + pid * Out.stride(1)\n    \n    # Load data from K and store it into Out\n    k_data = tl.load(K_ptrs)\n    tl.store(Out_ptrs, k_data)\n\n# Define the wrapper function\ndef destindex_copy_kv(K, Out, DestLoc, BLOCK_HEAD, BLOCK_DMODEL):\n    # Determine the grid size\n    grid = (K.shape[1] * K.shape[2] + BLOCK_DMODEL - 1) // BLOCK_DMODEL\n    \n    # Launch the kernel\n    _fwd_kernel_destindex_copy_kv[grid](\n        K, Out, DestLoc,\n        BLOCK_HEAD=BLOCK_HEAD,\n        BLOCK_DMODEL=BLOCK_DMODEL\n    )\n\n# Example usage\n# Assuming K is a 3D tensor of shape [batch, head, depth]\n# Out is a tensor of the same shape as K\n# DestLoc is a 1D tensor of length equal to the number of heads\nimport torch\n\nbatch_size = 32\nnum_heads = 8\ndepth = 64\n\nK = torch.randn((batch_size, num_heads, depth), device='cuda')\nOut = torch.empty_like(K)\nDestLoc = torch.randint(0, batch_size, (num_heads,), device='cuda')\n\nBLOCK_HEAD = 1  # Must be a power of two\nBLOCK_DMODEL = 64  # Must be a power of two\n\ndestindex_copy_kv(K, Out, DestLoc, BLOCK_HEAD, BLOCK_DMODEL)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    K,\n    Dest_loc,\n    Out,\n    stride_k_bs,\n    stride_k_h,\n    stride_k_d,\n    stride_o_bs,\n    stride_o_h,\n    stride_o_d,\n    head_num,\n    head_dim,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n\n    k = tl.load(k_ptrs, mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim), other=0.0)\n    tl.store(o_ptrs, k, mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim))\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_kv(K, DestLoc, Out):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        K,\n        DestLoc,\n        Out,\n        K.stride(0),\n        K.stride(1),\n        K.stride(2),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        head_num,\n        head_dim,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel named `_rotary_kernel` is designed to perform a rotary transformation on input tensors `Q` and `K` using cosine (`Cos`) and sine (`Sin`) embeddings. This transformation is often used in advanced transformer models for enhanced positional encoding. The kernel is executed on a 2D grid where the dimensions correspond to the head and sequence blocks.\n\n            Inputs:\n            - `Q`, `K`: Input tensors for query and key matrices.\n            - `Cos`, `Sin`: Tensors containing cosine and sine embeddings for rotary transformation.\n            - Several stride values for indexing in `Q`, `K`, `Cos`, and `Sin`.\n            - `max_total_len`, `HEAD_Q`, `HEAD_K`: Dimension specifications.\n            - Compile-time constants `BLOCK_HEAD`, `BLOCK_SEQ`, `BLOCK_DMODEL` to define processing block sizes.\n\n            The function loads slices from `Q` and `K`, applies the rotary transformation using `Cos` and `Sin`, and stores the results back into `Q` and `K`. The process uses triton `load` and `store` instructions with masks to handle boundary conditions and ensure safe memory access.\n\n            The wrapper function `rotary_emb_fwd` prepares inputs and launches the kernel. It determines the grid size for execution based on the number of query heads and sequence length and chooses the number of warps based on the dimension size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define compile-time constants\nBLOCK_HEAD = 1\nBLOCK_SEQ = 128\nBLOCK_DMODEL = 64\n\n@triton.jit\ndef _rotary_kernel(\n    Q_ptr, K_ptr, Cos_ptr, Sin_ptr,\n    stride_qh, stride_qs, stride_qd,\n    stride_kh, stride_ks, stride_kd,\n    stride_cos, stride_sin,\n    max_total_len, HEAD_Q, HEAD_K,\n    **meta\n):\n    # Define block indices\n    head_idx = tl.program_id(0)\n    seq_idx = tl.program_id(1)\n\n    # Compute offsets\n    q_offset = head_idx * stride_qh + seq_idx * stride_qs\n    k_offset = head_idx * stride_kh + seq_idx * stride_ks\n    cos_offset = seq_idx * stride_cos\n    sin_offset = seq_idx * stride_sin\n\n    # Load data for Q and K\n    Q = tl.load(Q_ptr + q_offset + tl.arange(0, BLOCK_DMODEL), mask=tl.arange(0, BLOCK_DMODEL) < max_total_len, other=0.0)\n    K = tl.load(K_ptr + k_offset + tl.arange(0, BLOCK_DMODEL), mask=tl.arange(0, BLOCK_DMODEL) < max_total_len, other=0.0)\n\n    # Load Cos and Sin\n    Cos = tl.load(Cos_ptr + cos_offset + tl.arange(0, BLOCK_DMODEL), mask=tl.arange(0, BLOCK_DMODEL) < max_total_len, other=0.0)\n    Sin = tl.load(Sin_ptr + sin_offset + tl.arange(0, BLOCK_DMODEL), mask=tl.arange(0, BLOCK_DMODEL) < max_total_len, other=0.0)\n\n    # Perform rotary transformation\n    Q_rotated = Q * Cos - K * Sin\n    K_rotated = K * Cos + Q * Sin\n\n    # Store the results back\n    tl.store(Q_ptr + q_offset + tl.arange(0, BLOCK_DMODEL), Q_rotated, mask=tl.arange(0, BLOCK_DMODEL) < max_total_len)\n    tl.store(K_ptr + k_offset + tl.arange(0, BLOCK_DMODEL), K_rotated, mask=tl.arange(0, BLOCK_DMODEL) < max_total_len)\n\ndef rotary_emb_fwd(Q, K, Cos, Sin, max_total_len, HEAD_Q, HEAD_K):\n    # Get the shape of the input tensors\n    num_heads, seq_len, d_model = Q.shape\n\n    # Determine grid size\n    grid = (HEAD_Q, (seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ)\n\n    # Launch kernel\n    triton.run(\n        _rotary_kernel,\n        grid=grid,\n        num_warps=4,  # Choose number of warps based on d_model\n        args=[\n            Q, K, Cos, Sin,\n            Q.stride(0), Q.stride(1), Q.stride(2),\n            K.stride(0), K.stride(1), K.stride(2),\n            Cos.stride(0), Sin.stride(0),\n            max_total_len, HEAD_Q, HEAD_K\n        ],\n        meta={\n            'BLOCK_HEAD': BLOCK_HEAD,\n            'BLOCK_SEQ': BLOCK_SEQ,\n            'BLOCK_DMODEL': BLOCK_DMODEL\n        }\n    )\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _rotary_kernel(\n    Q,\n    K,\n    Cos,\n    Sin,\n    stride_qbs,\n    stride_qh,\n    stride_qd,\n    stride_kbs,\n    stride_kh,\n    stride_kd,\n    stride_cosbs,\n    stride_cosd,\n    stride_sinbs,\n    stride_sind,\n    max_total_len,\n    HEAD_Q,\n    HEAD_K,\n    BLOCK_HEAD: tl.constexpr,\n    BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2) * 2\n    dim_range1 = tl.arange(0, BLOCK_DMODEL // 2) * 2 + 1\n\n    off_q0 = (\n        cur_seq_range[:, None, None] * stride_qbs\n        + cur_head_range[None, :, None] * stride_qh\n        + dim_range0[None, None, :] * stride_qd\n    )\n    off_q1 = (\n        cur_seq_range[:, None, None] * stride_qbs\n        + cur_head_range[None, :, None] * stride_qh\n        + dim_range1[None, None, :] * stride_qd\n    )\n\n    off_dimcos_sin0 = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[None, None, :] * stride_cosd\n    off_dimcos_sin1 = cur_seq_range[:, None, None] * stride_cosbs + dim_range1[None, None, :] * stride_cosd\n\n    q0 = tl.load(\n        Q + off_q0,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q),\n        other=0.0,\n    )\n    q1 = tl.load(\n        Q + off_q1,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q),\n        other=0.0,\n    )\n\n    cos0 = tl.load(Cos + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin0 = tl.load(Sin + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    cos1 = tl.load(Cos + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin1 = tl.load(Sin + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    out0 = q0 * cos0 - q1 * sin0\n    out1 = q0 * sin1 + q1 * cos1\n\n    tl.store(\n        Q + off_q0, out0, mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q)\n    )\n    tl.store(\n        Q + off_q1, out1, mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_Q)\n    )\n\n    off_k0 = (\n        cur_seq_range[:, None, None] * stride_kbs\n        + cur_head_range[None, :, None] * stride_kh\n        + dim_range0[None, None, :] * stride_kd\n    )\n    off_k1 = (\n        cur_seq_range[:, None, None] * stride_kbs\n        + cur_head_range[None, :, None] * stride_kh\n        + dim_range1[None, None, :] * stride_kd\n    )\n\n    off_dimcos_sin0 = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[None, None, :] * stride_cosd\n    off_dimcos_sin1 = cur_seq_range[:, None, None] * stride_cosbs + dim_range1[None, None, :] * stride_cosd\n\n    k0 = tl.load(\n        K + off_k0,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n        other=0.0,\n    )\n    k1 = tl.load(\n        K + off_k1,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n        other=0.0,\n    )\n\n    cos0 = tl.load(Cos + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin0 = tl.load(Sin + off_dimcos_sin0, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    cos1 = tl.load(Cos + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n    sin1 = tl.load(Sin + off_dimcos_sin1, mask=cur_seq_range[:, None, None] < max_total_len, other=0.0)\n\n    out_k0 = k0 * cos0 - k1 * sin0\n    out_k1 = k0 * sin1 + k1 * cos1\n\n    tl.store(\n        K + off_k0,\n        out_k0,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n    )\n    tl.store(\n        K + off_k1,\n        out_k1,\n        mask=(cur_seq_range[:, None, None] < max_total_len) & (cur_head_range[None, :, None] < HEAD_K),\n    )\n    return\n\n@torch.no_grad()\ndef rotary_emb_fwd(q, k, cos, sin, partial_rotary_factor=1.0):\n    total_len = q.shape[0]\n    head_num_q, head_num_k = q.shape[1], k.shape[1]\n    head_dim = int(q.shape[2] * partial_rotary_factor)\n    assert q.shape[0] == cos.shape[0] and q.shape[0] == sin.shape[0], f\"q shape {q.shape} cos shape {cos.shape}\"\n    assert k.shape[0] == cos.shape[0] and k.shape[0] == sin.shape[0], f\"k shape {k.shape} cos shape {cos.shape}\"\n\n    BLOCK_SEQ = 16\n    BLOCK_HEAD = 4\n    if head_dim >= 128:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    grid = (triton.cdiv(head_num_q, BLOCK_HEAD), triton.cdiv(total_len, BLOCK_SEQ))\n    _rotary_kernel[grid](\n        q,\n        k,\n        cos,\n        sin,\n        q.stride(0),\n        q.stride(1),\n        q.stride(2),\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        cos.stride(0),\n        cos.stride(1),\n        sin.stride(0),\n        sin.stride(1),\n        total_len,\n        head_num_q,\n        head_num_k,\n        BLOCK_HEAD=BLOCK_HEAD,\n        BLOCK_SEQ=BLOCK_SEQ,\n        BLOCK_DMODEL=head_dim,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel function `_fwd_kernel_flash_decode_stage2` is designed to perform a specific computation involving a batch of sequences and their corresponding head dimensions. The main logic is implemented in a parallel kernel that runs on each batch and head. The function loads the sequence length for the current batch, calculates a block size for sequences, and initializes several accumulators and logic variables. It processes each block of sequence data by loading values and logic sums, computing scaling factors, and accumulating results. The final output is stored by normalizing the accumulated value by the sum of exponentials. The kernel is launched by the `flash_decode_stage2` function, which determines grid size and other parameters before executing the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    output_ptr,       # Pointer to the output tensor\n    input_ptr,        # Pointer to the input tensor\n    seq_len_ptr,      # Pointer to the sequence lengths\n    num_heads,        # Number of heads\n    head_dim,         # Dimension of each head\n    batch_stride,     # Stride between batches\n    head_stride,      # Stride between heads\n    BLOCK_SIZE: tl.constexpr,  # Block size for processing\n):\n    # Get the program ID\n    pid = tl.program_id(0)\n\n    # Calculate batch and head indices\n    batch_id = pid // num_heads\n    head_id = pid % num_heads\n\n    # Load sequence length for the current batch\n    seq_len = tl.load(seq_len_ptr + batch_id)\n\n    # Initialize accumulators and logic variables\n    acc_value = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    acc_sum_exp = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Process each block of sequence data\n    for i in range(0, seq_len, BLOCK_SIZE):\n        # Calculate block start and end indices\n        block_start = i\n        block_end = min(i + BLOCK_SIZE, seq_len)\n\n        # Load input values and logic sums\n        input_offset = batch_id * batch_stride + head_id * head_stride + block_start * head_dim\n        input_values = tl.load(input_ptr + input_offset)\n\n        # Compute scaling factors (example: using softmax scaling)\n        max_value = tl.max(input_values, axis=0)\n        exp_values = tl.exp(input_values - max_value)\n        sum_exp_values = tl.sum(exp_values, axis=0)\n\n        # Accumulate results\n        acc_value += exp_values * input_values\n        acc_sum_exp += sum_exp_values\n\n    # Normalize the accumulated value by the sum of exponentials\n    normalized_value = acc_value / acc_sum_exp\n\n    # Store the final output\n    output_offset = batch_id * batch_stride + head_id * head_stride\n    tl.store(output_ptr + output_offset, normalized_value)\n\ndef flash_decode_stage2(\n    output, input, seq_len, num_heads, head_dim, BLOCK_SIZE=128\n):\n    # Get the number of batches from the shape of the input tensor\n    num_batches = input.shape[0]\n\n    # Calculate grid size\n    grid_size = num_batches * num_heads\n\n    # Launch the Triton kernel\n    _fwd_kernel_flash_decode_stage2[grid_size](\n        output, input, seq_len, num_heads, head_dim,\n        input.stride(0), input.stride(1), BLOCK_SIZE=BLOCK_SIZE\n    )\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_flash_decode_stage2(\n    B_Seqlen,\n    Mid_O,  # [batch, head, seq_block_num, head_dim]\n    Mid_O_LogExpSum,  # [batch, head, seq_block_num]\n    Out,  # [batch, head, head_dim]\n    stride_mid_ob,\n    stride_mid_oh,\n    stride_mid_os,\n    stride_mid_od,\n    stride_mid_o_eb,\n    stride_mid_o_eh,\n    stride_mid_o_es,\n    stride_obs,\n    stride_oh,\n    stride_od,\n    head_dim,\n    BLOCK_SEQ: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ\n\n    sum_exp = 0.0\n    max_logic = -float(\"inf\")\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os, mask=offs_d < head_dim, other=0.0)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n\n    tl.store(Out + cur_batch * stride_obs + cur_head * stride_oh + offs_d, acc / sum_exp, mask=offs_d < head_dim)\n    return\n\n@torch.no_grad()\ndef flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, Out, block_seq):\n    Lk = mid_out.shape[-1]\n    head_dim = Lk\n    batch, head_num = mid_out.shape[0], mid_out.shape[1]\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    grid = (batch, head_num)\n\n    _fwd_kernel_flash_decode_stage2[grid](\n        B_Seqlen,\n        mid_out,\n        mid_out_logexpsum,\n        Out,\n        mid_out.stride(0),\n        mid_out.stride(1),\n        mid_out.stride(2),\n        mid_out.stride(3),\n        mid_out_logexpsum.stride(0),\n        mid_out_logexpsum.stride(1),\n        mid_out_logexpsum.stride(2),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        head_dim,\n        BLOCK_SEQ=block_seq,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        num_warps=4,\n        num_stages=2,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code defines a kernel for performing matrix multiplication using GPU with optional leaky ReLU activation. The `matmul_kernel` function performs the matrix multiplication of two input matrices, `a` and `b`, and optionally applies the leaky ReLU activation to the result. It utilizes Triton's autotuning to optimize the computation based on different configurations, specifically for CUDA and HIP backends.\n\n            The main function `matmul` is a wrapper that prepares input matrices for the kernel, checks for dimensional compatibility, and invokes the kernel using specified configurations. It returns the result matrix `c`.\n\n            Key parameters:\n            - `a_ptr`, `b_ptr`, `c_ptr`: Pointers to input matrices A, B, and output matrix C.\n            - `M`, `N`, `K`: Dimensions of the matrices involved.\n            - `stride_am`, `stride_ak`, `stride_bk`, `stride_bn`, `stride_cm`, `stride_cn`: Stride values for indexing matrices.\n            - `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, `BLOCK_SIZE_K`: Block sizes for the computation.\n            - `GROUP_SIZE_M`: Number of groups in the grid.\n            - `ACTIVATION`: Type of activation function applied.\n\n            The `leaky_relu` function is used when specified, modifying negative outputs to 0.01 times their value.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel for matrix multiplication with optional leaky ReLU activation\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_warps=4),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K,\n                  stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n                  BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n                  GROUP_SIZE_M: tl.constexpr, ACTIVATION: tl.constexpr):\n    # Block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate block row and column\n    block_row = pid_m * BLOCK_SIZE_M\n    block_col = pid_n * BLOCK_SIZE_N\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a = tl.load(a_ptr + (block_row + tl.arange(0, BLOCK_SIZE_M)[:, None]) * stride_am + (k + tl.arange(0, BLOCK_SIZE_K)[None, :]) * stride_ak)\n        b = tl.load(b_ptr + (k + tl.arange(0, BLOCK_SIZE_K)[:, None]) * stride_bk + (block_col + tl.arange(0, BLOCK_SIZE_N)[None, :]) * stride_bn)\n\n        # Matrix multiplication\n        acc += tl.dot(a, b)\n\n    # Optional leaky ReLU activation\n    if ACTIVATION == 'leaky_relu':\n        acc = tl.where(acc > 0, acc, 0.01 * acc)\n\n    # Store result in C\n    c = c_ptr + (block_row + tl.arange(0, BLOCK_SIZE_M)[:, None]) * stride_cm + (block_col + tl.arange(0, BLOCK_SIZE_N)[None, :]) * stride_cn\n    tl.store(c, acc)\n\n\ndef matmul(a, b, activation=None):\n    # Extract dimensions\n    M, K = a.shape\n    K_, N = b.shape\n    assert K == K_, \"Incompatible dimensions for matrix multiplication.\"\n\n    # Strides for input matrices\n    stride_am, stride_ak = a.stride()\n    stride_bk, stride_bn = b.stride()\n\n    # Allocate output matrix\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    stride_cm, stride_cn = c.stride()\n\n    # Define grid size\n    grid = (triton.cdiv(M, 128), triton.cdiv(N, 128))\n\n    # Launch kernel\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32,\n        GROUP_SIZE_M=8,\n        ACTIVATION=activation\n    )\n\n    return c\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Autotune configuration functions\ndef is_cuda():\n    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n\ndef get_cuda_autotune_config():\n    return [\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        # Other configurations omitted for brevity\n    ]\n\ndef get_hip_autotune_config():\n    return [\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2}, num_warps=4, num_stages=2),\n        # Other configurations omitted for brevity\n    ]\n\ndef get_autotune_config():\n    if is_cuda():\n        return get_cuda_autotune_config()\n    else:\n        return get_hip_autotune_config()\n\n# Triton kernel for matrix multiplication with autotuning\n@triton.autotune(\n    configs=get_autotune_config(),\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,  \n        stride_bk, stride_bn,  \n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  \n        GROUP_SIZE_M: tl.constexpr,  \n        ACTIVATION: tl.constexpr  \n):\n    \"\"\"Kernel for computing the matrix multiplication C = A x B.\n    A has shape (M, K), B has shape (K, N), and C has shape (M, N).\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n@triton.jit\ndef leaky_relu(x):\n    \"\"\"Applies the Leaky ReLU activation function.\"\"\"\n    return tl.where(x >= 0, x, 0.01 * x)\n\ndef matmul(a, b, activation=\"\"):\n    \"\"\"Wrapper function for matrix multiplication using Triton.\n    a: Tensor with shape (M, K)\n    b: Tensor with shape (K, N)\n    activation: Activation function to be applied (optional)\n    Returns the result of the matrix multiplication.\n    \"\"\"\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n    matmul_kernel[grid](\n        a, b, c,  \n        M, N, K,  \n        a.stride(0), a.stride(1),  \n        b.stride(0), b.stride(1),  \n        c.stride(0), c.stride(1),  \n        ACTIVATION=activation  \n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code implements two main kernels, `prefill_cache_kernel` and `decoding_cache_kernel`, designed for efficiently managing cosine and sine caches used in sequence processing.\n\n            - `prefill_cache_kernel`: This kernel is used when `is_prompts` is `True`. It copies segments of cosine and sine cache data to output tensors based on provided cumulative sequence lengths. Inputs include `cos_cache`, `sin_cache`, `cumsum_lengths`, `cos_output`, `sin_output`, `cache_stride`, `hidden_stride`, `total_length`, with constants `HIDDEN_DIM`, `N_ELEMENTS`, and `BLOCK_SIZE`. The logic computes the original sequence index for each element, extracts relevant cache parts, and stores them in the output tensors.\n\n            - `decoding_cache_kernel`: This kernel is used when `is_prompts` is `False`. It handles the copying of cache segments for decoding purposes. It uses input lengths to compute original sequence indices for each block and retrieves corresponding cache parts. Inputs are `cos_cache`, `sin_cache`, `lengths`, `cos_output`, `sin_output`, `cache_stride`, `hidden_stride`, with constants `HIDDEN_DIM`, `NUM_SEQS`, and `BLOCK_SIZE`.\n\n            The function `get_xine_cache` coordinates these kernels. It calculates necessary parameters such as `cache_stride`, `hidden_stride`, and sets up output tensors. Depending on `is_prompts`, it either executes `prefill_cache_kernel` for prompt filling or `decoding_cache_kernel` for sequence decoding.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants\nHIDDEN_DIM = 1024  # Example hidden dimension\nN_ELEMENTS = 64    # Example number of elements per block\nBLOCK_SIZE = 128   # Example block size\n\n@triton.jit\ndef prefill_cache_kernel(\n    cos_cache_ptr, sin_cache_ptr, cumsum_lengths_ptr,\n    cos_output_ptr, sin_output_ptr,\n    cache_stride, hidden_stride, total_length,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    # Compute the start index for the current block\n    start_idx = pid * BLOCK_SIZE\n    \n    # Load the cumulative sum lengths for determining sequence start\n    cumsum_length = tl.load(cumsum_lengths_ptr + pid)\n    \n    # Calculate the original sequence index\n    original_idx = start_idx - cumsum_length\n    \n    # Load cache values\n    cos_cache = tl.load(cos_cache_ptr + original_idx * cache_stride)\n    sin_cache = tl.load(sin_cache_ptr + original_idx * cache_stride)\n    \n    # Store to output\n    tl.store(cos_output_ptr + start_idx * hidden_stride, cos_cache)\n    tl.store(sin_output_ptr + start_idx * hidden_stride, sin_cache)\n\n@triton.jit\ndef decoding_cache_kernel(\n    cos_cache_ptr, sin_cache_ptr, lengths_ptr,\n    cos_output_ptr, sin_output_ptr,\n    cache_stride, hidden_stride,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    \n    # Compute the start index for the current block\n    start_idx = pid * BLOCK_SIZE\n    \n    # Load the lengths to determine the sequence index\n    length = tl.load(lengths_ptr + pid)\n    \n    # Calculate the original sequence index\n    original_idx = start_idx - length\n    \n    # Load cache values\n    cos_cache = tl.load(cos_cache_ptr + original_idx * cache_stride)\n    sin_cache = tl.load(sin_cache_ptr + original_idx * cache_stride)\n    \n    # Store to output\n    tl.store(cos_output_ptr + start_idx * hidden_stride, cos_cache)\n    tl.store(sin_output_ptr + start_idx * hidden_stride, sin_cache)\n\ndef get_xine_cache(\n    cos_cache, sin_cache, cumsum_lengths, lengths,\n    total_length, is_prompts\n):\n    # Determine strides\n    cache_stride = HIDDEN_DIM\n    hidden_stride = N_ELEMENTS\n    \n    # Prepare output tensors\n    cos_output = torch.empty((total_length, HIDDEN_DIM), dtype=torch.float32, device='cuda')\n    sin_output = torch.empty((total_length, HIDDEN_DIM), dtype=torch.float32, device='cuda')\n    \n    # Launch the appropriate kernel\n    if is_prompts:\n        prefill_cache_kernel[(total_length // BLOCK_SIZE,)](\n            cos_cache, sin_cache, cumsum_lengths,\n            cos_output, sin_output,\n            cache_stride, hidden_stride, total_length,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n    else:\n        decoding_cache_kernel[(total_length // BLOCK_SIZE,)](\n            cos_cache, sin_cache, lengths,\n            cos_output, sin_output,\n            cache_stride, hidden_stride,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n    \n    return cos_output, sin_output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef prefill_cache_kernel(\n    cos_cache,\n    sin_cache,\n    cumsum_lengths,\n    cos_output,\n    sin_output,\n    cache_stride,\n    hidden_stride,\n    total_length,\n    HIDDEN_DIM: tl.constexpr,\n    N_ELEMENTS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    idx0 = tl.program_id(axis=0)\n    idx1 = tl.program_id(axis=1)\n    idx = idx0 * BLOCK_SIZE + idx1\n\n    # original seq_idx and pos\n    cumsum_lens = tl.load(cumsum_lengths + tl.arange(0, N_ELEMENTS))\n    ori_seq_idx = idx - tl.max(tl.where(cumsum_lens <= idx, cumsum_lens, 0))\n    cos_cache_part = tl.load(\n        cos_cache + ori_seq_idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride, mask=idx < total_length\n    )\n    sin_cache_part = tl.load(\n        sin_cache + ori_seq_idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride, mask=idx < total_length\n    )\n    tl.store(\n        cos_output + idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride,\n        cos_cache_part,\n        mask=idx < total_length,\n    )\n    tl.store(\n        sin_output + idx * cache_stride + tl.arange(0, HIDDEN_DIM) * hidden_stride,\n        sin_cache_part,\n        mask=idx < total_length,\n    )\n\n\n@triton.jit\ndef decoding_cache_kernel(\n    cos_cache,\n    sin_cache,\n    lengths,\n    cos_output,\n    sin_output,\n    cache_stride,\n    hidden_stride,\n    HIDDEN_DIM: tl.constexpr,\n    NUM_SEQS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    idx = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    ori_seq_idx = tl.load(lengths + idx, mask=(idx < NUM_SEQS), other=None)  # [BLOCK_SIZE,]\n    cos_cache_part = tl.load(\n        cos_cache + ori_seq_idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n    sin_cache_part = tl.load(\n        sin_cache + ori_seq_idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n    tl.store(\n        cos_output + (idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride),\n        cos_cache_part,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n    tl.store(\n        sin_output + (idx[:, None] * cache_stride + tl.arange(0, HIDDEN_DIM)[None, :] * hidden_stride),\n        sin_cache_part,\n        mask=idx[:, None] < NUM_SEQS,\n    )\n\n\ndef get_xine_cache(lengths: torch.Tensor, cos_cache: torch.Tensor, sin_cache: torch.Tensor, is_prompts: bool = False):\n    assert cos_cache.shape[1] == sin_cache.shape[1]\n    _, hidden_dim = cos_cache.shape\n    num_seqs = lengths.numel()\n\n    if hidden_dim >= 256:\n        num_warps = 16\n    elif hidden_dim >= 128:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    cache_stride = cos_cache.stride(0)\n    hidden_stride = cos_cache.stride(1)\n\n    if is_prompts:\n        BLOCK_SIZE = 16\n        total_length = lengths.sum().item()\n        cumsum_lens = torch.cumsum(lengths, dim=0)\n        cos_output = torch.empty((total_length, hidden_dim), dtype=cos_cache.dtype, device=cos_cache.device)\n        sin_output = torch.empty((total_length, hidden_dim), dtype=sin_cache.dtype, device=sin_cache.device)\n        grid = (triton.cdiv(total_length, BLOCK_SIZE), BLOCK_SIZE)\n        prefill_cache_kernel[grid](\n            cos_cache,\n            sin_cache,\n            cumsum_lens,\n            cos_output,\n            sin_output,\n            cache_stride,\n            hidden_stride,\n            total_length,\n            HIDDEN_DIM=hidden_dim,\n            N_ELEMENTS=triton.next_power_of_2(num_seqs),\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n    else:\n        BLOCK_SIZE = 4\n        nlengths = torch.as_tensor(lengths) - 1\n        cos_output = torch.empty((num_seqs, hidden_dim), dtype=cos_cache.dtype, device=cos_cache.device)\n        sin_output = torch.empty((num_seqs, hidden_dim), dtype=sin_cache.dtype, device=sin_cache.device)\n        grid = (triton.cdiv(num_seqs, BLOCK_SIZE),)\n        decoding_cache_kernel[grid](\n            cos_cache,\n            sin_cache,\n            nlengths,\n            cos_output,\n            sin_output,\n            cache_stride,\n            hidden_stride,\n            HIDDEN_DIM=hidden_dim,\n            NUM_SEQS=num_seqs,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n\n    return cos_output, sin_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This code implements parallel attention mechanisms using Triton for GPU acceleration. The main components are the `parallel_retention_fwd_kernel` and `parallel_retention_bwd_kernel` functions, which handle the forward and backward passes, respectively. The kernels utilize block pointers for efficient memory access and leverage parallelism across sequences and heads. The `forward` function computes scaled dot-product attention, where `q`, `k`, and `v` are the query, key, and value tensors. It saves these tensors for backward pass. The `backward` function calculates the gradients for `q`, `k`, and `v` using stored tensors from the forward pass. The implementation handles decay factors and strides for multi-head attention computations.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef parallel_retention_fwd_kernel(\n    Q_ptr, K_ptr, V_ptr, Out_ptr,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_oz, stride_oh, stride_om, stride_ok,\n    n_heads, seq_len, d_head, decay_factor,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Block pointers for Q, K, V, and output\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    q_ptrs = Q_ptr + offs_m[:, None] * stride_qm + offs_n[None, :] * stride_qk\n    k_ptrs = K_ptr + offs_m[:, None] * stride_kn + offs_n[None, :] * stride_kk\n    v_ptrs = V_ptr + offs_m[:, None] * stride_vn + offs_n[None, :] * stride_vk\n    out_ptrs = Out_ptr + offs_m[:, None] * stride_om + offs_n[None, :] * stride_ok\n\n    # Load Q, K, V\n    Q = tl.load(q_ptrs, mask=offs_m[:, None] < seq_len)\n    K = tl.load(k_ptrs, mask=offs_n[None, :] < seq_len)\n    V = tl.load(v_ptrs, mask=offs_n[None, :] < seq_len)\n\n    # Compute scaled dot-product attention\n    scores = tl.dot(Q, K.T) * decay_factor\n    scores = tl.softmax(scores, axis=-1)\n    output = tl.dot(scores, V)\n\n    # Store output\n    tl.store(out_ptrs, output, mask=offs_m[:, None] < seq_len)\n\n@triton.jit\ndef parallel_retention_bwd_kernel(\n    dOut_ptr, Q_ptr, K_ptr, V_ptr, dQ_ptr, dK_ptr, dV_ptr,\n    stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_dqz, stride_dqh, stride_dqm, stride_dqk,\n    stride_dkz, stride_dkh, stride_dkn, stride_dkk,\n    stride_dvz, stride_dvh, stride_dvn, stride_dvk,\n    n_heads, seq_len, d_head, decay_factor,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n\n    # Block pointers for dOut, Q, K, V, and gradients\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    dOut_ptrs = dOut_ptr + offs_m[:, None] * stride_dom + offs_n[None, :] * stride_dok\n    q_ptrs = Q_ptr + offs_m[:, None] * stride_qm + offs_n[None, :] * stride_qk\n    k_ptrs = K_ptr + offs_m[:, None] * stride_kn + offs_n[None, :] * stride_kk\n    v_ptrs = V_ptr + offs_m[:, None] * stride_vn + offs_n[None, :] * stride_vk\n    dq_ptrs = dQ_ptr + offs_m[:, None] * stride_dqm + offs_n[None, :] * stride_dqk\n    dk_ptrs = dK_ptr + offs_m[:, None] * stride_dkn + offs_n[None, :] * stride_dkk\n    dv_ptrs = dV_ptr + offs_m[:, None] * stride_dvn + offs_n[None, :] * stride_dvk\n\n    # Load dOut, Q, K, V\n    dOut = tl.load(dOut_ptrs, mask=offs_m[:, None] < seq_len)\n    Q = tl.load(q_ptrs, mask=offs_m[:, None] < seq_len)\n    K = tl.load(k_ptrs, mask=offs_n[None, :] < seq_len)\n    V = tl.load(v_ptrs, mask=offs_n[None, :] < seq_len)\n\n    # Compute gradients\n    dV = tl.dot(dOut.T, K)\n    dK = tl.dot(Q.T, dOut)\n    dQ = tl.dot(dOut, V.T)\n\n    # Store gradients\n    tl.store(dq_ptrs, dQ, mask=offs_m[:, None] < seq_len)\n    tl.store(dk_ptrs, dK, mask=offs_n[None, :] < seq_len)\n    tl.store(dv_ptrs, dV, mask=offs_n[None, :] < seq_len)\n\ndef forward(Q, K, V, decay_factor):\n    # Allocate output tensor\n    Out = torch.empty_like(Q)\n\n    # Launch kernel\n    grid = (triton.cdiv(Q.shape[0], BLOCK_SIZE_M), triton.cdiv(Q.shape[1], BLOCK_SIZE_N))\n    parallel_retention_fwd_kernel[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n        K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n        V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n        Out.stride(0), Out.stride(1), Out.stride(2), Out.stride(3),\n        Q.shape[1], Q.shape[2], Q.shape[3], decay_factor,\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128\n    )\n\n    # Save tensors for backward pass\n    ctx.save_for_backward(Q, K, V, Out)\n    return Out\n\ndef backward(ctx, dOut):\n    Q, K, V, Out = ctx.saved_tensors\n\n    # Allocate gradient tensors\n    dQ = torch.empty_like(Q)\n    dK = torch.empty_like(K)\n    dV = torch.empty_like(V)\n\n    # Launch kernel\n    grid = (triton.cdiv(dOut.shape[0], BLOCK_SIZE_M), triton.cdiv(dOut.shape[1], BLOCK_SIZE_N))\n    parallel_retention_bwd_kernel[grid](\n        dOut, Q, K, V, dQ, dK, dV,\n        dOut.stride(0), dOut.stride(1), dOut.stride(2), dOut.stride(3),\n        Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n        K.stride(0), K.stride(1), K.stride(2), K.stride(3),\n        V.stride(0), V.stride(1), V.stride(2), V.stride(3),\n        dQ.stride(0), dQ.stride(1), dQ.stride(2), dQ.stride(3),\n        dK.stride(0), dK.stride(1), dK.stride(2), dK.stride(3),\n        dV.stride(0), dV.stride(1), dV.stride(2), dV.stride(3),\n        Q.shape[1], Q.shape[2], Q.shape[3], ctx.decay_factor,\n        BLOCK_SIZE_M=128, BLOCK_SIZE_N=128\n    )\n\n    return dQ, dK, dV\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom torch.amp import custom_bwd, custom_fwd\nimport functools\n\ndef contiguous(fn):\n    @functools.wraps(fn)\n    def wrapper(ctx, *args, **kwargs):\n        return fn(ctx,\n                  *(i if not isinstance(i, torch.Tensor) else i.contiguous() for i in args),\n                  **{k: (v if not isinstance(v, torch.Tensor) else v.contiguous()) for k, v in kwargs.items()})\n    return wrapper\n\n@triton.jit\ndef parallel_retention_fwd_kernel(\n    # B: batch_size, H: n_heads, T: seq_len, D: d_head\n    q,  # query [B, H, L, D_head_K]\n    k,  # key [B, H, L, D_head_V]\n    v,  # value [B, H, L, D_head_V]\n    o,  # output [B, H, L, D_head_V]\n    s_qk_h,  # stride size: L * D_head_K\n    s_qk_t,  # stride size: D_head_K\n    s_qk_d,  # stride size: 1\n    s_vo_h,  # stride size: L * D_head_V\n    s_vo_t,  # stride size: D_head_V\n    s_vo_d,  # stride size: 1\n    B,  # batch size\n    H,  # n_heads\n    T,  # seq_len\n    scale,  # D_head_K ** -0.5\n    BTL: tl.constexpr,  # BLOCK SIZE along the sequence dimension for Q\n    BTS: tl.constexpr,  # BLOCK SIZE along the sequence dimension for K/V\n    BK: tl.constexpr,  # BLOCK SIZE along the K dimension\n    BV: tl.constexpr,  # BLOCK SIZE along the V dimension\n    DK: tl.constexpr,  # D_head_K\n    DV: tl.constexpr,  # D_head_V\n):\n    # i_c: chunk index. used for sequence parallelism\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n    i_h = i_bh % H\n    # decay rate given the head index\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    # cumulative decay from the end of the chunk\n    o_k = tl.arange(0, BTS)\n    d_h = tl.math.exp2((BTS - o_k) * b_b)\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK),\n                            (s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T),\n                            (s_qk_d, s_qk_t), (i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV),\n                            (s_vo_t, s_vo_d), (0, i_v * BV), (BTS, BV), (1, 0))\n\n    # [BQ, BD] block Q, in the shared memory throughout the whole kernel\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n\n    # Q block and K block have no overlap\n    # no need for mask, thereby saving flops\n    for _ in range(0, i_c * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_s = tl.dot(b_q, (b_k), allow_tf32=False) * d_h[None, :]\n        # [BQ, BD]\n        b_o = b_o * tl.math.exp2(b_b * BTS)\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n\n    # # rescale interchunk output\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    d_q = tl.math.exp2(tl.arange(0, BTL) * b_b)\n    b_o *= d_q[:, None]\n    # # sync threads, easy for compiler to optimize\n    # tl.debug_barrier()\n\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T),\n                            (s_qk_d, s_qk_t), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV),\n                            (s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2(\n            (o_q[:, None] - o_k[None, :]) * b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        # [BTL, BV]\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV),\n                            (s_vo_t, s_vo_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef _parallel_retention_bwd_dq(\n    i_bh, i_c, i_k, i_v, i_h,\n    k, v, do, dq, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale,\n    BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    DK: tl.constexpr,  DV: tl.constexpr,\n):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n                             (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK),\n                            (s_qk_t, s_qk_d), (0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T),\n                            (s_vo_d, s_vo_t), (i_v * BV, 0), (BV, BTS), (0, 1))\n    # decay rate given the head index\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    # overall decay rate for an entire block\n    d_b = tl.math.exp2(b_b * BTS)\n    # cumulative decay from the end of the chunk\n    d_h = tl.math.exp2((BTS - tl.arange(0, BTS)) * b_b)\n    for _ in range(0, i_c * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        # [BQ, BD]\n        b_dq *= d_b\n        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= tl.math.exp2(tl.arange(0, BTL) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK),\n                            (s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T),\n                            (s_vo_d, s_vo_t), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2(\n            (o_q[:, None] - o_k[None, :]) * b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        # [BTL, BK]\n        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK),\n                             (s_qk_t, s_qk_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n\n@triton.jit\ndef _parallel_retention_bwd_dkv(\n    i_bh, i_c, i_k, i_v, i_h,\n    q, k, v, do, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale,\n    BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    DK: tl.constexpr,  DV: tl.constexpr,\n):\n    # no overlap. no need for mask.\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    # overall decay rate for an entire block\n    d_b = tl.math.exp2(b_b * BTS)\n    # compute dk dv\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d),\n                            (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n                            (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(\n        p_v, boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(\n        [BTL, BV], dtype=tl.float32)\n    d_h = tl.math.exp2((BTL - tl.arange(0, BTL)) * b_b)\n    b_kd = (b_k * d_h[:, None]).to(b_k.dtype)\n    d_q = tl.math.exp2(tl.arange(0, BTS) * b_b)\n    for i in range((tl.cdiv(T, BTS) * BTS)-BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BK, BTS]\n        b_do = tl.load(p_do, boundary_check=(0, 1))  # [BV, BTS]\n        b_do = (b_do * d_q[None, :]).to(b_do.dtype)\n\n        b_dv *= d_b\n        b_s = tl.dot(b_kd.to(b_q.dtype), b_q, allow_tf32=False)  # [BTL, BTS]\n        b_dv += tl.dot(b_s.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n\n        b_dk *= d_b\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        b_dk += tl.dot(b_ds.to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c*BTL, (i_c+1)*BTL, BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BD, BQ]\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        # [BK, BQ]\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2(\n            (-o_k[:, None] + o_q[None, :]) * b_b.to(tl.float32)), 0) * scale\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * d_s\n        # [BK, BD]\n        b_dk += tl.dot(b_ds.to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h,\n                             (T, DK), (s_qk_t, s_qk_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h,\n                             (T, DV), (s_vo_t, s_vo_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n\n@triton.jit\ndef parallel_retention_bwd_kernel(\n    q, k, v, do, dq, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale,\n    BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    DK: tl.constexpr,  DV: tl.constexpr,\n):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n    i_h = i_bh % H\n    _parallel_retention_bwd_dq(\n        i_bh, i_c, i_k, i_v, i_h,\n        k, v, do, dq, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d, B, H, T, scale,  BTL=BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV\n    )\n    tl.debug_barrier()\n    _parallel_retention_bwd_dkv(\n        i_bh, i_c, i_k, i_v, i_h,\n        q, k, v, do, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d, B, H, T, scale, BTL, BTS, BK, BV, DK, DV\n    )\n\n\nclass ParallelRetentionFunction(torch.autograd.Function):\n    @staticmethod\n    @contiguous\n    @custom_fwd(device_type='cuda')\n    def forward(ctx, q, k, v):\n        BTL, BTS = 128, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n        num_stages = 3 if d_head_qk <= 64 else 2\n        num_warps = 4\n        NK = triton.cdiv(d_head_qk, BK)\n        NV = triton.cdiv(d_head_v, BV)\n\n        grid = (NK * NV, triton.cdiv(seq_len, BTL), batch_size * n_heads)\n        scale = d_head_qk ** -0.5\n        o = torch.empty(NK, batch_size, n_heads, seq_len,\n                        d_head_v, dtype=q.dtype, device=q.device)\n        parallel_retention_fwd_kernel[grid](\n            q, k, v, o,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV, DK=d_head_qk, DV=d_head_v,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        ctx.save_for_backward(q, k, v)\n        return o.sum(0).to(q.dtype)\n\n    @staticmethod\n    @contiguous\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, do):\n        q, k, v = ctx.saved_tensors\n        BTL, BTS = 64, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        batch_size, n_heads, seq_len, d_head_qk = q.shape\n        d_head_v = v.shape[-1]\n        num_stages = 3 if d_head_qk <= 64 else 2\n        num_warps = 4\n        NK = triton.cdiv(d_head_qk, BK)\n        NV = triton.cdiv(d_head_v, BV)\n        grid = (NK * NV, triton.cdiv(seq_len, BTL), batch_size * n_heads)\n        scale = d_head_qk ** -0.5\n\n        dq = torch.empty(NV, batch_size, n_heads, seq_len,\n                         d_head_qk, dtype=q.dtype, device=q.device)\n        dk = torch.empty(NV, batch_size, n_heads, seq_len,\n                         d_head_qk, dtype=q.dtype, device=q.device)\n        dv = torch.empty(NK, batch_size, n_heads, seq_len,\n                         d_head_v, dtype=q.dtype, device=q.device)\n\n        parallel_retention_bwd_kernel[grid](\n            q, k, v, do, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            batch_size, n_heads, seq_len, scale,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV, DK=d_head_qk, DV=d_head_v,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        return dq.sum(0).to(q.dtype), dk.sum(0).to(k.dtype), dv.sum(0).to(v.dtype)\n\n\nparallel_retention = ParallelRetentionFunction.apply\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a fast RMS Layernorm using Triton, which is a GPU programming language. It includes forward and backward kernels for computing layer normalization with improved speed on GPUs. The main function, `Fast_RMS_Layernorm`, is a PyTorch autograd function that performs the forward and backward pass for the RMS Layernorm. The forward pass calculates the normalized output and stores necessary variables for the backward pass. The `fast_rms_layernorm` function applies this layernorm operation to input data using a defined layernorm layer. The code also includes a simple layernorm module for testing purposes.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_layernorm_forward_kernel(\n    X, Y, mean, inv_std, N, eps, BLOCK_SIZE: tl.constexpr\n):\n    # Compute the block index\n    block_idx = tl.program_id(axis=0)\n    # Define the range of elements this block will process\n    start = block_idx * BLOCK_SIZE\n    end = tl.min(start + BLOCK_SIZE, N)\n    # Load the input data for this block\n    x = tl.load(X + start, mask=start < N)\n    # Compute mean\n    mean_val = tl.sum(x, axis=0) / N\n    # Compute variance and inverse standard deviation\n    var = tl.sum((x - mean_val) ** 2, axis=0) / N\n    inv_std_val = 1 / tl.sqrt(var + eps)\n    # Normalize\n    y = (x - mean_val) * inv_std_val\n    # Store results\n    tl.store(Y + start, y, mask=start < N)\n    tl.store(mean + block_idx, mean_val)\n    tl.store(inv_std + block_idx, inv_std_val)\n\n@triton.jit\ndef rms_layernorm_backward_kernel(\n    dY, X, dX, mean, inv_std, N, BLOCK_SIZE: tl.constexpr\n):\n    block_idx = tl.program_id(axis=0)\n    start = block_idx * BLOCK_SIZE\n    end = tl.min(start + BLOCK_SIZE, N)\n    # Load necessary data\n    dy = tl.load(dY + start, mask=start < N)\n    x = tl.load(X + start, mask=start < N)\n    mean_val = tl.load(mean + block_idx)\n    inv_std_val = tl.load(inv_std + block_idx)\n    # Compute gradients\n    dx = inv_std_val * (dy - tl.sum(dy, axis=0) / N - (x - mean_val) * tl.sum(dy * (x - mean_val), axis=0) / (N * inv_std_val ** 2))\n    # Store gradient\n    tl.store(dX + start, dx, mask=start < N)\n\nclass FastRMSLayernorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, eps=1e-5):\n        N = input.shape[-1]\n        output = torch.empty_like(input)\n        mean = torch.empty(input.shape[0], device=input.device)\n        inv_std = torch.empty(input.shape[0], device=input.device)\n\n        # Launch the forward kernel\n        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n        rms_layernorm_forward_kernel[grid](\n            input, output, mean, inv_std, N, eps, BLOCK_SIZE=1024\n        )\n\n        # Save for backward\n        ctx.save_for_backward(input, mean, inv_std)\n        ctx.eps = eps\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, mean, inv_std = ctx.saved_tensors\n        N = input.shape[-1]\n        grad_input = torch.empty_like(input)\n\n        # Launch the backward kernel\n        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n        rms_layernorm_backward_kernel[grid](\n            grad_output, input, grad_input, mean, inv_std, N, BLOCK_SIZE=1024\n        )\n\n        return grad_input, None\n\ndef fast_rms_layernorm(input, eps=1e-5):\n    return FastRMSLayernorm.apply(input, eps)\n\n# Simple layernorm module for testing\nclass SimpleLayerNorm(torch.nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5):\n        super(SimpleLayerNorm, self).__init__()\n        self.eps = eps\n        self.weight = torch.nn.Parameter(torch.ones(normalized_shape))\n        self.bias = torch.nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, input):\n        return self.weight * fast_rms_layernorm(input, self.eps) + self.bias\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\nimport torch.nn as nn\n\n\nnext_power_of_2 = triton.next_power_of_2\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int,):\n    BLOCK_SIZE : int = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _rms_layernorm_forward(\n    Y, Y_row_stride,\n    X, X_row_stride,\n    W, W_row_stride,\n    r, r_row_stride,\n    n_cols, eps,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\n        Fast RMS Layernorm kernel\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    normed = normed.to(W_row.dtype)\n    output = normed * W_row\n    tl.store(Y + col_offsets, output, mask=mask)\n\n@triton.heuristics({\"GEMMA\": lambda args: args[\"GEMMA\"],})\n@triton.jit\ndef _rms_layernorm_backward(\n    dY, dY_row_stride,\n    X, X_row_stride,\n    W, W_row_stride,\n    r, r_row_stride,\n    dW, dW_row_stride,\n    n_cols, eps,\n    GEMMA: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    \"\"\"\n        Fast RMS Layernorm kernel for the backward pass\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dY += row_idx * dY_row_stride\n    X  += row_idx *  X_row_stride\n    r  += row_idx *  r_row_stride\n\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0).to(tl.float32)\n    X_row  = tl.load(X  + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row  = tl.load(W  + col_offsets, mask=mask, other=0).to(tl.float32)\n\n    inv_var = tl.load(r).to(tl.float32)\n    normed = X_row * inv_var\n\n    if GEMMA: dY_W = dY_row * (W_row + 1.0)\n    else:     dY_W = dY_row * W_row\n\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)\n    tl.store(dY + col_offsets, output, mask=mask)\n\n@triton.jit\ndef _gemma_rms_layernorm_forward(\n    Y, Y_row_stride,\n    X, X_row_stride,\n    W, W_row_stride,\n    r, r_row_stride,\n    n_cols, eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n\n    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\n\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    output = normed * (W_row + 1.0)\n\n    tl.store(Y + col_offsets, output, mask=mask)\n\nclass Fast_RMS_Layernorm(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, X, W, eps, gemma=False):\n        shape = X.shape\n        dim = shape[-1]\n        X = X.view(-1, dim)\n        n_rows, n_cols = X.shape\n        BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n        Y = torch.empty((n_rows, n_cols), dtype=X.dtype, device=\"cuda:0\")\n        r = torch.empty(n_rows, dtype=torch.float32, device=\"cuda:0\")\n\n        fx = _gemma_rms_layernorm_forward if gemma else _rms_layernorm_forward\n        fx[(n_rows,)](\n            Y, Y.stride(0),\n            X, X.stride(0),\n            W, W.stride(0),\n            r, r.stride(0),\n            n_cols, eps,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n        ctx.eps = eps\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps = num_warps\n        ctx.GEMMA = gemma\n        ctx.save_for_backward(X, W, r)\n        return Y.view(*shape)\n\n    @staticmethod\n    def backward(ctx, dY):\n        shape = dY.shape\n        dim = shape[-1]\n        dY = dY.view(-1, dim)\n        X, W, r = ctx.saved_tensors\n        n_rows, n_cols = dY.shape\n        dW = X\n\n        _rms_layernorm_backward[(n_rows,)](\n            dY, dY.stride(0),\n            X,  X.stride(0),\n            W,  W.stride(0),\n            r,  r.stride(0),\n            dW, dW.stride(0),\n            n_cols, ctx.eps,\n            GEMMA=ctx.GEMMA,\n            BLOCK_SIZE=ctx.BLOCK_SIZE,\n            num_warps=ctx.num_warps,\n        )\n        dX = dY.view(*shape)\n        return dX, None, None, None\n\ndef fast_rms_layernorm(layernorm, X, gemma=False):\n    W = layernorm.weight\n    eps = layernorm.variance_epsilon if \\\n        hasattr(layernorm, \"variance_epsilon\") \\\n        else layernorm.eps\n    out = Fast_RMS_Layernorm.apply(X, W, eps, gemma)\n    return out\n\n\n# Define a simple layernorm module for testing\nclass SimpleLayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5):\n        super(SimpleLayerNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape).cuda())\n        self.eps = eps\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel for the forward pass of an attention mechanism, which is a common operation in transformer-based neural networks. The main function, `forward`, takes in query (`q`), key (`k`), and value (`v`) tensors along with scaling factors for query and key (`q_scale`, `k_scale`). It performs a block-wise computation over these inputs to compute the attention scores and the resulting output tensor `o`.\n\n            The `_attn_fwd` kernel is called within the `forward` function, which is responsible for the main attention computation. It uses block-wise parallelism to handle large tensor operations efficiently. The kernel computes the dot product of `q` and `k`, scales it, applies a softmax-like normalization, and finally computes the weighted sum with `v`.\n\n            The helper function `_attn_fwd_inner` is called within the `_attn_fwd` kernel to handle the computations for each block of the input tensors. It iteratively updates accumulated results (`acc`) and normalization factors (`l_i`) for attention score calculations over the context size `N_CTX`.\n\n            The function returns the computed output tensor `o`.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for the forward pass of attention mechanism\n@triton.jit\ndef _attn_fwd(q_ptr, k_ptr, v_ptr, o_ptr, q_scale, k_scale, N_CTX, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Compute offsets for each block\n    q_offset = pid * BLOCK_SIZE\n    k_offset = 0  # Keys are shared across all queries\n    v_offset = 0  # Values are shared across all queries\n\n    # Load blocks of q, k, v\n    q = tl.load(q_ptr + q_offset)\n    k = tl.load(k_ptr + k_offset)\n    v = tl.load(v_ptr + v_offset)\n\n    # Initialize accumulators\n    acc = tl.zeros([BLOCK_SIZE, BLOCK_SIZE], dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_SIZE], dtype=tl.float32) - float('inf')\n\n    # Compute attention for each block\n    for n in range(0, N_CTX, BLOCK_SIZE):\n        # Compute dot product and scale\n        qk = tl.dot(q, k) * q_scale * k_scale\n        \n        # Update normalization factor\n        l_i = tl.maximum(l_i, qk)\n\n        # Compute exp and accumulate results\n        exp_qk = tl.exp(qk - l_i[:, None])\n        acc += tl.dot(exp_qk, v)\n\n        # Update k and v offsets\n        k_offset += BLOCK_SIZE\n        v_offset += BLOCK_SIZE\n\n        # Load next blocks of k, v\n        k = tl.load(k_ptr + k_offset)\n        v = tl.load(v_ptr + v_offset)\n\n    # Normalize and write output\n    acc = acc / tl.exp(l_i[:, None])\n    tl.store(o_ptr + q_offset, acc)\n\n# Wrapper function to call the Triton kernel\ndef forward(q, k, v, q_scale, k_scale, BLOCK_SIZE=128):\n    # Ensure inputs are contiguous\n    q = q.contiguous()\n    k = k.contiguous()\n    v = v.contiguous()\n\n    # Allocate output tensor\n    o = torch.empty_like(q)\n\n    # Launch the Triton kernel\n    grid = (q.shape[0] // BLOCK_SIZE,)\n    _attn_fwd[grid](q, k, v, o, q_scale, k_scale, q.shape[1], BLOCK_SIZE=BLOCK_SIZE)\n\n    return o\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Triton kernel for the inner loop of attention forward pass\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask = k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask = (offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        \n        acc += tl.dot(p, v.to(tl.float16), out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i\n\n# Triton kernel for the attention forward pass\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr  \n              ):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N) \n    \n\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX \n                                    )\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\n# Function to call the Triton kernel\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.bfloat16)\n    stage = 1\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=3)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel computes a custom operation on a batched input tensor `x` with dimensions [batch, M, K], generating an output tensor of the same shape. The operation is a form of position-dependent complex transformation using cosine and sine functions, parameterized by a constant `THETA`. The kernel is split into 2D blocks, processing blocks of size `BLOCK_SIZE_M` by `BLOCK_SIZE_K`, where `BLOCK_SIZE_M` is 2 and `BLOCK_SIZE_K` is 1024. The input tensor is divided among Triton program instances, each handling specific indices. The result is stored back in an output tensor `out` of the same shape. The main function `rbe_triton_wrapper` sets up the grid and executes the kernel, given a tensor `x` and a starting position `pos`.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define constants\nBLOCK_SIZE_M = 2\nBLOCK_SIZE_K = 1024\nTHETA = 0.5  # You can adjust this value as needed\n\n@triton.jit\ndef custom_transform_kernel(x_ptr, out_ptr, pos, batch, M, K,\n                            stride_xm, stride_xk,\n                            stride_om, stride_ok,\n                            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    # Define the block indices\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n\n    # Calculate the start indices for this block\n    m_start = pid_m * BLOCK_SIZE_M\n    k_start = pid_k * BLOCK_SIZE_K\n\n    # Create offsets for the block\n    offsets_m = m_start + tl.arange(0, BLOCK_SIZE_M)\n    offsets_k = k_start + tl.arange(0, BLOCK_SIZE_K)\n\n    # Load the input data for this block\n    x = tl.load(x_ptr + offsets_m[:, None] * stride_xm + offsets_k[None, :] * stride_xk, mask=(offsets_m[:, None] < M) & (offsets_k[None, :] < K), other=0.0)\n\n    # Calculate position-dependent transformation\n    # Here, `pos` is assumed to be a tensor with shape [batch, M, K] that indicates the position\n    # of each element in the input tensor `x`. You can modify the position logic as needed.\n    theta_m = THETA * (pos + offsets_m[:, None])\n    cos_theta = tl.cos(theta_m)\n    sin_theta = tl.sin(theta_m)\n\n    # Apply the transformation\n    transformed_x = x * cos_theta - x * sin_theta\n\n    # Store the result back to the output tensor\n    tl.store(out_ptr + offsets_m[:, None] * stride_om + offsets_k[None, :] * stride_ok, transformed_x, mask=(offsets_m[:, None] < M) & (offsets_k[None, :] < K))\n\ndef rbe_triton_wrapper(x, pos):\n    # Extract dimensions\n    batch, M, K = x.shape\n\n    # Create output tensor\n    out = torch.empty_like(x)\n\n    # Calculate grid size\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(K, BLOCK_SIZE_K))\n\n    # Launch the Triton kernel\n    custom_transform_kernel[grid](\n        x_ptr=x,\n        out_ptr=out,\n        pos=pos,\n        batch=batch,\n        M=M,\n        K=K,\n        stride_xm=x.stride(1),\n        stride_xk=x.stride(2),\n        stride_om=out.stride(1),\n        stride_ok=out.stride(2),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n\n    return out\n\n# Example usage\nx = torch.randn(1, 4, 1024, device='cuda')  # Example input tensor\npos = torch.arange(0, x.shape[1], device='cuda')  # Example position tensor\nout = rbe_triton_wrapper(x, pos)\nprint(out)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef get_freq_multi_tokens(offs_cn, starting_idx, theta: tl.constexpr, NB_TOKENS: tl.constexpr):\n    DIM: tl.constexpr = 128  # in model, dim = self.params.dim // self.params.n_heads\n    freqs = offs_cn % DIM\n    freqs = freqs.to(tl.float32) / DIM\n    freqs = tl.extra.cuda.libdevice.pow(theta, freqs)\n    freqs = (tl.arange(0, NB_TOKENS) + starting_idx)[:, None] / freqs[None, :]\n    return tl.cos(freqs), tl.sin(freqs)\n\n\n@triton.jit\ndef rbe_triton(x_ptr, out_ptr,\n               M, K,\n               stride_x_batch, stride_x_m, stride_x_n,\n               stride_out_batch, stride_out_m, stride_out_n,\n               start_token_position,\n               THETA: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_n = pid % tl.cdiv(K, BLOCK_SIZE_K)\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K // 2) * 2  # take only even numbers\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + stride_x_m * offs_m[:, None] + stride_x_n * offs_n[None, :])\n    x_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    real = tl.load(x_ptrs, mask=x_real_mask, other=0.0)\n    x_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    imag = tl.load(x_ptrs + 1, mask=x_imag_mask, other=0.0)\n    tl.debug_barrier()\n    start_block = start_token_position + pid_m * BLOCK_SIZE_M\n    cos, sin = get_freq_multi_tokens(offs_cn=offs_n, starting_idx=start_block, theta=THETA, NB_TOKENS=BLOCK_SIZE_M)\n\n    out_real = real * cos - imag * sin\n    out_imag = real * sin + imag * cos\n    tl.debug_barrier()\n    out_ptrs = out_ptr + (\n            pid_batch * stride_out_batch + stride_out_m * offs_m[:, None] + stride_out_n * offs_n[None, :])\n    out_real_mask = (offs_m[:, None] < M) & (offs_n[None, :] < K)\n    tl.store(out_ptrs, out_real, mask=out_real_mask)\n    out_imag_mask = (offs_m[:, None] < M) & (1 + offs_n[None, :] < K)\n    tl.store(out_ptrs + 1, out_imag, mask=out_imag_mask)\n\n\ndef rbe_triton_wrapper(x: torch.Tensor, pos: int) -> torch.Tensor:\n    batch, M, K = x.shape\n    out = torch.empty_like(x)\n    grid = lambda META: (\n        batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"K\"], META[\"BLOCK_SIZE_K\"]),)\n\n    rbe_triton[grid](x, out,\n                     M, K,\n                     *x.stride(),\n                     *out.stride(),\n                     start_token_position=pos, THETA=10000., BLOCK_SIZE_M=2, BLOCK_SIZE_K=1024)\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code implements a matrix multiplication kernel using Triton, designed for GPUs. The main function is `iv_dependent_matmul_kernel`, which multiplies two matrices `a` and `b`, and stores the result in `c`. It supports different data loading strategies specified by the `type` parameter (like 'pre_load', 'post_load', etc.). A wrapper function `iv_dependent_matmul_wrapper` sets up the necessary parameters and invokes this kernel. The wrapper accepts matrix dimensions (`M`, `K`, `N`), block sizes, and kernel type to determine how data is loaded and processed.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef iv_dependent_matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bn, stride_bk, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, type: tl.constexpr):\n    # Get program IDs\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the start of the block for each dimension\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    # Initialize accumulation\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_K):\n        # Load blocks of a and b\n        if type == 'pre_load':\n            a = tl.load(a_ptr + (offs_m[:, None] * stride_am + (k + offs_k)[None, :] * stride_ak), mask=offs_m[:, None] < M)\n            b = tl.load(b_ptr + ((k + offs_k)[:, None] * stride_bk + offs_n[None, :] * stride_bn), mask=offs_n[None, :] < N)\n        elif type == 'post_load':\n            a = tl.load(a_ptr + (offs_m[:, None] * stride_am + (k + offs_k)[None, :] * stride_ak))\n            b = tl.load(b_ptr + ((k + offs_k)[:, None] * stride_bk + offs_n[None, :] * stride_bn))\n        else:\n            raise ValueError(f\"Unsupported type: {type}\")\n\n        # Matrix multiplication\n        acc += tl.dot(a, b)\n\n    # Store the result\n    c = acc.to(tl.float32)\n    c_ptr += offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n    tl.store(c_ptr, c, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\n# Define the wrapper function\ndef iv_dependent_matmul_wrapper(a, b, c, M, N, K, BLOCK_M=128, BLOCK_N=128, BLOCK_K=32, type='pre_load'):\n    # Determine the number of blocks\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n\n    # Launch the kernel\n    iv_dependent_matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n        type=type\n    )\n\n# Example usage:\n# Assume a, b, c are Triton-managed tensors with appropriate strides\n# iv_dependent_matmul_wrapper(a, b, c, M, N, K)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef iv_dependent_matmul_kernel(a_ptr, b_ptr, c_ptr,  #\n                                M, N, K,  #\n                                stride_am, stride_ak,  #\n                                stride_bk, stride_bn,  #\n                                stride_cm, stride_cn,  #\n                                BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n                                type: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptr = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptr = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    a_ptrs = a_ptr\n    b_ptrs = b_ptr\n    if type == \"post_load_two_iters\":\n        a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n        b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n    elif type == \"post_load_three_iters\":\n        a_ptrs_next = a_ptr + BLOCK_SIZE_K * stride_ak\n        b_ptrs_next = b_ptr + BLOCK_SIZE_K * stride_bk\n        a_ptrs_next_next = a_ptr + 2 * BLOCK_SIZE_K * stride_ak\n        b_ptrs_next_next = b_ptr + 2 * BLOCK_SIZE_K * stride_bk\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        if type == \"pre_load\":\n            a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n            b_ptrs = b_ptr + k * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_pre_mixed\":\n            a_ptrs = a_ptr + k * BLOCK_SIZE_K * stride_ak\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        if type == \"post_load\":\n            a_ptrs = a_ptr + (k + 1) * BLOCK_SIZE_K * stride_ak\n            b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_pre_mixed\":\n            b_ptrs = b_ptr + (k + 1) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_load_two_iters\":\n            a_ptrs = a_ptrs_next\n            b_ptrs = b_ptrs_next\n            a_ptrs_next = a_ptr + (k + 2) * BLOCK_SIZE_K * stride_ak\n            b_ptrs_next = b_ptr + (k + 2) * BLOCK_SIZE_K * stride_bk\n        elif type == \"post_load_three_iters\":\n            a_ptrs = a_ptrs_next\n            b_ptrs = b_ptrs_next\n            a_ptrs_next = a_ptrs_next_next\n            b_ptrs_next = b_ptrs_next_next\n            a_ptrs_next_next = a_ptr + (k + 3) * BLOCK_SIZE_K * stride_ak\n            b_ptrs_next_next = b_ptr + (k + 3) * BLOCK_SIZE_K * stride_bk\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\ndef iv_dependent_matmul_wrapper(\n    M: int, \n    K: int, \n    N: int, \n    BLOCK_SIZE_M: int, \n    BLOCK_SIZE_N: int, \n    BLOCK_SIZE_K: int, \n    type: str = \"pre_load\",  # Kernel type for scheduling (\"pre_load\", \"post_load\", etc.)\n    device: torch.device = \"cuda\"  # Device to run the test (defaults to \"cuda\")\n):\n    # Ensure the device is correct\n    device = torch.device(device)\n\n    # Generate random input matrices a and b on the specified device\n    a = torch.rand((M, K), device=device)\n    b = torch.rand((K, N), device=device)\n\n    # Create an empty tensor to store the Triton result\n    triton_output = torch.empty((M, N), device=device)\n\n    # Define Triton grid configuration\n    def grid(META):\n        return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n\n    # Set the number of stages based on the kernel type\n    num_stages = 4 if type == \"post_load_three_iters\" else 3\n\n    # Run the Triton kernel\n    iv_dependent_matmul_kernel[grid](\n        a, b, triton_output, M, N, K,  #\n        a.stride(0), a.stride(1), b.stride(0), b.stride(1),  #\n        triton_output.stride(0), triton_output.stride(1),  #\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K, type=type,  #\n        num_stages=num_stages\n    )\n\n    # Optionally print the result for inspection\n    # print(triton_output)\n\n    return triton_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel `_swiglu_bwd_kernel` is designed to compute the backward pass of the Swish-Gated Linear Units (Swiglu) operation. It takes input tensors `X` and `Y`, the derivative `DOUT`, and optionally the output `OUT` to compute gradients `DX` and `DY`. The main logic involves iterating over columns using the program ID to determine rows and column blocks. The computations involve sigmoid and element-wise operations for gradient calculation, and the results are stored in `DX` and `DY`. The function `_swiglu_bwd` serves as a Python wrapper, reshaping inputs and invoking the Triton kernel. It handles contiguous memory alignment, splits input into two tensors, and sets up the execution grid for the Triton kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _swiglu_bwd_kernel(X, Y, DOUT, DX, DY, OUT, stride_x, stride_y, stride_dout, stride_dx, stride_dy, stride_out, n_cols, BLOCK_SIZE: tl.constexpr):\n    # Program ID\n    pid = tl.program_id(axis=0)\n\n    # Offsets\n    row_start = pid * BLOCK_SIZE\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n\n    # Load data\n    X_ptrs = X + row_start * stride_x + col_offsets\n    Y_ptrs = Y + row_start * stride_y + col_offsets\n    DOUT_ptrs = DOUT + row_start * stride_dout + col_offsets\n\n    # Conditionally load OUT if it's not None\n    if OUT is not None:\n        OUT_ptrs = OUT + row_start * stride_out + col_offsets\n        out = tl.load(OUT_ptrs, mask=col_offsets < n_cols, other=0.0)\n    else:\n        out = None\n\n    x = tl.load(X_ptrs, mask=col_offsets < n_cols, other=0.0)\n    y = tl.load(Y_ptrs, mask=col_offsets < n_cols, other=0.0)\n    dout = tl.load(DOUT_ptrs, mask=col_offsets < n_cols, other=0.0)\n\n    # Compute sigmoid\n    sigmoid_y = 1 / (1 + tl.exp(-y))\n\n    # Compute gradients\n    dx = dout * y * sigmoid_y * (1 + x * (1 - sigmoid_y))\n    dy = dout * x * sigmoid_y * (1 - sigmoid_y)\n\n    # Store results\n    DX_ptrs = DX + row_start * stride_dx + col_offsets\n    DY_ptrs = DY + row_start * stride_dy + col_offsets\n\n    tl.store(DX_ptrs, dx, mask=col_offsets < n_cols)\n    tl.store(DY_ptrs, dy, mask=col_offsets < n_cols)\n\nimport torch\n\ndef _swiglu_bwd(X, Y, DOUT, OUT=None):\n    assert X.shape == Y.shape == DOUT.shape, \"Input tensors must have the same shape\"\n\n    # Reshape inputs to ensure contiguous memory layout\n    X = X.contiguous()\n    Y = Y.contiguous()\n    DOUT = DOUT.contiguous()\n    if OUT is not None:\n        OUT = OUT.contiguous()\n\n    # Get dimensions\n    n_rows, n_cols = X.shape\n\n    # Allocate output tensors\n    DX = torch.empty_like(X)\n    DY = torch.empty_like(Y)\n\n    # Define grid and block size\n    BLOCK_SIZE = 128  # This can be tuned based on the GPU architecture\n    grid = (triton.cdiv(n_rows, BLOCK_SIZE),)\n\n    # Launch the Triton kernel\n    _swiglu_bwd_kernel[grid](\n        X, Y, DOUT, DX, DY, OUT,\n        X.stride(0), Y.stride(0), DOUT.stride(0), DX.stride(0), DY.stride(0), OUT.stride(0) if OUT is not None else 0,\n        n_cols, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return DX, DY\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Backward Triton kernel for Swish-Gated Linear Units (Swiglu)\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_N': 32}),\n        triton.Config({'BLOCK_N': 64}),\n        triton.Config({'BLOCK_N': 128}),\n        triton.Config({'BLOCK_N': 256}),\n        triton.Config({'BLOCK_N': 512}),\n        triton.Config({'BLOCK_N': 1024}),\n    ],\n    key=['ncols'],\n)\n@triton.heuristics({\"RECOMPUTE_OUTPUT\": lambda args: args[\"OUT\"] is not None})\n@triton.jit\ndef _swiglu_bwd_kernel(\n    X, Y, DOUT, OUT, DX, DY, stride_x_row, stride_y_row, stride_dout_row,\n    stride_out_row, stride_dx_row, stride_dy_row, ncols, BLOCK_N: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.).to(tl.float32)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)\n\n# Function to invoke the backward kernel\ndef _swiglu_bwd(xy, dout, dxy=None, recompute_output=False, out=None):\n    if xy.stride(-1) != 1:\n        xy = xy.contiguous()\n    if dout.stride(-1) != 1:\n        dout = dout.contiguous()\n    batch_shape = xy.shape[:-1]\n    xy = xy.reshape(-1, xy.shape[-1])\n    x, y = xy.chunk(2, dim=-1)\n    dout = dout.reshape(-1, dout.shape[-1])\n    assert dout.shape == x.shape\n    if dxy is None:\n        dxy = torch.empty_like(xy)\n    else:\n        dxy = dxy.reshape(-1, dxy.shape[-1])\n        assert dxy.shape == xy.shape\n    dx, dy = dxy.chunk(2, dim=-1)\n    assert dx.stride(-1) == 1\n    assert dy.stride(-1) == 1\n    if recompute_output:\n        if out is None:\n            out = torch.empty_like(x)\n        else:\n            out = out.reshape(-1, out.shape[-1])\n            assert out.shape == x.shape\n        assert out.stride(-1) == 1\n    M, N = x.shape\n    grid = lambda META: (M, triton.cdiv(N, META['BLOCK_N']))\n    with torch.cuda.device(x.device.index):\n        _swiglu_bwd_kernel[grid](\n            x, y, dout, out if recompute_output else None, dx, dy, x.stride(0), y.stride(0),\n            dout.stride(0), out.stride(0) if recompute_output else 0, dx.stride(0),\n            dy.stride(0), N\n        )\n    if not recompute_output:\n        return dxy.reshape(*batch_shape, dxy.shape[-1])\n    else:\n        return dxy.reshape(*batch_shape, dxy.shape[-1]), out.reshape(*batch_shape, out.shape[-1])\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe given Triton operator consists of a kernel function and a wrapper function. The kernel function named 'kernel' transposes a matrix 'M' of size (SIZE_M, D_HEAD) to an output matrix 'Out' of size (D_HEAD, SIZE_M). Inputs are 'M' (input matrix), 'Out' (output matrix), strides of the input and output matrices, and dimensions 'SIZE_M' and 'D_HEAD'. The wrapper function initializes the input matrix with random values and the output matrix with zeros, both on the GPU. It then launches the kernel to perform the matrix transpose and returns the transposed matrix.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef kernel(M, Out, stride_m, stride_out, SIZE_M, D_HEAD, BLOCK_SIZE: tl.constexpr):\n    # Program ID is the unique identifier for each program instance\n    pid_m = tl.program_id(0)\n    pid_d = tl.program_id(1)\n\n    # Calculate the starting index of each block\n    offset_m = pid_m * BLOCK_SIZE\n    offset_d = pid_d * BLOCK_SIZE\n\n    # Create pointers to the input and output matrices\n    ptr_m = M + offset_m * stride_m + offset_d\n    ptr_out = Out + offset_d * stride_out + offset_m\n\n    # Create a block of indices\n    m_idx = tl.arange(0, BLOCK_SIZE)\n    d_idx = tl.arange(0, BLOCK_SIZE)\n\n    # Load a block from the input matrix\n    mask_m = (offset_m + m_idx) < SIZE_M\n    mask_d = (offset_d + d_idx) < D_HEAD\n    block = tl.load(ptr_m + m_idx[:, None] * stride_m + d_idx[None, :], mask=mask_m[:, None] & mask_d[None, :])\n\n    # Store the transposed block into the output matrix\n    tl.store(ptr_out + d_idx[:, None] * stride_out + m_idx[None, :], block, mask=mask_d[:, None] & mask_m[None, :])\n\ndef transpose_matrix(M, SIZE_M, D_HEAD):\n    # Define block size for tiling\n    BLOCK_SIZE = 32\n\n    # Initialize output matrix with zeros\n    Out = torch.zeros((D_HEAD, SIZE_M), device='cuda', dtype=M.dtype)\n\n    # Calculate strides\n    stride_m = M.stride(0)\n    stride_out = Out.stride(0)\n\n    # Launch the kernel\n    grid = (triton.cdiv(SIZE_M, BLOCK_SIZE), triton.cdiv(D_HEAD, BLOCK_SIZE))\n    kernel[grid](M, Out, stride_m, stride_out, SIZE_M, D_HEAD, BLOCK_SIZE=BLOCK_SIZE)\n\n    return Out\n\n# Example usage\nSIZE_M = 128\nD_HEAD = 64\nM = torch.rand((SIZE_M, D_HEAD), device='cuda', dtype=torch.float32)\n\n# Transpose the matrix\nOut = transpose_matrix(M, SIZE_M, D_HEAD)\n\n# Verify correctness\nprint(\"Input Matrix (M):\")\nprint(M.cpu().numpy())\nprint(\"\\nTransposed Matrix (Out):\")\nprint(Out.cpu().numpy())\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel(\n    M,\n    Out,\n    matrix_stridex,\n    matrix_stridey,\n    out_stridex,\n    out_stridey,\n    SIZE_M: tl.constexpr,\n    D_HEAD: tl.constexpr,\n):\n    size_m_arange = tl.arange(0, SIZE_M)\n    d_head_arange = tl.arange(0, D_HEAD)\n    # transpose\n    matrix_ptr = M + d_head_arange[None, :] * matrix_stridey + size_m_arange[:, None] * matrix_stridex\n    out_ptr = Out + d_head_arange[None, :] * out_stridex + size_m_arange[:, None] * out_stridey\n    matrix = tl.load(matrix_ptr)\n    tl.store(out_ptr, matrix)\n\ndef wrapper(size_m, d_head):\n    matrix = torch.randn((size_m, d_head), dtype=torch.float16, device=\"cuda\")\n    out = torch.zeros((d_head, size_m), dtype=torch.float16, device=\"cuda\")\n\n    grid = (1,)\n    kernel[grid](\n        matrix,\n        out,\n        *matrix.stride(),\n        *out.stride(),\n        size_m,\n        d_head,\n    )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The `softmax_kernel_online_v2` function computes the softmax of input data using Triton language for optimization. It processes data in tiles (sub-arrays) to handle potentially large inputs efficiently. The function takes pointers to the input and output tensors, as well as the dimensions M (number of rows) and N (number of columns) and TILE_N (tile size, power of 2). The function calculates the maximum and the sum of exponentials for stable softmax computation, and normalizes the exponentiated inputs by this sum to produce the final softmax output stored in the output pointer.\n        The helper function `prev_multiple_of` calculates the largest multiple of `b` less than `a`, used for tile alignment.\n        The `softmax` function is a wrapper to invoke the Triton kernel on a given 2D torch tensor `x`. It initializes the output tensor `out` and determines the tile size, then calls the Triton kernel, returning the computed softmax results.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel_online_v2(input_ptr, output_ptr, M, N, TILE_N, **meta):\n    # Compute the row and column indices for this program\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, TILE_N)\n\n    # Load the current row of the input matrix\n    row_start = row_idx * N\n    input_row = tl.load(input_ptr + row_start + col_idx, mask=col_idx < N, other=-float('inf'))\n\n    # Compute the maximum value for numerical stability\n    row_max = tl.max(input_row, axis=0)\n\n    # Subtract the maximum and exponentiate\n    exp_row = tl.exp(input_row - row_max)\n\n    # Compute the sum of exponentials\n    sum_exp = tl.sum(exp_row, axis=0)\n\n    # Normalize the exponentials to get the softmax probabilities\n    softmax_row = exp_row / sum_exp\n\n    # Store the result in the output matrix\n    tl.store(output_ptr + row_start + col_idx, softmax_row, mask=col_idx < N)\n\ndef prev_multiple_of(a, b):\n    return (a // b) * b\n\ndef softmax(x):\n    # Get the dimensions of the input tensor\n    M, N = x.shape\n\n    # Determine the tile size (must be a power of 2)\n    TILE_N = 1 << (N - 1).bit_length()  # Closest power of 2 greater than or equal to N\n\n    # Initialize the output tensor\n    out = torch.empty_like(x)\n\n    # Launch the Triton kernel\n    grid = (M,)  # One program per row\n    num_warps = 4  # You can tune this based on your hardware\n    softmax_kernel_online_v2[grid](x, out, M, N, TILE_N, num_warps=num_warps)\n\n    return out\n\n# Example usage\nx = torch.randn(128, 512, device='cuda')  # Example input tensor\nout = softmax(x)\n", "label": "import triton\nfrom triton import language as tl\nimport torch\n\n@triton.jit\ndef prev_multiple_of(a, b):\n    # the largest x<a that x%b ==0\n    return tl.cdiv(a, b) * b - b\n\n@triton.jit\ndef softmax_kernel_online_v2(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    TILE_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    m = tl.full((TILE_N,), value=-float(\"inf\"), dtype=output_ptr.dtype.element_ty)\n    z = tl.full((TILE_N,), value=0, dtype=output_ptr.dtype.element_ty)\n    prev_multiple = prev_multiple_of(N, TILE_N)\n    for start_n in range(0, prev_multiple, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs).to(output_ptr.dtype.element_ty)\n        new_m = tl.maximum(m, inp)\n        new_z = tl.exp(m - new_m) * z + tl.exp(inp - new_m)\n        m = new_m\n        z = new_z\n    for start_n in range(prev_multiple, N, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(output_ptr.dtype.element_ty)\n        new_m = tl.maximum(m, inp)\n        new_z = tl.exp(m - new_m) * z + tl.exp(inp - new_m)\n        m = new_m\n        z = new_z\n    final_m = tl.max(m, 0)\n    z = tl.sum(tl.exp(m - final_m) * z)\n    m = final_m\n\n    prev_multiple = prev_multiple_of(N, TILE_N)\n    for start_n in range(0, prev_multiple, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs).to(output_ptr.dtype.element_ty)\n        e = tl.exp(inp - m)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out)\n    for start_n in range(prev_multiple, N, TILE_N):\n        n_offsets = start_n + tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(output_ptr.dtype.element_ty)\n        e = tl.exp(inp - m)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n\ndef softmax(x):\n    M, N = x.shape\n    out = torch.empty_like(x)\n    TILE_N = min(4096, triton.next_power_of_2(N))\n    grid = (M, 1, 1)\n    softmax_kernel_online_v2[grid](out, x, M, N, TILE_N)\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This code defines a Triton kernel `_bgmv_expand_kernel` and a wrapper function `_bgmv_expand`. The kernel performs a batched generalized matrix-vector multiplication (GroupGEMV) with optional input addition and dtype casting. The wrapper function configures the kernel launch.\n\n    The kernel `_bgmv_expand_kernel` takes as inputs the pointers to the input matrix, LoRA weights, output matrix, and several parameters describing the problem size and strides. It computes the product of a portion of the input matrix and LoRA weights, storing the result in the output matrix. If `ADD_INPUTS` is set, it adds the existing output values to the computed results.\n\n    The wrapper function `_bgmv_expand` is a PyTorch inference mode function. It prepares the inputs and launches the Triton kernel with appropriate configuration. The function checks the input tensor properties, configures the execution grid, and handles type casting if necessary.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_kernel(\n    input_ptr,  # pointer to the input matrix\n    lora_ptr,   # pointer to the LoRA weights\n    output_ptr, # pointer to the output matrix\n    BATCH,      # number of batches\n    M,          # number of rows in the input matrix\n    N,          # number of columns in the input matrix\n    K,          # number of columns in the LoRA matrix\n    stride_in_batch, stride_in_m, stride_in_n,  # strides for input matrix\n    stride_lora_k, stride_lora_n,               # strides for LoRA weights\n    stride_out_batch, stride_out_m,             # strides for output matrix\n    ADD_INPUTS: tl.constexpr,                   # whether to add input values\n    BLOCK_SIZE: tl.constexpr                    # block size for the computation\n):\n    # Program IDs\n    batch_id = tl.program_id(0)\n    row_id = tl.program_id(1)\n\n    # Pointers to the start of the batch and row\n    input_ptr = input_ptr + batch_id * stride_in_batch + row_id * stride_in_m\n    output_ptr = output_ptr + batch_id * stride_out_batch + row_id * stride_out_m\n\n    # Initialize result\n    result = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Iterate over the LoRA columns\n    for k in range(0, K, BLOCK_SIZE):\n        # Load input and LoRA blocks\n        input_block = tl.load(input_ptr + k * stride_in_n + tl.arange(0, BLOCK_SIZE))\n        lora_block = tl.load(lora_ptr + k * stride_lora_k + tl.arange(0, BLOCK_SIZE))\n\n        # Compute the dot product\n        result += input_block * lora_block\n\n    # If ADD_INPUTS is set, add existing output values\n    if ADD_INPUTS:\n        existing_output = tl.load(output_ptr + tl.arange(0, BLOCK_SIZE))\n        result += existing_output\n\n    # Store the result\n    tl.store(output_ptr + tl.arange(0, BLOCK_SIZE), result)\n\nimport torch\n\ndef _bgmv_expand(input_matrix, lora_weights, output_matrix, add_inputs=False):\n    # Ensure input tensors are on the same device\n    assert input_matrix.device == lora_weights.device == output_matrix.device\n    device = input_matrix.device\n\n    # Get the dimensions and strides\n    BATCH, M, N = input_matrix.shape\n    _, _, K = lora_weights.shape\n\n    stride_in_batch, stride_in_m, stride_in_n = input_matrix.stride()\n    stride_lora_k, stride_lora_n = lora_weights.stride()[-2:]\n    stride_out_batch, stride_out_m = output_matrix.stride()\n\n    # Define block size\n    BLOCK_SIZE = 128  # This can be tuned based on the GPU architecture\n\n    # Configure grid size\n    grid = (BATCH, M)\n\n    # Launch the kernel\n    triton._C.launch(\n        _bgmv_expand_kernel,\n        grid=grid,\n        num_warps=4,  # This can be tuned\n        BLOCK_SIZE=BLOCK_SIZE,\n        input_ptr=input_matrix.data_ptr(),\n        lora_ptr=lora_weights.data_ptr(),\n        output_ptr=output_matrix.data_ptr(),\n        BATCH=BATCH,\n        M=M,\n        N=N,\n        K=K,\n        stride_in_batch=stride_in_batch,\n        stride_in_m=stride_in_m,\n        stride_in_n=stride_in_n,\n        stride_lora_k=stride_lora_k,\n        stride_lora_n=stride_lora_n,\n        stride_out_batch=stride_out_batch,\n        stride_out_m=stride_out_m,\n        ADD_INPUTS=add_inputs\n    )\n\n# Example usage\ninput_matrix = torch.randn(10, 128, 256, device='cuda')\nlora_weights = torch.randn(10, 256, 64, device='cuda')\noutput_matrix = torch.zeros(10, 128, 64, device='cuda')\n\n_bgmv_expand(input_matrix, lora_weights, output_matrix, add_inputs=True)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _bgmv_expand_kernel(\n    input_ptr,\n    lora_ptr,\n    out_ptr,\n    N,\n    K,\n    lora_indices,\n    xm_stride,\n    xk_stride,\n    l0_stride,\n    lora_k_stride,\n    lora_n_stride,\n    cm_stride,\n    cn_stride,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    SPLIT_N: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ADD_INPUTS: tl.constexpr,\n    CAST_TYPE: tl.constexpr,\n):\n    \"\"\"\n    GroupGEMV, additionally, introducing SPLIT_N can improve large hidden_size's\n    performance\n    \"\"\"\n    pid_sn = tl.program_id(axis=0)\n    cur_batch = tl.program_id(axis=1)\n    lora_index = tl.load(lora_indices + cur_batch)\n    if lora_index == -1:\n        return\n    offset_k = tl.arange(0, BLOCK_K)\n    offset_n = tl.arange(0, BLOCK_N)\n    if EVEN_K:\n        tiled_a = tl.load(input_ptr + cur_batch * xm_stride +\n                          offset_k * xk_stride, )  # [BLOCK_K]\n    else:\n        tiled_a = tl.load(\n            input_ptr + cur_batch * xm_stride + offset_k * xk_stride,\n            mask=offset_k < K,\n            other=0,\n        )  # [BLOCK_K]\n    # N must be divisible by SPLIT_N\n    split_n_length = tl.cdiv(N, SPLIT_N)\n    if CAST_TYPE:\n        tiled_a = tiled_a.to(lora_ptr.dtype.element_ty)\n    # sliding  to  next row-block\n    b_ptr = (lora_ptr + l0_stride * lora_index +\n             pid_sn * split_n_length * lora_k_stride)\n    c_ptr = out_ptr + cur_batch * cm_stride + pid_sn * split_n_length\n    for n in range(0, split_n_length, BLOCK_N):\n        current_n = n + offset_n\n        current_n_c = tl.max_contiguous(current_n, BLOCK_N)\n        b_ptr_mask = (current_n[:, None] < split_n_length) & (offset_k[None, :]\n                                                              < K)\n        c_mask = current_n < split_n_length\n        tiled_b = tl.load(\n            b_ptr + current_n_c[:, None] * lora_k_stride +\n            offset_k[None, :] * lora_n_stride,\n            mask=b_ptr_mask,\n            other=0.0,\n        )  # [BLOCK_N,BLOCK_K]\n        if ADD_INPUTS:\n            tiled_out = tl.load(c_ptr + current_n * cn_stride, mask=c_mask)\n            accumulator = tl.sum(tiled_a * tiled_b, 1) + tiled_out\n        else:\n            accumulator = tl.sum(tiled_a * tiled_b, 1)\n\n        tl.store(c_ptr + current_n * cn_stride, accumulator, mask=c_mask)\n\n\n@torch.inference_mode()\ndef _bgmv_expand(\n    inputs: torch.Tensor,\n    lora_b_weights: torch.Tensor,\n    output_tensor: torch.Tensor,\n    lora_indices_tensor: torch.Tensor,\n    add_inputs: bool = True,\n) -> None:\n    \"\"\"\n    Args:\n        inputs (torch.Tensor): input tensor\n        lora_b_weights (torch.Tensor): lora'a weight\n        output_tensor (torch.Tensor): output tensor\n        lora_indices_tensor (torch.Tensor): (batch_size,). The LoRA index\n            corresponding to each batch, An index of -1 means no lora should be\n            applied.\n        batches (int): batch size\n        add_inputs (bool, optional):  Defaults to False, adds the final lora \n            results to the output.\n    \"\"\"\n    assert inputs.dtype in [torch.float16, torch.bfloat16, torch.float32]\n    assert lora_b_weights.dtype in [\n        torch.float16,\n        torch.bfloat16,\n    ]\n    assert inputs.size(1) == lora_b_weights.size(-1)\n\n    assert inputs.is_contiguous()\n    assert output_tensor.is_contiguous()\n\n    if lora_b_weights.ndim == 4:  # shape:(lora_num,1,size,rank)\n        assert lora_b_weights.size(1) == 1\n        lora_b_weights = lora_b_weights.squeeze(dim=1)\n    else:\n        assert lora_b_weights.ndim == 3  # shape:(lora_num,size,rank)\n    assert lora_b_weights.is_contiguous()\n\n    # TODO tuning this config\n    N, K = lora_b_weights.shape[-2:]  # K= rank,N=hidden_size\n    BLOCK_K = triton.next_power_of_2(K)\n    EVEN_K = K % BLOCK_K == 0\n    ADD_INPUTS = add_inputs\n    CAST_TYPE = False\n    if inputs.dtype == torch.float32 and lora_b_weights.dtype in [\n            torch.float16,\n            torch.bfloat16,\n    ]:\n        CAST_TYPE = True\n    batches = lora_indices_tensor.size(0)\n    # config = get_lora_op_configs(\"expand\", batches, N)\n    grid = lambda META: (\n        META[\"SPLIT_N\"],\n        batches,\n    )\n    _bgmv_expand_kernel[grid](\n        inputs,\n        lora_b_weights,\n        output_tensor,\n        N,\n        K,\n        lora_indices_tensor,\n        inputs.stride(0),\n        inputs.stride(1),\n        lora_b_weights.stride(0),\n        lora_b_weights.stride(1),\n        lora_b_weights.stride(2),\n        output_tensor.stride(0),\n        output_tensor.stride(1),\n        BLOCK_K=BLOCK_K,\n        BLOCK_N=256,\n        SPLIT_N=lora_b_weights.shape[-2:][0],\n        EVEN_K=EVEN_K,\n        ADD_INPUTS=ADD_INPUTS,\n        CAST_TYPE=CAST_TYPE,\n        # **config,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code implements a forward pass for the attention mechanism. The core function is `_attn_fwd`, a Triton JIT-compiled kernel for efficient computation of attention outputs, with support for sliding windows in attention calculation. The kernel operates on blocks of data, indicated by `_BLOCK_M`, `_BLOCK_N`, and `BLOCK_DMODEL`, optimizing GPU resources usage. `_attn_fwd_inner` handles the blockwise computations for attention scores (QK) and updates the accumulated results. It employs a mask for sliding window calculations. The `_forward` function serves as a Python wrapper that sets up grid dimensions, manages exceptions, and calls the Triton kernel. Inputs include query (Q), key (K), and value (V) matrices, and parameters for handling batch dimensions, context lengths, and optional sliding window settings. Outputs are the attention-weighted matrix, along with normalization parameters.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Constants for block sizes\n_BLOCK_M = 128  # Block size for batch dimension\n_BLOCK_N = 128  # Block size for sequence length dimension\n_BLOCK_DMODEL = 64  # Block size for model dimension\n\n@triton.jit\ndef _attn_fwd(\n    Q_ptr, K_ptr, V_ptr, Out_ptr,\n    stride_qm, stride_qn, stride_qd,\n    stride_km, stride_kn, stride_kd,\n    stride_vm, stride_vn, stride_vd,\n    stride_om, stride_on, stride_od,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    context_len: tl.constexpr, sliding_window: tl.constexpr\n):\n    # Block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Offsets for the blocks\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    # Pointers to the Q, K, V matrices\n    Q = tl.load(Q_ptr + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qd)\n    K = tl.load(K_ptr + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kd)\n    V = tl.load(V_ptr + offs_n[:, None] * stride_vn + offs_d[None, :] * stride_vd)\n\n    # Compute attention scores (QK^T)\n    QK = tl.dot(Q, K.T)\n\n    # Apply sliding window mask if enabled\n    if sliding_window:\n        mask = (offs_n[:, None] - offs_m[None, :]) < context_len\n        QK = tl.where(mask, QK, float('-inf'))\n\n    # Softmax normalization\n    QK_max = tl.max(QK, axis=1)\n    QK_exp = tl.exp(QK - QK_max[:, None])\n    QK_sum = tl.sum(QK_exp, axis=1)\n    QK_softmax = QK_exp / QK_sum[:, None]\n\n    # Compute attention output\n    Out = tl.dot(QK_softmax, V)\n\n    # Store the result\n    tl.store(Out_ptr + offs_m[:, None] * stride_om + offs_d[None, :] * stride_od, Out)\n\ndef _forward(Q, K, V, context_len, sliding_window=False):\n    # Ensure inputs are contiguous\n    Q = Q.contiguous()\n    K = K.contiguous()\n    V = V.contiguous()\n\n    # Get shapes\n    batch_size, seq_len, d_model = Q.shape\n\n    # Allocate output tensor\n    Out = torch.empty((batch_size, seq_len, d_model), device=Q.device, dtype=Q.dtype)\n\n    # Define grid dimensions\n    grid = (triton.cdiv(batch_size, _BLOCK_M), triton.cdiv(seq_len, _BLOCK_N))\n\n    # Launch the Triton kernel\n    _attn_fwd[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1), Q.stride(2),\n        K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        _BLOCK_M=_BLOCK_M, _BLOCK_N=_BLOCK_N, BLOCK_DMODEL=_BLOCK_DMODEL,\n        context_len=context_len, sliding_window=sliding_window\n    )\n\n    return Out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n_BLOCK_N = 64\n_BLOCK_M = 64\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q,\n                    K_block_ptr, V_block_ptr,\n                    start_m, qk_scale, N_CTX,\n                    sliding_window_offset, sliding_window_size,\n                    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, SLIDING_WINDOW: tl.constexpr,\n                    IS_EVEN_M: tl.constexpr, IS_EVEN_N: tl.constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr\n                ):\n    # range of values handled by this stage\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = (((start_m + 1) * BLOCK_M + sliding_window_offset - sliding_window_size + BLOCK_N - 1) // BLOCK_N) * BLOCK_N\n        else:\n            lo = ((start_m * BLOCK_M + sliding_window_offset - sliding_window_size + 1) // BLOCK_N) * BLOCK_N\n            hi = ((((start_m + 1) * BLOCK_M - 1) + sliding_window_offset + BLOCK_N) // BLOCK_N) * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n\n    # loop over k, v and update accumulator\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        # -- compute qk ----\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[None, :] \\\n                   + start_m * BLOCK_M - start_n + sliding_window_offset\n\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = (dist >= sliding_window_size)\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n\n            qk = tl.where(mask, qk, float(\"-inf\"))\n\n        if not IS_EVEN_N:\n            qk = tl.where(((tl.arange(0, BLOCK_N) + start_n) < N_CTX)[None, :], qk, float(\"-inf\"))\n\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n\n        if not IS_EVEN_N:\n            p = tl.where(((tl.arange(0, BLOCK_N) + start_n) < N_CTX)[None, :], p, 0)\n\n        l_ij = tl.sum(p, 1)\n        # -- update m_i and l_i\n        tmp = m_i - m_ij\n        alpha_mask = (tmp != tmp)  # check nan\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1., alpha)\n        l_i = l_i * alpha + l_ij\n        # -- update output accumulator --\n        acc = acc * alpha[:, None]\n        # update acc\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n        acc += tl.dot(p.to(v.dtype), v)\n        # update m_i and l_i\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n\n    return acc, l_i, m_i\n\n\n@triton.heuristics(\n    {\n        \"IS_EVEN_M\": lambda args: args[\"N_CTX\"] % args[\"BLOCK_M\"] == 0,\n        \"IS_EVEN_N\": lambda args: args[\"NKV_CTX\"] % args[\"BLOCK_N\"] == 0,\n    }\n)\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L,#\n              stride_qz, stride_qh, stride_qm, stride_qk,  #\n              stride_kz, stride_kh, stride_kn, stride_kk,  #\n              stride_vz, stride_vh, stride_vk, stride_vn,  #\n              stride_oz, stride_oh, stride_om, stride_on,  #\n              Z, H, H_KV, #\n              N_CTX,  #\n              ROUND_CTX,\n              NKV_CTX,\n              sliding_window_offset,\n              sliding_window_size,\n              IS_EVEN_M: tl.constexpr,\n              IS_EVEN_N: tl.constexpr,\n              BLOCK_M: tl.constexpr,  #\n              BLOCK_DMODEL: tl.constexpr,  #\n              BLOCK_N: tl.constexpr,  #\n              END: tl.constexpr,\n              INIT: tl.constexpr,\n              SLIDING_WINDOW: tl.constexpr,\n              COMPLEMENT_SLIDING_WINDOW: tl.constexpr\n            ):\n\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H//H_KV)\n    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\n    v_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64) * stride_vh\n    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\n\n    # block pointers\n    Q_block_ptr = tl.make_block_ptr(\n        base=Q + q_offset,\n        shape=(N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    V_block_ptr = tl.make_block_ptr(\n        base=V + v_offset,\n        shape=(NKV_CTX, BLOCK_DMODEL),\n        strides=(stride_vk, stride_vn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    K_block_ptr = tl.make_block_ptr(\n        base=K + k_offset,\n        shape=(BLOCK_DMODEL, NKV_CTX),\n        strides=(stride_kk, stride_kn),\n        offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N),\n        order=(0, 1),\n    )\n    O_block_ptr = tl.make_block_ptr(\n        base=Out + o_offset,\n        shape=(ROUND_CTX, BLOCK_DMODEL),\n        strides=(stride_om, stride_on),\n        offsets=(start_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL),\n        order=(1, 0),\n    )\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    # initialize pointer to m and l\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        # don't have to check boundary for q len\n        m_i = tl.load(m_ptrs).to(tl.float32)\n        l_i = tl.load(l_ptrs).to(tl.float32)\n        acc = tl.load(O_block_ptr).to(tl.float32)\n\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634   # 1/log(2)\n    # load q: it will stay in SRAM throughout\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, #\n                                    start_m, qk_scale, NKV_CTX, #\n                                    sliding_window_offset, sliding_window_size,\n                                    BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW, IS_EVEN_M, IS_EVEN_N,\n                                    COMPLEMENT_SLIDING_WINDOW)\n    # epilogue\n    if (END):\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty))\n\ndef _forward(\n    q, k, v, sm_scale,\n    o=None, m=None, l=None, end=False,\n    sliding_window=None, init=False,\n    complement_sliding_window=False\n):\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n\n    q_round_len = math.ceil(q.shape[2] / 64) * 64\n\n    if sliding_window is not None:\n        sliding_window_offset, sliding_window_size = sliding_window\n    else:\n        sliding_window_offset, sliding_window_size = None, None\n\n    grid = lambda META: (\n        triton.cdiv(q.shape[2], META[\"BLOCK_M\"]),\n        q.shape[0] * q.shape[1],\n    )\n\n    global _BLOCK_N\n    global _BLOCK_M\n\n    try:\n        with torch.cuda.device(q.device):\n            _attn_fwd[grid](\n                q, k, v, sm_scale, m, o, l, #\n                q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n                k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n                v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n                o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n                q.shape[0], q.shape[1], k.shape[1], #\n                q.shape[2],  #\n                q_round_len,\n                k.shape[2],\n                sliding_window_offset,\n                sliding_window_size,\n                BLOCK_DMODEL=Lk,  #\n                END=end,\n                INIT=init,\n                BLOCK_M=_BLOCK_M,\n                BLOCK_N=_BLOCK_N,\n                SLIDING_WINDOW=(sliding_window is not None),\n                COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n                num_warps=4,\n                num_stages=4\n            )\n    except triton.OutOfResources as E:\n        _BLOCK_N = _BLOCK_N // 2\n        _BLOCK_M = _BLOCK_M // 2\n        from warnings import warn\n        warn(f\"Triton Attention Output Resources. {E}\\nUse smaller block size {_BLOCK_N}.\")\n        with torch.cuda.device(q.device):\n            _attn_fwd[grid](\n                q, k, v, sm_scale, m, o, l, #\n                q.stride(0), q.stride(1), q.stride(2), q.stride(3),  #\n                k.stride(0), k.stride(1), k.stride(2), k.stride(3),  #\n                v.stride(0), v.stride(1), v.stride(2), v.stride(3),  #\n                o.stride(0), o.stride(1), o.stride(2), o.stride(3),  #\n                q.shape[0], q.shape[1], k.shape[1], #\n                q.shape[2],  #\n                q_round_len,\n                k.shape[2],\n                sliding_window_offset,\n                sliding_window_size,\n                BLOCK_DMODEL=Lk,  #\n                END=end,\n                INIT=init,\n                BLOCK_M=_BLOCK_M,\n                BLOCK_N=_BLOCK_N,\n                SLIDING_WINDOW=(sliding_window is not None),\n                COMPLEMENT_SLIDING_WINDOW=complement_sliding_window,\n                num_warps=4,\n                num_stages=4\n            )\n\n    if end:\n        o = o[:, :, :q.shape[2], :].contiguous().to(q.dtype)\n\n    return o, m, l\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel and its associated wrapper function, `destindex_copy_kv`, are designed to copy values from source tensors (`KV_nope` and `KV_rope`) to destination tensors (`O_nope` and `O_rope`) based on the indices provided in `DestLoc`. The `_fwd_kernel_destindex_copy_kv` kernel function is executed for each element in the sequence, determined by `DestLoc`, and uses Triton's `tl.load` and `tl.store` operations to perform the copying based on strides calculated for batch, head, and dimension. The main logic includes loading values from the source at the current index, fetching the destination index from `DestLoc`, and then storing the values to the appropriate position in the destination tensor. The wrapper function prepares the parameters for the kernel call, ensuring all necessary strides and tensor shapes are passed correctly. The dimensions are aligned to the next power of two for efficient memory access in Triton.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(KV_nope_ptr, KV_rope_ptr, O_nope_ptr, O_rope_ptr, DestLoc_ptr,\n                                  batch_stride, head_stride, dim_stride, seq_len,\n                                  BLOCK_SIZE: tl.constexpr):\n    # Get the program ID\n    pid = tl.program_id(0)\n    \n    # Compute the index for each element in the sequence\n    index = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we do not go out of bounds\n    mask = index < seq_len\n    \n    # Load destination indices\n    dest_index = tl.load(DestLoc_ptr + index, mask=mask, other=0)\n    \n    # Compute the offsets for each tensor\n    offset_kv_nope = index * dim_stride\n    offset_kv_rope = index * dim_stride\n    offset_o_nope = dest_index * dim_stride\n    offset_o_rope = dest_index * dim_stride\n    \n    # Load data from source tensors\n    kv_nope_val = tl.load(KV_nope_ptr + offset_kv_nope, mask=mask)\n    kv_rope_val = tl.load(KV_rope_ptr + offset_kv_rope, mask=mask)\n    \n    # Store data to destination tensors\n    tl.store(O_nope_ptr + offset_o_nope, kv_nope_val, mask=mask)\n    tl.store(O_rope_ptr + offset_o_rope, kv_rope_val, mask=mask)\n\ndef destindex_copy_kv(KV_nope, KV_rope, O_nope, O_rope, DestLoc, seq_len, block_size=128):\n    # Ensure that the dimensions are aligned to the next power of two\n    assert KV_nope.shape == KV_rope.shape\n    assert O_nope.shape == O_rope.shape\n    assert KV_nope.shape[0] == DestLoc.shape[0]\n    \n    # Calculate strides\n    batch_stride = KV_nope.stride(0)\n    head_stride = KV_nope.stride(1)\n    dim_stride = KV_nope.stride(2)\n    \n    # Launch the kernel\n    grid = (triton.cdiv(seq_len, block_size),)\n    _fwd_kernel_destindex_copy_kv[grid](\n        KV_nope, KV_rope, O_nope, O_rope, DestLoc,\n        batch_stride, head_stride, dim_stride, seq_len,\n        BLOCK_SIZE=block_size\n    )\n\n# Example usage:\n# KV_nope, KV_rope, O_nope, O_rope, DestLoc should be Triton-compatible tensors\n# seq_len is the length of the sequence\n# destindex_copy_kv(KV_nope, KV_rope, O_nope, O_rope, DestLoc, seq_len)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    KV_nope,\n    KV_rope,\n    Dest_loc,\n    O_nope,\n    O_rope,\n    stride_kv_nope_bs,\n    stride_kv_nope_h,\n    stride_kv_nope_d,\n    stride_kv_rope_bs,\n    stride_kv_rope_h,\n    stride_kv_rope_d,\n    stride_o_nope_bs,\n    stride_o_nope_h,\n    stride_o_nope_d,\n    stride_o_rope_bs,\n    stride_o_rope_h,\n    stride_o_rope_d,\n    kv_nope_head_num,\n    kv_rope_head_num,\n    BLOCK_DMODEL_NOPE: tl.constexpr,\n    BLOCK_DMODEL_ROPE: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    offs_d_nope = tl.arange(0, BLOCK_DMODEL_NOPE)\n    offs_d_rope = tl.arange(0, BLOCK_DMODEL_ROPE)\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    kv_nope_ptrs = KV_nope + cur_index * stride_kv_nope_bs + stride_kv_nope_d * offs_d_nope[None, :]\n    kv_rope_ptrs = KV_rope + cur_index * stride_kv_rope_bs + stride_kv_rope_d * offs_d_rope[None, :]\n\n    o_nope_ptrs = O_nope + dest_index * stride_o_nope_bs + stride_o_nope_d * offs_d_nope[None, :]\n    o_rope_ptrs = O_rope + dest_index * stride_o_rope_bs + stride_o_rope_d * offs_d_rope[None, :]\n\n    kv_nope = tl.load(kv_nope_ptrs)\n    kv_rope = tl.load(kv_rope_ptrs)\n\n    tl.store(o_nope_ptrs, kv_nope)\n    tl.store(o_rope_ptrs, kv_rope)\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_kv(KV_nope, KV_rope, DestLoc, O_nope, O_rope):\n    seq_len = DestLoc.shape[0]\n    kv_nope_head_num = KV_nope.shape[1]\n    kv_rope_head_num = KV_rope.shape[1]\n\n    kv_nope_head_dim = KV_nope.shape[2]\n    kv_rope_head_dim = KV_rope.shape[2]\n\n    aligned_d_nope = triton.next_power_of_2(kv_nope_head_dim) # \u8c03\u6574\u4e3a2\u7684\u5e42\u6b21\u65b9\n    aligned_d_rope = triton.next_power_of_2(kv_rope_head_dim) # \u8c03\u6574\u4e3a2\u7684\u5e42\u6b21\u65b9\n\n    assert KV_nope.shape[1] == O_nope.shape[1]\n    assert KV_nope.shape[2] == O_nope.shape[2]\n    assert KV_rope.shape[1] == O_rope.shape[1]\n    assert KV_rope.shape[2] == O_rope.shape[2]\n    grid = (seq_len,)\n    num_warps = 2\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        KV_nope,\n        KV_rope,\n        DestLoc,\n        O_nope,\n        O_rope,\n        KV_nope.stride(0),\n        KV_nope.stride(1),\n        KV_nope.stride(2),\n        KV_rope.stride(0),\n        KV_rope.stride(1),\n        KV_rope.stride(2),\n        O_nope.stride(0),\n        O_nope.stride(1),\n        O_nope.stride(2),\n        O_rope.stride(0),\n        O_rope.stride(1),\n        O_rope.stride(2),\n        kv_nope_head_num,\n        kv_rope_head_num,\n        # BLOCK_DMODEL_NOPE=kv_nope_head_dim,\n        # BLOCK_DMODEL_ROPE=kv_rope_head_dim,\n        BLOCK_DMODEL_NOPE=aligned_d_nope,  # \u4f20\u9012\u5bf9\u9f50\u540e\u7684\u503c\n        BLOCK_DMODEL_ROPE=aligned_d_rope,  # \u4f20\u9012\u5bf9\u9f50\u540e\u7684\u503c\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The function `fused_native_layer_norm` normalizes input tensors using the Triton kernel `triton_red_fused_native_layer_norm_0`. It processes three input tensors: `primals_1`, `primals_2`, and `primals_3`, where `primals_3` is the main input tensor with shape `(S, D)`. The function computes a layer normalization which includes computing the mean and variance of the input across a specified dimension, and then normalizes the input tensor accordingly. It uses two auxiliary buffers `buf0` and `buf3` for intermediate results and `buf4` for the final normalized output tensor. The normalized result (`buf4`), the unchanged input tensor (`primals_3`), the mean (`buf0`), and an auxiliary output (`buf3`) are returned.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef triton_red_fused_native_layer_norm_0(\n    primals_3_ptr, primals_1_ptr, primals_2_ptr,\n    buf0_ptr, buf3_ptr, buf4_ptr,\n    S, D,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    \n    # Compute row index\n    row_idx = pid\n    col_idx = tl.arange(0, BLOCK_SIZE)\n    \n    # Compute offset for the current row\n    offset = row_idx * D + col_idx\n    \n    # Load input data\n    primals_3 = tl.load(primals_3_ptr + offset, mask=col_idx < D, other=0.0)\n    \n    # Compute mean\n    mean = tl.sum(primals_3, axis=0) / D\n    tl.store(buf0_ptr + row_idx, mean)\n    \n    # Compute variance\n    var = tl.sum((primals_3 - mean) ** 2, axis=0) / D\n    \n    # Compute normalized output\n    norm_output = (primals_3 - mean) / tl.sqrt(var + 1e-5)\n    \n    # Store normalized output\n    tl.store(buf4_ptr + offset, norm_output, mask=col_idx < D)\n    \n    # Store auxiliary output (e.g., variance or any other necessary computation)\n    tl.store(buf3_ptr + row_idx, var)\n\n\nimport torch\n\ndef fused_native_layer_norm(primals_1, primals_2, primals_3):\n    S, D = primals_3.shape\n    \n    # Allocate buffers\n    buf0 = torch.empty((S,), dtype=primals_3.dtype, device=primals_3.device)\n    buf3 = torch.empty((S,), dtype=primals_3.dtype, device=primals_3.device)\n    buf4 = torch.empty_like(primals_3)\n    \n    # Launch the Triton kernel\n    BLOCK_SIZE = 1024  # Example block size, tune this for your hardware\n    grid = (S,)\n    \n    triton_red_fused_native_layer_norm_0[grid](\n        primals_3, primals_1, primals_2,\n        buf0, buf3, buf4,\n        S, D,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return buf4, primals_3, buf0, buf3\n\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch._C import _cuda_getCurrentRawStream as get_raw_stream\nfrom torch._inductor.runtime import triton_helpers\nfrom torch._inductor.runtime.triton_helpers import libdevice\n\nempty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\nreinterpret_tensor = torch.ops.inductor._reinterpret_tensor\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 1024,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n        triton.Config(\n            {\n                \"XBLOCK\": 1,\n                \"RBLOCK\": 2048,\n            },\n            num_stages=1,\n            num_warps=8,\n        ),\n    ],\n    key=[\"xnumel\", \"rnumel\"],\n)\n@triton.jit\ndef triton_red_fused_native_layer_norm_0(\n    in_out_ptr0,\n    in_ptr0,\n    in_ptr1,\n    in_ptr2,\n    out_ptr0,\n    out_ptr1,\n    xnumel,\n    rnumel,\n    XBLOCK: tl.constexpr,\n    RBLOCK: tl.constexpr,\n):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"\n        ).to(tl.float32)\n        tmp1 = tmp0.to(tl.float32)\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(\n            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0\n        )\n        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)\n        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)\n        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)\n    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(\n        tmp3_mean, tmp3_m2, tmp3_weight, 1\n    )\n    tmp3 = tmp3_tmp[:, None]\n    tmp4 = tmp4_tmp[:, None]\n    tmp5 = tmp5_tmp[:, None]\n    tl.store(out_ptr0 + (x0), tmp3, None)\n    tmp6 = rnumel\n    tmp7 = tmp4 / tmp6\n    tmp8 = 1e-05\n    tmp9 = tmp7 + tmp8\n    tmp10 = libdevice.rsqrt(tmp9)\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + (x0), tmp10, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp11 = tl.load(\n            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"\n        ).to(tl.float32)\n        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(\n            tl.float32\n        )\n        tmp12 = tmp11.to(tl.float32)\n        tmp13 = tmp12 - tmp3\n        tmp14 = tmp13 * tmp10\n        tmp16 = tmp15.to(tl.float32)\n        tmp17 = tmp14 * tmp16\n        tmp19 = tmp18.to(tl.float32)\n        tmp20 = tmp17 + tmp19\n        tmp21 = tmp20.to(tl.float32)\n        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)\n\ndef fused_native_layer_norm(primals_1, primals_2, primals_3):\n    S, D = primals_3.shape\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0)\n        buf0 = empty_strided_cuda((S, 1), (1, 1), torch.float32)\n        buf1 = empty_strided_cuda((S, 1), (1, S), torch.float32)\n        buf3 = reinterpret_tensor(buf1, (S, 1), (1, 1), 0)\n        del buf1  # reuse\n        buf4 = empty_strided_cuda((S, D), (D, 1), torch.bfloat16)\n        stream0 = get_raw_stream(0)\n        grid = lambda META: (triton.cdiv(S, META[\"XBLOCK\"]),)\n        triton_red_fused_native_layer_norm_0[grid](\n            buf3, primals_3, primals_1, primals_2, buf0, buf4, S, D\n        )\n    return (\n        buf4,\n        primals_3,\n        buf0,\n        buf3,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The Triton kernel named `spinning_lock_kernel` is designed to perform a parallel reduction across multiple blocks. \n    It uses atomic operations to implement a locking mechanism for synchronization between threads. The kernel accumulates \n    values from a temporary storage `P` into an accumulator `acc` and stores the result in `C`. The main loop iterates \n    for 9 cycles and performs locking on specific threads using a 'compare and swap' operation on `locks` to ensure that \n    only one thread performs the addition at a time. The function `spinning_lock` sets up the grid and launches this \n    kernel using specified block sizes.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef spinning_lock_kernel(P, acc, C, locks, num_blocks, num_elements, **meta):\n    pid = tl.program_id(0)\n    block_size = meta['BLOCK_SIZE']\n\n    # Initialize accumulators for each block\n    acc_block = tl.zeros([block_size], dtype=tl.float32)\n\n    # Main loop for reduction across multiple blocks\n    for _ in range(9):\n        # Each thread accumulates its portion of the data\n        start = pid * block_size\n        end = start + block_size\n        acc_block += P[start:end]\n\n        # Locking mechanism: attempt to acquire lock\n        while tl.atomic_cas(locks + pid, 0, 1) != 0:\n            pass  # Spin until the lock is acquired\n\n        # Critical section: update the global accumulator\n        acc[pid] += tl.sum(acc_block)\n\n        # Release the lock\n        tl.atomic_xchg(locks + pid, 0)\n\n    # Store the result in C\n    C[pid] = acc[pid]\n\ndef spinning_lock(P, C, num_blocks, num_elements, block_size):\n    # Initialize accumulators and locks\n    acc = triton.zeros([num_blocks], dtype=tl.float32)\n    locks = triton.zeros([num_blocks], dtype=tl.int32)\n\n    # Set up grid\n    grid = (num_blocks,)\n\n    # Launch kernel\n    spinning_lock_kernel[grid](P, acc, C, locks, num_blocks, num_elements, BLOCK_SIZE=block_size)\n\n# Example usage\nnum_blocks = 1024\nnum_elements = 1024 * 256\nblock_size = 256\n\n# Initialize input and output arrays\nP = triton.testing.rand([num_elements], dtype=tl.float32)\nC = triton.zeros([num_blocks], dtype=tl.float32)\n\n# Launch the spinning lock reduction\nspinning_lock(P, C, num_blocks, num_elements, block_size)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef spinning_lock_kernel(P, C, locks, num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n    pid = tl.program_id(0)\n    pid_m = pid // num_sms\n    pid_n = pid % num_sms\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)  # Assuming acc initialization\n\n    # Perform reduction for every kth pid\n    for iters in range(1, 10):\n        if (pid % k == 0):\n            next_pid = pid + 1\n\n            while next_pid < pid + k and next_pid < num_sms:\n                while tl.atomic_cas(locks + next_pid, 1, 1) != 1:\n                    pass\n\n                rm1 = tl.arange(0, BLOCK_SIZE_M)\n                rn1 = tl.arange(0, BLOCK_SIZE_N)\n                P_ = P + next_pid * BLOCK_SIZE_M * BLOCK_SIZE_N + rm1[:, None] * BLOCK_SIZE_N + rn1[None, :]\n                acc1 = tl.load(P_)\n                acc += acc1\n\n                next_pid += 1\n              \n        # Store results using temporary storage P for every k-1 pids\n        else:\n            rm1 = tl.arange(0, BLOCK_SIZE_M)\n            rn1 = tl.arange(0, BLOCK_SIZE_N)\n            P_ = P + pid * BLOCK_SIZE_M * BLOCK_SIZE_N + rm1[:, None] * BLOCK_SIZE_N + rn1[None, :]\n            tl.store(P_, acc)\n            tl.atomic_xchg(locks + pid, 1)\n\n        # Store final results in C\n        rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n        mask = (rm < M)[:, None] & (rn < N)[None, :]\n        tl.store(C_, acc, mask=mask)\n\n\ndef spinning_lock(P, C, locks, num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M, BLOCK_SIZE_N):\n    grid = (num_sms,)\n    spinning_lock_kernel[grid](\n        P, C, locks, num_sms, k, M, N, stride_cm, stride_cn, BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N,)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel `rmsnorm_triton` performs RMS normalization on a 3D tensor `x`. It takes in input tensors `x` and `rms_weights`, and outputs the normalized tensor `out`. The kernel normalizes each element in the `K` dimension by dividing it by the root mean square (RMS) calculated over that dimension. The normalized values are then scaled by the corresponding elements in `rms_weights`. The `rmsnorm_wrapper` is a Python function wrapping this kernel call, setting up necessary parameters and handling the kernel launch.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_weights_ptr, out_ptr, M, N, K, BLOCK_SIZE: tl.constexpr):\n    # Calculate the grid and block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    # Calculate the offsets for the current block\n    offset_m = pid_m * BLOCK_SIZE\n    offset_n = pid_n * BLOCK_SIZE\n    \n    # Create a block of data from the input tensor\n    x_block = tl.load(x_ptr + offset_m * N * K + offset_n * K + tl.arange(0, BLOCK_SIZE))\n    \n    # Compute the square of the elements\n    x_squared = x_block * x_block\n    \n    # Sum the squared values across the K dimension\n    sum_x_squared = tl.sum(x_squared, axis=0)\n    \n    # Compute the RMS\n    rms = tl.sqrt(sum_x_squared / K)\n    \n    # Normalize the block by the RMS\n    normalized_block = x_block / rms\n    \n    # Load the RMS weights\n    rms_weights = tl.load(rms_weights_ptr + offset_n * K + tl.arange(0, BLOCK_SIZE))\n    \n    # Scale the normalized values by the RMS weights\n    scaled_block = normalized_block * rms_weights\n    \n    # Store the result in the output tensor\n    tl.store(out_ptr + offset_m * N * K + offset_n * K + tl.arange(0, BLOCK_SIZE), scaled_block)\n\nimport torch\n\ndef rmsnorm_wrapper(x, rms_weights):\n    # Ensure input tensors are contiguous\n    x = x.contiguous()\n    rms_weights = rms_weights.contiguous()\n    \n    # Get the dimensions of the input tensor\n    M, N, K = x.shape\n    \n    # Allocate output tensor\n    out = torch.empty_like(x)\n    \n    # Define block size\n    BLOCK_SIZE = 128  # Adjust this based on your GPU architecture\n    \n    # Launch the kernel\n    grid = (M // BLOCK_SIZE, N // BLOCK_SIZE)\n    rmsnorm_triton[grid](x, rms_weights, out, M, N, K, BLOCK_SIZE)\n    \n    return out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, out_ptr,\n                   stride_x_batch, stride_x_m, stride_x_k,\n                   stride_rms_w,\n                   stride_out_batch, stride_out_m, stride_out_k,\n                   N_SIZE: tl.constexpr, eps: tl.constexpr, BLOCK_N_SIZE: tl.constexpr):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n\n    # parallel at m dimension\n    offset_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_n_size = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    # parallel between blocks\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=x_ptr_mask, other=0.)  # careful stride_x_k\n        xf = x.to(tl.float32)\n        var += xf*xf\n    var = tl.sum(var, axis=0) / N_SIZE  # reduce between wrap\n    std = tl.sqrt(var + eps)\n\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n\n        rms_w_offset = tl.load(rms_w_ptr + offset_n * stride_rms_w, mask=x_ptr_mask)\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=x_ptr_mask, other=0.)\n\n        x_new = x / std\n        out = x_new * rms_w_offset\n        out_offset = pid_batch * stride_out_batch + pid_m * stride_out_m + offset_n * stride_out_k\n        tl.store(out_ptr + out_offset, out, mask=x_ptr_mask)\n\n\ndef rmsnorm_wrapper(x, rms_weights, eps=1e-6):\n    batch, M, K = x.shape\n    out = torch.empty_like(x)\n    rmsnorm_triton[(batch, M,)](x, rms_weights, out,\n                                *x.stride(),\n                                *rms_weights.stride(),  # 1\n                                *out.stride(),\n                                N_SIZE=K, eps=eps, BLOCK_N_SIZE=4096,\n                                num_warps=16\n                                )\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel, named `chunk_simple_gla_bwd_kernel_dqkg`, is designed to compute gradients for Q, K, V, and a gating term in a transformer-like architecture. It processes input tensors `q`, `k`, `v`, `g`, `h`, and `do` along with other parameters. The kernel utilizes Triton's automatic tuning feature to choose between two configurations with different warp numbers. Outputs are stored in `dq`, `dk`, and `dg`. The kernel iteratively computes intermediate results like `b_dq`, `b_dk`, and `b_dg` using matrix multiplications and reductions. The host function `chunk_bwd_dqkg_fn` prepares the data, defines grid dimensions, and launches the Triton kernel with appropriate parameters.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'num_warps': 4}),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'num_warps': 8}),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(\n    q_ptr, k_ptr, v_ptr, g_ptr, h_ptr, do_ptr,\n    dq_ptr, dk_ptr, dg_ptr,\n    M, N, K,\n    stride_qm, stride_qk,\n    stride_km, stride_kn,\n    stride_vm, stride_vk,\n    stride_gm, stride_gk,\n    stride_hm, stride_hk,\n    stride_dom, stride_don,\n    stride_dqm, stride_dqk,\n    stride_dkm, stride_dkn,\n    stride_dgm, stride_dgk,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    # Program IDs\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Create pointers for the current block\n    q = tl.load(q_ptr + pid_m * BLOCK_SIZE_M * stride_qm + tl.arange(0, BLOCK_SIZE_M)[:, None] * stride_qk)\n    k = tl.load(k_ptr + pid_n * BLOCK_SIZE_N * stride_kn + tl.arange(0, BLOCK_SIZE_N) * stride_km)\n    v = tl.load(v_ptr + pid_n * BLOCK_SIZE_N * stride_vk + tl.arange(0, BLOCK_SIZE_N) * stride_vm)\n    g = tl.load(g_ptr + pid_m * BLOCK_SIZE_M * stride_gm + tl.arange(0, BLOCK_SIZE_M) * stride_gk)\n    h = tl.load(h_ptr + pid_m * BLOCK_SIZE_M * stride_hm + tl.arange(0, BLOCK_SIZE_M) * stride_hk)\n    do = tl.load(do_ptr + pid_m * BLOCK_SIZE_M * stride_dom + tl.arange(0, BLOCK_SIZE_M) * stride_don)\n\n    # Compute gradients\n    b_dq = tl.zeros((BLOCK_SIZE_M, K), dtype=tl.float32)\n    b_dk = tl.zeros((BLOCK_SIZE_N, K), dtype=tl.float32)\n    b_dg = tl.zeros((BLOCK_SIZE_M, K), dtype=tl.float32)\n\n    for k_idx in range(0, K, BLOCK_SIZE_K):\n        # Load a block of the matrix\n        q_block = tl.load(q_ptr + pid_m * BLOCK_SIZE_M * stride_qm + k_idx * stride_qk)\n        k_block = tl.load(k_ptr + pid_n * BLOCK_SIZE_N * stride_kn + k_idx * stride_km)\n        v_block = tl.load(v_ptr + pid_n * BLOCK_SIZE_N * stride_vk + k_idx * stride_vm)\n\n        # Perform matrix multiplication and reductions\n        b_dq += tl.dot(q_block, k_block)\n        b_dk += tl.dot(k_block, v_block)\n        b_dg += tl.dot(v_block, q_block)\n\n    # Store results\n    tl.store(dq_ptr + pid_m * BLOCK_SIZE_M * stride_dqm + tl.arange(0, BLOCK_SIZE_M) * stride_dqk, b_dq)\n    tl.store(dk_ptr + pid_n * BLOCK_SIZE_N * stride_dkn + tl.arange(0, BLOCK_SIZE_N) * stride_dkm, b_dk)\n    tl.store(dg_ptr + pid_m * BLOCK_SIZE_M * stride_dgm + tl.arange(0, BLOCK_SIZE_M) * stride_dgk, b_dg)\n\n\ndef chunk_bwd_dqkg_fn(q, k, v, g, h, do):\n    # Extract shapes and strides\n    M, K = q.shape\n    _, N = k.shape\n\n    # Allocate output tensors\n    dq = torch.empty_like(q)\n    dk = torch.empty_like(k)\n    dg = torch.empty_like(g)\n\n    # Define grid dimensions\n    grid = (triton.cdiv(M, 128), triton.cdiv(N, 64))\n\n    # Launch the Triton kernel\n    chunk_simple_gla_bwd_kernel_dqkg[grid](\n        q, k, v, g, h, do,\n        dq, dk, dg,\n        M, N, K,\n        q.stride(0), q.stride(1),\n        k.stride(0), k.stride(1),\n        v.stride(0), v.stride(1),\n        g.stride(0), g.stride(1),\n        h.stride(0), h.stride(1),\n        do.stride(0), do.stride(1),\n        dq.stride(0), dq.stride(1),\n        dk.stride(0), dk.stride(1),\n        dg.stride(0), dg.stride(1)\n    )\n\n    return dq, dk, dg\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8)\n    ],\n    key=[\"BT\", \"BK\", \"BV\"],\n)\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(\n    q,\n    k,\n    v,\n    h,\n    g,\n    do,\n    dh,\n    dq,\n    dk,\n    dg,\n    s_k_h,\n    s_k_t,\n    s_v_h,\n    s_v_t,\n    s_h_h,\n    s_h_t,\n    scale,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr\n):\n    # Kernel implementation\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    last_idx = min(i_t * BT + BT, T) - 1\n    b_g_last = tl.load(g + i_bh * T + last_idx)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg_last = tl.zeros([1,], dtype=tl.float32)\n    b_dg = tl.zeros([BT,], dtype=tl.float32)\n\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        # [BV, BK]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_dg_last += (tl.sum(b_h * b_dh))\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg_last *= tl.exp(b_g_last)\n    b_dq = b_dq * tl.exp(b_g)[:, None] * scale\n    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale * tl.exp(b_g[:, None] - b_g[None, :]), 0)\n    b_ds = b_ds.to(b_k.dtype)\n    # [BT, BK]\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)\n    # (SY 09/21) revcumsum in a separate kernel due to strange triton compiler issue\n    # b_dg = tl.dot(tl.where(o_i[:, None] <= o_i[None, :], 1., 0.), b_dg, allow_tf32=False) + b_dg_last)\n    b_dg = tl.where(o_i < min(BT, T-i_t*BT) - 1, b_dg, b_dg + b_dg_last)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_k*n_bh + i_bh) * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\n\ndef chunk_bwd_dqkg_fn(do, q, k, v, g, h, dh, scale):\n    # Kernel call\n    B, H, T, K, V = *k.shape, v.shape[-1]\n    BT = 64\n    BK = min(triton.next_power_of_2(K), 64)\n    BV = min(triton.next_power_of_2(V), 64)\n    NT, NK = triton.cdiv(T, BT), triton.cdiv(K, BK)\n    grid = (NK, NT, B * H)\n    dq = torch.empty_like(q)\n    dk = torch.empty_like(k)\n    dg = torch.empty(NK, B, H, T, dtype=torch.float32, device=g.device).fill_(-1e9)\n    chunk_simple_gla_bwd_kernel_dqkg[grid](\n        q, k, v, h, g, do, dh, dq, dk, dg,\n        q.stride(1), q.stride(2),\n        v.stride(1), v.stride(2),\n        dh.stride(1), dh.stride(2),\n        scale,\n        T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT\n    )\n    return dq, dk, dg\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The code implements two Triton kernels named `fifth_order_fwd` and `fifth_order_bwd` for forward and backward computations of fifth-order spherical harmonics projections. These kernels operate on 3D input coordinate tensors to compute spherical harmonics and their gradients. The main entry point is the `FifthOrderSphericalHarmonic` class, which provides static methods for forward and backward passes. The `forward` method computes the harmonics projections, and the `backward` method computes gradients w.r.t. input coordinates. The kernels use Triton's parallelization to distribute computation across blocks, which are determined by a block size. The class saves input coordinates during the forward pass to use in the backward computation.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Kernel for forward computation of fifth-order spherical harmonics\n@triton.jit\ndef fifth_order_fwd(coords_ptr, output_ptr, num_coords, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Offsets for the block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load coordinates\n    x = tl.load(coords_ptr + offsets, mask=offsets < num_coords, other=0.0)\n    y = tl.load(coords_ptr + offsets + num_coords, mask=offsets < num_coords, other=0.0)\n    z = tl.load(coords_ptr + offsets + 2 * num_coords, mask=offsets < num_coords, other=0.0)\n\n    # Compute spherical harmonics for fifth order\n    # Placeholder for actual computation\n    result = x**5 + y**5 + z**5  # Replace with actual spherical harmonics computation\n\n    # Store result\n    tl.store(output_ptr + offsets, result, mask=offsets < num_coords)\n\n\n# Kernel for backward computation (gradient w.r.t. input coordinates)\n@triton.jit\ndef fifth_order_bwd(coords_ptr, grad_output_ptr, grad_input_ptr, num_coords, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Offsets for the block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load coordinates and gradient output\n    x = tl.load(coords_ptr + offsets, mask=offsets < num_coords, other=0.0)\n    y = tl.load(coords_ptr + offsets + num_coords, mask=offsets < num_coords, other=0.0)\n    z = tl.load(coords_ptr + offsets + 2 * num_coords, mask=offsets < num_coords, other=0.0)\n    grad_output = tl.load(grad_output_ptr + offsets, mask=offsets < num_coords, other=0.0)\n\n    # Compute gradients\n    # Placeholder for actual gradient computation\n    grad_x = 5 * x**4 * grad_output  # Replace with actual gradient computation\n    grad_y = 5 * y**4 * grad_output\n    grad_z = 5 * z**4 * grad_output\n\n    # Store gradients\n    tl.store(grad_input_ptr + offsets, grad_x, mask=offsets < num_coords)\n    tl.store(grad_input_ptr + offsets + num_coords, grad_y, mask=offsets < num_coords)\n    tl.store(grad_input_ptr + offsets + 2 * num_coords, grad_z, mask=offsets < num_coords)\n\n\nclass FifthOrderSphericalHarmonic:\n    @staticmethod\n    def forward(coords):\n        num_coords = coords.shape[0]\n        output = torch.empty(num_coords, dtype=coords.dtype, device=coords.device)\n\n        # Launch the forward kernel\n        BLOCK_SIZE = 128\n        grid = lambda META: (triton.cdiv(num_coords, META['BLOCK_SIZE']),)\n        fifth_order_fwd[grid](coords, output, num_coords, BLOCK_SIZE=BLOCK_SIZE)\n\n        # Save input coordinates for backward computation\n        FifthOrderSphericalHarmonic.saved_coords = coords.clone()\n\n        return output\n\n    @staticmethod\n    def backward(grad_output):\n        coords = FifthOrderSphericalHarmonic.saved_coords\n        num_coords = coords.shape[0]\n        grad_input = torch.empty_like(coords)\n\n        # Launch the backward kernel\n        BLOCK_SIZE = 128\n        grid = lambda META: (triton.cdiv(num_coords, META['BLOCK_SIZE']),)\n        fifth_order_bwd[grid](coords, grad_output, grad_input, num_coords, BLOCK_SIZE=BLOCK_SIZE)\n\n        return grad_input\n", "label": "import triton\nimport torch\nfrom triton import language as tl\nimport math\n\ndef calculate_lastdim_num_blocks(input_tensor: torch.Tensor, block_size: int) -> int:\n    \"\"\"\n    Calculate the number of blocks for a tensor, assuming we\n    stride along the last dimension, and a given block size.\n\n    The corresponding pointer arithmetic looks like this:\n\n    ```python\n    block_id = tl.program_id(0)\n    striding = tl.arange(0, block_size) * stride\n    offset = (striding + (block_size * stride * block_id))\n    ```\n\n    This function is used to work out the amount of parallel\n    work that needs to be done, given as the total number of\n    elements divided by the last dimension stride, and a specified\n    block size that will then divvy up the work.\n\n    Parameters\n    ----------\n    input_tensor : torch.Tensor\n        Torch N-d tensor to operate over.\n\n    Returns\n    -------\n    int\n        Number of blocks of work, given a block size.\n    \"\"\"\n    # get the stride of the last dimension\n    stride = input_tensor.stride(-2)\n    numel = input_tensor.numel()\n    total_blocks = math.ceil(numel / stride)\n    return total_blocks\n\n\n# This kernel computes the fifth order spherical harmonics projections.\n@triton.jit\ndef fifth_order_fwd(\n    coord_ptr: tl.tensor,  # Pointer to input coordinates tensor.\n    output_ptr: tl.tensor,  # Pointer to output tensor.\n    block_size: tl.constexpr,  # Number of elements in each block.\n    coord_numel: tl.constexpr,  # Total number of elements in the coordinates tensor.\n    output_numel: tl.constexpr,  # Total number of elements in the output tensor.\n    col_offset: tl.constexpr,  # Offset for the output tensor.\n    output_stride: tl.constexpr,  # Stride of the output tensor.\n):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + (block_size * coord_stride * block_id)\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset < coord_numel)\n    y = tl.load(\n        coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 < coord_numel\n    )\n    z = tl.load(\n        coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 < coord_numel\n    )\n    # -------------------- variable and constant definitions\n    CONST000 = 1.73430461568895\n    CONST001 = 2.32681380862329\n    CONST002 = 1.60565407233314\n    CONST003 = 3.21130814466628\n    CONST004 = 3.31662479035540\n    CONST005 = 6.21867148191637\n    CONST006 = 6.21867148191637\n    CONST007 = 1.60565407233314\n    CONST009 = 11.6340690431164\n    CONST010 = 12.8452325786651\n    CONST011 = 12.4373429638327\n    CONST012 = 12.8452325786651\n    CONST013 = 13.8744369255116\n    CONST017 = 33.9852909359329\n    CONST018 = 7.35803132638072\n    CONST020 = -44.1481879582843\n    CONST021 = -41.6233107765348\n    CONST022 = -29.4321253055229\n    CONST023 = -23.2681380862329\n    CONST024 = -19.2678488679977\n    CONST025 = -19.2678488679977\n    CONST026 = -16.9926454679664\n    CONST027 = -16.9926454679664\n    CONST028 = -13.8744369255116\n    CONST029 = -16.5831239517770\n    CONST030 = 3.46860923137790\n    CONST031 = -8.49632273398321\n    CONST032 = -5.20291384706685\n    CONST033 = -3.46860923137790\n    CONST034 = -1.73430461568895\n    VAR05 = x * x * x * x * x\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR14 = y * y * y * y * y\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR23 = z * z * z * z * z\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    # -------------------- kernel implementations\n    Y00 = CONST001 * VAR05 + CONST009 * VAR24 * x + CONST023 * VAR07 * VAR26\n    Y01 = y * (CONST022 * VAR07 * z - CONST022 * VAR25 * x)\n    Y02 = (\n        CONST000 * VAR05\n        + VAR07 * (CONST028 * VAR17 + CONST033 * VAR26)\n        + x * (-CONST021 * VAR17 * VAR26 + CONST032 * VAR24)\n    )\n    Y03 = CONST027 * VAR07 * y * z + x * (CONST017 * VAR16 * z + CONST026 * VAR25 * y)\n    Y04 = (\n        CONST002 * VAR05\n        + VAR07 * (CONST003 * VAR26 + CONST025 * VAR17)\n        + x * (CONST002 * VAR24 + CONST010 * VAR15 + CONST024 * VAR17 * VAR26)\n    )\n    Y05 = (\n        CONST004 * VAR14\n        + VAR16 * (CONST029 * VAR08 + CONST029 * VAR26)\n        + y * (CONST005 * VAR06 + CONST006 * VAR24 + CONST011 * VAR08 * VAR26)\n    )\n    Y06 = (\n        CONST002 * VAR23\n        + VAR25 * (CONST003 * VAR08 + CONST024 * VAR17)\n        + z * (CONST007 * VAR06 + CONST012 * VAR15 + CONST024 * VAR08 * VAR17)\n    )\n    Y07 = VAR16 * (CONST026 * VAR08 - CONST026 * VAR26) + y * (\n        -CONST031 * VAR06 + CONST031 * VAR24\n    )\n    Y08 = (\n        CONST034 * VAR23\n        + VAR25 * (CONST013 * VAR17 + CONST030 * VAR08)\n        + z * (CONST021 * VAR08 * VAR17 - CONST032 * VAR06)\n    )\n    Y09 = y * (CONST018 * VAR06 + CONST018 * VAR24 + CONST020 * VAR08 * VAR26)\n    Y10 = CONST001 * VAR23 + CONST009 * VAR06 * z + CONST023 * VAR08 * VAR25\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (\n        output_striding + (block_size * output_stride * block_id) + col_offset\n    )\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset < output_numel)\n    tl.store(\n        output_ptr + output_row_offset + 1,\n        Y01,\n        mask=output_row_offset + 1 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 2,\n        Y02,\n        mask=output_row_offset + 2 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 3,\n        Y03,\n        mask=output_row_offset + 3 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 4,\n        Y04,\n        mask=output_row_offset + 4 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 5,\n        Y05,\n        mask=output_row_offset + 5 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 6,\n        Y06,\n        mask=output_row_offset + 6 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 7,\n        Y07,\n        mask=output_row_offset + 7 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 8,\n        Y08,\n        mask=output_row_offset + 8 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 9,\n        Y09,\n        mask=output_row_offset + 9 < output_numel,\n    )\n    tl.store(\n        output_ptr + output_row_offset + 10,\n        Y10,\n        mask=output_row_offset + 10 < output_numel,\n    )\n\n\n# This kernel computes the gradients of the fifth order spherical harmonics projections.\n@triton.jit\ndef fifth_order_bwd(\n    coord_ptr: tl.tensor,  # Pointer to input coordinates tensor.\n    coord_grad_ptr: tl.tensor,  # Pointer to the gradient output tensor.\n    sph_grad_ptr: tl.tensor,  # Pointer to the gradient of the spherical harmonic.\n    block_size: tl.constexpr,  # Number of elements in each block.\n    coord_numel: tl.constexpr,  # Total number of elements in the coordinates tensor.\n    output_numel: tl.constexpr,  # Total number of elements in the output tensor.\n    col_offset: tl.constexpr,  # Offset for the output tensor.\n    output_stride: tl.constexpr,  # Stride of the output tensor.\n):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + (block_size * coord_stride * block_id)\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset < coord_numel)\n    y = tl.load(\n        coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 < coord_numel\n    )\n    z = tl.load(\n        coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 < coord_numel\n    )\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (\n        output_striding + (block_size * output_stride * block_id) + col_offset\n    )\n    g_0 = tl.load(\n        sph_grad_ptr + output_row_offset, mask=output_row_offset < output_numel\n    )\n    g_1 = tl.load(\n        sph_grad_ptr + output_row_offset + 1, mask=output_row_offset + 1 < output_numel\n    )\n    g_2 = tl.load(\n        sph_grad_ptr + output_row_offset + 2, mask=output_row_offset + 2 < output_numel\n    )\n    g_3 = tl.load(\n        sph_grad_ptr + output_row_offset + 3, mask=output_row_offset + 3 < output_numel\n    )\n    g_4 = tl.load(\n        sph_grad_ptr + output_row_offset + 4, mask=output_row_offset + 4 < output_numel\n    )\n    g_5 = tl.load(\n        sph_grad_ptr + output_row_offset + 5, mask=output_row_offset + 5 < output_numel\n    )\n    g_6 = tl.load(\n        sph_grad_ptr + output_row_offset + 6, mask=output_row_offset + 6 < output_numel\n    )\n    g_7 = tl.load(\n        sph_grad_ptr + output_row_offset + 7, mask=output_row_offset + 7 < output_numel\n    )\n    g_8 = tl.load(\n        sph_grad_ptr + output_row_offset + 8, mask=output_row_offset + 8 < output_numel\n    )\n    g_9 = tl.load(\n        sph_grad_ptr + output_row_offset + 9, mask=output_row_offset + 9 < output_numel\n    )\n    g_10 = tl.load(\n        sph_grad_ptr + output_row_offset + 10,\n        mask=output_row_offset + 10 < output_numel,\n    )\n    # -------------------- variable and constant definitions\n    CONST000 = 1.60565407233314\n    CONST001 = 3.00000000000000\n    CONST002 = 3.21130814466628\n    CONST003 = 1.60565407233314\n    CONST004 = 6.42261628933256\n    CONST005 = 6.42261628933256\n    CONST006 = 8.67152307844476\n    CONST007 = 8.02827036166571\n    CONST008 = 6.93721846275580\n    CONST009 = 11.6340690431164\n    CONST010 = 12.8452325786651\n    CONST011 = 6.21867148191637\n    CONST012 = 6.21867148191637\n    CONST014 = 12.4373429638327\n    CONST017 = 12.8452325786651\n    CONST018 = 13.8744369255116\n    CONST019 = 24.8746859276655\n    CONST020 = 24.8746859276655\n    CONST021 = 27.7488738510232\n    CONST024 = 29.4321253055229\n    CONST027 = 7.35803132638072\n    CONST029 = 46.5362761724657\n    CONST030 = 51.3809303146605\n    CONST031 = 51.3809303146605\n    CONST034 = 101.955872807799\n    CONST036 = -8.67152307844475\n    CONST037 = 3.46860923137790\n    CONST038 = -88.2963759165686\n    CONST039 = -83.2466215530696\n    CONST040 = -69.8044142586986\n    CONST041 = -50.9779364038993\n    CONST042 = -50.9779364038993\n    CONST043 = -46.5362761724657\n    CONST044 = -44.1481879582843\n    CONST045 = -41.6233107765348\n    CONST046 = -38.5356977359954\n    CONST047 = -38.5356977359954\n    CONST048 = -33.1662479035540\n    CONST049 = -33.9852909359329\n    CONST050 = 6.42261628933257\n    CONST051 = -33.9852909359329\n    CONST052 = -29.4321253055229\n    CONST053 = -27.7488738510232\n    CONST054 = -20.8116553882674\n    CONST055 = -19.2678488679977\n    CONST056 = -19.2678488679977\n    CONST057 = -16.9926454679664\n    CONST058 = -16.9926454679664\n    CONST059 = -13.8744369255116\n    CONST060 = -16.5831239517770\n    CONST061 = -8.49632273398321\n    CONST062 = -6.93721846275580\n    CONST063 = -5.20291384706685\n    CONST064 = -3.46860923137790\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    # -------------------- kernel implementations\n    g_x = tl.load(\n        coord_grad_ptr + coord_row_offset, mask=coord_row_offset < coord_numel\n    )\n    g_y = tl.load(\n        coord_grad_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 < coord_numel\n    )\n    g_z = tl.load(\n        coord_grad_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 < coord_numel\n    )\n    g_x += (\n        g_0 * (CONST009 * VAR06 + CONST009 * VAR24 + CONST040 * VAR08 * VAR26)\n        + g_1 * y * (CONST038 * VAR08 * z - CONST052 * VAR25)\n        + g_10 * (CONST029 * VAR07 * z + CONST043 * VAR25 * x)\n        + g_2\n        * (\n            CONST001 * VAR08 * (CONST059 * VAR17 + CONST064 * VAR26)\n            + CONST006 * VAR06\n            - CONST045 * VAR17 * VAR26\n            + CONST063 * VAR24\n        )\n        + g_3 * (CONST041 * VAR08 * y * z - CONST049 * VAR16 * z + CONST057 * VAR25 * y)\n        + g_4\n        * (\n            CONST000 * VAR24\n            + CONST001 * VAR08 * (CONST002 * VAR26 + CONST055 * VAR17)\n            + CONST007 * VAR06\n            + CONST010 * VAR15\n            + CONST056 * VAR17 * VAR26\n        )\n        + g_5 * (CONST048 * VAR16 * x + y * (CONST019 * VAR07 + CONST019 * VAR26 * x))\n        + g_6 * (CONST005 * VAR25 * x + z * (CONST004 * VAR07 + CONST046 * VAR17 * x))\n        + g_7 * (CONST049 * VAR16 * x - CONST051 * VAR07 * y)\n        + g_8 * (CONST008 * VAR25 * x + z * (CONST039 * VAR17 * x - CONST054 * VAR07))\n        + g_9 * y * (CONST024 * VAR07 + CONST038 * VAR26 * x)\n    )\n    g_y += (\n        g_1 * (CONST052 * VAR07 * z - CONST052 * VAR25 * x)\n        + g_2 * (-CONST039 * VAR26 * x * y + CONST053 * VAR07 * y)\n        + g_3 * (CONST058 * VAR07 * z + x * (CONST034 * VAR17 * z + CONST057 * VAR25))\n        + g_4 * (CONST047 * VAR07 * y + x * (CONST030 * VAR16 + CONST046 * VAR26 * y))\n        + g_5\n        * (\n            CONST001 * VAR17 * (CONST060 * VAR08 + CONST060 * VAR26)\n            + CONST011 * VAR06\n            + CONST012 * VAR24\n            + CONST014 * VAR08 * VAR26\n            - CONST060 * VAR15\n        )\n        + g_6 * (CONST046 * VAR25 * y + z * (CONST031 * VAR16 + CONST046 * VAR08 * y))\n        + g_7\n        * (\n            CONST001 * VAR17 * (CONST057 * VAR08 - CONST057 * VAR26)\n            - CONST061 * VAR06\n            + CONST061 * VAR24\n        )\n        + g_8 * (CONST021 * VAR25 * y + CONST039 * VAR08 * y * z)\n        + g_9 * (CONST027 * VAR06 + CONST027 * VAR24 + CONST044 * VAR08 * VAR26)\n    )\n    g_z += (\n        g_0 * (CONST029 * VAR25 * x + CONST043 * VAR07 * z)\n        + g_1 * y * (-CONST038 * VAR26 * x + CONST052 * VAR07)\n        + g_10 * (CONST009 * VAR06 + CONST009 * VAR24 + CONST040 * VAR08 * VAR26)\n        + g_2 * (CONST062 * VAR07 * z + x * (-CONST039 * VAR17 * z + CONST054 * VAR25))\n        + g_3 * (CONST058 * VAR07 * y + x * (CONST042 * VAR26 * y - CONST049 * VAR16))\n        + g_4 * (CONST005 * VAR07 * z + x * (CONST046 * VAR17 * z + CONST050 * VAR25))\n        + g_5 * (CONST048 * VAR16 * z + y * (CONST019 * VAR08 * z + CONST020 * VAR25))\n        + g_6\n        * (\n            CONST001 * VAR26 * (CONST002 * VAR08 + CONST056 * VAR17)\n            + CONST003 * VAR06\n            + CONST007 * VAR24\n            + CONST017 * VAR15\n            + CONST056 * VAR08 * VAR17\n        )\n        + g_7 * (-CONST049 * VAR16 * z + CONST051 * VAR25 * y)\n        + g_8\n        * (\n            CONST001 * VAR26 * (CONST018 * VAR17 + CONST037 * VAR08)\n            + CONST036 * VAR24\n            + CONST045 * VAR08 * VAR17\n            - CONST063 * VAR06\n        )\n        + g_9 * y * (CONST024 * VAR25 + CONST038 * VAR08 * z)\n    )\n    tl.store(\n        coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset < coord_numel\n    )\n    tl.store(\n        coord_grad_ptr + coord_row_offset + 1,\n        g_y,\n        mask=coord_row_offset + 1 < coord_numel,\n    )\n    tl.store(\n        coord_grad_ptr + coord_row_offset + 2,\n        g_z,\n        mask=coord_row_offset + 2 < coord_numel,\n    )\n\n\nclass FifthOrderSphericalHarmonic(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx,\n        coords: torch.Tensor,  # Input coordinates tensor.\n        output_tensor: torch.Tensor | None = None,  # Output tensor placeholder.\n        mask: torch.Tensor | None = None,  # Mask tensor (not used in this implementation).\n        block_size: int = 64,  # Block size for Triton kernels.\n        col_offset: int = 0,  # Column offset for the output tensor.\n    ):\n        if not isinstance(output_tensor, torch.Tensor):\n            output_tensor = torch.empty(\n                (*coords.shape[:-1], 11), dtype=coords.dtype, device=coords.device\n            )\n        coord_numel = coords.numel()\n        output_numel = output_tensor.numel()\n        num_blocks = calculate_lastdim_num_blocks(coords, block_size)\n        fifth_order_fwd[num_blocks,](\n            coords,\n            output_tensor,\n            block_size,\n            coord_numel,\n            output_numel,\n            col_offset,\n            output_tensor.stride(-2),\n        )\n        ctx.save_for_backward(coords)\n        return output_tensor\n\n    @staticmethod\n    def backward(\n        ctx,\n        sph_grad_tensor: torch.Tensor,  # Gradient of the output tensor.\n        block_size: int = 64,  # Block size for Triton kernels.\n        col_offset: int = 0,  # Column offset for the output tensor.\n    ) -> torch.Tensor:\n        (coords,) = ctx.saved_tensors\n        coord_grad_output = torch.zeros_like(coords)\n        num_blocks = calculate_lastdim_num_blocks(coords, block_size)\n        fifth_order_bwd[num_blocks,](\n            coords,\n            coord_grad_output,\n            sph_grad_tensor,\n            block_size,\n            coords.numel(),\n            sph_grad_tensor.numel(),\n            col_offset,\n            sph_grad_tensor.stride(-2),\n        )\n        return coord_grad_output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton kernel implements a fused recurrent operation for RWKV-6 model. It is designed to handle the forward pass of a recurrent neural network computation with customizable features such as initial state usage and reverse processing. The main function `fused_recurrent_rwkv6` accepts input tensors `r`, `k`, `v`, `w`, and `u`, and optionally, a scaling factor `scale`, an `initial_state`, and a flag `output_final_state`. It returns the output tensor `o` and optionally the `final_state`. This function leverages a Triton kernel to efficiently execute the computation in parallel across specified grid dimensions.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef fused_recurrent_rwkv6_kernel(\n    r_ptr, k_ptr, v_ptr, w_ptr, u_ptr, o_ptr, final_state_ptr,\n    scale, initial_state_ptr, n, reverse, output_final_state,\n    BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    block_end = min(block_start + BLOCK_SIZE, n)\n\n    # Load initial state if provided\n    if initial_state_ptr != 0:\n        state = tl.load(initial_state_ptr)\n    else:\n        state = tl.zeros((1,), dtype=tl.float32)\n\n    for i in range(block_start, block_end):\n        idx = n - 1 - i if reverse else i\n        r = tl.load(r_ptr + idx)\n        k = tl.load(k_ptr + idx)\n        v = tl.load(v_ptr + idx)\n        w = tl.load(w_ptr + idx)\n        u = tl.load(u_ptr + idx)\n\n        # Compute the fused recurrent operation\n        state = state * w + r * tl.exp(k * scale) * v\n        output = state + u\n\n        # Store the result\n        tl.store(o_ptr + idx, output)\n\n    # Store the final state if needed\n    if output_final_state:\n        tl.store(final_state_ptr, state)\n\n# Define the Python wrapper function\ndef fused_recurrent_rwkv6(r, k, v, w, u, scale=1.0, initial_state=None, reverse=False, output_final_state=False):\n    n = r.shape[0]\n    BLOCK_SIZE = 128  # Adjust this based on your GPU capabilities\n\n    # Allocate output tensors\n    o = torch.empty_like(r)\n    final_state = torch.empty(1, dtype=r.dtype) if output_final_state else None\n\n    # Prepare pointers\n    r_ptr = r.data_ptr()\n    k_ptr = k.data_ptr()\n    v_ptr = v.data_ptr()\n    w_ptr = w.data_ptr()\n    u_ptr = u.data_ptr()\n    o_ptr = o.data_ptr()\n    initial_state_ptr = initial_state.data_ptr() if initial_state is not None else 0\n    final_state_ptr = final_state.data_ptr() if final_state is not None else 0\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(n, BLOCK_SIZE),)\n    fused_recurrent_rwkv6_kernel[grid](\n        r_ptr, k_ptr, v_ptr, w_ptr, u_ptr, o_ptr, final_state_ptr,\n        scale, initial_state_ptr, n, reverse, output_final_state,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return (o, final_state) if output_final_state else o\n\n# Example usage\nr = torch.randn(1024, dtype=torch.float32, device='cuda')\nk = torch.randn(1024, dtype=torch.float32, device='cuda')\nv = torch.randn(1024, dtype=torch.float32, device='cuda')\nw = torch.randn(1024, dtype=torch.float32, device='cuda')\nu = torch.randn(1024, dtype=torch.float32, device='cuda')\ninitial_state = torch.tensor([0.0], dtype=torch.float32, device='cuda')\n\no, final_state = fused_recurrent_rwkv6(r, k, v, w, u, scale=0.5, initial_state=initial_state, reverse=False, output_final_state=True)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel(\n    q, k, v, w, u, o, h0, ht, s_k_h, s_v_h, scale, B: tl.constexpr, H: tl.constexpr, T: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, REVERSE: tl.constexpr,\n):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T-1) * K if REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T-1) * K if REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T-1) * V if REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + ((T-1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T-1) * K if REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n\n    mask_bk = (i_k * BK + tl.arange(0, BK)) < K\n    mask_bv = (i_v * BV + tl.arange(0, BV)) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\n\n    b_u = tl.load(p_u, mask=mask_bk, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0).to(tl.float32)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)\n\nclass FusedRecurrentRWKV6Function(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, r, k, v, w, u, scale=None, initial_state=None, output_final_state=False, reverse=False):\n        q = r\n        B, H, T, K, V = *q.shape, v.shape[-1]\n\n        BK, BV = min(triton.next_power_of_2(K), 32), min(triton.next_power_of_2(V), 32)\n        NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 1\n\n        final_state = q.new_empty(B, H, K, V) if output_final_state else None\n\n        o = q.new_empty(NK, B, H, T, V, dtype=torch.float32)\n        grid = (NV, NK, B * H)\n        fused_recurrent_rwkv6_fwd_kernel[grid](\n            q, k, v, w, u, o, initial_state, final_state,\n            k.stride(1),\n            v.stride(1),\n            scale,\n            B=B, H=H, T=T, K=K, V=V, BK=BK, BV=BV,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None,\n            REVERSE=reverse,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        o = o.sum(0)\n        ctx.save_for_backward(q, k, v, w, u, initial_state)\n        ctx.scale = scale\n        ctx.reverse = reverse\n        return o.to(q.dtype), final_state\n\ndef fused_recurrent_rwkv6(\n    r: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    u: torch.Tensor,\n    scale: float = -1,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if scale == -1:\n        scale = r.shape[-1] ** -0.5\n    o, final_state = FusedRecurrentRWKV6Function.apply(r, k, v, w, u, scale, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel function 'softmax_kernel' computes the softmax operation for each row of an input matrix 'x'. It stores the result in an output matrix. It uses parameters: 'output_ptr', 'input_ptr', 'input_row_stride', 'output_row_stride', 'n_cols', and 'BLOCK_SIZE'. The row index is derived from 'tl.program_id(axis=0)'. The row is loaded into SRAM using 'tl.load'. The maximum value in the row is found using 'tl.max' for stability during exponentiation. The numerator for the softmax is calculated by subtracting the max and applying 'tl.exp'. The denominator is the sum of the numerator values computed via 'tl.sum'. The normalized softmax result is obtained by dividing the numerator by the denominator and stored back using 'tl.store'. The wrapper function 'triton_softmax' sets up the computation by determining block size and launching the kernel for all rows.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    # Obtain the row index for this program instance\n    row_idx = tl.program_id(axis=0)\n    \n    # Compute the starting pointer for the row in input and output\n    input_row_ptr = input_ptr + row_idx * input_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    \n    # Load the row into SRAM\n    row = tl.load(input_row_ptr + tl.arange(0, n_cols), mask=tl.arange(0, n_cols) < n_cols)\n    \n    # Compute the maximum value of the row for numerical stability\n    row_max = tl.max(row, axis=0)\n    \n    # Compute the numerator by subtracting the max and applying exp\n    numerator = tl.exp(row - row_max)\n    \n    # Compute the denominator by summing the numerator\n    denominator = tl.sum(numerator, axis=0)\n    \n    # Compute the softmax result by dividing the numerator by the denominator\n    softmax_result = numerator / denominator\n    \n    # Store the result back to the output matrix\n    tl.store(output_row_ptr + tl.arange(0, n_cols), softmax_result, mask=tl.arange(0, n_cols) < n_cols)\n\ndef triton_softmax(input_matrix, BLOCK_SIZE):\n    # Determine the number of rows and columns in the input matrix\n    n_rows, n_cols = input_matrix.shape\n    \n    # Allocate output matrix\n    output_matrix = torch.empty_like(input_matrix)\n    \n    # Launch the Triton kernel for each row\n    grid = (n_rows,)\n    \n    softmax_kernel[grid](\n        output_matrix,\n        input_matrix,\n        input_matrix.stride(0),\n        output_matrix.stride(0),\n        n_cols,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return output_matrix\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(\n    output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Get the program ID\n    row_idx = tl.program_id(axis=0)\n\n    # Compute the memory offsets for this row\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    out_row_start_ptr = output_ptr + row_idx * output_row_stride\n\n    # Load the row into SRAM\n    row = tl.load(row_start_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < n_cols, other=-float('inf'))\n\n    # Compute max for numerical stability\n    row_max = tl.max(row, axis=0)\n    \n    # Subtract max from row and exponentiate\n    numerator = tl.exp(row - row_max)\n    \n    # Compute sum for normalization\n    denominator = tl.sum(numerator, axis=0)\n    \n    # Normalize\n    softmax_output = numerator / denominator\n    \n    # Store the output\n    tl.store(out_row_start_ptr + tl.arange(0, BLOCK_SIZE), softmax_output, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\ndef triton_softmax(x):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    \n    # Determine the block size\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    BLOCK_SIZE = min(BLOCK_SIZE, 1024)  \n    \n    # Launch the Triton kernel\n    grid = (n_rows,)\n    softmax_kernel[grid](\n        output, x,\n        x.stride(0), output.stride(0),\n        n_cols, BLOCK_SIZE=BLOCK_SIZE\n    )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel performs a chunked global cumulative sum (cumsum) on a 3D tensor. The main kernel function is `chunk_global_cumsum_scalar_kernel`. It is applied along the last dimension of the input tensor `s`, outputting a tensor `o` of the same shape. It computes cumsum per block of size `BT` and maintains a running total across blocks. The helper function `chunk_global_cumsum_scalar` sets up this kernel by defining the grid size and preparing an output tensor `z` to store the results.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Kernel definition\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(s_ptr, o_ptr, n_elements, BT, BLOCK_SIZE: tl.constexpr):\n    # Compute the block index and the starting index for this block\n    block_idx = tl.program_id(0)\n    start_idx = block_idx * BLOCK_SIZE\n\n    # Initialize a running total for the cumulative sum\n    running_total = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    # Iterate over the elements in chunks\n    for i in range(0, n_elements, BT):\n        # Load a chunk of data from the input tensor\n        offset = start_idx + i\n        data = tl.load(s_ptr + offset, mask=offset < n_elements, other=0.0)\n\n        # Compute the cumulative sum for this chunk\n        running_total += data\n\n        # Store the result in the output tensor\n        tl.store(o_ptr + offset, running_total, mask=offset < n_elements)\n\n# Wrapper function\ndef chunk_global_cumsum_scalar(s, BT):\n    # Get the shape of the input tensor\n    B, H, W = s.shape\n\n    # Flatten the first two dimensions to simplify the kernel launch\n    s_flat = s.view(-1, W)\n    n_elements = s_flat.shape[1]\n\n    # Prepare an output tensor of the same shape\n    o_flat = torch.empty_like(s_flat)\n\n    # Define the block size and grid size\n    BLOCK_SIZE = 1024  # You can adjust this value based on your hardware\n    grid_size = (s_flat.shape[0],)\n\n    # Launch the kernel\n    chunk_global_cumsum_scalar_kernel[grid_size](\n        s_flat, o_flat, n_elements, BT, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    # Reshape the output tensor to the original shape\n    o = o_flat.view(B, H, W)\n    return o\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a 3D tensor\n    B, H, W = 4, 3, 8  # Example dimensions\n    s = torch.rand((B, H, W), dtype=torch.float32).cuda()\n\n    # Define the chunk size\n    BT = 4\n\n    # Perform chunked global cumsum\n    o = chunk_global_cumsum_scalar(s, BT)\n\n    # Print the result\n    print(o)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=4),\n    ],\n    key=[]\n)\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(\n    s,\n    o,\n    T: tl.constexpr,\n    BT: tl.constexpr,\n):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n        b_o = tl.cumsum(b_s, axis=0) + b_z[None]\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))\n\ndef chunk_global_cumsum_scalar(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T = s.shape\n    dtype = dtype or s.dtype\n    grid = (B * H,)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_cumsum_scalar_kernel[grid](\n        s, z,\n        T=T\n    )\n    return z\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a ReLU (Rectified Linear Unit) operation implemented using the Triton library for efficient GPU execution. The main functionality is to perform an element-wise ReLU operation on input tensors. The key components include:\n            - `heuristics_for_tile_size`: Determines the tile sizes for partitioning the computation workload based on input dimensions.\n            - `heuristics_for_num_warps`: Determines the optimal number of warps (thread groups) based on the tile size.\n            - `StridedBuffer`: A custom class for handling tensors with arbitrary strides, including negative strides for operations like tensor flipping.\n            - `relu_forward_wrapper_rank_1`: A wrapper function that sets up kernel execution for 1D tensors using `relu_forward_kernel_rank_1`.\n            - `relu_forward_kernel_rank_1`: A Triton kernel that performs the ReLU operation, handling data loading, computation, and storing results back to memory.\n            ", "predict": "import triton\nimport triton.language as tl\n\ndef heuristics_for_tile_size(input_shape):\n    # Heuristic to determine tile size based on input dimensions\n    # This is a simple example, and you might want to make this more sophisticated\n    return min(1024, input_shape[0])\n\ndef heuristics_for_num_warps(tile_size):\n    # Heuristic to determine the number of warps\n    # A simple heuristic is to use more warps for larger tile sizes\n    return max(1, tile_size // 256)\n\nclass StridedBuffer:\n    def __init__(self, data, stride):\n        self.data = data\n        self.stride = stride\n\n@triton.jit\ndef relu_forward_kernel_rank_1(in_ptr, out_ptr, stride, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Block start index\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n\n    # Create a block of indices\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load data from input, applying stride\n    x = tl.load(in_ptr + offsets * stride, mask=offsets < n_elements, other=0.0)\n\n    # Apply ReLU\n    y = tl.maximum(x, 0.0)\n\n    # Store the result\n    tl.store(out_ptr + offsets * stride, y, mask=offsets < n_elements)\n\ndef relu_forward_wrapper_rank_1(input_data, output_data):\n    n_elements = input_data.shape[0]\n    stride = input_data.stride(0)\n    \n    # Determine tile size and number of warps\n    tile_size = heuristics_for_tile_size(input_data.shape)\n    num_warps = heuristics_for_num_warps(tile_size)\n\n    # Launch the kernel\n    grid = (triton.cdiv(n_elements, tile_size),)\n    relu_forward_kernel_rank_1[grid](input_data.data_ptr(), output_data.data_ptr(), stride, n_elements, BLOCK_SIZE=tile_size, num_warps=num_warps)\n\n# Example usage\nimport torch\n\ninput_tensor = torch.tensor([-1.0, 2.0, -3.0, 4.0], dtype=torch.float32, device='cuda')\noutput_tensor = torch.empty_like(input_tensor)\n\nrelu_forward_wrapper_rank_1(input_tensor, output_tensor)\n\nprint(output_tensor)  # Should print: tensor([0.0, 2.0, 0.0, 4.0], device='cuda')\n", "label": "import math\nfrom typing import Union\nimport torch\nimport triton\nfrom triton import language as tl\n\ndef heuristics_for_tile_size(max_tile_size, *sizes):\n    ndim = len(sizes)\n    tile_sizes = [0 for _ in range(ndim)]\n    for i in range(ndim):\n        size = sizes[ndim - 1 - i]\n        tile_size = min(max_tile_size, triton.next_power_of_2(size))\n        tile_sizes[ndim - 1 - i] = tile_size\n        max_tile_size = max(1, max_tile_size // tile_size)\n    return tuple(tile_sizes)\n\ndef heuristics_for_num_warps(tile_size):\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n    \n\nclass StridedBuffer:\n    \"\"\"A drop-in replacement of torch.Tensor that can be used in wrapper generated by\n    PointwiseDynamicFunction. It allows us to use a different shape, stride, data\n    pointer that that of the base tensor.\n\n    It is a kind of reinterpretation of the base tensor. We make this class since we\n    cannot get a Tensor view with negative strides via torch APIs, while we need this\n    to implement flip op.\n\n    Although generated code can accept torch.Tensor & StridedBuffer, but StridedBuffer\n    may not have all the methods as torch.Tensors do. We add some attributes & methods\n    with the same name as torch.Tensor, which are used in the generated code. But we\n    may not cover all the methods, add one if what you need is missing here.\n\n    And can also be used in triton kernels since it also has dtype & data_ptr().\n    \"\"\"\n\n    def __init__(\n        self, base: torch.Tensor, shape=None, strides=None, dtype=None, offset=0\n    ):\n        self._base = base\n        self.dtype = dtype or base.dtype\n        if offset == 0:\n            self._data_ptr = self._base.data_ptr()\n        else:\n            offset = self.dtype.itemsize * offset\n            self._data_ptr = self._base.data_ptr() + offset\n        self.shape = tuple(shape if shape is not None else self._base.shape)\n        self._strides = tuple(strides if strides is not None else self._base.stride())\n        self.device = self._base.device\n        self.ndim = len(self.shape)\n\n    def stride(self):\n        return self._strides\n\n    def size(self):\n        return self.shape\n\n    def element_size(self):\n        return self.dtype.itemsize\n\n    def numel(self):\n        return math.prod(self.shape)\n\n    def dim(self):\n        return self.ndim\n\n    def unwrap(self):\n        return self._base\n\n    def data_ptr(self):\n        return self._data_ptr\n\n\n\ndef relu_forward_wrapper_rank_1(in0: Union[torch.Tensor, StridedBuffer], /, *, out0: Union[torch.Tensor, StridedBuffer]): \n    \"\"\"Generated wrapper function with Pointwise: StridedBuffer, StridedBuffer(a1!) -> StridedBuffer(a1!)\"\"\"\n    assert in0.shape == out0.shape, 'operand shapes mismatch'\n    # task partitioning\n    shape = out0.shape\n    num_tasks = out0.numel()\n    tile_sizes = heuristics_for_tile_size(512, *shape)\n    tile_size = math.prod(tile_sizes)\n    num_tiles = math.prod(triton.cdiv(size, tile_size) for size, tile_size in zip(shape, tile_sizes))\n    num_ctas = min(65536, num_tiles)\n    tiles_per_cta = triton.cdiv(num_tiles, num_ctas)\n    num_warps = heuristics_for_num_warps(tile_size)\n    one_tile_per_cta = tiles_per_cta==1\n    grid = (num_ctas, 1, 1)\n    # kernel launch\n    in0_strides = in0.stride()\n    in0_stride_order = (0,)\n    out0_strides = out0.stride()\n    out0_stride_order = (0,)\n    with torch.cuda._DeviceGuard(in0.device.index):\n        relu_forward_kernel_rank_1[grid](\n            in0, out0,\n            in0_strides[0], # stride for in0\n            in0_stride_order[0], # stride order for in0\n            out0_strides[0], # stride for out0\n            out0_stride_order[0], # stride orderfor out0\n            shape[0], # task indexing space\n            num_tasks, # num tasks\n            tiles_per_cta=tiles_per_cta, # tiles_per_cta\n            tile_size0=tile_sizes[0],\n            one_tile_per_cta=one_tile_per_cta,\n            num_warps=num_warps,\n        )\n    return out0\n\n@triton.jit\ndef relu_forward(x):\n    return tl.where(x > 0, x, 0)\n\n@triton.jit\ndef relu_forward_kernel_rank_1(\n    in0_ptr: tl.tensor, # of tl.pointer_type\n    out0_ptr: tl.tensor, # of tl.pointer_type\n    in0_stride0: int, # strides for in0\n    in0_stride_order0: tl.constexpr, # stride order for in0\n    out0_stride0: int, # strides for out0\n    out0_stride_order0: tl.constexpr, # stride order for out0\n    s0: int, # task_space\n    num_tasks: int,\n    tiles_per_cta: int,\n    tile_size0: tl.constexpr,\n    one_tile_per_cta: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    num_tiles0 = tl.cdiv(s0, tile_size0)\n    if one_tile_per_cta: # monolitic kernel style\n        tile_id = pid\n        # pid multi index recontruction: we use c ordering, right axes changes fastest\n        tile_id0 = tile_id\n\n        # tile offsets\n        offset0 = tile_id0 * tile_size0\n        # loads\n        in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n        in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n        # compute\n        out0 = relu_forward(in0)\n\n        # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n        out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n        tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n    else: # grid-stride-loop style kernel\n        num_ctas = tl.num_programs(0)\n        for j in range(0, tiles_per_cta):\n            tile_id = pid + j * num_ctas\n            # pid multi index recontruction: we use c ordering, right axes changes fastest\n            tile_id0 = tile_id\n\n            # tile offsets\n            offset0 = tile_id0 * tile_size0\n            # loads\n            in0_bptr = tl.make_block_ptr(in0_ptr, (s0,), (in0_stride0,), (offset0,), (tile_size0,), order=(in0_stride_order0,))\n            in0 = tl.load(in0_bptr, boundary_check=(in0_stride_order0,)).to(in0_ptr.type.element_ty) # workaround the bug on bool, we should use the original pointer's dtype(instead of block pointer's)\n\n            # compute\n            out0 = relu_forward(in0)\n\n            # stores, note that store to block pointer does not automatically cast the value to the pointer's dtype\n            out0_bptr = tl.make_block_ptr(out0_ptr, (s0,), (out0_stride0,), (offset0,), (tile_size0,), order=(out0_stride_order0,))\n            tl.store(out0_bptr, out0.to(out0_bptr.type.element_ty), boundary_check=(out0_stride_order0,))\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel and its associated Python function compute the Rotary Positional Encoding (RoPE) on input tensors, commonly used in transformer models to incorporate positional information into token embeddings. The main kernel, `_rope_embedding`, takes input tensors `Q`, `cos`, and `sin` along with their respective strides and dimensions to compute the RoPE transformation. The kernel is vectorized using Triton's parallel programming model to execute efficiently on GPUs. The process involves loading segments of the input tensor `Q`, applying rotations using precomputed `cos` and `sin` values, and storing the transformed outputs back to `Q`. The `Fast_RoPE_Embedding` class acts as an autograd-compatible wrapper, providing forward and backward methods to compute RoPE embeddings and their gradients, respectively. The `fast_rope_embedding` function serves as the API to transform both query (`Q`) and key (`K`) embeddings.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _rope_embedding(Q_ptr, cos_ptr, sin_ptr, stride_qz, stride_qh, stride_qm, stride_qk, \n                    stride_cos, stride_sin, n_heads, seq_len, d_head, BLOCK_SIZE: tl.constexpr):\n    # Offsets for the block of data this program instance will process\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    offset = tl.arange(0, BLOCK_SIZE)\n    \n    # Compute the memory offsets\n    q_offset = batch_idx * stride_qz + head_idx * stride_qh + offset * stride_qk\n    cos_offset = head_idx * stride_cos + offset\n    sin_offset = head_idx * stride_sin + offset\n    \n    # Load data from memory\n    Q = tl.load(Q_ptr + q_offset, mask=offset < d_head)\n    cos = tl.load(cos_ptr + cos_offset, mask=offset < d_head)\n    sin = tl.load(sin_ptr + sin_offset, mask=offset < d_head)\n    \n    # Apply RoPE transformation\n    Q_real = Q[0::2]\n    Q_imag = Q[1::2]\n    Q_real_rot = Q_real * cos - Q_imag * sin\n    Q_imag_rot = Q_real * sin + Q_imag * cos\n    \n    # Interleave real and imaginary parts\n    Q_rot = tl.cat((Q_real_rot, Q_imag_rot), axis=0)\n    \n    # Store results back to memory\n    tl.store(Q_ptr + q_offset, Q_rot, mask=offset < d_head)\n\n\nimport torch\nimport triton\n\nclass Fast_RoPE_Embedding(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, cos, sin):\n        # Ensure inputs are on the correct device\n        assert Q.is_cuda and cos.is_cuda and sin.is_cuda, \"All inputs must be on CUDA.\"\n        \n        # Extract dimensions\n        batch_size, n_heads, seq_len, d_head = Q.shape\n        BLOCK_SIZE = 128  # Choose a block size suitable for your GPU\n        \n        # Launch the Triton kernel\n        grid = (batch_size, n_heads)\n        _rope_embedding[grid](Q, cos, sin, \n                              Q.stride(0), Q.stride(1), Q.stride(2), Q.stride(3),\n                              cos.stride(0), sin.stride(0),\n                              n_heads, seq_len, d_head, BLOCK_SIZE)\n        \n        # Save for backward\n        ctx.save_for_backward(cos, sin)\n        return Q\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retrieve saved tensors\n        cos, sin = ctx.saved_tensors\n        \n        # Compute gradients\n        grad_Q = grad_output.clone()  # Placeholder, actual gradient computation needs to be implemented\n        grad_cos = None  # Typically, cos and sin are constants and do not require gradients\n        grad_sin = None\n        \n        return grad_Q, grad_cos, grad_sin\n\ndef fast_rope_embedding(Q, cos, sin):\n    return Fast_RoPE_Embedding.apply(Q, cos, sin)\n\n# Example usage\nQ = torch.randn(batch_size, n_heads, seq_len, d_head, device='cuda')\ncos = torch.randn(n_heads, d_head // 2, device='cuda')\nsin = torch.randn(n_heads, d_head // 2, device='cuda')\n\n# Apply RoPE\nQ_transformed = fast_rope_embedding(Q, cos, sin)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\nROPE_GROUP_SIZE = 4\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int,):\n    BLOCK_SIZE : int = triton.next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _rope_embedding(\n    Q,     Q_row_stride,\n    cos, cos_row_stride,\n    sin, sin_row_stride,\n    seqlen,\n    head_dim      : tl.constexpr,\n    n_heads       : tl.constexpr,\n    BACKWARD_PASS : tl.constexpr,\n    BLOCK_SIZE    : tl.constexpr,\n):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    ROPE_GROUP_SIZE = 4\n    row_position  = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets  = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n\n    sin1 = tl.load(sin + (row_position % seqlen)*sin_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n    cos1 = tl.load(cos + (row_position % seqlen)*cos_row_stride + \\\n                   half_head_dim*0 + col_offsets, mask = mask, other = 0)\n\n    if BACKWARD_PASS:\n        # See our blog post for more info.\n        sin1 = -sin1\n    \n    # [TODO] Autotune ROPE_GROUP_SIZE to be 1, 2, 4, 8\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)\n\n    # 10% Faster kernel from [HuyNguyen-hust](https://github.com/unslothai/unsloth/pull/238)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n\n        # For Gemma - sometimes RoPE must be done in float32 and not bfloat16\n        Q1 = tl.load(Q + offs_q1, mask = mask, other = 0).to(sin1.dtype)\n        Q2 = tl.load(Q + offs_q2, mask = mask, other = 0).to(sin1.dtype)\n\n        tl.store(Q + offs_q1, Q1*cos1 - Q2*sin1, mask = mask)\n        tl.store(Q + offs_q2, Q2*cos1 + Q1*sin1, mask = mask)\n    \n\nclass Fast_RoPE_Embedding(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, cos, sin):\n        cos, sin = cos.squeeze(), sin.squeeze()\n        batch, seq_len, n_heads, head_dim = Q.shape\n        Q = Q.reshape(batch*seq_len, n_heads*head_dim)\n        n_rows, n_cols = Q.shape\n        assert(seq_len <= cos.shape[0])\n\n        # [TODO] Changing blocksize to head_dim//2 seems to have\n        # some concurrency / un-deterministic issues.\n        BLOCK_SIZE, num_warps = calculate_settings(head_dim//2) # (head_dim//2)\n        \n        # group_size = 4 # 4 or 8, too large group_size can hurt performance.\n        div, mod = divmod(n_heads, ROPE_GROUP_SIZE)\n        n_groups = div + (mod != 0)\n\n        _rope_embedding[(n_rows, n_groups, )](\n              Q,   Q.stride(0),\n            cos, cos.stride(0),\n            sin, sin.stride(0),\n            seq_len,\n            head_dim, n_heads,\n            BACKWARD_PASS = False,\n            BLOCK_SIZE = BLOCK_SIZE,\n            num_warps  = num_warps,\n        )\n        ctx.BLOCK_SIZE = BLOCK_SIZE\n        ctx.num_warps  = num_warps\n        ctx.n_groups = n_groups\n        ctx.cos = cos\n        ctx.sin = sin\n        return Q.view(batch, seq_len, n_heads, head_dim)\n    \n    @staticmethod\n    def backward(ctx, dY):\n        batch, seq_len, n_heads, head_dim = dY.shape\n        dY = dY.reshape(batch*seq_len, n_heads*head_dim)\n        # Must be reshape not view\n        n_rows, n_cols = dY.shape\n\n        cos = ctx.cos\n        sin = ctx.sin\n\n        _rope_embedding[(n_rows, ctx.n_groups, )](\n            dY,  dY .stride(0),\n            cos, cos.stride(0),\n            sin, sin.stride(0),\n            seq_len, head_dim, n_heads,\n            BACKWARD_PASS = True,\n            BLOCK_SIZE = ctx.BLOCK_SIZE,\n            num_warps  = ctx.num_warps,\n        )\n        dY = dY.view(batch, seq_len, n_heads, head_dim)\n        return dY, None, None,\n    \n\ndef fast_rope_embedding(Q, K, cos, sin):\n    Q = Fast_RoPE_Embedding.apply(Q.transpose(1, 2), cos, sin).transpose(1, 2)\n    K = Fast_RoPE_Embedding.apply(K.transpose(1, 2), cos, sin).transpose(1, 2)\n    return Q, K\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe function `chunk_global_reversed_cumsum_vector` uses a Triton kernel to compute a reversed cumulative sum across a specified dimension of a 4D input tensor `s`. The kernel iterates in reverse over the time dimension `T`, accumulating and storing results in an output tensor `z`. It operates on blocks of data with a configurable block size `BT` for temporal dimensions and `BS` for spatial dimensions. The input tensor has dimensions [B, H, T, S], where B is the batch size, H is the number of heads, T is the sequence length, and S is the number of features. The Triton kernel, `chunk_global_reversed_cumsum_vector_kernel`, performs the main computation, leveraging parallel processing. It first initializes a block-wise cumulative sum `b_z` to zero and iterates backward through time blocks, updating `b_z` using the dot product of a mask matrix `m_s` and the loaded input block `b_s`. The results are stored in the output tensor `z`. \n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(s_ptr, z_ptr, m_s_ptr, B, H, T, S, BT, BS, **meta):\n    # Calculate block indices\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    s_idx = tl.program_id(2)\n    \n    # Calculate start indices for each dimension\n    batch_start = b_idx * BT\n    head_start = h_idx\n    spatial_start = s_idx * BS\n    \n    # Initialize block-wise cumulative sum to zero\n    b_z = tl.zeros([BT, BS], dtype=tl.float32)\n    \n    # Iterate over time dimension in reverse\n    for t_offset in range(0, T, BT):\n        t_idx = T - t_offset - BT - 1  # Reverse index\n        \n        # Load input block\n        b_s = tl.load(s_ptr + (batch_start * H * T * S) + (head_start * T * S) + (t_idx * S) + spatial_start)\n        \n        # Load mask block\n        m_s = tl.load(m_s_ptr + (batch_start * H * T * S) + (head_start * T * S) + (t_idx * S) + spatial_start)\n        \n        # Update block-wise cumulative sum using mask and input\n        b_z += tl.dot(m_s, b_s)\n        \n        # Store result in output tensor\n        tl.store(z_ptr + (batch_start * H * T * S) + (head_start * T * S) + (t_idx * S) + spatial_start, b_z)\n\ndef chunk_global_reversed_cumsum_vector(s, m_s, BT, BS):\n    B, H, T, S = s.shape\n    z = torch.zeros_like(s)\n    \n    # Launch the Triton kernel\n    grid = (B, H, S // BS)\n    chunk_global_reversed_cumsum_vector_kernel[grid](\n        s, z, m_s,\n        B, H, T, S,\n        BT, BS\n    )\n    \n    return z\n\n# Example usage\nB, H, T, S = 2, 4, 8, 16\nBT, BS = 2, 4\ns = torch.randn(B, H, T, S, device='cuda')\nm_s = torch.ones(B, H, T, S, device='cuda')  # Example mask\nz = chunk_global_reversed_cumsum_vector(s, m_s, BT, BS)\nprint(z)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 16}, num_warps=4),\n        triton.Config({'BT': 16}, num_warps=8),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=4),\n        triton.Config({'BT': 64}, num_warps=8),\n    ],\n    key=['S']\n)\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(\n    s,\n    z,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr\n):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1., 0.)\n\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        # [BT, BS]\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n\ndef chunk_global_reversed_cumsum_vector(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T, S = s.shape\n    BS = 32\n    dtype = dtype or s.dtype\n    grid = (triton.cdiv(S, BS), B * H)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_reversed_cumsum_vector_kernel[grid](\n        s, z,\n        s.stride(1), s.stride(2), s.stride(3),\n        T=T, S=S, BS=BS\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator computes the log-sum-exp over the last dimension of a given input tensor. The function `logsumexp_fwd` wraps around the Triton kernel `logsumexp_fwd_kernel`, which efficiently computes this operation in parallel.\n\n        The kernel `logsumexp_fwd_kernel` reads a segment of the input tensor, optionally scales it, computes the maximum value, and then the log-sum-exp using Triton's specialized operations. It writes the result into an output tensor. The function accepts parameters including the input tensor `x`, an optional scale factor `scale`, and constants `D` and `B` representing the dimension size and block size, respectively. The kernel has configurations for various warp sizes to optimize performance.\n\n        The main function `logsumexp_fwd` reshapes the input tensor, determines the appropriate block size `B`, creates an empty output tensor `z`, and launches the kernel. It then reshapes the result back to the original tensor shape minus the last dimension and optionally casts it to a specified data type.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef logsumexp_fwd_kernel(\n    x_ptr, z_ptr, scale_ptr,\n    D, B,\n    BLOCK_SIZE: tl.constexpr,\n    ):\n    # Get the program ID for the current block\n    pid = tl.program_id(0)\n\n    # Compute the starting index for this block\n    block_start = pid * BLOCK_SIZE\n\n    # Create a pointer for the input block\n    x_block_ptr = x_ptr + block_start\n\n    # Load the input block\n    x = tl.load(x_block_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < D, other=-float('inf'))\n\n    # Optionally apply the scale\n    if scale_ptr is not None:\n        scale = tl.load(scale_ptr)\n        x *= scale\n\n    # Compute the maximum value in the block\n    max_x = tl.max(x, axis=0)\n\n    # Compute the exponential and sum it\n    exp_x = tl.exp(x - max_x)\n    sum_exp_x = tl.sum(exp_x, axis=0)\n\n    # Compute the log-sum-exp\n    log_sum_exp = max_x + tl.log(sum_exp_x)\n\n    # Store the result in the output tensor\n    z_block_ptr = z_ptr + pid\n    tl.store(z_block_ptr, log_sum_exp)\n\n\ndef logsumexp_fwd(x, scale=None):\n    # Get the dimensions of the input tensor\n    shape = x.shape\n    D = shape[-1]\n    num_blocks = (D + 1024 - 1) // 1024  # Compute the number of blocks needed\n    B = min(D, 1024)  # Set block size, typically a power of two\n\n    # Create the output tensor\n    z = torch.empty(shape[:-1], dtype=x.dtype, device=x.device)\n\n    # Launch the Triton kernel\n    grid = (num_blocks,)\n    logsumexp_fwd_kernel[grid](\n        x_ptr=x,\n        z_ptr=z,\n        scale_ptr=scale,\n        D=D,\n        B=B,\n        BLOCK_SIZE=B\n    )\n\n    return z\n", "label": "import torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=['D']\n)\n@triton.heuristics({\n    'HAS_SCALE': lambda args: args['scale'] is not None\n})\n@triton.jit\ndef logsumexp_fwd_kernel(\n    x,\n    z,\n    scale,\n    D: tl.constexpr,\n    B: tl.constexpr,\n    HAS_SCALE: tl.constexpr\n):\n    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)\n\ndef logsumexp_fwd(\n    x,\n    scale: Optional[float] = None,\n    dtype: Optional[torch.dtype] = None\n):\n    r\"\"\"\n    Compute the logsumexp of the input tensor over the last dimension.\n\n    Args:\n        x (Tensor):\n            The input tensor of any shape.\n        scale (Optional[float]):\n            The scale applied to the input tensor. Default: `None`.\n        dtype (Optional[torch.dtype]):\n            The data type of the output tensor. Default: `None`.\n    Returns:\n        Tensor: The logsumexp of the input tensor.\n    \"\"\"\n\n    shape = x.shape\n    x = x.view(-1, shape[-1])\n    N, D = x.shape\n    B = min(triton.next_power_of_2(D), 64 * 1024)\n    ND = triton.cdiv(D, B)\n\n    z = x.new_empty(N, ND, dtype=torch.float)\n    logsumexp_fwd_kernel[(N, ND)](\n        x=x,\n        z=z,\n        scale=scale,\n        D=D,\n        B=B\n    )\n    z = z.logsumexp(-1).view(*shape[:-1])\n    if dtype is not None and dtype != torch.float:\n        z = z.to(dtype)\n    return z\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton operator is designed for parallel computations on tensors in a Transformer-like architecture. The main functions are kernels that compute the forward and backward passes for a parallel rebased operation. The `parallel_rebased_fwd_kernel` performs matrix multiplications between query, key, and value tensors, storing results in an output tensor 'o' and a normalizer tensor 'z'. The `parallel_rebased_bwd_kernel` calculates gradients for these operations. The `ParallelBasedFunction` class encapsulates these operations for use as an autograd function in PyTorch, enabling gradient computation. Its `forward` method computes the output by applying the forward kernel, while the `backward` method computes gradients by invoking the backward kernel. The wrapper function `parallel_rebased` provides an interface, allowing customization of scale and normalization settings.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef parallel_rebased_fwd_kernel(Q, K, V, O, Z, M, N, L, scale, stride_q, stride_k, stride_v, stride_o, stride_z):\n    # Define the block size\n    BLOCK_SIZE = 128\n\n    # Define program IDs for parallel execution\n    pid = tl.program_id(0)\n\n    # Compute the offsets for Q, K, V, O, and Z\n    offs_q = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_k = tl.arange(0, N)\n    offs_v = tl.arange(0, L)\n\n    # Load Q, K, V\n    q = tl.load(Q + offs_q[:, None] * stride_q, mask=offs_q[:, None] < M)\n    k = tl.load(K + offs_k[None, :] * stride_k, mask=offs_k[None, :] < N)\n    v = tl.load(V + offs_v[None, :] * stride_v, mask=offs_v[None, :] < L)\n\n    # Compute attention scores\n    attn_scores = tl.dot(q, k) * scale\n\n    # Compute softmax normalization\n    max_score = tl.max(attn_scores, axis=1)\n    attn_scores = attn_scores - max_score[:, None]\n    exp_scores = tl.exp(attn_scores)\n    z = tl.sum(exp_scores, axis=1)\n\n    # Store normalizer\n    tl.store(Z + offs_q, z, mask=offs_q < M)\n\n    # Compute weighted sum\n    o = tl.dot(exp_scores, v)\n\n    # Store output\n    tl.store(O + offs_q[:, None] * stride_o, o, mask=offs_q[:, None] < M)\n\n@triton.jit\ndef parallel_rebased_bwd_kernel(Q, K, V, dO, dQ, dK, dV, Z, M, N, L, scale, stride_q, stride_k, stride_v, stride_do, stride_dq, stride_dk, stride_dv):\n    # Define the block size\n    BLOCK_SIZE = 128\n\n    # Define program IDs for parallel execution\n    pid = tl.program_id(0)\n\n    # Compute the offsets for Q, K, V, dO, dQ, dK, dV\n    offs_q = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_k = tl.arange(0, N)\n    offs_v = tl.arange(0, L)\n\n    # Load Q, K, V, dO, Z\n    q = tl.load(Q + offs_q[:, None] * stride_q, mask=offs_q[:, None] < M)\n    k = tl.load(K + offs_k[None, :] * stride_k, mask=offs_k[None, :] < N)\n    v = tl.load(V + offs_v[None, :] * stride_v, mask=offs_v[None, :] < L)\n    do = tl.load(dO + offs_q[:, None] * stride_do, mask=offs_q[:, None] < M)\n    z = tl.load(Z + offs_q, mask=offs_q < M)\n\n    # Compute gradients\n    dq = tl.zeros_like(q)\n    dk = tl.zeros_like(k)\n    dv = tl.zeros_like(v)\n\n    # Backprop through weighted sum\n    dv += tl.dot(do, v)\n\n    # Backprop through softmax\n    grad_exp_scores = tl.dot(do, k) * scale / z[:, None]\n    grad_attn_scores = grad_exp_scores - tl.sum(grad_exp_scores, axis=1)[:, None]\n    \n    # Backprop through dot product\n    dq += tl.dot(grad_attn_scores, k)\n    dk += tl.dot(q, grad_attn_scores)\n\n    # Store gradients\n    tl.store(dQ + offs_q[:, None] * stride_dq, dq, mask=offs_q[:, None] < M)\n    tl.store(dK + offs_k[None, :] * stride_dk, dk, mask=offs_k[None, :] < N)\n    tl.store(dV + offs_v[None, :] * stride_dv, dv, mask=offs_v[None, :] < L)\n\nimport torch\n\nclass ParallelBasedFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, Q, K, V, scale):\n        M, N, L = Q.shape[0], K.shape[1], V.shape[1]\n        O = torch.empty((M, L), device=Q.device, dtype=Q.dtype)\n        Z = torch.empty((M,), device=Q.device, dtype=Q.dtype)\n        \n        # Launch the forward kernel\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE']),)\n        parallel_rebased_fwd_kernel[grid](\n            Q, K, V, O, Z, M, N, L, scale,\n            Q.stride(0), K.stride(0), V.stride(0),\n            O.stride(0), Z.stride(0)\n        )\n        \n        # Save context for backward\n        ctx.save_for_backward(Q, K, V, Z)\n        ctx.scale = scale\n        \n        return O\n\n    @staticmethod\n    def backward(ctx, dO):\n        Q, K, V, Z = ctx.saved_tensors\n        M, N, L = Q.shape[0], K.shape[1], V.shape[1]\n        dQ = torch.empty_like(Q)\n        dK = torch.empty_like(K)\n        dV = torch.empty_like(V)\n        \n        # Launch the backward kernel\n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE']),)\n        parallel_rebased_bwd_kernel[grid](\n            Q, K, V, dO, dQ, dK, dV, Z, M, N, L, ctx.scale,\n            Q.stride(0), K.stride(0), V.stride(0),\n            dO.stride(0), dQ.stride(0), dK.stride(0), dV.stride(0)\n        )\n        \n        return dQ, dK, dV, None\n\ndef parallel_rebased(Q, K, V, scale=1.0):\n    return ParallelBasedFunction.apply(Q, K, V, scale)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef parallel_rebased_fwd_kernel(\n    q,  # query [B, H, L, D_head_K]\n    k,  # key [B, H, L, D_head_V]\n    v,  # value [B, H, L, D_head_V]\n    o,  # output [B, H, L, D_head_V]\n    z,  # normalizer [B, H, L]\n    s_qk_h,  # stride size: L * D_head_K\n    s_qk_t,  # stride size: D_head_K\n    s_qk_d,  # stride size: 1\n    s_vo_h,  # stride size: L * D_head_V\n    s_vo_t,  # stride size: D_head_V\n    s_vo_d,  # stride size: 1\n    scale,  # D_head_K ** -0.5\n    B,  # batch size\n    H,  # H\n    T,  # T\n    K: tl.constexpr,  # D_head_K\n    V: tl.constexpr,  # D_head_V\n    BTL: tl.constexpr,  # BLOCK SIZE along the sequence dimension for Q\n    BTS: tl.constexpr,  # BLOCK SIZE along the sequence dimension for K/V\n    BK: tl.constexpr,  # BLOCK SIZE along the K dimension\n    BV: tl.constexpr,  # BLOCK SIZE along the V dimension\n):\n    # i_c: chunk index. used for sequence parallelism\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0, i_v * BV), (BTS, BV), (1, 0))\n\n    # [BQ, BD] block Q, in the shared memory throughout the whole kernel\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n\n    # Q block and K block have no overlap\n    # no need for mask, thereby saving flops\n    for _ in range(0, i_c * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_s = tl.dot(b_q, (b_k), allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n\n        # [BQ, BD]\n        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n\n    # # rescale interchunk output\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    # # sync threads, easy for compiler to optimize\n    # tl.debug_barrier()\n\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BK, BTS]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BTS, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        # [BTL, BV]\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_z, b_z.to(p_z.dtype.element_ty),\n             mask=((i_c * BTL + tl.arange(0, BTL)) < T))\n\n@triton.jit\ndef _parallel_rebased_bwd_dq(\n    i_bh,\n    i_c,\n    i_k,\n    i_v,\n    i_h,\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dq,\n    s_k_h,\n    s_k_t,\n    s_k_d,\n    s_v_h,\n    s_v_t,\n    s_v_d,\n    scale,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n                             (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + (i_bh) * s_k_h, (T, K),\n                            (s_k_t, s_k_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K),\n                            (s_k_t, s_k_d), (0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T),\n                            (s_v_d, s_v_t), (i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=(i_c * BTL + tl.arange(0, BTL)) < T)\n\n    for _ in range(0, i_c * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        # [BQ, BD]\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K),\n                            (s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T),\n                            (s_v_d, s_v_t), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    # Q block and K block have overlap. masks required\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        # [BTS, BK]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BV, BTS]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BTL, BTS]\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        # [BTL, BK]\n        b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype),\n                       b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_k_h, (T, K),\n                             (s_k_t, s_k_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n@triton.jit\ndef _parallel_rebased_bwd_dkv(\n    i_bh, i_c, i_k, i_v, i_h,\n    q, k, v, do, dz, dk, dv, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d,\n    scale,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n):\n    # compute dk dv\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n                            (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n                            (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(\n        p_v, boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(\n        [BTL, BV], dtype=tl.float32)\n\n    for i in range((tl.cdiv(T, BTS) * BTS)-BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BK, BTS]\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)  # [BV, BTS]\n        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)\n        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * \\\n            scale  # [BTL, BTS]\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype),\n                       tl.trans(b_q), allow_tf32=False)\n\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c*BTL, (i_c+1)*BTL, BTS):\n        p_q = tl.make_block_ptr(\n            q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(\n            do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))  # [BD, BQ]\n        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)\n        # [BK, BQ]\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        # [BK, BD]\n        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype),\n                       tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_k_h,\n                             (T, K), (s_k_t, s_k_d), (i_c*BTL, i_k*BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_v_h,\n                             (T, V), (s_v_t, s_v_d), (i_c*BTL, i_v*BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    return\n\n@triton.jit\ndef parallel_rebased_bwd_kernel(\n    q,\n    k,\n    v,\n    do,\n    dz,\n    dq,\n    dk,\n    dv,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    scale,\n    B: tl.constexpr,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BTL: tl.constexpr,\n    BTS: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // (NV)\n    i_v = i_kv % (NV)\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(\n        i_bh, i_c, i_k, i_v, i_h,\n        q, k, v, do, dz, dq, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d, scale,\n        B=B, H=H, T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV\n    )\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(\n        i_bh, i_c, i_k, i_v, i_h,\n        q, k, v, do, dz, dk, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n        s_vo_t, s_vo_d,\n        scale,\n        B=B, H=H, T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV\n    )\n\nclass ParallelBasedFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, scale):\n        BTL, BTS = 128, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        BK, BV = max(BK, 16), max(BV, 16)\n        B, H, T, K, V = *k.shape, v.shape[-1]\n        num_stages = 2\n        num_warps = 4\n        NK = triton.cdiv(K, BK)\n        NV = triton.cdiv(V, BV)\n        grid = (NK * NV, triton.cdiv(T, BTL), B * H)\n\n        assert NK == 1, \"will encounter some synchronization issue if not.\"\n\n        o = torch.empty(NK, B, H, T, V, device=q.device)\n        z = torch.empty(NK, B, H, T, device=q.device)\n        parallel_rebased_fwd_kernel[grid](\n            q, k, v, o, z,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        ctx.save_for_backward(q, k, v)\n        ctx.scale = scale\n        return o.sum(0).to(q.dtype), z.sum(0).to(q.dtype)\n\n    @staticmethod\n    def backward(ctx, do, dz):\n        q, k, v = ctx.saved_tensors\n        scale = ctx.scale\n        BTL, BTS = 64, 32\n        assert BTL % BTS == 0\n        BK = min(128, triton.next_power_of_2(k.shape[-1]))\n        BV = min(128, triton.next_power_of_2(v.shape[-1]))\n        BK, BV = max(BK, 16), max(BV, 16)\n        B, H, T, K, V = *k.shape, v.shape[-1]\n        num_stages = 2\n        num_warps = 4\n        NK = triton.cdiv(K, BK)\n        NV = triton.cdiv(V, BV)\n        grid = (NK * NV, triton.cdiv(T, BTL), B * H)\n\n        assert NK == 1, \"will encounter some synchronization issue if not\"\n\n        dq = torch.empty(NV, B, H, T, K, dtype=q.dtype, device=q.device)\n        dk = torch.empty(NV, B, H, T, K, dtype=q.dtype, device=q.device)\n        dv = torch.empty(NK, B, H, T, V, dtype=q.dtype, device=q.device)\n\n        parallel_rebased_bwd_kernel[grid](\n            q, k, v, do, dz, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            scale,\n            B=B, H=H, T=T, K=K, V=V,\n            BTL=BTL, BTS=BTS, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        return dq.sum(0).to(q.dtype), dk.sum(0).to(k.dtype), dv.sum(0).to(v.dtype), None\n\ntriton_parallel_based = ParallelBasedFunction.apply\n\ndef parallel_rebased(q, k, v, eps=1e-5, use_scale=True, use_normalize=True, return_both=False):\n    assert q.shape[-1] <= 128, \"only support feature dim up to 128\"\n    if use_scale:\n        scale = q.shape[-1] ** -0.5\n    else:\n        scale = 1\n    o, z = triton_parallel_based(q, k, v, scale)\n    if return_both:\n        return o, z\n    if use_normalize:\n        o = o / (z[..., None] + eps)\n    else:\n        o = o\n    return o.to(q.dtype)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines two Triton kernels: `matmul4_kernel` and `dequantize_kernel`, each decorated with `triton.autotune` to optimize for different configurations.\n\n        `matmul4_kernel` performs a specialized matrix multiplication where:\n        - Matrix `A` is of shape `(M, K)` with `float16` elements.\n        - Matrix `B` is stored in a packed 4-bit integer format, shaped `(K//8, N)`.\n        - It computes the matrix multiplication `C = A x B` where `C` is `(M, N)` in `float16`.\n        - It uses scales and zeros arrays for quantization adjustments.\n        - `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K` define the tile sizes for computation.\n        - It processes blocks of the matrix in a loop, unpacking 4-bit integers from `B`.\n\n        `dequantize_kernel` converts a packed 4-bit integer matrix `B` to full precision:\n        - Input `b` is `(K//8, N)` with packed int4 format.\n        - It outputs `fpb` as a full-precision matrix `(K, N)`.\n        - The dequantization uses scales and zero-points for adjustment.\n        - The kernel processes matrix tiles of size `BLOCK_SIZE_K x BLOCK_SIZE_N`.\n\n        The functions `dequantize_int4` and `matmul_dequantize_int4_s1` wrap these kernels for higher-level operations:\n        - `dequantize_int4` dequantizes matrix `B` using `dequantize_kernel`.\n        - `matmul_dequantize_int4_s1` dequantizes `B` and then performs matrix multiplication with `A`.\n\n        The code provides a function `quantize_int4` for quantizing weights into int4 format with associated scales and zero-points. It processes 8 elements into an `int32` and handles group-wise statistics for quantization. The output is a packed version of `B`, its scales, and zero-points.\n        ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}),\n        # Add more configurations if needed\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul4_kernel(\n    A_ptr, B_ptr, C_ptr, scales_ptr, zeros_ptr,\n    M, N, K, stride_am, stride_ak, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Define the ranges for the blocks\n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float16)\n\n    # Loop over K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        rk = k + tl.arange(0, BLOCK_SIZE_K)\n\n        # Load A and B blocks\n        A = tl.load(A_ptr + rm[:, None] * stride_am + rk[None, :] * stride_ak, mask=rk[None, :] < K)\n        packed_B = tl.load(B_ptr + (rk // 8)[:, None] * stride_bn + rn[None, :], mask=rk[None, :] < K)\n\n        # Unpack B\n        B = tl.zeros((BLOCK_SIZE_K, BLOCK_SIZE_N), dtype=tl.float16)\n        for i in range(8):\n            B += (((packed_B >> (i * 4)) & 0xF) - zeros_ptr[i]) * scales_ptr[i]\n\n        # Perform the matrix multiplication\n        acc += tl.dot(A, B)\n\n    # Store the result\n    tl.store(C_ptr + rm[:, None] * stride_cm + rn[None, :] * stride_cn, acc)\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 128}),\n        # Add more configurations if needed\n    ],\n    key=['K', 'N']\n)\n@triton.jit\ndef dequantize_kernel(\n    B_ptr, fpb_ptr, scales_ptr, zeros_ptr,\n    K, N, stride_bn, stride_fpbk, stride_fpbn,\n    BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr\n):\n    pid_k = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Define the ranges for the blocks\n    rk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\n    # Load packed B\n    packed_B = tl.load(B_ptr + (rk // 8)[:, None] * stride_bn + rn[None, :], mask=rk[:, None] < K)\n\n    # Unpack and dequantize\n    fpb = tl.zeros((BLOCK_SIZE_K, BLOCK_SIZE_N), dtype=tl.float16)\n    for i in range(8):\n        fpb += (((packed_B >> (i * 4)) & 0xF) - zeros_ptr[i]) * scales_ptr[i]\n\n    # Store the result\n    tl.store(fpb_ptr + rk[:, None] * stride_fpbk + rn[None, :] * stride_fpbn, fpb)\n\ndef dequantize_int4(B, scales, zeros, K, N):\n    fpb = torch.empty((K, N), dtype=torch.float16, device=B.device)\n    grid = (triton.cdiv(K, BLOCK_SIZE_K), triton.cdiv(N, BLOCK_SIZE_N))\n    dequantize_kernel[grid](B, fpb, scales, zeros, K, N, B.stride(0), fpb.stride(0), fpb.stride(1))\n    return fpb\n\ndef matmul_dequantize_int4_s1(A, B, scales, zeros, M, N, K):\n    C = torch.empty((M, N), dtype=torch.float16, device=A.device)\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n    matmul4_kernel[grid](\n        A, B, C, scales, zeros,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), C.stride(0), C.stride(1)\n    )\n    return C\n\ndef quantize_int4(weights, num_groups):\n    K, N = weights.shape\n    packed_B = torch.empty((K // 8, N), dtype=torch.int32, device=weights.device)\n    scales = torch.empty((num_groups,), dtype=torch.float32, device=weights.device)\n    zeros = torch.empty((num_groups,), dtype=torch.int32, device=weights.device)\n\n    # Compute group-wise statistics and quantize\n    for i in range(num_groups):\n        group_weights = weights[i::num_groups]\n        min_val = group_weights.min()\n        max_val = group_weights.max()\n        scale = (max_val - min_val) / 15.0\n        zero_point = -min_val / scale\n        scales[i] = scale\n        zeros[i] = zero_point\n\n        # Quantize\n        for k in range(0, K, 8):\n            packed_value = 0\n            for j in range(8):\n                idx = k + j\n                if idx < K:\n                    quantized_value = int((group_weights[idx] - min_val) / scale)\n                    packed_value |= (quantized_value & 0xF) << (j * 4)\n            packed_B[k // 8, :] = packed_value\n\n    return packed_B, scales, zeros\n", "label": "import time\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n\tconfigs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), \n    ],\n\tkey=['M', 'N', 'K', 'NO_GROUPS'],\n)\n@triton.jit\ndef matmul4_kernel(\n\ta_ptr, b_ptr, c_ptr,\n\tscales_ptr, zeros_ptr,\n\tM, N, K,\n\tstride_am, stride_ak,\n\tstride_bk, stride_bn,\n\tstride_cm, stride_cn,\n\tstride_scales_g, stride_scales_n,\n\tstride_zeros_g, stride_zeros_n,\n\tgroupsize, NO_GROUPS: tl.constexpr,\n\tBLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n\tGROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    # b_ptrs is set up such that it repeats elements along the K axis 8 times\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n   # (BLOCK_SIZE_N,)\n    # zeros_ptrs is set up such that it repeats elements along the N axis 8 times\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)   # (BLOCK_SIZE_N,)\n    # shifter is used to extract the 4 bits of each element in the 32-bit word from B and zeros\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    # If G == 1, scales and zeros are the same for all K, so we can load them once\n    if NO_GROUPS:\n        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n        # Unpack zeros\n        zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n        # zeros = (zeros + 1) * scales  # (BLOCK_SIZE_N,) float16\n        zeros = zeros * scales\n    # Now calculate a block of output of shape (BLOCK_SIZE_M, BLOCK_SIZE_N)\n    # M is along the batch dimension, N is along the outfeatures dimension, K is along the infeatures dimension\n    # So this loop is along the infeatures dimension (K)\n    # It's calculating BLOCK_SIZE_M batches in parallel, and for each batch, BLOCK_SIZE_N outfeatures in parallel\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)  # (BLOCK_SIZE_N,)\n            ptr = zeros_ptrs + g_id * stride_zeros_g   # (BLOCK_SIZE_N,)\n            zeros = tl.load(ptr)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n            # Unpack zeros\n            zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n            zeros = (zeros) * scales  # (BLOCK_SIZE_N,) float16\t\n        # Now we need to unpack b (which is 4-bit values) into 32-bit values\n        b = (b >> shifter[:, None]) & 0xF  # Extract the 4-bit values\n        b = b * scales[None, :] - zeros[None, :]  # Scale and shift\n        # print(\"data type\", a, b)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk  \n    c = accumulator.to(c_ptr.dtype.element_ty)  \n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    ],\n    key=['K', 'N'],\n)\n@triton.jit\ndef dequantize_kernel(\n    # Pointers to matrices\n    b_ptr, b_scale_ptr, b_zp_ptr, fpb_ptr,\n    # Matrix dimensions\n    K, N, group_size,\n    stride_bk, stride_bn,\n    stride_bsk, stride_bsn,\n    stride_bzpk, stride_bzpn,\n    stride_fpbk, stride_fpbn,\n    # Meta-parameters\n    BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr,\n):\n    \"\"\"Dequantize tile [BLOCK_SIZE_K, BLOCK_SIZE_N] in full precision.\n    We should assert BLOCK_SIZE_N % 8 == 0.\n    weight[K // 8, N], scale[K // group_size, N], zp[K // group_size, N // group_size]\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn\n    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn\n    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (offs_n[None, :] // 8) * stride_bzpn\n    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[None, :] * stride_bsn\n    n_mask = offs_n[None, :] < N\n    k_mask = offs_k[:, None] < K\n    mask = n_mask & k_mask\n    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)\n    b_shift = (offs_k[:, None] % 8) * 4\n    bzp_shift = (offs_n[None, :] % 8) * 4\n    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b\n    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)\n\n\ndef dequantize_int4(b, b_scale, b_zero_point, device, dtype, group_size):\n    Kw, N = b.shape\n    K = Kw * 8\n    fp_b = torch.ones((K, N), device=device, dtype=dtype)\n    grid = lambda META: (\n        triton.cdiv(K, META['BLOCK_SIZE_K']),\n        triton.cdiv(N, META['BLOCK_SIZE_N']), \n    )\n    dequantize_kernel[grid](\n        b, b_scale, b_zero_point, fp_b,\n        K, N, group_size,\n        b.stride(0), b.stride(1),\n        b_scale.stride(0), b_scale.stride(1),\n        b_zero_point.stride(0), b_zero_point.stride(1),\n        fp_b.stride(0), fp_b.stride(1)\n    )\n    return fp_b\n\n\ndef matmul_dequantize_int4_s1(a, b, b_scale, b_zero_point, group_size=128, out=None):\n    \"\"\"\n    Matmul dequantize int4 s1 dequantize weight to `fp_b` and do fp16 torch.mm,\n    this is for `prefill` stage, since weight size is fixed so is dequantize overhead,\n    perfill stage have more tokens to amortize dequant cost.\n    \"\"\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    # assert b.is_contiguous(), \"Matrix B must be contiguous\"\n    M, K = a.shape\n    Kw, N = b.shape\n    if out is None:\n        # Allocates output.\n        out = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    fp_b = dequantize_int4(b, b_scale, b_zero_point, a.device, a.dtype, group_size)\n    torch.mm(a, fp_b, out=out)\n    fp_b = None\n    return out\n\n\ndef quantize_int4(weight, group_size=128, tp_rank=0):\n    # Weight shape: [H1 // 8, H2]\n    # Scale shape: [H1 // group_size, H2]\n    # zero_pint shape: [H1 // group_size, H2 // 8]\n\n    weight = weight.transpose(1, 0)\n    h1, h2 = weight.shape\n    assert h1 % 8 == 0 and h2 % 8 == 0, \"H1 {} H2 {}\".format(h1, h2)\n    assert h2 % group_size == 0, \"H1 {} H2 {}\".format(h1, h2)\n    weight = weight.contiguous().view(-1, group_size).cuda(tp_rank)\n    weight_max = weight.amax(-1, keepdim=True)\n    weight_max = torch.where(weight_max < 0, 0, weight_max)\n    weight_min = weight.amin(-1, keepdim=True)\n    weight_min = torch.where(weight_min > 0, 0, weight_min)\n    weight_range = weight_max - weight_min \n    scale = weight_range / (2 ** 4 - 1)\n    zero_point = (-weight_min / scale).round().clamp(0, 15).to(torch.int32)\n    weight = (weight / scale + zero_point).round().clamp(0, 15).to(torch.int32).view(h1, h2)\n    int_weight = torch.empty(h1, h2 // 8).to(torch.int32).to(weight.device)\n    int_zero_point = torch.zeros(h1 // 8, h2 // group_size).to(torch.int32).to(weight.device)\n    zero_point = zero_point.view(h1, -1)\n    scale = scale.view(h1, -1)\n    # pack 8 int4 in an int32 number.\n    # Weight pack in row.\n    for pack in range(0, h2, 8):\n        for i in range(8):\n            int_weight[:, pack // 8] += weight[:, pack + i] << (i * 4)\n    # zero point pack in col.\n    for pack in range(0, h1, 8):\n        for i in range(8):\n            int_zero_point[pack // 8, :] += zero_point[pack + i, :] << (i * 4)\n    '''\n    fp_weight = torch.zeros(h1, h2).half().to(weight.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_weight[pack * 8 + i, :] = \\\n                ((int_weight[pack, :] << (28 - i * 4) >> 28) + 16) % 16\n    print((fp_weight - weight).abs().sum())\n\n    fp_zp = torch.zeros(zero_point.shape).half().to(zero_point.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zp[pack * 8 + i, :] = \\\n                (int_zero_point[pack, :] >> (i * 4)) & 15\n\n    print((fp_zp - zero_point).abs().sum())\n    '''\n    weight = None\n    return int_weight.transpose(1, 0).contiguous(), scale.transpose(1, 0).contiguous(), int_zero_point.transpose(1, 0).contiguous(), group_size\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel '_fwd_kernel_destindex_copy_quantize_kv' is designed to perform quantization of input data from tensor 'K' and store the results into 'Out' and 'Out_scale', based on indices provided by 'Dest_loc'. The kernel reads each element from 'K', calculates the scale for quantization, converts data to int8, and writes quantized data and scaling factors to output tensors. The function 'destindex_copy_quantize_kv' sets up and calls this kernel, configuring grid dimensions based on input shapes.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K_ptr, Dest_loc_ptr, Out_ptr, Out_scale_ptr, \n                                           M, N, BLOCK_SIZE: tl.constexpr):\n    # Pointers to blocks of data\n    pid = tl.program_id(0)\n    # Compute start position of this block\n    row_start = pid * BLOCK_SIZE\n    # Load data from K\n    k_vals = tl.load(K_ptr + row_start * N + tl.arange(0, BLOCK_SIZE)[:, None] * N + tl.arange(0, N)[None, :], mask=row_start + tl.arange(0, BLOCK_SIZE) < M, other=0.0)\n    \n    # Load destination indices\n    dest_indices = tl.load(Dest_loc_ptr + row_start + tl.arange(0, BLOCK_SIZE), mask=row_start + tl.arange(0, BLOCK_SIZE) < M, other=0)\n    \n    # Compute quantization scale (e.g., max absolute value)\n    max_val = tl.max(tl.abs(k_vals), axis=1)\n    scale = max_val / 127.0\n    scale = tl.where(max_val == 0, 1.0, scale)  # Avoid division by zero\n    \n    # Quantize the data\n    k_quantized = tl.cast(tl.round(k_vals / scale[:, None]), tl.int8)\n    \n    # Write quantized data and scales to the output tensors\n    for i in range(BLOCK_SIZE):\n        if row_start + i < M:\n            dest_idx = dest_indices[i]\n            tl.store(Out_ptr + dest_idx * N + tl.arange(0, N), k_quantized[i, :])\n            tl.store(Out_scale_ptr + dest_idx, scale[i])\n\ndef destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale):\n    # Input dimensions\n    M, N = K.shape\n    \n    # Define block size for the kernel\n    BLOCK_SIZE = 128  # You can adjust this based on your hardware\n    \n    # Launch the Triton kernel\n    grid = (triton.cdiv(M, BLOCK_SIZE),)\n    _fwd_kernel_destindex_copy_quantize_kv[grid](K, Dest_loc, Out, Out_scale, M, N, BLOCK_SIZE=BLOCK_SIZE)\n\n# Example usage:\nM, N = 256, 128  # Example dimensions\nK = torch.randn((M, N), dtype=torch.float32, device='cuda')\nDest_loc = torch.randint(0, M, (M,), dtype=torch.int32, device='cuda')\nOut = torch.empty((M, N), dtype=torch.int8, device='cuda')\nOut_scale = torch.empty((M,), dtype=torch.float32, device='cuda')\n\ndestindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(\n    K,\n    Dest_loc,\n    Out,\n    Out_scale,\n    stride_k_bs,\n    stride_k_h,\n    stride_k_d,\n    stride_o_bs,\n    stride_o_h,\n    stride_o_d,\n    stride_os_bs,\n    stride_os_h,\n    stride_os_d,\n    head_num,\n    head_dim,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr,\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(\n        K + cur_index * stride_k_bs + offs_h[:, None] * stride_k_h + stride_k_d * offs_d[None, :],\n        mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim),\n        other=0.0,\n    )\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0).to(Out_scale.dtype.element_ty)[:, None]\n    q_src_data = (src_data / data_scale).to(tl.int8)\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[:, None]\n    tl.store(o_ptrs, q_src_data, mask=(offs_h[:, None] < head_num) & (offs_d[None, :] < head_dim))\n    tl.store(os_ptrs, data_scale, mask=(offs_h[:, None] < head_num))\n\n\n@torch.no_grad()\ndef destindex_copy_quantize_kv(K, DestLoc, Out, Out_scale):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    BLOCK_DMODEL = triton.next_power_of_2(head_dim)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_quantize_kv[grid](\n        K,\n        DestLoc,\n        Out,\n        Out_scale,\n        K.stride(0),\n        K.stride(1),\n        K.stride(2),\n        Out.stride(0),\n        Out.stride(1),\n        Out.stride(2),\n        Out_scale.stride(0),\n        Out_scale.stride(1),\n        Out_scale.stride(2),\n        head_num,\n        head_dim,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel `rotary_kernel` applies rotary positional embeddings to a tensor `X`. It computes the cosine and sine rotations for the specified rotary dimension, allowing for an efficient implementation of rotary position encoding in transformer models. The function `apply_rotary` serves as a Python interface to call the Triton kernel, configuring the execution parameters and managing input/output tensors. This setup allows it to handle both fixed and variable sequence lengths.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rotary_kernel(X_ptr, cos_ptr, sin_ptr, out_ptr, n_heads, head_dim, rotary_dim, seq_len, BLOCK_SIZE: tl.constexpr):\n    # Obtain the block and thread indices\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    seq_idx = tl.program_id(2)\n\n    # Calculate the offsets\n    offset = batch_idx * n_heads * seq_len * head_dim + head_idx * seq_len * head_dim + seq_idx * head_dim\n    rotary_offset = offset + rotary_dim\n\n    # Load the rotary dimension data\n    x_rotary = tl.load(X_ptr + rotary_offset, mask=rotary_offset < seq_len * head_dim)\n    cos_rotary = tl.load(cos_ptr + rotary_offset, mask=rotary_offset < seq_len * head_dim)\n    sin_rotary = tl.load(sin_ptr + rotary_offset, mask=rotary_offset < seq_len * head_dim)\n\n    # Apply rotary transformation\n    x_rotated = x_rotary * cos_rotary + tl.math.fma(-x_rotary, sin_rotary, 0)\n\n    # Store the result\n    tl.store(out_ptr + rotary_offset, x_rotated, mask=rotary_offset < seq_len * head_dim)\n\n### Python Wrapper: `apply_rotary`\n\nThis function will set up the input tensors, call the Triton kernel, and manage the execution parameters.\n\n", "label": "from typing import Optional, Union\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef rotary_kernel(\n    OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen, nheads, rotary_dim, seqlen_ro,\n    CACHE_KEY_SEQLEN, stride_out_batch, stride_out_seqlen, stride_out_nheads,\n    stride_out_headdim, stride_x_batch, stride_x_seqlen, stride_x_nheads,\n    stride_x_headdim, BLOCK_K: tl.constexpr, IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,\n    IS_VARLEN: tl.constexpr, INTERLEAVED: tl.constexpr, CONJUGATE: tl.constexpr, BLOCK_M: tl.constexpr,\n):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = OUT + start_idx * stride_out_seqlen + pid_head * stride_out_nheads\n\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] * stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:, None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + ((rk + 1) % 2) * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] * stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] * stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half), other=1.0).to(tl.float32)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[None, :] < rotary_dim_half), other=0.0).to(tl.float32)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim), other=0.0).to(tl.float32)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] < rotary_dim), other=0.0).to(tl.float32)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] * stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] < rotary_dim))\n\ndef apply_rotary(\n    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, seqlen_offsets: Union[int, torch.Tensor] = 0,\n    cu_seqlens: Optional[torch.Tensor] = None, max_seqlen: Optional[int] = None,\n    interleaved=False, inplace=False, conjugate=False\n) -> torch.Tensor:\n    is_varlen = cu_seqlens is not None\n    if not is_varlen:\n        batch, seqlen, nheads, headdim = x.shape\n    else:\n        total_seqlen, nheads, headdim = x.shape\n        batch = cu_seqlens.shape[0] - 1\n        seqlen = max_seqlen\n    seqlen_ro, rotary_dim = cos.shape\n    rotary_dim *= 2\n\n    cos, sin = cos.contiguous(), sin.contiguous()\n    if isinstance(seqlen_offsets, torch.Tensor):\n        seqlen_offsets = seqlen_offsets.contiguous()\n    else:\n        seqlen_offsets += seqlen\n\n    output = torch.empty_like(x) if not inplace else x\n    if rotary_dim < headdim and not inplace:\n        output[..., rotary_dim:].copy_(x[..., rotary_dim:])\n\n    BLOCK_K = 32 if rotary_dim <= 32 else (64 if rotary_dim <= 64 else (128 if rotary_dim <= 128 else 256))\n    grid = lambda META: (triton.cdiv(seqlen, META[\"BLOCK_M\"]), batch, nheads)\n    BLOCK_M = 4 if interleaved else (8 if rotary_dim <= 64 else 4)\n\n    with torch.cuda.device(x.device.index):\n        rotary_kernel[grid](\n            output, x, cos, sin, cu_seqlens, seqlen_offsets, seqlen, nheads, rotary_dim,\n            seqlen_ro, seqlen // 128, output.stride(0) if not is_varlen else 0, output.stride(-3),\n            output.stride(-2), output.stride(-1), x.stride(0) if not is_varlen else 0,\n            x.stride(-3), x.stride(-2), x.stride(-1), BLOCK_K,\n            isinstance(seqlen_offsets, torch.Tensor), is_varlen, interleaved, conjugate, BLOCK_M\n        )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel, named `_fwd_kernel_apply_penalty`, adjusts the logits for each batch based on specified penalties: presence, frequency, and repetition. The kernel takes various input tensors such as `Logits`, penalty coefficients, token IDs, token counts, and sequence lengths. The main logic involves loading batch-specific penalties, applying them to logits using arithmetic operations, and storing the results back in the logits tensor. The Python function `apply_penalty` configures and calls this kernel with appropriate parameters, including calculating the block size based on maximum sequence length in the batch.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_apply_penalty(\n    logits_ptr,  # pointer to the logits tensor\n    presence_penalty_ptr,  # pointer to the presence penalty coefficients\n    frequency_penalty_ptr,  # pointer to the frequency penalty coefficients\n    repetition_penalty_ptr,  # pointer to the repetition penalty coefficients\n    token_ids_ptr,  # pointer to the token IDs\n    token_counts_ptr,  # pointer to the token counts\n    seq_lengths_ptr,  # pointer to the sequence lengths\n    num_batches,  # number of batches\n    num_tokens,  # number of tokens\n    BLOCK_SIZE: tl.constexpr  # block size for the kernel\n):\n    batch_id = tl.program_id(0)\n    token_id = tl.program_id(1)\n\n    # Calculate offsets\n    logits_offset = batch_id * num_tokens + token_id\n    token_id_offset = batch_id * num_tokens + token_id\n\n    # Load data\n    logits = tl.load(logits_ptr + logits_offset)\n    presence_penalty = tl.load(presence_penalty_ptr + batch_id)\n    frequency_penalty = tl.load(frequency_penalty_ptr + batch_id)\n    repetition_penalty = tl.load(repetition_penalty_ptr + batch_id)\n    token_id = tl.load(token_ids_ptr + token_id_offset)\n    token_count = tl.load(token_counts_ptr + token_id)\n\n    # Apply penalties\n    logits = logits - presence_penalty\n    logits = logits - frequency_penalty * token_count\n    logits = logits - repetition_penalty * (token_id == token_id_offset)\n\n    # Store the result\n    tl.store(logits_ptr + logits_offset, logits)\n\ndef apply_penalty(logits, presence_penalty, frequency_penalty, repetition_penalty, token_ids, token_counts, seq_lengths):\n    # Ensure inputs are contiguous\n    logits = logits.contiguous()\n    presence_penalty = presence_penalty.contiguous()\n    frequency_penalty = frequency_penalty.contiguous()\n    repetition_penalty = repetition_penalty.contiguous()\n    token_ids = token_ids.contiguous()\n    token_counts = token_counts.contiguous()\n    seq_lengths = seq_lengths.contiguous()\n\n    # Get shapes\n    num_batches, num_tokens = logits.shape\n\n    # Calculate block size based on maximum sequence length\n    max_seq_length = seq_lengths.max().item()\n    block_size = min(max_seq_length, 1024)  # Triton typically supports block sizes up to 1024\n\n    # Launch kernel\n    grid = (num_batches, num_tokens)\n    _fwd_kernel_apply_penalty[grid](\n        logits, presence_penalty, frequency_penalty, repetition_penalty,\n        token_ids, token_counts, seq_lengths,\n        num_batches, num_tokens,\n        BLOCK_SIZE=block_size\n    )\n\n# Example usage:\n# logits = torch.randn((batch_size, num_tokens), device='cuda')\n# presence_penalty = torch.randn((batch_size,), device='cuda')\n# frequency_penalty = torch.randn((batch_size,), device='cuda')\n# repetition_penalty = torch.randn((batch_size,), device='cuda')\n# token_ids = torch.randint(0, vocab_size, (batch_size, num_tokens), device='cuda')\n# token_counts = torch.randint(0, 10, (vocab_size,), device='cuda')\n# seq_lengths = torch.randint(1, num_tokens, (batch_size,), device='cuda')\n# apply_penalty(logits, presence_penalty, frequency_penalty, repetition_penalty, token_ids, token_counts, seq_lengths)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_apply_penalty(\n    Logits, presence_penalty, freqency_penalty, repetition_penalty,\n    p_token_ids, p_token_counts, p_cumsum_seq_len, \n    stride_logit_b, stride_logit_s,\n    BLOCK_P: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_repetition = tl.load(repetition_penalty + cur_batch)\n\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=cur_batch_id_offset<cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=cur_batch_id_offset<cur_batch_end_index, other=0)\n    \n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset<cur_batch_end_index, other=0.0)\n    rep_logits = tl.where(cur_logits > 0, cur_logits / cur_repetition, cur_logits * cur_repetition)\n    freq_logits = rep_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset<cur_batch_end_index)\n\n    return\n\n@torch.no_grad()\ndef apply_penalty(Logits, presence_penalty, freqency_penalty, repetition_penalty, p_token_ids, p_token_counts, p_cumsum_seq_len, p_max_len_in_batch):\n    assert Logits.is_contiguous()\n    BLOCK = triton.next_power_of_2(p_max_len_in_batch)\n    if BLOCK <= 512:\n        BLOCK = 512\n    elif BLOCK <= 1024:\n        BLOCK = 1024\n    num_warps = 8\n    _fwd_kernel_apply_penalty[(Logits.shape[0], )](\n        Logits, presence_penalty, freqency_penalty, repetition_penalty,\n        p_token_ids, p_token_counts, p_cumsum_seq_len,\n        Logits.stride(0), Logits.stride(1),\n        num_warps=num_warps,\n        BLOCK_P=BLOCK\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided Triton code implements a custom kernel named `chunk_delta_rule_fwd_kernel_h` and a wrapper function `chunk_fwd_h_fn` for processing tensor data in a highly parallel manner. The kernel performs computations involving matrices `k`, `v`, `d` to produce an output `v_new` while optionally storing intermediate results in `initial_state` and `final_state`. The kernel is designed to handle batch processing of inputs using block pointers and takes advantage of Triton's grid and block mechanisms to operate efficiently across multiple dimensions.\n\n    Key inputs for the kernel include: \n    - `k`, `v`, `d`: Input matrices representing different dimensions and data.\n    - `v_new`: Output matrix to store the updated values after processing.\n    - `h`: Intermediate storage for cumulative sum calculations.\n    - `initial_state` and `final_state`: Optional states for storing data across iterations.\n\n    The kernel is configured for different numbers of warps using `triton.autotune` to optimize performance based on the input size. The main processing loop iterates over a time dimension (`NT`) and computes block-wise operations involving dot products and cumulative sums using Triton's block pointer mechanics.\n\n    The function `chunk_fwd_h_fn` serves as a wrapper for setting up kernel execution. It initializes output tensors, calculates grid and block sizes based on input dimensions, and invokes the Triton kernel with appropriate parameters. This function handles batching and reshaping of input data for parallel processing.\n\n    Both the kernel and the wrapper function heavily utilize Triton features such as `tl.program_id` for grid configuration and `tl.make_block_ptr` for efficient memory access, enabling high performance on GPU architectures.\n    ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_warps=2),\n    ],\n    key=['NT', 'D']\n)\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(\n    k_ptr, v_ptr, d_ptr, v_new_ptr, h_ptr,\n    initial_state_ptr, final_state_ptr,\n    B, N, D, NT, **meta\n):\n    pid = tl.program_id(axis=0)\n    bid = tl.program_id(axis=1)\n\n    BLOCK_SIZE_M = meta['BLOCK_SIZE_M']\n    BLOCK_SIZE_N = meta['BLOCK_SIZE_N']\n\n    # Calculate the starting index for this block\n    m_start = pid * BLOCK_SIZE_M\n    n_start = bid * BLOCK_SIZE_N\n\n    # Create block pointers for the input and output matrices\n    k_block_ptr = tl.make_block_ptr(k_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n    v_block_ptr = tl.make_block_ptr(v_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n    d_block_ptr = tl.make_block_ptr(d_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n    v_new_block_ptr = tl.make_block_ptr(v_new_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n    h_block_ptr = tl.make_block_ptr(h_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n\n    # Optionally handle initial and final states\n    if initial_state_ptr is not None:\n        initial_state_block_ptr = tl.make_block_ptr(initial_state_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n    if final_state_ptr is not None:\n        final_state_block_ptr = tl.make_block_ptr(final_state_ptr, shape=(B, N, D), strides=(N * D, D, 1), offsets=(0, m_start, 0))\n\n    # Iterate over the time dimension\n    for t in range(NT):\n        # Load the necessary blocks\n        k_block = tl.load(k_block_ptr)\n        v_block = tl.load(v_block_ptr)\n        d_block = tl.load(d_block_ptr)\n\n        # Perform the desired computation\n        result = tl.dot(k_block, v_block) + d_block\n\n        # Store the result\n        tl.store(v_new_block_ptr, result)\n\n        # Optionally store intermediate states\n        if initial_state_ptr is not None:\n            tl.store(initial_state_block_ptr, result)\n        if final_state_ptr is not None:\n            tl.store(final_state_block_ptr, result)\n\n        # Move to the next block\n        k_block_ptr = tl.advance(k_block_ptr, 1)\n        v_block_ptr = tl.advance(v_block_ptr, 1)\n        d_block_ptr = tl.advance(d_block_ptr, 1)\n        v_new_block_ptr = tl.advance(v_new_block_ptr, 1)\n        h_block_ptr = tl.advance(h_block_ptr, 1)\n        if initial_state_ptr is not None:\n            initial_state_block_ptr = tl.advance(initial_state_block_ptr, 1)\n        if final_state_ptr is not None:\n            final_state_block_ptr = tl.advance(final_state_block_ptr, 1)\n\n\ndef chunk_fwd_h_fn(k, v, d, NT, initial_state=None, final_state=None):\n    B, N, D = k.shape\n\n    # Prepare output tensors\n    v_new = torch.empty_like(v)\n    h = torch.empty_like(v)\n\n    # Define grid and block sizes\n    grid = (triton.cdiv(N, 128), triton.cdiv(NT, 128))\n\n    # Launch the kernel\n    chunk_delta_rule_fwd_kernel_h[grid](\n        k, v, d, v_new, h,\n        initial_state, final_state,\n        B, N, D, NT\n    )\n\n    return v_new, h\n\n# Example usage\nB, N, D, NT = 32, 128, 64, 10\nk = torch.randn((B, N, D), device='cuda')\nv = torch.randn((B, N, D), device='cuda')\nd = torch.randn((B, N, D), device='cuda')\n\nv_new, h = chunk_fwd_h_fn(k, v, d, NT)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"BT\", \"BK\", \"BV\"], \n)\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(\n    k,\n    v,\n    d, \n    v_new,\n    h,\n    initial_state,\n    final_state,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BC: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_h_cumsum = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC)):\n            p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n            p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n            p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            p_v_new = tl.make_block_ptr(v_new + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))   \n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_v -= tl.dot(b_d, b_h.to(b_k.dtype), allow_tf32=False)\n            tl.store(p_v_new, b_v.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n            b_h_cumsum += tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)\n        b_h += b_h_cumsum      \n        \n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\ndef chunk_fwd_h_fn(k, w, u, BT, initial_state, final_state):\n    B, H, T, K, V = *k.shape, u.shape[-1]\n\n    BK = triton.next_power_of_2(K)\n    assert BK <= 256, \"current kernel does not support head dimension larger than 256.\"\n    BV = 16 if BK > 128 else 32        \n    BV = 64 if BK <= 64 else BV\n    BC = 16 if BK > 128 else 32 \n    BC = 64 if BK <= 64 else BC\n    BC = min(BT, BC)\n    NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n    assert NK == 1, 'NK > 1 is not supported because it involves time-consuming synchronization'\n\n    h = k.new_empty(B, H, NT * K, V)\n    grid = (NK, NV, B * H)\n    v_new = torch.empty_like(u)\n    chunk_delta_rule_fwd_kernel_h[grid](\n        k, u, w, v_new, h, initial_state, final_state,\n        k.stride(1), k.stride(2), k.stride(3),\n        u.stride(1), u.stride(2), u.stride(3),\n        h.stride(1), h.stride(2),\n        H=H, T=T, K=K, V=V, BT=BT, BC=BC, BK=BK, BV=BV, NT=NT,\n        USE_INITIAL_STATE=initial_state is not None,\n        STORE_FINAL_STATE=final_state is not None,\n        )\n    return h, v_new\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided code defines a Triton kernel for matrix multiplication. The `matmul_kernel` function performs matrix multiplication on input matrices A and B, storing the result in matrix C. It supports an optional activation function, specifically 'leaky_relu'. The kernel is designed to efficiently compute the product by iterating over blocks of A and B, using blocks of size `BLOCK_SIZE_M`, `BLOCK_SIZE_N`, and `BLOCK_SIZE_K`. The `matmul` function is a wrapper around `matmul_kernel`, handling input validation, setting up the grid for execution, and calling the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define constants for block sizes\nBLOCK_SIZE_M = 128\nBLOCK_SIZE_N = 128\nBLOCK_SIZE_K = 32\n\n@triton.jit\ndef matmul_kernel(A, B, C, M, N, K, alpha, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    # Define the block indices\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the start of the block in the output matrix\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n\n    # Initialize an accumulator for the block\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Iterate over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B into shared memory\n        a_block = tl.load(A + (start_m + tl.arange(0, BLOCK_SIZE_M))[:, None] * K + (k + tl.arange(0, BLOCK_SIZE_K)))\n        b_block = tl.load(B + (k + tl.arange(0, BLOCK_SIZE_K))[:, None] * N + (start_n + tl.arange(0, BLOCK_SIZE_N)))\n\n        # Compute the matrix multiplication for the block\n        acc += tl.dot(a_block, b_block)\n\n    # Apply the optional activation function (leaky_relu)\n    acc = tl.where(acc > 0, acc, acc * alpha)\n\n    # Write the result to the output matrix\n    tl.store(C + (start_m + tl.arange(0, BLOCK_SIZE_M))[:, None] * N + (start_n + tl.arange(0, BLOCK_SIZE_N)), acc)\n\ndef matmul(A, B, M, N, K, activation='none', alpha=0.01):\n    # Ensure input matrices are compatible for multiplication\n    assert A.shape == (M, K)\n    assert B.shape == (K, N)\n\n    # Allocate output matrix\n    C = triton.empty((M, N), dtype=triton.float32)\n\n    # Determine grid size based on block sizes\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        A, B, C, M, N, K, alpha,\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n\n    return C\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n        GROUP_SIZE_M: tl.constexpr,\n        ACTIVATION: tl.constexpr\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    # Create pointers for the first blocks of A and B\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    # Compute a block of the C matrix\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    \n    # Apply activation function if specified\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    \n    c = accumulator.to(tl.float16)\n\n    # Write back the block of the output matrix C with masks\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n\ndef matmul(a, b, activation=\"\"):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n    matmul_kernel[grid](\n        a, b, c,\n        M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=32, BLOCK_SIZE_N=32, BLOCK_SIZE_K=32,\n        GROUP_SIZE_M=4,\n        ACTIVATION=activation\n    )\n    return c\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The given Triton kernel `_fwd_kernel` performs the forward pass of an attention mechanism, similar to the scaled dot-product attention often used in transformer models. The kernel computes the output based on queries (Q), keys (K), and values (V), taking into account a scaling factor (`sm_scale`) and a potential causal mask (`IS_CAUSAL`). The kernel also supports flexible data types and stride formats. A key feature is the ability to manage different block sizes (`BLOCK_M`, `BLOCK_N`, `BLOCK_DMODEL`) and float precision using `USE_FP8`. The wrapper function `triton_fa` manages the setup of this kernel, preparing inputs and configuring execution parameters like grid size and warps.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel(Q_ptr, K_ptr, V_ptr, Out_ptr, sm_scale, stride_qm, stride_qk, stride_qd, \n                stride_km, stride_kn, stride_kd, stride_vm, stride_vn, stride_vd, \n                stride_om, stride_on, stride_od, \n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr, \n                IS_CAUSAL: tl.constexpr, USE_FP8: tl.constexpr):\n    \n    # Define the program IDs for blocks\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Calculate the offsets for each block\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    # Load Q, K, V blocks\n    Q = tl.load(Q_ptr + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qd)\n    K = tl.load(K_ptr + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kd)\n    V = tl.load(V_ptr + offs_n[:, None] * stride_vn + offs_d[None, :] * stride_vd)\n\n    # Compute QK^T\n    qk = tl.dot(Q, tl.trans(K)) * sm_scale\n\n    # Apply causal mask if necessary\n    if IS_CAUSAL:\n        mask = tl.arange(0, BLOCK_M)[:, None] >= tl.arange(0, BLOCK_N)[None, :]\n        qk = tl.where(mask, qk, float('-inf'))\n\n    # Compute softmax along the N dimension\n    qk = tl.softmax(qk, axis=1)\n\n    # Compute output\n    Out = tl.dot(qk, V)\n\n    # Store the result\n    tl.store(Out_ptr + offs_m[:, None] * stride_om + offs_d[None, :] * stride_od, Out)\n\n### Wrapper Function\n\ndef triton_fa(Q, K, V, sm_scale, BLOCK_M=128, BLOCK_N=128, BLOCK_DMODEL=64, IS_CAUSAL=False, USE_FP8=False):\n    # Get the shapes and strides of the input tensors\n    Q_shape, K_shape, V_shape = Q.shape, K.shape, V.shape\n    Q_strides, K_strides, V_strides = Q.stride(), K.stride(), V.stride()\n\n    # Allocate output tensor\n    Out = torch.empty((Q_shape[0], V_shape[1], Q_shape[2]), device=Q.device, dtype=Q.dtype)\n\n    # Calculate grid size\n    grid = (triton.cdiv(Q_shape[0], BLOCK_M), triton.cdiv(K_shape[1], BLOCK_N))\n\n    # Launch the kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out, sm_scale,\n        Q_strides[0], Q_strides[1], Q_strides[2],\n        K_strides[0], K_strides[1], K_strides[2],\n        V_strides[0], V_strides[1], V_strides[2],\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=BLOCK_DMODEL,\n        IS_CAUSAL=IS_CAUSAL, USE_FP8=USE_FP8\n    )\n\n    return Out\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport math\n\n@triton.jit\ndef _fwd_kernel(\n    Q, K, V, sm_scale,\n    Out,\n    stride_qz, stride_qh, stride_qm, stride_qk,\n    stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on,\n    N_HEAD, H, N_CTX,\n    start_position,  # <- ADDED\n    IS_CAUSAL: tl.constexpr,  # <- ADDED\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n    USE_FP8: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n\n    head_idx = tl.program_id(1)\n    batch_id = head_idx // N_HEAD\n    off_hz = head_idx % N_HEAD\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = batch_id * stride_qz + off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk  # <- stride fixed\n    off_k = batch_id * stride_kz + off_hz * stride_kh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk  # <- stride fixed\n    off_v = batch_id * stride_vz + off_hz * stride_vh + offs_n[:, None] * stride_vk + offs_d[None, :] * stride_vn  # <- stride fixed\n    # Initialize pointers to Q, K, V\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    # initialize pointer to m and l\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    # load q: it will stay in SRAM throughout\n    q = tl.load(q_ptrs, offs_m[:, None] < H, other=0.0)\n    # loop over k, v and update accumulator\n    block_n_end = N_CTX  # <- ADDED (including the IF)\n    if IS_CAUSAL:\n        # in causal mode, we expect that BLOCK_M_SIZE == BLOCK_N_SIZE\n        # autotune will prune shapes not matching this rule\n        block_n_end = (start_m + 1) * BLOCK_N + start_position\n    for start_n in range(0, block_n_end, BLOCK_N):\n        block_n_offs = start_n + offs_n  # <- ADDED\n        # -- compute qk ----\n        k = tl.load(k_ptrs, block_n_offs[:, None] < N_CTX, 0.)\n        if USE_FP8:\n            k = k.to(tl.float8e5, bitcast=True)\n            k = k.to(tl.float16)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk = tl.where(offs_n[None, :] < N_CTX, qk, float(\"-inf\"))  # <- ADDED\n        qk *= sm_scale\n        if IS_CAUSAL:  # <- ADDED\n            qk = tl.where(offs_m[:, None] >= (block_n_offs[None, :] + start_position), qk, float(\"-inf\"))\n            \n        # compute new m\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        # correct old l\n        l_prev *= tl.exp(m_prev - m_curr)\n        # attention weights\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        # rescale operands of matmuls\n        l_rcp = 1. / l_curr\n        p *= l_rcp[:, None]\n        acc *= (l_prev * l_rcp)[:, None]\n        # update acc\n        p = p.to(Q.dtype.element_ty)\n        v = tl.load(v_ptrs, block_n_offs[:, None] < N_CTX, 0.0)\n        if USE_FP8:\n            v = v.to(tl.float8e5, bitcast=True)\n            v = v.to(tl.float16)\n        acc += tl.dot(p, v)\n        # update m_i and l_i\n        l_prev = l_curr\n        m_prev = m_curr\n        # update pointers\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vk\n    # rematerialize offsets to save registers\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    # initialize pointers to output\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_o = batch_id * stride_oz + off_hz * stride_oh + offs_m[:, None] * stride_om + offs_d[None, :] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, offs_m[:, None] < H)\n\n\ndef triton_fa(q, k, v, sm_scale, is_causal, start_position):\n    assert q.dtype == torch.float16\n    assert k.dtype == v.dtype and k.dtype in [torch.float16, torch.int8]\n\n    BLOCK = 64\n    # shape constraints\n    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n    assert Lq == Lk and Lk == Lv\n    assert Lk in {16, 32, 64, 128}\n    o = torch.empty_like(q)\n    num_warps = 4 if Lk <= 64 else 8\n    batch, head_size, m_size, dhead = q.size()\n    grid = (triton.cdiv(m_size, BLOCK), head_size * batch)\n    n_size = k.size(2)\n    _fwd_kernel[grid](\n        q, k, v, sm_scale,\n        o,\n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n        head_size, m_size, n_size,\n        start_position=start_position,\n        IS_CAUSAL=is_causal,\n        BLOCK_M=BLOCK,\n        BLOCK_N=BLOCK,\n        BLOCK_DMODEL=Lk,\n        USE_FP8=k.dtype == torch.int8,  # USE_FP8\n        num_warps=num_warps,\n        num_stages=2,\n    )\n\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel function and a corresponding Python wrapper to perform an operation involving index-based copying of a tensor. The Triton kernel `_fwd_kernel_destindex_copy_kv` takes three main tensors: `K`, `Dest_loc`, and `Out`. `K` contains the input data, `Dest_loc` specifies destination indices, and `Out` stores the result. The kernel iterates over `seq_len`, determined by the length of `Dest_loc`. It loads data from `K` into `Out` based on indices from `Dest_loc`, using a head dimension loop governed by `head_num`. Key parameters include strides for navigating tensors and block sizes for dimensions, which are optimized for GPU processing. The wrapper function `destindex_copy_kv` sets up the kernel launch, determining grid size and checking shape consistency.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, seq_len, head_num, stride_k_head, stride_k_seq, stride_out_head, stride_out_seq, BLOCK_SIZE: tl.constexpr):\n    # Define the block indices\n    head_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n\n    # Compute the start position for this block\n    start_idx = block_idx * BLOCK_SIZE\n\n    # Loop over the sequence length\n    for i in range(seq_len):\n        # Calculate the index for this position\n        dest_idx = tl.load(Dest_loc + i)\n\n        # Compute the offset in the K tensor\n        k_offset = head_idx * stride_k_head + start_idx + i * stride_k_seq\n        # Compute the offset in the Out tensor\n        out_offset = head_idx * stride_out_head + dest_idx * stride_out_seq\n\n        # Load the value from K\n        value = tl.load(K + k_offset)\n\n        # Store the value in the Out tensor\n        tl.store(Out + out_offset, value)\n\n\nimport torch\n\ndef destindex_copy_kv(K, Dest_loc, head_num, BLOCK_SIZE=128):\n    # Determine the dimensions\n    batch_size, _, seq_len = K.shape\n    assert Dest_loc.shape[0] == seq_len, \"Dest_loc length must match sequence length of K\"\n\n    # Prepare the output tensor\n    Out = torch.empty_like(K)\n\n    # Compute strides\n    stride_k_head = K.stride(1)\n    stride_k_seq = K.stride(2)\n    stride_out_head = Out.stride(1)\n    stride_out_seq = Out.stride(2)\n\n    # Launch the Triton kernel\n    grid = (head_num, (seq_len + BLOCK_SIZE - 1) // BLOCK_SIZE)\n    _fwd_kernel_destindex_copy_kv[grid](\n        K, Dest_loc, Out, seq_len, head_num,\n        stride_k_head, stride_k_seq, stride_out_head, stride_out_seq,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return Out\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_destindex_copy_kv(\n    K, Dest_loc,\n    Out,\n    stride_k_bs, stride_k_h, stride_k_d,\n    stride_o_bs, stride_o_h, stride_o_d,\n    head_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_HEAD: tl.constexpr\n):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    dest_index = tl.load(Dest_loc + cur_index)\n\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None] + stride_o_d * offs_d[None, :]\n\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n\n\n@torch.no_grad()\ndef destindex_copy_kv(K, DestLoc, Out):\n    seq_len = DestLoc.shape[0]\n    head_num = K.shape[1]\n    head_dim = K.shape[2]\n    assert K.shape[1] == Out.shape[1] and K.shape[2] == Out.shape[2]\n    BLOCK_HEAD = triton.next_power_of_2(head_num)\n    grid = (seq_len,)\n    num_warps = 1\n\n    _fwd_kernel_destindex_copy_kv[grid](\n        K, DestLoc, Out,\n        K.stride(0), K.stride(1), K.stride(2),\n        Out.stride(0), Out.stride(1), Out.stride(2),\n        head_num,\n        BLOCK_DMODEL=head_dim,\n        BLOCK_HEAD=BLOCK_HEAD,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator defines a kernel `_fwd_kernel` used for attention computation. The kernel takes input tensors Q, K, V and outputs the result to Out. It uses a blocking strategy with block size BLOCK_M, BLOCK_DMODEL, and BLOCK_N to optimize memory usage and parallelism. The context_attention_fwd function prepares the arguments and launches the kernel, setting the grid size based on input dimensions. It enforces head dimension Lk constraints and uses scales computed from Q's dimension to adjust the dot product computations within the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define block sizes\nBLOCK_M = 128  # Block size for query dimension\nBLOCK_DMODEL = 64  # Block size for model dimension\nBLOCK_N = 128  # Block size for key/value dimension\n\n@triton.jit\ndef _fwd_kernel(Q, K, V, Out, stride_qm, stride_qk, stride_km, stride_kn, stride_vm, stride_vn, stride_om, stride_on, Lk, scale, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Compute program IDs\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Define ranges for blocks\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    # Load Q, K, V blocks\n    Q_block = tl.load(Q + rm[:, None] * stride_qm + tl.arange(0, BLOCK_DMODEL) * stride_qk)\n    K_block = tl.load(K + rn[None, :] * stride_kn + tl.arange(0, BLOCK_DMODEL) * stride_km)\n    V_block = tl.load(V + rn[None, :] * stride_vn + tl.arange(0, BLOCK_DMODEL) * stride_vm)\n\n    # Compute scaled dot product\n    QK = tl.dot(Q_block, K_block, trans_b=True)\n    QK_scaled = QK * scale\n\n    # Apply softmax to the scaled dot product\n    QK_softmax = tl.softmax(QK_scaled, axis=1)\n\n    # Compute attention output\n    Out_block = tl.dot(QK_softmax, V_block)\n\n    # Store the result\n    tl.store(Out + rm[:, None] * stride_om + rn[None, :] * stride_on, Out_block)\n\ndef context_attention_fwd(Q, K, V, Lk, BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL, BLOCK_N=BLOCK_N):\n    # Check dimensions\n    assert Q.shape[1] == K.shape[1] == V.shape[1] == Lk, \"Mismatch in head dimension Lk\"\n    \n    # Compute scale factor\n    scale = 1.0 / (Q.shape[1] ** 0.5)\n\n    # Prepare output tensor\n    Out = torch.empty((Q.shape[0], V.shape[0]), device=Q.device, dtype=Q.dtype)\n\n    # Define grid size\n    grid = (triton.cdiv(Q.shape[0], BLOCK_M), triton.cdiv(V.shape[0], BLOCK_N))\n\n    # Launch the kernel\n    _fwd_kernel[grid](\n        Q, K, V, Out,\n        Q.stride(0), Q.stride(1),\n        K.stride(0), K.stride(1),\n        V.stride(0), V.stride(1),\n        Out.stride(0), Out.stride(1),\n        Lk, scale,\n        BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL, BLOCK_N=BLOCK_N\n    )\n\n    return Out\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\nif triton.__version__ >= \"2.1.0\":\n    @triton.jit\n    def _fwd_kernel(\n        Q, K, V, sm_scale, B_Start_Loc, B_Seqlen,  # B_LOC \u5185\u90e8\u8bb0\u5f55\u6bcf\u4e2abatch \u8f93\u5165\u7684\u771f\u5b9e\u4f4d\u7f6e\uff0c B_SEQ_len \u8bb0\u5f55\u5f53\u524d\u8f93\u5165\u7684\u771f\u5b9e\u957f\u5ea6\n        Out,\n        stride_qbs, stride_qh, stride_qd,\n        stride_kbs, stride_kh, stride_kd,\n        stride_vbs, stride_vh, stride_vd,\n        stride_obs, stride_oh, stride_od,\n        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n        BLOCK_N: tl.constexpr,\n    ):\n        cur_batch = tl.program_id(0)\n        cur_head = tl.program_id(1)\n        start_m = tl.program_id(2)\n\n        cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n        cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n        block_start_loc = BLOCK_M * start_m\n\n        # initialize offsets\n        offs_n = tl.arange(0, BLOCK_N)\n        offs_d = tl.arange(0, BLOCK_DMODEL)\n        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        off_q = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n        off_k = offs_n[None, :] * stride_kbs + cur_head * stride_kh + offs_d[:, None] * stride_kd\n        off_v = offs_n[:, None] * stride_vbs + cur_head * stride_vh + offs_d[None, :] * stride_vd\n\n        q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n\n        k_ptrs = K + off_k\n        v_ptrs = V + off_v\n\n        # initialize pointer to m and l\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n\n        block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n\n        for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n            start_n = tl.multiple_of(start_n, BLOCK_N)\n            # -- compute qk ----\n            k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,\n                        mask=(start_n + offs_n[None, :]) < cur_batch_seq_len, other=0.0)\n            # mask = tl.load(mask_ptrs + start_n, mask=start_n + offs_n < cur_batch_end_loc, other=0.0)\n\n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, k)\n            qk *= sm_scale\n            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n\n            # -- compute m_ij, p, l_ij\n            m_ij = tl.max(qk, 1)\n            p = tl.exp(qk - m_ij[:, None])\n            l_ij = tl.sum(p, 1)\n            # -- update m_i and l_i\n            m_i_new = tl.maximum(m_i, m_ij)\n            alpha = tl.exp(m_i - m_i_new)\n            beta = tl.exp(m_ij - m_i_new)\n            l_i_new = alpha * l_i + beta * l_ij\n            # -- update output accumulator --\n            # scale p\n            p_scale = beta / l_i_new\n            p = p * p_scale[:, None]\n            # scale acc\n            acc_scale = l_i / l_i_new * alpha\n            acc = acc * acc_scale[:, None]\n            # update acc\n            v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,\n                        mask=(start_n + offs_n[:, None]) < cur_batch_seq_len, other=0.0)\n\n            p = p.to(v.dtype)\n            acc += tl.dot(p, v)\n            # update m_i and l_i\n            l_i = l_i_new\n            m_i = m_i_new\n        # initialize pointers to output\n        off_o = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n        out_ptrs = Out + off_o\n        tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n        return\n\n    @torch.no_grad()\n    def context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len):\n        BLOCK = 128\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        assert Lq == Lk and Lk == Lv\n        assert Lk in {16, 32, 64, 128}\n\n        sm_scale = 1.0 / (Lq**0.5)  # \u8ba1\u7b97scale\u7cfb\u6570\n        batch, head = b_seq_len.shape[0], q.shape[1]\n\n        grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n\n        num_warps = 4 if Lk <= 64 else 8\n        _fwd_kernel[grid](\n            q, k, v, sm_scale, b_start_loc, b_seq_len,\n            o,\n            q.stride(0), q.stride(1), q.stride(2),\n            k.stride(0), k.stride(1), k.stride(2),\n            v.stride(0), v.stride(1), v.stride(2),\n            o.stride(0), o.stride(1), o.stride(2),\n            BLOCK_M=BLOCK,\n            BLOCK_DMODEL=Lk,\n            BLOCK_N=BLOCK,\n            num_warps=num_warps,\n            num_stages=1,\n        )\n        return\n\nelse:\n    raise Exception(\"error triton version!\")\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines a Triton kernel function called `kernel_function` and a Python function `call_kernel` to apply the kernel. \n            The kernel computes the sine of elements in an input tensor `x` and stores the results in an output tensor.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef kernel_function(x_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Calculate the block index\n    block_idx = tl.program_id(0)\n    \n    # Calculate the starting index for this block\n    start_idx = block_idx * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = start_idx + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n    \n    # Load input data from global memory\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Compute the sine of each element\n    y = tl.sin(x)\n    \n    # Store the result back to global memory\n    tl.store(y_ptr + offsets, y, mask=mask)\n\nimport torch\n\ndef call_kernel(x):\n    # Ensure input is a Torch tensor on the GPU\n    assert x.is_cuda, \"Input tensor must be on the GPU\"\n    \n    # Allocate output tensor on the GPU\n    y = torch.empty_like(x)\n    \n    # Define the block size (you can adjust this for performance tuning)\n    BLOCK_SIZE = 1024\n    \n    # Calculate the number of blocks needed\n    n_elements = x.numel()\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the kernel\n    kernel_function[grid](x, y, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# Kernel function using Triton\n@triton.jit\ndef kernel_function(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # x_ptr: pointer to input data\n    # output_ptr: pointer to output data\n    # n_elements: number of elements to process\n    # BLOCK_SIZE: block size for Triton kernel\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.math.sin(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# Function to call the Triton kernel\ndef call_kernel(x):\n    # x: input tensor\n    n_elements = x.numel()\n    output = torch.empty_like(x)\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    kernel_function[grid](x, output, n_elements, BLOCK_SIZE=1024)\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton-based operator computes cross-entropy loss with optional label smoothing and scaling for logits. The `cross_entropy_fwd_kernel` and `cross_entropy_bwd_kernel` are the forward and backward kernels, respectively. \n            \n            The `cross_entropy_loss` function wraps around the `CrossEntropyLoss` class, which implements these kernels. The forward method calculates the loss and logs-sum-exp values and can handle both split scenarios and parallelism. The backward method computes gradients with respect to logits. The inputs include logits, labels, optional smoothing and scaling factors, an ignored index, and a process group for distributed computation. The outputs are the computed losses and any auxiliary loss values (like z-losses for stability). \n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    logits_ptr, labels_ptr, loss_ptr, lse_ptr,  # pointers to data\n    n_classes, smoothing, scale, ignore_index,  # parameters\n    BLOCK_SIZE: tl.constexpr\n):\n    # Define block index\n    block_idx = tl.program_id(0)\n    \n    # Compute the offset for this block\n    start_idx = block_idx * BLOCK_SIZE\n    \n    # Load logits and labels\n    logits = tl.load(logits_ptr + start_idx, mask=start_idx < n_classes)\n    labels = tl.load(labels_ptr + block_idx, mask=block_idx < n_classes)\n\n    # Compute log-sum-exp for numerical stability\n    max_logits = tl.max(logits, axis=0)\n    exp_logits = tl.exp(logits - max_logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n    lse = max_logits + tl.log(sum_exp_logits)\n    \n    # Store log-sum-exp\n    tl.store(lse_ptr + block_idx, lse, mask=block_idx < n_classes)\n\n    # Compute cross-entropy loss\n    correct_logits = logits[labels]\n    loss = lse - correct_logits\n\n    # Apply label smoothing if specified\n    if smoothing > 0.0:\n        loss = (1.0 - smoothing) * loss + smoothing * lse / n_classes\n\n    # Apply scaling if specified\n    if scale != 1.0:\n        loss *= scale\n\n    # Ignore certain indices\n    if labels == ignore_index:\n        loss = 0.0\n\n    # Store the loss\n    tl.store(loss_ptr + block_idx, loss, mask=block_idx < n_classes)\n\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    logits_ptr, labels_ptr, grads_ptr, d_logits_ptr,  # pointers to data\n    n_classes, smoothing, scale, ignore_index,  # parameters\n    BLOCK_SIZE: tl.constexpr\n):\n    # Define block index\n    block_idx = tl.program_id(0)\n    \n    # Compute the offset for this block\n    start_idx = block_idx * BLOCK_SIZE\n    \n    # Load logits and labels\n    logits = tl.load(logits_ptr + start_idx, mask=start_idx < n_classes)\n    labels = tl.load(labels_ptr + block_idx, mask=block_idx < n_classes)\n    \n    # Load the gradients\n    grads = tl.load(grads_ptr + block_idx, mask=block_idx < n_classes)\n\n    # Compute probabilities\n    max_logits = tl.max(logits, axis=0)\n    exp_logits = tl.exp(logits - max_logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n    probs = exp_logits / sum_exp_logits\n\n    # Compute gradient with respect to logits\n    d_logits = probs\n    d_logits[labels] -= 1.0\n\n    # Apply label smoothing if specified\n    if smoothing > 0.0:\n        d_logits = (1.0 - smoothing) * d_logits + smoothing / n_classes\n\n    # Apply scaling if specified\n    if scale != 1.0:\n        d_logits *= scale\n\n    # Ignore certain indices\n    if labels == ignore_index:\n        d_logits = 0.0\n\n    # Store the gradients\n    tl.store(d_logits_ptr + start_idx, d_logits * grads, mask=start_idx < n_classes)\n\nimport torch\n\nclass CrossEntropyLoss:\n    def __init__(self, smoothing=0.0, scale=1.0, ignore_index=-1):\n        self.smoothing = smoothing\n        self.scale = scale\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, labels):\n        # Allocate memory for outputs\n        loss = torch.empty_like(labels, dtype=logits.dtype)\n        lse = torch.empty_like(labels, dtype=logits.dtype)\n        \n        # Launch the forward kernel\n        grid = (labels.numel(),)\n        cross_entropy_fwd_kernel[grid](\n            logits, labels, loss, lse,\n            logits.shape[-1], self.smoothing, self.scale, self.ignore_index,\n            BLOCK_SIZE=128\n        )\n        \n        return loss, lse\n\n    def backward(self, logits, labels, grad_output):\n        # Allocate memory for gradients\n        d_logits = torch.empty_like(logits)\n        \n        # Launch the backward kernel\n        grid = (labels.numel(),)\n        cross_entropy_bwd_kernel[grid](\n            logits, labels, grad_output, d_logits,\n            logits.shape[-1], self.smoothing, self.scale, self.ignore_index,\n            BLOCK_SIZE=128\n        )\n        \n        return d_logits\n\ndef cross_entropy_loss(logits, labels, smoothing=0.0, scale=1.0, ignore_index=-1):\n    loss_fn = CrossEntropyLoss(smoothing, scale, ignore_index)\n    return loss_fn.forward(logits, labels)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_fwd_kernel(\n    loss_ptr,  # data ptrs\n    lse_ptr,\n    z_loss_ptr,\n    logits_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    n_rows,\n    logits_row_stride,  # strides\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n    SPLIT: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(\n            n_cols, (col_block_idx + 1) * BLOCK_SIZE\n        ):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (\n                    (lse if not SPLIT else 0.0)\n                    - smoothing * sum_logits / total_classes\n                    - (1 - smoothing) * logits_label\n                )\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        else:\n            if HAS_SMOOTHING:\n                loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n            else:\n                loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n\n@triton.heuristics(\n    {\n        \"HAS_SMOOTHING\": lambda args: args[\"smoothing\"] > 0.0,\n    }\n)\n@triton.jit\ndef cross_entropy_bwd_kernel(\n    dlogits_ptr,  # data ptrs\n    dloss_ptr,\n    logits_ptr,\n    lse_ptr,\n    labels_ptr,\n    smoothing,\n    logit_scale,\n    lse_square_scale,\n    ignored_index,\n    total_classes,\n    class_start_idx,  # Useful for tensor parallel when each rank only has a subset of classes\n    n_cols,  # shapes\n    logits_row_stride,  # strides\n    dlogits_row_stride,\n    dloss_row_stride,\n    BLOCK_SIZE: tl.constexpr,\n    HAS_SMOOTHING: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")).to(\n        tl.float32\n    ) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, (dloss * logit_scale) * probs, mask=col_offsets < n_cols)\n\nclass CrossEntropyLoss(torch.autograd.Function):\n\n    @staticmethod\n    def forward(\n        ctx,\n        logits,\n        labels,\n        smoothing=0.0,\n        logit_scale=1.0,\n        lse_square_scale=0.0,\n        ignored_index=-100,\n        inplace_backward=False,\n        process_group=None,\n    ):\n        n_rows, n_cols = logits.shape\n        assert labels.shape == (n_rows,)\n        world_size = 1 if process_group is None else torch.distributed.get_world_size(process_group)\n        total_classes = world_size * n_cols\n        rank = 0 if process_group is None else torch.distributed.get_rank(process_group)\n        class_start_idx = rank * n_cols\n\n        if logits.stride(-1) != 1:\n            logits = logits.contiguous()\n        MAX_BLOCK_SIZE = 64 * 1024\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), MAX_BLOCK_SIZE)\n        num_warps = (\n            4\n            if BLOCK_SIZE < 2048\n            else (8 if BLOCK_SIZE < 8192 else (16 if BLOCK_SIZE < 128 * 1024 else 32))\n        )\n        split = world_size > 1 or n_cols > MAX_BLOCK_SIZE\n        n_splits = (n_cols + BLOCK_SIZE - 1) // BLOCK_SIZE\n        loss_shape = (n_splits, n_rows) if n_splits > 1 else (n_rows,)\n        losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        lse = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        z_losses = torch.empty(*loss_shape, dtype=torch.float, device=logits.device)\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_fwd_kernel[(n_rows, n_splits)](\n                losses,  # data ptrs\n                lse,\n                z_losses,\n                logits,\n                labels,\n                smoothing,\n                logit_scale,\n                lse_square_scale,\n                ignored_index,\n                total_classes,\n                class_start_idx,\n                n_cols,  # shapes\n                n_rows,\n                logits.stride(0),  # strides\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n                SPLIT=split,\n            )\n\n        if split:\n            if n_splits > 1:\n                lse = torch.logsumexp(lse, dim=0)\n                losses = losses.sum(dim=0)\n            if world_size > 1:\n                lse_allgather = torch.empty(world_size, n_rows, dtype=lse.dtype, device=lse.device)\n                torch.distributed.all_gather_into_tensor(lse_allgather, lse, group=process_group)\n                handle_losses = torch.distributed.all_reduce(\n                    losses, op=torch.distributed.ReduceOp.SUM, group=process_group, async_op=True\n                )\n                lse = torch.logsumexp(lse_allgather, dim=0)\n                handle_losses.wait()\n            losses += lse\n            if lse_square_scale != 0.0:\n                z_losses = lse_square_scale * lse.square()\n                z_losses.masked_fill_(labels == ignored_index, 0.0)\n                losses += z_losses\n            else:\n                z_losses = torch.zeros_like(losses)\n            losses.masked_fill_(labels == ignored_index, 0.0)\n\n        ctx.save_for_backward(logits, lse, labels)\n        ctx.mark_non_differentiable(z_losses)\n        ctx.smoothing = smoothing\n        ctx.logit_scale = logit_scale\n        ctx.lse_square_scale = lse_square_scale\n        ctx.ignored_index = ignored_index\n        ctx.total_classes = total_classes\n        ctx.class_start_idx = class_start_idx\n        ctx.inplace_backward = inplace_backward\n\n        return losses, z_losses\n\n    @staticmethod\n    def backward(ctx, grad_losses, grad_z_losses):\n        del grad_z_losses  # z_losses are only for logging.\n\n        logits, lse, labels = ctx.saved_tensors\n        dlogits = logits if ctx.inplace_backward else torch.empty_like(logits)\n        n_rows, n_cols = logits.shape\n        BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 4 * 1024)\n        num_warps = 4 if BLOCK_SIZE < 2048 else (8 if BLOCK_SIZE < 8192 else 16)\n        grid = lambda META: (n_rows, triton.cdiv(n_cols, META[\"BLOCK_SIZE\"]))  # noqa\n        with torch.cuda.device(logits.device.index):\n            cross_entropy_bwd_kernel[grid](\n                dlogits,  # data ptrs\n                grad_losses,\n                logits,\n                lse,\n                labels,\n                ctx.smoothing,\n                ctx.logit_scale,\n                ctx.lse_square_scale,\n                ctx.ignored_index,\n                ctx.total_classes,\n                ctx.class_start_idx,\n                n_cols,  # shapes\n                logits.stride(0),  # strides\n                dlogits.stride(0),\n                grad_losses.stride(0),\n                BLOCK_SIZE=BLOCK_SIZE,  # constants\n                num_warps=num_warps,\n            )\n        return dlogits, None, None, None, None, None, None, None, None\n\ndef cross_entropy_loss(\n    logits: torch.Tensor,\n    labels: torch.Tensor,\n    label_smoothing: float = 0.0,\n    logit_scale: float = 1.0,\n    lse_square_scale: float = 0.0,\n    ignored_index=-100,\n    inplace_backward: bool = False,\n    process_group=None,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    return CrossEntropyLoss.apply(\n        logits,\n        labels,\n        label_smoothing,\n        logit_scale,\n        lse_square_scale,\n        ignored_index,\n        inplace_backward,\n        process_group,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton code defines a kernel function called `add_kernel` and a wrapper function `add_wrapper`. The main function of `add_kernel` is to add two input tensors element-wise and store the result in an output tensor. The kernel handles elements in blocks of a defined size, `BLOCK_SIZE`. The wrapper function `add_wrapper` initializes the output tensor and launches the kernel with the necessary parameters. Inputs to the functions are pointers to input tensors, the output tensor, number of elements, and block size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the block size for processing elements\nBLOCK_SIZE = 128\n\n@triton.jit\ndef add_kernel(\n    x_ptr, y_ptr, output_ptr, n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Calculate the block index and the range of elements this block will handle\n    block_start = tl.program_id(0) * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n\n    # Load input elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    # Perform element-wise addition\n    result = x + y\n\n    # Store the result in the output tensor\n    tl.store(output_ptr + offsets, result, mask=mask)\n\ndef add_wrapper(x, y, output, n_elements):\n    # Calculate the number of blocks needed\n    grid = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[grid](\n        x, y, output, n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    import torch\n\n    # Initialize input tensors\n    n_elements = 1024\n    x = torch.rand(n_elements, dtype=torch.float32).cuda()\n    y = torch.rand(n_elements, dtype=torch.float32).cuda()\n    output = torch.empty_like(x)\n\n    # Call the wrapper function\n    add_wrapper(x, y, output, n_elements)\n\n    # Verify the result\n    assert torch.allclose(output, x + y)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    in_ptr0,\n    in_ptr1,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef add_wrapper(x, y):\n    out = torch.zeros_like(x)\n    \n    BLOCK_SIZE = 4\n    n_elements = x.numel()\n\n    # Calculate the number of blocks needed\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)\n\n    return out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton kernel, `block_sparse_attention_kernel`, computes block-sparse attention for a transformer model during the prompt phase, assuming no past sequence length. The key inputs are Q (queries), K (keys), V (values), and layout information in CSR format. The outputs are stored in 'out'. The kernel processes data in blocks defined by BLOCK_M, BLOCK_N, and BLOCK_D, iterating over query sequence length blocks. It computes query-key products, applies a softmax, and uses the results to produce weighted value sums for the output. It handles scenarios with multiple data blocks when NUM_D_BLOCKS is greater than 1.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Kernel function for block-sparse attention\n@triton.jit\ndef block_sparse_attention_kernel(\n    Q_ptr, K_ptr, V_ptr, out_ptr, row_ptr, col_indices, \n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_D: tl.constexpr,\n    NUM_D_BLOCKS: tl.constexpr, num_heads: tl.constexpr, head_dim: tl.constexpr\n):\n    # Get the program ID and calculate the block indices\n    pid = tl.program_id(axis=0)\n    num_q_blocks = tl.num_programs(axis=0)\n\n    # Calculate row and column block indices\n    row_block_idx = pid // num_heads\n    col_block_idx = pid % num_heads\n\n    # Calculate the start index for this block\n    row_start = row_ptr[row_block_idx]\n    row_end = row_ptr[row_block_idx + 1]\n\n    # Iterate over the column indices for this row block\n    for col_idx in range(row_start, row_end):\n        # Get the column index from CSR format\n        col = col_indices[col_idx]\n\n        # Calculate block offsets\n        q_offset = (row_block_idx * BLOCK_M) * head_dim + col_block_idx * BLOCK_D\n        k_offset = (col * BLOCK_N) * head_dim + col_block_idx * BLOCK_D\n        v_offset = (col * BLOCK_N) * head_dim + col_block_idx * BLOCK_D\n\n        # Load Q, K, V blocks\n        Q_block = tl.load(Q_ptr + q_offset)\n        K_block = tl.load(K_ptr + k_offset)\n        V_block = tl.load(V_ptr + v_offset)\n\n        # Compute the query-key product\n        qk_product = tl.dot(Q_block, K_block.T)\n\n        # Apply softmax\n        qk_softmax = tl.softmax(qk_product)\n\n        # Compute the weighted sum for the output\n        out_block = tl.dot(qk_softmax, V_block)\n\n        # Store the result in the output tensor\n        out_offset = (row_block_idx * BLOCK_M) * head_dim + col_block_idx * BLOCK_D\n        tl.store(out_ptr + out_offset, out_block)\n\n# Wrapper function for the kernel\ndef block_sparse_attention(Q, K, V, layout, BLOCK_M, BLOCK_N, BLOCK_D, num_heads, head_dim):\n    # Extract row pointers and column indices from the CSR layout\n    row_ptr, col_indices = layout\n\n    # Define grid size\n    num_q_blocks = len(row_ptr) - 1\n    grid = (num_q_blocks * num_heads,)\n\n    # Launch the Triton kernel\n    block_sparse_attention_kernel[grid](\n        Q, K, V, out, row_ptr, col_indices,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_D=BLOCK_D,\n        NUM_D_BLOCKS=(head_dim // BLOCK_D), num_heads=num_heads, head_dim=head_dim\n    )\n\n# Example usage\n# Assuming Q, K, V, and layout are pre-defined tensors and CSR format respectively\n# BLOCK_M, BLOCK_N, BLOCK_D, num_heads, and head_dim are defined constants\n# block_sparse_attention(Q, K, V, layout, BLOCK_M, BLOCK_N, BLOCK_D, num_heads, head_dim)\n", "label": "\nimport triton\nimport triton.language as tl\n\n# This kernel is for prompt only and assume that past sequence length is 0. It only supports right padding.\n@triton.jit\ndef block_sparse_attention_kernel(\n    out,  # output [B, H, M, D]. Note that B is batch_size, H is num_heads, M is q_seq_len, and D is head_size\n    Q,  # query [B, H, M, D]\n    K,  # key [B, H_kv, N, D]. Note that N is max_seq_len for kv cache, H_kv is num_kv_heads\n    V,  # value [B, H_kv, N, D]\n    layout_csr_row_indices,  # block mask CSR format. Shape is [L, num_rows + 1] where num_rows = max_seq_len / BLOCK_M\n    layout_csr_col_indices,  # block mask CSR format. Shape is [L, num_rows * num_cols] where num_cols = max_seq_len / BLOCK_N\n    layout_csr_row_stride_h,  # stride per head for csr_row_indices, i.e. num_rows + 1\n    layout_csr_col_stride_h,  # stride per head for csr_col_indices, i.e. num_rows * num_cols\n    num_layout,  # number of sparse layout (L)\n    softmax_scale,\n    stride_qb,\n    stride_qh,\n    stride_qm,\n    stride_kb,\n    stride_kh,\n    stride_kn,\n    stride_vb,\n    stride_vh,\n    stride_vn,\n    stride_ob,\n    stride_oh,\n    stride_om,\n    num_heads,\n    num_kv_heads,\n    total_seq_len,  # Total sequence length including past sequence length and query sequence length.\n    BLOCK_M: tl.constexpr,  # block size for q_seq_len\n    EVEN_M: tl.constexpr,  # whether q_seq_len % BLOCK_M == 0\n    BLOCK_N: tl.constexpr,  # block size for k_seq_len\n    EVEN_N: tl.constexpr,  # whether k_seq_len % BLOCK_N == 0\n    BLOCK_D: tl.constexpr,  # block size for D\n    NUM_D_BLOCKS: tl.constexpr,  # number of data blocks =  D / BLOCK_D\n):\n    tl.static_print(f\"{BLOCK_M=} {BLOCK_N=} {BLOCK_D=} {EVEN_M=} {EVEN_N=} {NUM_D_BLOCKS=}\")\n\n    # Past sequence length is 0 since this kernel is for prompt only.\n    q_seq_len = total_seq_len\n\n    # Grid is [CDiv(q_seq_len, BLOCK_M), batch_size * num_heads]\n    start_m = tl.program_id(0)\n    off_bh = tl.program_id(1)\n\n    off_h = off_bh % num_heads\n    off_b = off_bh // num_heads\n\n    # For group query attention, map the query head index to the corresponding one for key and value.\n    head_groups = num_heads // num_kv_heads\n    off_h_kv = off_h // head_groups\n\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h_kv * stride_kh\n    V += off_b * stride_vb + off_h_kv * stride_vh\n\n    # Initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_D)\n    off_q = offs_m[:, None] * stride_qm + offs_d[None, :]  # [BLOCK_M, BLOCK_D]\n    off_k = offs_n[None, :] * stride_kn + offs_d[:, None]  # [BLOCK_D, BLOCK_N]\n    off_v = offs_n[:, None] * stride_vn + offs_d[None, :]  # [BLOCK_N, BLOCK_D]\n\n    # Initialize pointers to query, key, value\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n\n    # Initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n    if NUM_D_BLOCKS >= 2:\n        acc2 = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)\n\n    # Load q: it will stay in SRAM throughout\n    if EVEN_M:\n        q = tl.load(q_ptrs)\n        if NUM_D_BLOCKS >= 2:\n            q2 = tl.load(q_ptrs + BLOCK_D)\n    else:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < q_seq_len)\n        if NUM_D_BLOCKS >= 2:\n            q2 = tl.load(q_ptrs + BLOCK_D, mask=offs_m[:, None] < q_seq_len)\n\n    layout_h = off_h % num_layout\n\n    # This assumes that past sequence length is 0, otherwise need + (past_seq_len + 1) // BLOCK_M.\n    layout_ptr = layout_csr_row_indices + layout_h * layout_csr_row_stride_h + start_m\n    start_l = tl.load(layout_ptr).to(tl.int32)\n    end_l = tl.load(layout_ptr + 1).to(tl.int32)\n\n    # Loop over k, v and update accumulator\n    for col_idx_idx in range(start_l, end_l):\n        col_idx = tl.load(layout_csr_col_indices + layout_h * layout_csr_col_stride_h + col_idx_idx).to(tl.int32)\n        start_n = col_idx * BLOCK_N\n        # -- compute qk ----\n        if EVEN_N:\n            k = tl.load(k_ptrs + start_n * stride_kn)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_n[None, :] + start_n < total_seq_len)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n\n        if NUM_D_BLOCKS >= 2:\n            if EVEN_N:\n                k = tl.load(k_ptrs + start_n * stride_kn + BLOCK_D)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn + BLOCK_D, mask=offs_n[None, :] + start_n < total_seq_len)\n            qk += tl.dot(q2, k)\n\n        qk *= softmax_scale\n\n        # This assumes that past sequence length is 0, otherwise need offs_m[:, None] + past_seq_len >= ...\n        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n        # -- compute m_ij, p, l_ij\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        # -- update m_i and l_i\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        # -- update output accumulator --\n        # scale p\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        # scale acc\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        if NUM_D_BLOCKS >= 2:\n            acc2 = acc2 * acc_scale[:, None]\n        p = p.to(Q.dtype.element_ty)\n        # update acc\n        if EVEN_N:\n            v = tl.load(v_ptrs + start_n * stride_vn)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_n[:, None] + start_n < total_seq_len)\n        acc += tl.dot(p, v)\n\n        if NUM_D_BLOCKS >= 2:\n            if EVEN_N:\n                v = tl.load(v_ptrs + start_n * stride_vn + BLOCK_D)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn + BLOCK_D, mask=offs_n[:, None] + start_n < total_seq_len)\n            acc2 += tl.dot(p, v)\n\n        # update m_i and l_i\n        l_i = l_i_new\n        m_i = m_i_new\n\n    off_o = off_b * stride_ob + off_h * stride_oh + offs_m[:, None] * stride_om + offs_d[None, :]\n    out_ptrs = out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < q_seq_len)\n    if NUM_D_BLOCKS >= 2:\n        tl.store(out_ptrs + BLOCK_D, acc2, mask=offs_m[:, None] < q_seq_len)\n\n# Define the wrapper function to call the Triton kernel\ndef block_sparse_attention(\n    Q, K, V, layout_csr_row_indices, layout_csr_col_indices, layout_csr_row_stride_h, layout_csr_col_stride_h,\n    num_layout, softmax_scale, num_heads, num_kv_heads, total_seq_len, BLOCK_M, EVEN_M, BLOCK_N, EVEN_N, BLOCK_D, NUM_D_BLOCKS\n):\n    # Determine the grid size\n    q_seq_len = total_seq_len\n    grid = (triton.cdiv(q_seq_len, BLOCK_M), Q.shape[0] * num_heads)\n\n    # Allocate output tensor\n    out = torch.empty((Q.shape[0], num_heads, q_seq_len, Q.shape[-1]), device=Q.device, dtype=Q.dtype)\n\n    # Call the Triton kernel\n    block_sparse_attention_kernel[grid](\n        out, Q, K, V, layout_csr_row_indices, layout_csr_col_indices, layout_csr_row_stride_h, layout_csr_col_stride_h,\n        num_layout, softmax_scale, Q.stride(0), Q.stride(1), Q.stride(2), K.stride(0), K.stride(1), K.stride(2),\n        V.stride(0), V.stride(1), V.stride(2), out.stride(0), out.stride(1), out.stride(2), num_heads, num_kv_heads,\n        total_seq_len, BLOCK_M=BLOCK_M, EVEN_M=EVEN_M, BLOCK_N=BLOCK_N, EVEN_N=EVEN_N, BLOCK_D=BLOCK_D, NUM_D_BLOCKS=NUM_D_BLOCKS\n    )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator code defines custom kernels for integer matrix multiplication and scaled integer matrix multiplication using Triton, a language for writing highly efficient GPU code. The main functions are `matmul_kernel_with_block_pointers` and `scaled_matmul_kernel_with_block_pointers`, both decorated with `@triton.jit`, which compiles them for execution on GPUs.\n\n            - `matmul_kernel_with_block_pointers` computes the matrix multiplication C = A x B. It takes pointers to matrices A, B, and C, along with their dimensions M, N, K, and their strides. The kernel splits the matrices into blocks defined by `BLOCK_M`, `BLOCK_N`, and `BLOCK_K`, and accumulates the results block by block to ensure efficient use of GPU resources. The kernel supports boundary checks when loading and storing data to handle out-of-bound accesses safely.\n\n            - `scaled_matmul_kernel_with_block_pointers` performs a scaled matrix multiplication where the result is multiplied by a scale factor stored in `scales1`. The implementation is similar to the previous kernel but includes additional operations to multiply the result by the scale matrix.\n\n            - `int_matmul_kernel` and `int_scaled_matmul_kernel` are the functions that set up and launch these kernels. They calculate the grid size based on matrix dimensions and use Triton's kernel launch syntax to execute the kernels on the GPU. The kernels are configured using an instance of the `Config` class, which holds parameters like `num_warps`, `num_stages`, and `num_ctas` to control the kernel's execution configuration.\n\n            - The `Config` class is a simple configuration holder that allows setting these execution parameters easily. It passes them to the kernel launch call through `config.kwargs`.\n\n            These kernels are specialized for integer arithmetic and are structured to maximize data locality and performance on modern GPUs using Triton's APIs and language features.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Configuration class to hold kernel execution parameters\nclass Config:\n    def __init__(self, num_warps=4, num_stages=3, num_ctas=1):\n        self.kwargs = {\n            'num_warps': num_warps,\n            'num_stages': num_stages,\n            'num_ctas': num_ctas\n        }\n\n# Kernel for integer matrix multiplication C = A x B\n@triton.jit\ndef matmul_kernel_with_block_pointers(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    # Compute block indices\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    block_m = pid // grid_n\n    block_n = pid % grid_n\n\n    # Initialize pointers for the current block\n    a_ptr = A_ptr + block_m * BLOCK_M * stride_am\n    b_ptr = B_ptr + block_n * BLOCK_N * stride_bn\n    c_ptr = C_ptr + block_m * BLOCK_M * stride_cm + block_n * BLOCK_N\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n\n    # Loop over K dimension\n    for k in range(0, K, BLOCK_K):\n        # Load A and B blocks\n        a_block = tl.load(a_ptr + k * stride_ak, mask=(k + tl.arange(0, BLOCK_K)[:, None] < K))\n        b_block = tl.load(b_ptr + k * stride_bk, mask=(k + tl.arange(0, BLOCK_K) < K))\n\n        # Matrix multiplication\n        acc += tl.dot(a_block, b_block)\n\n    # Store result\n    c_mask = (block_m * BLOCK_M + tl.arange(0, BLOCK_M)[:, None] < M) & (block_n * BLOCK_N + tl.arange(0, BLOCK_N) < N)\n    tl.store(c_ptr, acc, mask=c_mask)\n\n# Kernel for scaled integer matrix multiplication C = A x B * scales1\n@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(\n    A_ptr, B_ptr, C_ptr, scales1_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr\n):\n    # Compute block indices\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    block_m = pid // grid_n\n    block_n = pid % grid_n\n\n    # Initialize pointers for the current block\n    a_ptr = A_ptr + block_m * BLOCK_M * stride_am\n    b_ptr = B_ptr + block_n * BLOCK_N * stride_bn\n    c_ptr = C_ptr + block_m * BLOCK_M * stride_cm + block_n * BLOCK_N\n    scales1_ptr = scales1_ptr + block_m * BLOCK_M\n\n    # Initialize accumulators\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n\n    # Loop over K dimension\n    for k in range(0, K, BLOCK_K):\n        # Load A and B blocks\n        a_block = tl.load(a_ptr + k * stride_ak, mask=(k + tl.arange(0, BLOCK_K)[:, None] < K))\n        b_block = tl.load(b_ptr + k * stride_bk, mask=(k + tl.arange(0, BLOCK_K) < K))\n\n        # Matrix multiplication\n        acc += tl.dot(a_block, b_block)\n\n    # Load scales and apply scaling\n    scales1 = tl.load(scales1_ptr)\n    acc *= scales1[:, None]\n\n    # Store result\n    c_mask = (block_m * BLOCK_M + tl.arange(0, BLOCK_M)[:, None] < M) & (block_n * BLOCK_N + tl.arange(0, BLOCK_N) < N)\n    tl.store(c_ptr, acc, mask=c_mask)\n\n# Wrapper function for integer matrix multiplication\ndef int_matmul_kernel(A, B, C, M, N, K, config):\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    matmul_kernel_with_block_pointers[grid](\n        A, B, C, M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_M=32, BLOCK_N=32, BLOCK_K=32,\n        **config.kwargs\n    )\n\n# Wrapper function for scaled integer matrix multiplication\ndef int_scaled_matmul_kernel(A, B, C, scales1, M, N, K, config):\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)\n    scaled_matmul_kernel_with_block_pointers[grid](\n        A, B, C, scales1, M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1),\n        BLOCK_M=32, BLOCK_N=32, BLOCK_K=32,\n        **config.kwargs\n    )\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef matmul_kernel_with_block_pointers(\n    # Pointers to matrices\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n    # by to get the element one row down (A has M rows).\n    stride_am,\n    stride_ak,  #\n    stride_bk,\n    stride_bn,  #\n    stride_cm,\n    stride_cn,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    # -----------------------------------------------------------\n    # Map program ids `pid` to the block of C it should compute.\n    # This is done in a grouped ordering to promote L2 data reuse.\n    # See the matrix multiplication tutorial for details.\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + (pid % GROUP_M)\n    pid_n = (pid % num_pid_in_group) // GROUP_M\n\n    # ----------------------------------------------------------\n    # Create block pointers for the first blocks of A and B.\n    # We will advance this pointer as we move in the K direction and accumulate.\n    # See above `Make a Block Pointer` section for details.\n    a_block_ptr = tl.make_block_ptr(\n        base=a_ptr,\n        shape=(M, K),\n        strides=(stride_am, stride_ak),\n        offsets=(pid_m * BLOCK_M, 0),\n        block_shape=(BLOCK_M, BLOCK_K),\n        order=(1, 0),\n    )\n    b_block_ptr = tl.make_block_ptr(\n        base=b_ptr,\n        shape=(K, N),\n        strides=(stride_bk, stride_bn),\n        offsets=(0, pid_n * BLOCK_N),\n        block_shape=(BLOCK_K, BLOCK_N),\n        order=(1, 0),\n    )\n\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_M, BLOCK_N]` block.\n    # of fp32 values for higher accuracy.\n    # `accumulator` will be converted back to fp16 after the loop.\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, K, BLOCK_K):\n        # Load with boundary checks, no need to calculate the mask manually.\n        # For better performance, you may remove some axis from the boundary\n        # check, if you can guarantee that the access is always in-bound in\n        # that axis.\n        # See above `Load/Store a Block Pointer` section for details.\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n        # We accumulate along the K dimension.\n        accumulator += tl.dot(a, b)\n        # Advance the block pointer to the next K block.\n        # See above `Advance a Block Pointer` section for details.\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    c = accumulator  # .to(tl.float16)\n\n    # ----------------------------------------------------------------\n    # Write back the block of the output matrix C with boundary checks.\n    # See above `Load/Store a Block Pointer` section for details.\n    c_block_ptr = tl.make_block_ptr(\n        base=c_ptr,\n        shape=(M, N),\n        strides=(stride_cm, stride_cn),\n        offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N),\n        order=(1, 0),\n    )\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n\n\n@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(\n    # Pointers to matrices\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    s1_ptr,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n    # by to get the element one row down (A has M rows).\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_s1m,\n    stride_s1n,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr = tl.int32,\n):\n    # based on triton.ops.matmul\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = a_ptr + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = b_ptr + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)  # , allow_tf32=ALLOW_TF32)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n\n    # rematerialize rm and rn to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = rm[:, None]\n    idx_n = rn[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n\n    # inductor generates a suffix\n    xindex = idx_n + (N * idx_m)\n    tmp0 = tl.load(\n        s1_ptr + (tl.broadcast_to(idx_m, mask.shape)),\n        mask,\n        eviction_policy=\"evict_last\",\n    )\n    tl.store(c_ptr + (tl.broadcast_to(xindex, mask.shape)), acc * tmp0, mask)\n\n\ndef int_matmul_kernel(a, b, c, config):\n    M, K = a.shape\n    K, N = b.shape\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),\n    )\n    matmul_kernel_with_block_pointers[grid](\n        a,\n        b,\n        c,  #\n        M,\n        N,\n        K,  #\n        a.stride(0),\n        a.stride(1),  #\n        b.stride(0),\n        b.stride(1),  #\n        c.stride(0),\n        c.stride(1),\n        num_warps=config.num_warps,\n        num_stages=config.num_stages,\n        num_ctas=config.num_ctas,\n        **config.kwargs,\n    )\n    return c\n\n\ndef int_scaled_matmul_kernel(a, b, scales1, c, config):\n    M, K = a.shape\n    K, N = b.shape\n    # print(\"a.sizes(): \", a.size(), \"a.strides(): \", a.stride(), \"a.dtype: \", a.dtype)\n    # print(\"b.sizes(): \", b.size(), \"b.strides(): \", b.stride(), \"b.dtype: \", b.dtype)\n    # print(\"c.sizes(): \", c.size(), \"c.strides(): \", c.stride(), \"c.dtype: \", c.dtype)\n    # print(\"scales1.sizes(): \", scales1.size(), \"scales1.strides(): \", scales1.stride(), \"scales1.dtype\", scales1.dtype)\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),\n    )\n    scaled_matmul_kernel_with_block_pointers[grid](\n        a,\n        b,\n        c,\n        scales1,\n        M,\n        N,\n        K,  #\n        a.stride(0),\n        a.stride(1),  #\n        b.stride(0),\n        b.stride(1),  #\n        c.stride(0),\n        c.stride(1),\n        scales1.stride(0),\n        scales1.stride(1),\n        num_warps=config.num_warps,\n        num_stages=config.num_stages,\n        num_ctas=config.num_ctas,\n        EVEN_K=(K % 2 == 0),\n        **config.kwargs,\n    )\n    return c\n\n\nclass Config:\n    def __init__(self, num_warps=4, num_stages=2, num_ctas=1, **kwargs):\n        self.num_warps = num_warps\n        self.num_stages = num_stages\n        self.num_ctas = num_ctas\n        self.kwargs = kwargs\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines three Triton kernels for operations related to decay cumulative sums: `fwd_decay_cumsum`, `prepare_qg_kg`, and `bwd_decay_global_cumsum`. The kernels are used for forward and backward operations on tensors for tasks often found in neural networks.\n\n        1. **fwd_decay_cumsum**: Computes the cumulative sum with decay. It reads input tensor `g` and writes the cumulative result to `g_o`. The kernel iterates over `BT` blocks, scaling and accumulating values from `g` using a decay constant, and stores the results in `g_o`.\n\n        2. **prepare_qg_kg**: Prepares transformed versions of input tensors `q` and `k` with additional tensor `g` to produce `qg` and `kg`. It uses exponential scaling on `q` and `k` derived from `g`, applying transformations that incorporate cumulative decay effects.\n\n        3. **bwd_decay_global_cumsum**: Computes gradients with respect to the decay sum using input gradients `dq_inner`, `dq_inter`, `dk_inner`, and `dk_inter`, along with original inputs `q`, `k`, and `g`. It updates `dg` with cumulative gradients for decay.\n\n        The launch functions (`launch_fwd_decay_cumsum`, `launch_prepare_qg_kg`, `launch_bwd_decay_global_cumsum`) handle the preparation of grid dimensions and stride calculations before launching each kernel, adapting input and output shapes.\n\n        Common parameters:\n        - `B`, `H`, `T`: Batch size, head size, sequence length.\n        - `scale`: A scaling factor for transformations.\n        - `BT`, `BK`, `DK`: Block and kernel size constants for configuring the grid.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# 1. Forward Decay Cumulative Sum Kernel\n@triton.jit\ndef fwd_decay_cumsum_kernel(g_ptr, g_o_ptr, decay, B, H, T, BT, **meta):\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    \n    # Calculate offsets\n    offset = b_idx * H * T + h_idx * T\n    g = g_ptr + offset\n    g_o = g_o_ptr + offset\n    \n    # Initialize cumulative sum\n    cumsum = tl.zeros((BT,), dtype=tl.float32)\n    \n    for t in range(0, T, BT):\n        g_tile = tl.load(g + t + tl.arange(0, BT))\n        cumsum = decay * cumsum + g_tile\n        tl.store(g_o + t + tl.arange(0, BT), cumsum)\n\n# 2. Prepare QG and KG Kernel\n@triton.jit\ndef prepare_qg_kg_kernel(q_ptr, k_ptr, g_ptr, qg_ptr, kg_ptr, scale, B, H, T, BT, **meta):\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    \n    # Calculate offsets\n    offset = b_idx * H * T + h_idx * T\n    q = q_ptr + offset\n    k = k_ptr + offset\n    g = g_ptr + offset\n    qg = qg_ptr + offset\n    kg = kg_ptr + offset\n    \n    for t in range(0, T, BT):\n        q_tile = tl.load(q + t + tl.arange(0, BT))\n        k_tile = tl.load(k + t + tl.arange(0, BT))\n        g_tile = tl.load(g + t + tl.arange(0, BT))\n        \n        qg_tile = q_tile * tl.exp(scale * g_tile)\n        kg_tile = k_tile * tl.exp(scale * g_tile)\n        \n        tl.store(qg + t + tl.arange(0, BT), qg_tile)\n        tl.store(kg + t + tl.arange(0, BT), kg_tile)\n\n# 3. Backward Decay Global Cumulative Sum Kernel\n@triton.jit\ndef bwd_decay_global_cumsum_kernel(dq_inner_ptr, dq_inter_ptr, dk_inner_ptr, dk_inter_ptr, q_ptr, k_ptr, g_ptr, dg_ptr, B, H, T, BT, **meta):\n    b_idx = tl.program_id(0)\n    h_idx = tl.program_id(1)\n    \n    # Calculate offsets\n    offset = b_idx * H * T + h_idx * T\n    dq_inner = dq_inner_ptr + offset\n    dq_inter = dq_inter_ptr + offset\n    dk_inner = dk_inner_ptr + offset\n    dk_inter = dk_inter_ptr + offset\n    q = q_ptr + offset\n    k = k_ptr + offset\n    g = g_ptr + offset\n    dg = dg_ptr + offset\n    \n    for t in range(0, T, BT):\n        dq_inner_tile = tl.load(dq_inner + t + tl.arange(0, BT))\n        dq_inter_tile = tl.load(dq_inter + t + tl.arange(0, BT))\n        dk_inner_tile = tl.load(dk_inner + t + tl.arange(0, BT))\n        dk_inter_tile = tl.load(dk_inter + t + tl.arange(0, BT))\n        \n        # Compute gradient contributions\n        grad_contrib = dq_inner_tile + dq_inter_tile + dk_inner_tile + dk_inter_tile\n        \n        # Update dg\n        dg_tile = tl.load(dg + t + tl.arange(0, BT))\n        dg_tile += grad_contrib\n        tl.store(dg + t + tl.arange(0, BT), dg_tile)\n\n\ndef launch_fwd_decay_cumsum(g, g_o, decay, B, H, T, BT):\n    grid = (B, H)\n    fwd_decay_cumsum_kernel[grid](g, g_o, decay, B, H, T, BT)\n\ndef launch_prepare_qg_kg(q, k, g, qg, kg, scale, B, H, T, BT):\n    grid = (B, H)\n    prepare_qg_kg_kernel[grid](q, k, g, qg, kg, scale, B, H, T, BT)\n\ndef launch_bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, B, H, T, BT):\n    grid = (B, H)\n    bwd_decay_global_cumsum_kernel[grid](dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, B, H, T, BT)\n", "label": "\nimport triton\nimport triton.language as tl\n\ninv_ln2 = tl.constexpr(1.44269504)\n\n# Forward decay cumulative sum kernel\n@triton.jit\ndef fwd_decay_cumsum(\n    g,\n    g_o, \n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    B,\n    H,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    DK: tl.constexpr\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_g = g + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_go = g_o + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    cum_decay = tl.zeros([BK], dtype=tl.float32)\n    mask = (i_k * BK + tl.arange(0, BK)) < DK\n\n    for i in range(BT):\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        cum_decay += _g * inv_ln2\n        tl.store(p_go, cum_decay.to(p_go.dtype.element_ty), mask=mask)\n        p_g += DK\n        p_go += DK\n\n# Prepare qg and kg kernel\n@triton.jit\ndef prepare_qg_kg(\n    q,\n    k,\n    g,\n    qg,\n    kg,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    B,\n    H,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    DK: tl.constexpr\n):\n\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    \n    mask = (i_k * BK + tl.arange(0, BK)) < DK\n\n    last_decay = tl.load(g + i_bh * s_qk_h + (i_c * BT + BT - 1) * DK + i_k * BK + tl.arange(0, BK))\n\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        _k = tl.load(p_k, mask=mask, other=0)\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        _q *= tl.math.exp2(_g) * scale\n        _k *= tl.math.exp2(last_decay - _g)\n        tl.store(p_kg, _k.to(p_kg.dtype.element_ty), mask=mask)\n        tl.store(p_qg, _q.to(p_qg.dtype.element_ty), mask=mask)\n        p_q += DK\n        p_g += DK\n        p_k += DK\n        p_kg += DK\n        p_qg += DK\n\n# Backward decay global cumulative sum kernel\n@triton.jit\ndef bwd_decay_global_cumsum(\n    dq_inner,\n    dq_inter,\n    dk_inner,\n    dk_inter,\n    q, k, g, dg,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    B,\n    H,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    DK: tl.constexpr\n):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_g = g + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dg = dg + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dq_inner = dq_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dk_inner = dk_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dq_inter = dq_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    p_dk_inter = dk_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * DK\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = (i_k * BK + tl.arange(0, BK)) < DK\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT-1, -1, -1):\n        _g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        if j == (BT-1):\n            last_g = _g\n        _dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        _dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        _dq2 *= tl.math.exp2(_g)\n        _dq = _dq1 + _dq2\n        tl.store(p_dq_inter, _dq, mask=mask)\n        _dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        _dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        _dk2 *= tl.math.exp2(last_g - _g)\n        _dk = _dk1 + _dk2\n        tl.store(p_dk_inter, _dk, mask=mask)\n        _q = tl.load(p_q, mask=mask, other=0)\n        _k = tl.load(p_k, mask=mask, other=0)\n        _dg = _dq * _q - _dk * _k\n        cum_grad_dg += _dg\n        tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)\n        p_g -= DK\n        p_k -= DK\n        p_q -= DK\n        p_dq_inner -= DK\n        p_dk_inner -= DK\n        p_dq_inter -= DK\n        p_dk_inter -= DK\n        p_dg -= DK\n\n# Define the kernel launch functions\ndef launch_fwd_decay_cumsum(g, g_o, B, H, T, scale, BT, BK, DK):\n    # Calculate strides\n    s_qk_h = H * T * DK\n    s_qk_t = T * DK\n    s_qk_d = DK\n\n    # Launch the kernel\n    grid = (DK // BK, T // BT, B * H)\n    fwd_decay_cumsum[grid](\n        g, g_o, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT=BT, BK=BK, DK=DK\n    )\n\ndef launch_prepare_qg_kg(q, k, g, qg, kg, B, H, T, scale, BT, BK, DK):\n    # Calculate strides\n    s_qk_h = H * T * DK\n    s_qk_t = T * DK\n    s_qk_d = DK\n\n    # Launch the kernel\n    grid = (DK // BK, T // BT, B * H)\n    prepare_qg_kg[grid](\n        q, k, g, qg, kg, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT=BT, BK=BK, DK=DK\n    )\n\ndef launch_bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, B, H, T, scale, BT, BK, DK):\n    # Calculate strides\n    s_qk_h = H * T * DK\n    s_qk_t = T * DK\n    s_qk_d = DK\n\n    # Launch the kernel\n    grid = (DK // BK, T // BT, B * H)\n    bwd_decay_global_cumsum[grid](\n        dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT=BT, BK=BK, DK=DK\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The `matmul_kernel_persistent` is a Triton kernel for performing matrix multiplication efficiently on CUDA-enabled GPUs. The kernel takes pointers to input matrices `a` and `b`, and outputs the result in matrix `c`. The kernel divides the matrices into blocks, each block being processed by a separate thread, and accumulates the result in a shared accumulator before storing it back to global memory. It supports different data types such as `float16` and `float8`.\n\n        The `matmul_persistent` function prepares and launches the kernel. It sets up the kernel's configuration based on the input data type and allocates memory for the output matrix. It ensures the input dimensions are compatible and defines the grid configuration for launching the kernel. The function calls the `matmul_kernel_persistent` with the correct configurations to perform the matrix multiplication.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel_persistent(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bn, stride_bk, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, dtype: tl.constexpr):\n    pid = tl.program_id(0)\n    \n    # Compute the starting indices of the block\n    block_row = pid // (N // BLOCK_SIZE_N)\n    block_col = pid % (N // BLOCK_SIZE_N)\n    \n    # Create pointers for a, b, c\n    a_block_ptr = a_ptr + block_row * BLOCK_SIZE_M * stride_am\n    b_block_ptr = b_ptr + block_col * BLOCK_SIZE_N * stride_bn\n    c_block_ptr = c_ptr + block_row * BLOCK_SIZE_M * stride_cm + block_col * BLOCK_SIZE_N * stride_cn\n    \n    # Create accumulators for the result\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=dtype)\n    \n    # Loop over the K dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load blocks of A and B\n        a_block = tl.load(a_block_ptr + k * stride_ak, mask=(k + tl.arange(0, BLOCK_SIZE_K) < K))\n        b_block = tl.load(b_block_ptr + k * stride_bk, mask=(k + tl.arange(0, BLOCK_SIZE_K) < K))\n        \n        # Compute matrix multiplication for the block\n        acc += tl.dot(a_block, b_block)\n    \n    # Store the result back to C\n    tl.store(c_block_ptr, acc)\n\n\ndef matmul_persistent(a, b, M, N, K, dtype):\n    import triton\n\n    # Determine block sizes\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 128\n    BLOCK_SIZE_K = 32\n\n    # Check input dimensions\n    assert a.shape == (M, K), \"Matrix A dimensions are incorrect\"\n    assert b.shape == (K, N), \"Matrix B dimensions are incorrect\"\n\n    # Allocate output matrix\n    c = triton.zeros((M, N), dtype=dtype)\n\n    # Determine grid size\n    grid = (M // BLOCK_SIZE_M) * (N // BLOCK_SIZE_N)\n\n    # Launch the kernel\n    matmul_kernel_persistent[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n        dtype=dtype\n    )\n\n    return c\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\ndef _matmul_launch_metadata(grid, kernel, args):\n    ret = {}\n    M, N, K = args[\"M\"], args[\"N\"], args[\"K\"]\n    ret[\"name\"] = f\"{kernel.name} [M={M}, N={N}, K={K}]\"\n    if \"c_ptr\" in args:\n        bytes_per_elem = args[\"c_ptr\"].element_size()\n    else:\n        bytes_per_elem = 1 if args[\"FP8_OUTPUT\"] else 2\n    ret[f\"flops{bytes_per_elem * 8}\"] = 2. * M * N * K\n    ret[\"bytes\"] = bytes_per_elem * (M * K + N * K + M * N)\n    return ret\n\n\n@triton.jit(launch_metadata=_matmul_launch_metadata)\ndef matmul_kernel_persistent(a_ptr, b_ptr, c_ptr,  #\n                             M, N, K,  #\n                             stride_am, stride_ak,  #\n                             stride_bk, stride_bn,  #\n                             stride_cm, stride_cn,  #\n                             BLOCK_SIZE_M: tl.constexpr,  #\n                             BLOCK_SIZE_N: tl.constexpr,  #\n                             BLOCK_SIZE_K: tl.constexpr,  #\n                             GROUP_SIZE_M: tl.constexpr,  #\n                             NUM_SMS: tl.constexpr,  #\n                             ):\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)\n    num_tiles = num_pid_m * num_pid_n\n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n\n    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\n\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\n    pid_m = 0\n    pid_n = 0\n    offs_am = tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = tl.arange(0, BLOCK_SIZE_N)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_SIZE_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n            pid_m = first_pid_m + (tile_id % group_size_m)\n            pid_n = (tile_id % num_pid_in_group) // group_size_m\n\n            start_m = pid_m * BLOCK_SIZE_M\n            start_n = pid_n * BLOCK_SIZE_N\n            offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n            offs_am = tl.where(offs_am < M, offs_am, 0)\n            offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n\n        if ki == k_tiles - 1:\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n            if (c_ptr.dtype.element_ty == tl.float8e4nv):\n                c = accumulator.to(tl.float8e4nv)\n            else:\n                c = accumulator.to(tl.float16)\n            tl.store(c_ptrs, c, mask=c_mask)\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n\ndef matmul_persistent(a, b):\n    configs = {\n        torch.float8_e4m3fn: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 128, \"GROUP_SIZE_M\": 8, \"num_stages\": 4,\n            \"num_warps\": 8\n        }, torch.float16: {\n            \"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8, \"num_stages\": 3,\n            \"num_warps\": 8\n        }\n    }\n    # Check constraints.\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.dtype == b.dtype, \"Incompatible dtypes\"\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    M, K = a.shape\n    K, N = b.shape\n    dtype = a.dtype\n    # Allocates output.\n    c = torch.empty((M, N), device=a.device, dtype=dtype)\n    # 1D launch kernel where each block gets its own program.\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"])), )\n    matmul_kernel_persistent[grid](\n        a, b, c,  #\n        M, N, K,  #\n        a.stride(0), a.stride(1),  #\n        b.stride(0), b.stride(1),  #\n        c.stride(0), c.stride(1),  #\n        BLOCK_SIZE_M=configs[dtype][\"BLOCK_SIZE_M\"],  #\n        BLOCK_SIZE_N=configs[dtype][\"BLOCK_SIZE_N\"],  #\n        BLOCK_SIZE_K=configs[dtype][\"BLOCK_SIZE_K\"],  #\n        GROUP_SIZE_M=configs[dtype][\"GROUP_SIZE_M\"],  #\n        NUM_SMS=NUM_SMS,  #\n        num_stages=configs[dtype][\"num_stages\"],  #\n        num_warps=configs[dtype][\"num_warps\"],  #\n    )\n    return c\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel to compute the forward and backward pass of the GEGLU (Gated Exponential Linear Unit) activation function using a tanh approximation. The `geglu_forward` function takes two input tensors `a` and `b`, reshapes them to align the columns, and applies the `_geglu_tanh_forward_kernel` to compute the output tensor `c`. Similarly, `geglu_backward` calculates the gradients of `a` and `b` using the `_geglu_tanh_backward_kernel`. `calculate_settings` determines the block size and number of warps, currently set to 128 and 4. The kernel utilizes `tanh` and approximates GELU through mathematical operations. Outputs are reshaped back to their original dimensions.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Forward Kernel\n@triton.jit\ndef _geglu_tanh_forward_kernel(a_ptr, b_ptr, c_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the program's starting point\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n\n    # Create a range of indices for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load inputs\n    a = tl.load(a_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    b = tl.load(b_ptr + offsets, mask=offsets < n_elements, other=0.0)\n\n    # Compute GEGLU using tanh approximation\n    gate = tl.sigmoid(b)\n    c = a * gate\n\n    # Store the result\n    tl.store(c_ptr + offsets, c, mask=offsets < n_elements)\n\n# Backward Kernel\n@triton.jit\ndef _geglu_tanh_backward_kernel(a_ptr, b_ptr, grad_output_ptr, grad_a_ptr, grad_b_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the program's starting point\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n\n    # Create a range of indices for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load inputs\n    a = tl.load(a_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    b = tl.load(b_ptr + offsets, mask=offsets < n_elements, other=0.0)\n    grad_output = tl.load(grad_output_ptr + offsets, mask=offsets < n_elements, other=0.0)\n\n    # Compute gate and its gradient\n    gate = tl.sigmoid(b)\n    grad_gate = gate * (1 - gate)\n\n    # Compute gradients\n    grad_a = grad_output * gate\n    grad_b = grad_output * a * grad_gate\n\n    # Store the gradients\n    tl.store(grad_a_ptr + offsets, grad_a, mask=offsets < n_elements)\n    tl.store(grad_b_ptr + offsets, grad_b, mask=offsets < n_elements)\n\n# Helper function to determine settings\ndef calculate_settings(n_elements):\n    BLOCK_SIZE = 128\n    num_warps = 4\n    return BLOCK_SIZE, num_warps\n\n# Wrapper for forward pass\ndef geglu_forward(a, b):\n    assert a.shape == b.shape, \"Input tensors must have the same shape\"\n    n_elements = a.numel()\n    c = torch.empty_like(a)\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_elements)\n\n    _geglu_tanh_forward_kernel[(n_elements // BLOCK_SIZE,)](a, b, c, n_elements, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n\n    return c\n\n# Wrapper for backward pass\ndef geglu_backward(a, b, grad_output):\n    assert a.shape == b.shape == grad_output.shape, \"All tensors must have the same shape\"\n    n_elements = a.numel()\n    grad_a = torch.empty_like(a)\n    grad_b = torch.empty_like(b)\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_elements)\n\n    _geglu_tanh_backward_kernel[(n_elements // BLOCK_SIZE,)](a, b, grad_output, grad_a, grad_b, n_elements, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n\n    return grad_a, grad_b\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom triton.language.extra.libdevice import tanh\n\n\ndef calculate_settings(n_cols):\n    # This function calculates the BLOCK_SIZE and num_warps based on n_cols.\n    # For simplicity, let's assume BLOCK_SIZE is 128 and num_warps is 4.\n    BLOCK_SIZE = 128\n    num_warps = 4\n    return BLOCK_SIZE, num_warps\n\n@triton.jit\ndef _geglu_tanh_forward_kernel(\n    a, b, c, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n\n    # tanh approximation form of GELU is computed with:\n    # 0.5 * a * (1 + tanh(sqrt(2 / pi) * (a + 0.044715 * a^3)))\n    sqrt_2_over_pi = 0.7978845608028654  # sqrt(2 / pi)\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n\n@triton.jit\ndef _geglu_tanh_backward_kernel(\n    dc, a, b, stride, n_cols: tl.constexpr, BLOCK_SIZE: tl.constexpr\n):\n    program_id = tl.program_id(0).to(tl.int64)\n\n    # locate start index\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0).to(tl.float32)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n\n    # recomputation to save memory\n    sqrt_2_over_pi = 0.7978845608028654  # sqrt(2 / pi)\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n\n    db_row = dc_row * geglu_a\n\n    # Gradient w.r.t. a can be computed with:\n    # b * (0.5 * (1 + tanh(z)) + 0.5 * a * (1 - tanh(z)^2) * (sqrt(2/pi) * (1 + 3 * 0.044715 * a^2)))\n    # where z = sqrt(2/pi) * (a + 0.044715 * a^3)\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = (\n        0.5\n        * a_row\n        * (1 - tanh_sq)\n        * (sqrt_2_over_pi * (1 + 3 * 0.044715 * a_row * a_row))\n    )\n    da_row = dc_row * b_row * (term1 + term2)\n\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n\ndef geglu_forward(a, b):\n    ori_shape = a.shape\n\n    n_cols = ori_shape[-1]\n    a = a.view(-1, n_cols)\n    b = b.view(-1, n_cols)\n    c = torch.empty_like(a)\n    n_rows = a.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _geglu_tanh_forward_kernel[(n_rows,)](\n        a,\n        b,\n        c,\n        c.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return a, b, c.view(*ori_shape)\n\ndef geglu_backward(a, b, dc):\n    ori_shape = dc.shape\n    n_cols = ori_shape[-1]\n    dc = dc.view(-1, n_cols)\n    n_rows = dc.shape[0]\n\n    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n\n    _geglu_tanh_backward_kernel[(n_rows,)](\n        dc,\n        a,\n        b,\n        dc.stride(-2),\n        n_cols=n_cols,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n\n    return a.view(*ori_shape), b.view(*ori_shape)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel function `_bmm_chunk_fwd_kernel` used for performing batched matrix-matrix multiplication with customizable block sizes. The kernel supports optimizations like chunked processing, handling of causal masking, and sequence indexing.\n\n            The helper function `_bmm_chunk_fwd` wraps around the Triton kernel, setting up the necessary environment and data structures before launching the kernel. It computes matrix products for input tensors `a` and `b`, chunking them according to `chunk_size` and optionally applying sequence indexing or causal masking. The result is stored in `out`.\n\n            The kernel's logic involves loading sub-matrices into local variables, computing their dot products iteratively, and handling special conditions like causal masking using program IDs to index into blocks of data.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _bmm_chunk_fwd_kernel(a_ptr, b_ptr, out_ptr,\n                          M, N, K,  # dimensions of the matrices\n                          stride_am, stride_ak,  # strides for matrix A\n                          stride_bn, stride_bk,  # strides for matrix B\n                          stride_om, stride_on,  # strides for output matrix\n                          BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                          causal: tl.constexpr):\n    # Program IDs\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Define the start of the block\n    block_start_m = pid_m * BLOCK_M\n    block_start_n = pid_n * BLOCK_N\n\n    # Create pointers for the current block of A and B\n    a_block_ptr = a_ptr + block_start_m * stride_am\n    b_block_ptr = b_ptr + block_start_n * stride_bn\n\n    # Initialize the accumulator\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    # Loop over the K dimension\n    for k in range(0, K, BLOCK_K):\n        # Load blocks of A and B\n        a_block = tl.load(a_block_ptr + k * stride_ak, mask=(block_start_m + tl.arange(0, BLOCK_M)[:, None] < M))\n        b_block = tl.load(b_block_ptr + k * stride_bk, mask=(block_start_n + tl.arange(0, BLOCK_N)[None, :] < N))\n\n        # Update the accumulator\n        acc += tl.dot(a_block, b_block)\n\n    # Apply causal masking if required\n    if causal:\n        mask = tl.arange(0, BLOCK_M)[:, None] >= tl.arange(0, BLOCK_N)[None, :]\n        acc = tl.where(mask, acc, 0)\n\n    # Store the result\n    out_ptr = out_ptr + block_start_m * stride_om + block_start_n * stride_on\n    tl.store(out_ptr, acc, mask=(block_start_m + tl.arange(0, BLOCK_M)[:, None] < M) & (block_start_n + tl.arange(0, BLOCK_N)[None, :] < N))\n\nimport torch\n\ndef _bmm_chunk_fwd(a, b, chunk_size, causal=False):\n    # Dimensions\n    B, M, K = a.shape\n    _, _, N = b.shape\n\n    # Allocate output\n    out = torch.empty((B, M, N), device=a.device, dtype=a.dtype)\n\n    # Strides\n    stride_am, stride_ak = a.stride(-2), a.stride(-1)\n    stride_bn, stride_bk = b.stride(-1), b.stride(-2)\n    stride_om, stride_on = out.stride(-2), out.stride(-1)\n\n    # Launch kernel\n    grid = (triton.cdiv(M, chunk_size), triton.cdiv(N, chunk_size), B)\n    _bmm_chunk_fwd_kernel[grid](\n        a, b, out,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bn, stride_bk,\n        stride_om, stride_on,\n        BLOCK_M=chunk_size, BLOCK_N=chunk_size, BLOCK_K=chunk_size,\n        causal=causal\n    )\n\n    return out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2),\n    ],\n    key=['chunk_size', 'K', 'IS_CAUSAL'],\n)\n@triton.jit\ndef _bmm_chunk_fwd_kernel(\n    a_ptr, b_ptr, out_ptr, seq_idx_ptr,\n    seqlen, chunk_size, K, ngroups,\n    stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak,\n    stride_b_batch, stride_b_seqlen, stride_b_head, stride_bk,\n    stride_out_batch, stride_out_chunk, stride_out_head, stride_outm, stride_outn,\n    stride_seq_idx_batch, stride_seq_idx_seqlen,\n    IS_CAUSAL: tl.constexpr,\n    dot_dtype: tl.constexpr,\n    HAS_SEQ_IDX: tl.constexpr,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\n    b_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h * stride_b_head\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < K - k * BLOCK_SIZE_K), other=0.0).to(dot_dtype)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) & (offs_n[None, :] < chunk_size_limit), other=0.0).to(dot_dtype)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen, mask=offs_n < chunk_size_limit, other=-2)\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc.to(out_ptr.dtype.element_ty)\n\n    out_ptr += pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))\n\ndef _bmm_chunk_fwd(a, b, chunk_size, seq_idx=None, causal=False, output_dtype=None):\n    has_groups = a.dim() == 4\n    if not has_groups:\n        batch, seqlen, k = a.shape\n    else:\n        batch, seqlen, ngroups, k = a.shape\n    assert b.shape == a.shape\n    if seq_idx is not None:\n        assert seq_idx.shape == (batch, seqlen)\n    if a.stride(-1) != 1 and a.stride(1) != 1:\n        a = a.contiguous()\n    if b.stride(-1) != 1 and b.stride(1) != 1:\n        b = b.contiguous()\n    nchunks = math.ceil(seqlen / chunk_size)\n    out_dtype = a.dtype if output_dtype is None else output_dtype\n    out = torch.empty((batch, nchunks, chunk_size, chunk_size) if not has_groups else (batch, nchunks, ngroups, chunk_size, chunk_size),\n                      device=a.device, dtype=out_dtype)\n    dot_dtype = (tl.bfloat16 if a.dtype == torch.bfloat16 or b.dtype == torch.bfloat16 else\n                 (tl.float16 if a.dtype == torch.float16 or b.dtype == torch.float16 else tl.float32))\n    grid = lambda META: (triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(chunk_size, META['BLOCK_SIZE_N']),\n                    batch, nchunks if not has_groups else nchunks * ngroups)\n    with torch.cuda.device(a.device.index):\n        _bmm_chunk_fwd_kernel[grid](\n            a, b, out, seq_idx,\n            int(seqlen), int(chunk_size), int(k), int(ngroups if has_groups else 1),\n            a.stride(0), a.stride(1), 0 if not has_groups else a.stride(2), a.stride(-1),\n            b.stride(0), b.stride(1), 0 if not has_groups else b.stride(2), b.stride(-1),\n            out.stride(0), out.stride(1), 0 if not has_groups else out.stride(2), out.stride(-2), out.stride(-1),\n            *((seq_idx.stride(0), seq_idx.stride(1)) if seq_idx is not None else (0, 0)),\n            causal,\n            dot_dtype,\n            HAS_SEQ_IDX=seq_idx is not None,\n        )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator consists of a kernel function '_chunk_cumsum_fwd_kernel' and a wrapper function '_chunk_cumsum_fwd'. The kernel computes the cumulative sum for each chunk of data, applying optional transformations such as bias addition and softplus activation. It processes data in blocks determined by configurations for optimal performance. The wrapper function manages data preparation and execution on the GPU, organizing data into appropriate shapes and launching the kernel.\n        Inputs:\n        - dt: Input tensor of shape (batch, seqlen, nheads).\n        - A: Scaling factors, 1D tensor of shape (nheads,).\n        - chunk_size: Size of each chunk for processing.\n        - dt_bias: Optional bias tensor for dt.\n        - dt_softplus: Boolean to apply softplus transformation.\n        - dt_limit: Clamping limits for dt.\n        Outputs:\n        - dA_cumsum: Cumulative sum result.\n        - dt_out: Modified dt.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _chunk_cumsum_fwd_kernel(\n    dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr, dA_cumsum_ptr,\n    chunk_size, dt_softplus, dt_limit,\n    batch, seqlen, nheads,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Compute the index of the current block\n    pid = tl.program_id(axis=0)\n    \n    # Compute offsets for each dimension\n    batch_id = pid // (seqlen * nheads)\n    seqlen_id = (pid // nheads) % seqlen\n    head_id = pid % nheads\n\n    # Calculate the start and end index for the current chunk\n    chunk_start = seqlen_id * chunk_size\n    chunk_end = min(chunk_start + chunk_size, seqlen)\n\n    # Load scaling factor for the current head\n    A = tl.load(A_ptr + head_id)\n\n    # Initialize cumulative sum\n    cumsum = 0.0\n\n    # Iterate over the chunk and compute cumulative sum\n    for i in range(chunk_start, chunk_end):\n        # Load the current value\n        idx = batch_id * seqlen * nheads + i * nheads + head_id\n        dt_value = tl.load(dt_ptr + idx)\n\n        # Apply optional bias\n        if dt_bias_ptr:\n            bias = tl.load(dt_bias_ptr + idx)\n            dt_value += bias\n\n        # Apply scaling\n        dt_value *= A\n\n        # Apply optional softplus\n        if dt_softplus:\n            dt_value = tl.log(1 + tl.exp(dt_value))\n\n        # Apply clamping\n        dt_value = tl.clamp(dt_value, dt_limit[0], dt_limit[1])\n\n        # Update cumulative sum\n        cumsum += dt_value\n\n        # Store the modified dt value\n        tl.store(dt_out_ptr + idx, dt_value)\n\n    # Store the cumulative sum result\n    dA_cumsum_idx = batch_id * nheads + head_id\n    tl.store(dA_cumsum_ptr + dA_cumsum_idx, cumsum)\n\nimport torch\n\ndef _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=None, dt_softplus=False, dt_limit=(-float('inf'), float('inf'))):\n    batch, seqlen, nheads = dt.shape\n\n    # Allocate output tensors\n    dt_out = torch.empty_like(dt)\n    dA_cumsum = torch.empty((batch, nheads), dtype=dt.dtype, device=dt.device)\n\n    # Define the number of blocks\n    num_blocks = batch * seqlen * nheads\n\n    # Launch the kernel\n    grid = (num_blocks,)\n    _chunk_cumsum_fwd_kernel[grid](\n        dt_ptr=dt,\n        A_ptr=A,\n        dt_bias_ptr=dt_bias,\n        dt_out_ptr=dt_out,\n        dA_cumsum_ptr=dA_cumsum,\n        chunk_size=chunk_size,\n        dt_softplus=dt_softplus,\n        dt_limit=dt_limit,\n        batch=batch,\n        seqlen=seqlen,\n        nheads=nheads,\n        BLOCK_SIZE=chunk_size\n    )\n\n    return dA_cumsum, dt_out\n", "label": "\nimport math\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_H': 1}),\n        triton.Config({'BLOCK_SIZE_H': 2}),\n        triton.Config({'BLOCK_SIZE_H': 4}),\n        triton.Config({'BLOCK_SIZE_H': 8}),\n        triton.Config({'BLOCK_SIZE_H': 16}),\n        triton.Config({'BLOCK_SIZE_H': 32}),\n        triton.Config({'BLOCK_SIZE_H': 64}),\n    ],\n    key=['chunk_size', 'nheads'],\n)\n@triton.jit\ndef _chunk_cumsum_fwd_kernel(\n    # Pointers to matrices\n    dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr, dA_cumsum_ptr,\n    # Matrix dimension\n    batch, seqlen, nheads, chunk_size,\n    dt_min, dt_max,\n    # Strides\n    stride_dt_batch, stride_dt_seqlen, stride_dt_head,\n    stride_A_head,\n    stride_dt_bias_head,\n    stride_dt_out_batch, stride_dt_out_chunk, stride_dt_out_head, stride_dt_out_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize,\n    # Meta-parameters\n    DT_SOFTPLUS: tl.constexpr,\n    HAS_DT_BIAS: tl.constexpr,\n    BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_CHUNK: tl.constexpr,\n):\n    # Triton kernel implementation for chunked cumulative sum forward pass\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\n\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen)\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize)\n    dA_cs_ptrs = dA_cumsum_ptr + (offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n\n    dt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0).to(tl.float32)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, tl.log(1 + tl.exp(dt)), dt)\n    # As of Triton 2.2.0, tl.clamp is not available yet\n    # dt = tl.clamp(dt, dt_min, dt_max)\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0)\n    tl.store(dt_out_ptrs, dt, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dA = dt * A[:, None]\n    dA_cs = tl.cumsum(dA, axis=1)\n    tl.store(dA_cs_ptrs, dA_cs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\n\ndef _chunk_cumsum_fwd(dt, A, chunk_size, dt_bias=None, dt_softplus=False, dt_limit=(0.0, float(\"inf\"))):\n    \"\"\"\n    Function to perform the forward cumulative sum operation in chunks.\n\n    Arguments:\n    - dt: (batch, seqlen, nheads), the input tensor.\n    - A: (nheads,), the scaling factors.\n    - chunk_size: The size of each chunk to process at a time.\n    - dt_bias: (nheads,), optional, biases for dt if applicable.\n    - dt_softplus: Boolean, whether to apply the softplus operation to dt.\n    - dt_limit: Tuple, (min, max) limits for clamping dt values.\n\n    Returns:\n    - dA_cumsum: Cumulative sum result.\n    - dt_out: Modified dt after processing.\n    \"\"\"\n    batch, seqlen, nheads = dt.shape\n    assert A.shape == (nheads,)\n    if dt_bias is not None:\n        assert dt_bias.shape == (nheads,)\n    nchunks = math.ceil(seqlen / chunk_size)\n    dt_out = torch.empty(batch, nheads, nchunks, chunk_size, device=dt.device, dtype=torch.float32)\n    dA_cumsum = torch.empty(batch, nheads, nchunks, chunk_size, device=dt.device, dtype=torch.float32)\n    grid_chunk_cs = lambda META: (batch, nchunks, triton.cdiv(nheads, META['BLOCK_SIZE_H']))\n    with torch.cuda.device(dt.device.index):\n        _chunk_cumsum_fwd_kernel[grid_chunk_cs](\n            dt, A, dt_bias, dt_out, dA_cumsum,\n            int(batch), int(seqlen), int(nheads), int(chunk_size),\n            dt_limit[0], dt_limit[1],\n            dt.stride(0), dt.stride(1), dt.stride(2),\n            A.stride(0),\n            dt_bias.stride(0) if dt_bias is not None else 0,\n            dt_out.stride(0), dt_out.stride(2), dt_out.stride(1), dt_out.stride(3),\n            dA_cumsum.stride(0), dA_cumsum.stride(2), dA_cumsum.stride(1), dA_cumsum.stride(3),\n            dt_softplus,\n            HAS_DT_BIAS=dt_bias is not None,\n            BLOCK_SIZE_CHUNK=triton.next_power_of_2(chunk_size),\n        )\n    return dA_cumsum, dt_out\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The code defines a Triton kernel named 'dequantize_kernel' and a Python function 'matmul_dequantize_int8'. The Triton kernel dequantizes an int8 matrix by multiplying it with a scale factor and stores the result as a float matrix. 'matmul_dequantize_int8' then computes the matrix multiplication of a float input matrix 'a' with the dequantized float matrix, resulting in a final matrix 'c'. Both functions ensure dimensions are compatible and handle device-specific memory allocation.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef dequantize_kernel(\n    int8_ptr, scale_ptr, float_ptr, \n    n_elements, BLOCK_SIZE: tl.constexpr\n):\n    # Program ID determines which block of data this instance will handle\n    pid = tl.program_id(0)\n    \n    # Define the range of indices this block will handle\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Load int8 values and scale factor\n    int8_vals = tl.load(int8_ptr + offsets, mask=offsets < n_elements, other=0).to(tl.float32)\n    scale = tl.load(scale_ptr)\n    \n    # Dequantize\n    float_vals = int8_vals * scale\n    \n    # Store result\n    tl.store(float_ptr + offsets, float_vals, mask=offsets < n_elements)\n\n\nimport torch\n\ndef matmul_dequantize_int8(a, int8_matrix, scale):\n    # Ensure the int8_matrix is on the correct device\n    assert int8_matrix.dtype == torch.int8\n    assert a.device == int8_matrix.device\n    \n    # Get dimensions\n    n, k = a.shape\n    k_, m = int8_matrix.shape\n    assert k == k_, \"Incompatible dimensions for matrix multiplication\"\n\n    # Allocate memory for the dequantized matrix\n    float_matrix = torch.empty((k, m), device=int8_matrix.device, dtype=torch.float32)\n\n    # Dequantize the int8 matrix\n    grid = lambda meta: (triton.cdiv(k * m, meta['BLOCK_SIZE']),)\n    BLOCK_SIZE = 1024  # Adjust based on your GPU's capabilities\n    dequantize_kernel[grid](int8_matrix, scale, float_matrix, k * m, BLOCK_SIZE=BLOCK_SIZE)\n    \n    # Perform matrix multiplication\n    c = torch.matmul(a, float_matrix)\n    \n    return c\n\n# Example usage\na = torch.randn(64, 128, device='cuda', dtype=torch.float32)\nint8_matrix = torch.randint(-128, 127, (128, 256), device='cuda', dtype=torch.int8)\nscale = torch.tensor(0.1, device='cuda', dtype=torch.float32)\n\nc = matmul_dequantize_int8(a, int8_matrix, scale)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    ],\n    key=['K', 'N'],\n)\n\n\n@triton.jit\ndef dequantize_kernel(\n    b_ptr, b_scale_ptr, fpb_ptr,\n    K, N,\n    stride_bk, stride_bn,\n    stride_fpbk, stride_fpbn,\n    BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + \\\n        (n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + \\\n        (n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n\ndef matmul_dequantize_int8(a, b, b_scale, out=None):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    if out == None:\n        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    else:\n        c = out\n    fp_b = torch.empty((K, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (\n        triton.cdiv(K, META['BLOCK_SIZE_K']), triton.cdiv(N, META['BLOCK_SIZE_N']),\n    )\n    dequantize_kernel[grid](\n        b, b_scale, fp_b,\n        K, N,\n        b.stride(0), b.stride(1),\n        fp_b.stride(0), fp_b.stride(1)\n    )\n    torch.mm(a, fp_b, out=c)\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This code defines a Triton kernel function and a PyTorch wrapper function for a fused operation involving addition, multiplication, and activation on tensors. The Triton kernel 'fused_add_mul_activation_kernel' takes three pointers to tensors and several constants as input. It computes a fused operation on each block of the input tensor. The function 'fused_add_mul_activation_torch' is a Python function that sets up the execution grid for the Triton kernel and calls it on given input tensors. It takes three PyTorch tensors as arguments and applies the specified operations using the Triton kernel, returning the modified tensor.\n        ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_add_mul_activation_kernel(\n    x_ptr, y_ptr, z_ptr, output_ptr,\n    N, M, BLOCK_SIZE: tl.constexpr\n):\n    # Define the program ID for grid\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the start position for this block\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a block of indices\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to handle boundaries\n    mask = offsets < N\n    \n    # Load data from x, y, z\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n    z = tl.load(z_ptr + offsets, mask=mask, other=0.0)\n    \n    # Perform the fused operation: (x + y) * z and apply activation (ReLU)\n    result = (x + y) * z\n    result = tl.maximum(result, 0.0)  # ReLU activation\n    \n    # Store the result\n    tl.store(output_ptr + offsets, result, mask=mask)\n\nimport torch\n\ndef fused_add_mul_activation_torch(x, y, z):\n    # Ensure inputs are on the same device and are contiguous\n    assert x.is_cuda and y.is_cuda and z.is_cuda, \"Inputs must be CUDA tensors\"\n    assert x.is_contiguous() and y.is_contiguous() and z.is_contiguous(), \"Inputs must be contiguous\"\n    \n    # Get the number of elements\n    N = x.numel()\n    \n    # Define block size\n    BLOCK_SIZE = 1024  # You can tune this based on your hardware\n    \n    # Allocate output tensor\n    output = torch.empty_like(x)\n    \n    # Calculate the number of blocks needed\n    grid = lambda meta: (triton.cdiv(N, BLOCK_SIZE),)\n    \n    # Launch the Triton kernel\n    fused_add_mul_activation_kernel[grid](\n        x_ptr=x, y_ptr=y, z_ptr=z, output_ptr=output,\n        N=N, M=0, BLOCK_SIZE=BLOCK_SIZE\n    )\n    \n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef fused_add_mul_activation_kernel(x_ptr, bias_ptr, in_ptr,\n                                    num_weights: tl.constexpr,\n                                    xnumel: tl.constexpr,\n                                    multiplier: tl.constexpr,\n                                    activation: tl.constexpr,\n                                    BLOCK_SIZE: tl.constexpr):\n    xoffset = tl.program_id(0) * BLOCK_SIZE\n    index = xoffset + tl.arange(0, BLOCK_SIZE)[:]\n    mask = index < xnumel\n    bias_index = index % num_weights\n    tmp0 = tl.load(x_ptr + index, mask)\n    tmp1 = tl.load(bias_ptr + bias_index, mask, eviction_policy='evict_last')\n    tmp3 = tl.load(in_ptr + index, mask)\n    activ_input = multiplier * tmp3 + tmp0 + tmp1\n    if activation == \"sigmoid\":\n        ma_result = tl.sigmoid(activ_input)\n        # option 2 - calculate sigmoid using exp\n        # ma_result = 1.0 / (1.0 + tl.exp(-sigmoid_input))\n        # option 3: fast sigmoid - inaccurate but faster\n        # ma_result = 1.0 / (1.0 + tl.abs(sigmoid_input))\n    elif activation == \"relu\":\n        ma_result = tl.maximum(0, activ_input)\n\n    tl.store(x_ptr + index, ma_result, mask)\n\n\ndef fused_add_mul_activation_torch(in_out_tensor: torch.Tensor, bias: torch.Tensor,\n                                   in_tensor: torch.Tensor) -> torch.Tensor:\n    # print(\"calling fused_add_mul_relu_torch\")\n    grid = lambda meta: (triton.cdiv(in_out_tensor.numel(), meta['BLOCK_SIZE']),)\n    BLOCK_SIZE = min(2048, in_out_tensor.numel())\n    fused_add_mul_activation_kernel[grid](in_out_tensor, bias, in_tensor,\n                                          bias.numel(),\n                                          in_out_tensor.numel(),\n                                          multiplier=0.5,\n                                          activation=\"sigmoid\",\n                                          BLOCK_SIZE=BLOCK_SIZE)\n    return in_out_tensor\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel `matmul_tma_load_store` performs matrix multiplication using block pointers to load matrices A and B, then computes the product matrix C. The kernel can optionally output C in float16. Inputs include pointers to matrices A, B, and C, their dimensions (M, N, K), strides, and block sizes (BLOCK_M, BLOCK_N, BLOCK_K). The wrapper `warpper_tma_load_store` prepares random input matrices and calls the kernel with specified parameters.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef matmul_tma_load_store(A_ptr, B_ptr, C_ptr,\n                          M, N, K,\n                          stride_am, stride_ak,\n                          stride_bk, stride_bn,\n                          stride_cm, stride_cn,\n                          BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                          OUTPUT_FLOAT16: tl.constexpr):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_K)\n\n    A = tl.load(A_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak), mask=(offs_am[:, None] < M) & (offs_k[None, :] < K))\n    B = tl.load(B_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn), mask=(offs_k[:, None] < K) & (offs_bn[None, :] < N))\n\n    C = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_K):\n        A_block = tl.load(A_ptr + (offs_am[:, None] * stride_am + (k + offs_k)[None, :] * stride_ak), mask=(offs_am[:, None] < M) & ((k + offs_k)[None, :] < K))\n        B_block = tl.load(B_ptr + ((k + offs_k)[:, None] * stride_bk + offs_bn[None, :] * stride_bn), mask=((k + offs_k)[:, None] < K) & (offs_bn[None, :] < N))\n        C += tl.dot(A_block, B_block)\n\n    if OUTPUT_FLOAT16:\n        C = C.to(tl.float16)\n\n    tl.store(C_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn), C, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n\ndef wrapper_tma_load_store(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, OUTPUT_FLOAT16=False):\n    # Prepare random input matrices\n    A = torch.randn((M, K), dtype=torch.float16, device='cuda')\n    B = torch.randn((K, N), dtype=torch.float16, device='cuda')\n    C = torch.empty((M, N), dtype=torch.float16 if OUTPUT_FLOAT16 else torch.float32, device='cuda')\n\n    # Define strides\n    stride_am, stride_ak = A.stride()\n    stride_bk, stride_bn = B.stride()\n    stride_cm, stride_cn = C.stride()\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n    matmul_tma_load_store[grid](\n        A, B, C,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n        OUTPUT_FLOAT16=OUTPUT_FLOAT16\n    )\n\n    return C\n\n# Example usage\nM, N, K = 128, 128, 128\nBLOCK_M, BLOCK_N, BLOCK_K = 16, 16, 16\nC = wrapper_tma_load_store(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, OUTPUT_FLOAT16=True)\nprint(C)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_tma_load_store(\n        a_ptr, b_ptr, c_ptr,\n        M, N, K,\n        stride_am, stride_ak,\n        stride_bk, stride_bn,\n        stride_cm, stride_cn,\n        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n        OUTPUT_F16: tl.constexpr\n):\n    # Create block pointers for A, B, and C matrices\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn), offsets=(0, 0),\n                                    block_shape=(BLOCK_K, BLOCK_N), order=(0, 1))\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn), offsets=(0, 0),\n                                    block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    # Load A and B blocks\n    a = tl.load(a_block_ptr)\n    b = tl.load(b_block_ptr)\n\n    # Compute matrix product\n    c = tl.dot(a, b)\n    # Optionally convert the result to float16\n    if OUTPUT_F16:\n        c = c.to(tl.float16)\n\n    # Store the result\n    tl.store(c_block_ptr, c)\n\n\ndef warpper_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F16):\n    # Prepare input matrices\n    if (TRANS_A):\n        a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n    else:\n        a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n    if (TRANS_B):\n        b = torch.randn((N, K), device='cuda', dtype=torch.float16).T\n    else:\n        b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n\n    # Prepare output matrix\n    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n    if OUTPUT_F16:\n        c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n\n    # Execute Triton kernel\n    matmul_tma_load_store[(1, 1)](\n        a_ptr=a, b_ptr=b, c_ptr=c,\n        M=M, N=N, K=K,\n        stride_am=a.stride(0), stride_ak=a.stride(1),\n        stride_bk=b.stride(0), stride_bn=b.stride(1),\n        stride_cm=c.stride(0), stride_cn=c.stride(1),\n        BLOCK_M=M, BLOCK_N=N, BLOCK_K=K,\n        num_warps=NUM_WARPS, num_ctas=NUM_CTAS,\n        OUTPUT_F16=OUTPUT_F16)\n    return c\n    \n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton code contains two main functions, `triton_mul2` and `triton_mul2_inplace`, which double the values in a PyTorch tensor. The `triton_mul2` function uses a kernel `mul2_kernel` to output the result to a new tensor, while `triton_mul2_inplace` modifies the input tensor directly with `mul2_inplace_kernel`. Both kernels use a block-based approach to process data in parallel, where `BLOCK_SIZE` determines the number of elements processed per block. The kernels load data from the input tensor, multiply each element by 2, and store the result. They ensure no out-of-bounds memory access by using masks based on `n_elements`, the total number of elements in the tensor.\n    ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n# Kernel to multiply elements by 2 and store in a new tensor\n@triton.jit\ndef mul2_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Program ID\n    pid = tl.program_id(axis=0)\n\n    # Create block of indices\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load input data with mask to prevent out-of-bounds access\n    mask = offsets < n_elements\n    input_data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n\n    # Multiply by 2\n    result = input_data * 2.0\n\n    # Store result in output tensor\n    tl.store(output_ptr + offsets, result, mask=mask)\n\n# Kernel to multiply elements by 2 in place\n@triton.jit\ndef mul2_inplace_kernel(input_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Program ID\n    pid = tl.program_id(axis=0)\n\n    # Create block of indices\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Load input data with mask to prevent out-of-bounds access\n    mask = offsets < n_elements\n    input_data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n\n    # Multiply by 2\n    result = input_data * 2.0\n\n    # Store result back in input tensor\n    tl.store(input_ptr + offsets, result, mask=mask)\n\ndef triton_mul2(input_tensor):\n    # Ensure input is a contiguous tensor\n    input_tensor = input_tensor.contiguous()\n    n_elements = input_tensor.numel()\n\n    # Create an output tensor\n    output_tensor = torch.empty_like(input_tensor)\n\n    # Launch kernel\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    mul2_kernel[grid](input_tensor, output_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return output_tensor\n\ndef triton_mul2_inplace(input_tensor):\n    # Ensure input is a contiguous tensor\n    input_tensor = input_tensor.contiguous()\n    n_elements = input_tensor.numel()\n\n    # Launch kernel\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    mul2_inplace_kernel[grid](input_tensor, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample tensor\n    x = torch.tensor([1.0, 2.0, 3.0, 4.0], device='cuda')\n\n    # Double the values using triton_mul2\n    y = triton_mul2(x)\n    print(\"Output (new tensor):\", y)\n\n    # Double the values in place using triton_mul2_inplace\n    triton_mul2_inplace(x)\n    print(\"Output (in place):\", x)\n", "label": "import triton\nfrom triton import language as tl\nimport torch\n\n\n@triton.jit\ndef mul2_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = 2 * x\n    tl.store(out_ptr + offsets, output, mask=mask)\n\n@triton.jit\ndef mul2_inplace_kernel(\n    ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(ptr + offsets, mask=mask)\n    output = 2 * x\n    tl.store(ptr + offsets, output, mask=mask)\n\n\ndef triton_mul2(x, BLOCK_SIZE=16):\n    output = torch.zeros_like(x)\n    n_elements = x.numel()\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    mul2_kernel[grid](x, output, n_elements, BLOCK_SIZE)\n    return output\n\ndef triton_mul2_inplace(x, BLOCK_SIZE=16):\n    n_elements = x.numel()\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n    mul2_inplace_kernel[grid](x, n_elements, BLOCK_SIZE)\n    return x\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs a parameter update operation for optimization. The kernel function `update_fn_kernel` is applied across blocks of elements defined by `BLOCK_SIZE`. It reads parameter (`p_ptr`), gradient (`grad_ptr`), and exponential moving average (`exp_avg_ptr`) from global memory, and updates them using weight decay, momentum, and learning rate. It stores the updated parameter and average back into global memory. The host function `update_fn` serves as a wrapper, ensuring the operation is performed on CUDA tensors and determines the execution grid size based on the number of elements.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef update_fn_kernel(\n    p_ptr, grad_ptr, exp_avg_ptr,\n    lr, momentum, weight_decay,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Compute the block index\n    block_start = tl.program_id(0) * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to ensure we don't go out of bounds\n    mask = offsets < n_elements\n\n    # Load parameters, gradients, and exponential moving averages\n    p = tl.load(p_ptr + offsets, mask=mask, other=0.0)\n    grad = tl.load(grad_ptr + offsets, mask=mask, other=0.0)\n    exp_avg = tl.load(exp_avg_ptr + offsets, mask=mask, other=0.0)\n\n    # Update exponential moving average\n    exp_avg = momentum * exp_avg + (1 - momentum) * grad\n\n    # Apply weight decay\n    p = p - lr * (exp_avg + weight_decay * p)\n\n    # Store updated parameters and averages back to global memory\n    tl.store(p_ptr + offsets, p, mask=mask)\n    tl.store(exp_avg_ptr + offsets, exp_avg, mask=mask)\n\nimport torch\n\ndef update_fn(p, grad, exp_avg, lr, momentum, weight_decay):\n    # Ensure inputs are CUDA tensors\n    assert p.is_cuda and grad.is_cuda and exp_avg.is_cuda\n\n    # Number of elements\n    n_elements = p.numel()\n\n    # Define block size\n    BLOCK_SIZE = 1024  # You can adjust this based on your GPU's capability\n\n    # Calculate grid size\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    update_fn_kernel[grid_size](\n        p, grad, exp_avg,\n        lr, momentum, weight_decay,\n        n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(configs = [\n    triton.Config({'BLOCK_SIZE': 128}, num_warps = 4),\n    triton.Config({'BLOCK_SIZE': 1024}, num_warps = 8),\n], key = ['n_elements'], restore_value=['p_ptr', 'exp_avg_ptr'])\n\n# Triton CUDA kernel\n\n@triton.jit\ndef update_fn_kernel(\n    p_ptr,\n    grad_ptr,\n    exp_avg_ptr,\n    lr,\n    wd,\n    beta1,\n    beta2,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    mask = offsets < n_elements\n\n    # Offsetted pointers\n    offset_p_ptr = p_ptr + offsets\n    offset_grad_ptr = grad_ptr + offsets\n    offset_exp_avg_ptr = exp_avg_ptr + offsets\n\n    # Load\n    p = tl.load(offset_p_ptr, mask=mask)\n    grad = tl.load(offset_grad_ptr, mask=mask)\n    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\n\n    # Stepweight decay\n    p = p * (1 - lr * wd)\n\n    # Diff between momentum running average and grad\n    diff = exp_avg - grad\n\n    # Weight update\n    update = diff * beta1 + grad\n\n    # torch.sign\n    can_update = update != 0\n    update_sign = tl.where(update > 0, -lr, lr)\n\n    p = p + update_sign * can_update\n\n    # Decay the momentum running average coefficient\n    exp_avg = diff * beta2 + grad\n\n    # Store new params and momentum running average coefficient\n    tl.store(offset_p_ptr, p, mask=mask)\n    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)\n\ndef update_fn(\n    p: torch.Tensor,\n    grad: torch.Tensor,\n    exp_avg: torch.Tensor,\n    lr: float,\n    wd: float,\n    beta1: float,\n    beta2: float\n):\n    assert all([t.is_cuda for t in (p, grad, exp_avg)])\n    n_elements = p.numel()\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    update_fn_kernel[grid](\n        p,\n        grad,\n        exp_avg,\n        lr,\n        wd,\n        beta1,\n        beta2,\n        n_elements\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        This Triton-based code provides an implementation of the argmax operation with three kernels. The `argmax` function determines the maximum value and its index across elements of a tensor. It is optimized for GPU execution.\n\n        There are three kernels: \n        1. `argmax_kernel_1`: Processes a block of the input tensor, finding the local maximum and its index in the block.\n        2. `argmax_kernel_2`: Aggregates the results from `argmax_kernel_1` to find the overall maximum value's index.\n        3. `argmax_kernel`: Handles multi-dimensional tensors, finding the maximum index along a specified dimension.\n\n        The `argmax` function adapts based on whether the operation is performed over the entire tensor or a specific dimension, adjusting intermediate block sizes and determining if int32 or int64 indices are required.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef argmax_kernel_1(input_ptr, output_val_ptr, output_idx_ptr, BLOCK_SIZE: tl.constexpr):\n    # Create a program index for each block\n    pid = tl.program_id(axis=0)\n    \n    # Compute the block start index\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load the block of data\n    data = tl.load(input_ptr + offsets, mask=offsets < input_ptr.shape[0])\n    \n    # Find the local maximum and its index within the block\n    local_max = tl.max(data, axis=0)\n    local_max_idx = tl.argmax(data, axis=0)\n    \n    # Write the local maximum and its index to the output\n    tl.store(output_val_ptr + pid, local_max)\n    tl.store(output_idx_ptr + pid, block_start + local_max_idx)\n\n@triton.jit\ndef argmax_kernel_2(input_val_ptr, input_idx_ptr, output_idx_ptr, BLOCK_SIZE: tl.constexpr):\n    # Create a program index for each block\n    pid = tl.program_id(axis=0)\n    \n    # Compute the block start index\n    block_start = pid * BLOCK_SIZE\n    \n    # Create a range of indices for this block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Load the block of maximum values and their indices\n    max_vals = tl.load(input_val_ptr + offsets, mask=offsets < input_val_ptr.shape[0])\n    max_indices = tl.load(input_idx_ptr + offsets, mask=offsets < input_idx_ptr.shape[0])\n    \n    # Find the overall maximum and its index within the block\n    overall_max_idx = tl.argmax(max_vals, axis=0)\n    \n    # Write the index of the overall maximum to the output\n    tl.store(output_idx_ptr, max_indices[overall_max_idx])\n\n@triton.jit\ndef argmax_kernel(input_ptr, output_idx_ptr, BLOCK_SIZE: tl.constexpr, dim: tl.constexpr):\n    # For simplicity, this kernel assumes a 2D tensor\n    # This kernel will find the argmax along the specified dimension\n    # Create a program index for each block\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the start index for the block\n    block_start = pid * BLOCK_SIZE\n    \n    # Load the data for this block\n    if dim == 0:\n        # Argmax along rows\n        row_indices = block_start + tl.arange(0, BLOCK_SIZE)\n        data = tl.load(input_ptr + row_indices[:, None], mask=row_indices[:, None] < input_ptr.shape[0])\n        max_indices = tl.argmax(data, axis=0)\n    elif dim == 1:\n        # Argmax along columns\n        col_indices = block_start + tl.arange(0, BLOCK_SIZE)\n        data = tl.load(input_ptr + col_indices, mask=col_indices < input_ptr.shape[1])\n        max_indices = tl.argmax(data, axis=1)\n    \n    # Store the indices of the maximum values\n    tl.store(output_idx_ptr + block_start, max_indices)\n\ndef argmax(input_tensor, dim=None):\n    import torch\n    \n    # Determine the size of the tensor and the block size\n    BLOCK_SIZE = 1024  # Example block size, tune for your GPU and tensor size\n    \n    # Allocate memory for the output indices\n    if dim is None:\n        # Argmax over the entire tensor\n        num_blocks = (input_tensor.numel() + BLOCK_SIZE - 1) // BLOCK_SIZE\n        output_val = torch.empty(num_blocks, dtype=input_tensor.dtype, device=input_tensor.device)\n        output_idx = torch.empty(num_blocks, dtype=torch.int32, device=input_tensor.device)\n        \n        # Launch the first kernel\n        argmax_kernel_1[(num_blocks,)](input_tensor, output_val, output_idx, BLOCK_SIZE=BLOCK_SIZE)\n        \n        # Launch the second kernel to find the global maximum index\n        final_idx = torch.empty(1, dtype=torch.int32, device=input_tensor.device)\n        argmax_kernel_2[(1,)](output_val, output_idx, final_idx, BLOCK_SIZE=num_blocks)\n        \n        return final_idx.item()\n    else:\n        # Argmax along a specific dimension\n        if dim >= input_tensor.ndim:\n            raise ValueError(\"Dimension out of range\")\n        \n        # Determine the size of the dimension\n        dim_size = input_tensor.size(dim)\n        \n        # Allocate memory for the output indices\n        output_idx = torch.empty(input_tensor.size()[:dim] + input_tensor.size()[dim+1:], dtype=torch.int32, device=input_tensor.device)\n        \n        # Launch the kernel\n        argmax_kernel[(output_idx.numel(),)](input_tensor, output_idx, BLOCK_SIZE=dim_size, dim=dim)\n        \n        return output_idx\n\n# Example usage\n# input_tensor = torch.randn(2048, device='cuda')\n# max_index = argmax(input_tensor)\n# print(\"Index of max value:\", max_index)\n", "label": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n\ndef can_use_int32_index(tensor):\n    # This function checks if the tensor can use int32 indices\n    return tensor.numel() < 2**31\n\n# Kernel 1: argmax_kernel_1\n@triton.jit\ndef argmax_kernel_1(\n    inp,\n    mid_value,\n    mid_index,\n    M,\n    BLOCK_SIZE: tl.constexpr,\n    INT64_INDEX: tl.constexpr = False,\n):\n    pid = tl.program_id(0)\n    if INT64_INDEX:\n        pid = pid.to(tl.int64)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    inp_ptrs = inp + offset\n    mask = offset < M\n    inp_val = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n    max_val, max_index = tl.max(inp_val, axis=0, return_indices=True)\n    max_index = max_index + pid * BLOCK_SIZE\n    mid_value_ptr = mid_value + pid\n    max_index_ptr = mid_index + pid\n    tl.store(mid_value_ptr, max_val)\n    tl.store(max_index_ptr, max_index)\n\n# Kernel 2: argmax_kernel_2\n@triton.jit\ndef argmax_kernel_2(mid_value, mid_index, out, mid_size, BLOCK_MID: tl.constexpr):\n    offset = tl.arange(0, BLOCK_MID)\n    mid_ptrs = mid_value + offset\n    mask = offset < mid_size\n    mid_val = tl.load(mid_ptrs, mask=mask, other=-float(\"inf\"))\n    index_val = tl.argmax(mid_val, axis=0)\n    mid_index_ptrs = mid_index + index_val\n    out_val = tl.load(mid_index_ptrs)\n    tl.store(out, out_val)\n\n# Kernel 3: argmax_kernel\n@triton.jit\ndef argmax_kernel(\n    inp,\n    out_index,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    INT64_INDEX: tl.constexpr = False,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    if INT64_INDEX:\n        pid_m = pid_m.to(tl.int64)\n        pid_k = pid_k.to(tl.int64)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n\n    max_values = tl.full([BLOCK_M], dtype=tl.float32, value=float(\"-inf\"))\n    argmax_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)\n    for start_n in range(0, N, BLOCK_N):\n        n_offset = start_n + tl.arange(0, BLOCK_N)\n        offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n        mask = m_offset[:, None] < M and n_offset[None, :] < N\n        inp_ptrs = inp + offset\n        inp_vals = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))\n        local_max, local_argmax = tl.max(\n            inp_vals, 1, return_indices=True, return_indices_tie_break_left=True\n        )\n        update = local_max > max_values\n        max_values = tl.where(update, local_max, max_values)\n        argmax_values = tl.where(update, start_n + local_argmax, argmax_values)\n\n    offset_index = m_offset * K + pid_k\n    out_index_ptrs = out_index + offset_index\n    mask1 = m_offset < M\n    tl.store(out_index_ptrs, argmax_values, mask=mask1)\n\n# Function calling the kernels\ndef argmax(inp, dim=None, keepdim=False, *, dtype=None):\n    if dim is None:\n        M = inp.numel()\n        if dtype is None:\n            dtype = inp.dtype\n        block_size = triton.next_power_of_2(math.ceil(math.sqrt(M)))\n        mid_size = triton.cdiv(M, block_size)\n        block_mid = triton.next_power_of_2(mid_size)\n        use_int64_index = not can_use_int32_index(inp)\n\n        mid_value = torch.empty((mid_size,), dtype=dtype, device=inp.device)\n        mid_index = torch.empty((mid_size,), dtype=torch.int64, device=inp.device)\n        if keepdim:\n            shape = list(inp.shape)\n            for i in range(0, inp.dim()):\n                shape[i] = 1\n            out = torch.empty(shape, dtype=torch.int64, device=inp.device)\n        else:\n            out = torch.empty([], dtype=torch.int64, device=inp.device)\n\n        with torch.cuda.device(inp.device):\n            argmax_kernel_1[(mid_size, 1, 1)](\n                inp,\n                mid_value,\n                mid_index,\n                M,\n                block_size,\n                INT64_INDEX=use_int64_index,\n            )\n            argmax_kernel_2[(1, 1, 1)](mid_value, mid_index, out, mid_size, block_mid)\n        return out\n    else:\n        assert dim >= -inp.ndim and dim < inp.ndim, \"Invalid dim\"\n        shape = inp.shape\n        dim = dim % inp.ndim\n        N = shape[dim]\n        M = math.prod(shape[:dim])\n        K = inp.numel() // M // N\n\n        inp = inp.contiguous()\n        use_int64_index = not can_use_int32_index(inp)\n\n        shape_list = list(shape)\n        shape_list[dim] = 1\n        out_index = torch.empty(shape_list, dtype=torch.int64, device=inp.device)\n        if not keepdim:\n            out_index = torch.squeeze(out_index, dim)\n\n        BLOCK_M = 128  # Example, adjust as needed\n        BLOCK_N = 128  # Example, adjust as needed\n\n        grid = lambda meta: (\n            triton.cdiv(M, meta[\"BLOCK_M\"]),\n            K,\n        )\n        with torch.cuda.device(inp.device):\n            argmax_kernel[grid](\n                inp,\n                out_index,\n                M,\n                N,\n                K,\n                BLOCK_M=BLOCK_M,\n                BLOCK_N=BLOCK_N,\n                INT64_INDEX=use_int64_index,\n            )\n\n        return out_index\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel function, `_layer_norm_fwd_1pass_kernel`, performs layer normalization with optional dropout and bias on input tensors. It also supports element-wise operations with additional tensors like `x1`, `weight1`, and `bias1`. Inputs include `X`, `Y`, `W`, `B` and optionally `RESIDUAL`, `X1`, `W1`, `B1`, among others. The function computes normalized outputs and stores them in `Y` and optionally in `Y1`. It also computes the mean and inverse standard deviation for normalization if `IS_RMS_NORM` is False. Outputs include `y`, `y1`, `mean`, `rstd`, `residual_out`, `seeds`, `dropout_mask`, and `dropout_mask1`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, Y, W, B, \n    RESIDUAL=None, X1=None, W1=None, B1=None,\n    y_ptr=None, y1_ptr=None, mean_ptr=None, rstd_ptr=None,\n    residual_out_ptr=None, seeds_ptr=None, dropout_mask_ptr=None, dropout_mask1_ptr=None,\n    NORM_SIZE: tl.constexpr, IS_RMS_NORM: tl.constexpr\n):\n    # Compute the mean and variance\n    row_id = tl.program_id(0)\n    offset = row_id * NORM_SIZE\n    x = tl.load(X + offset, mask=True)\n\n    # Compute mean\n    mean = tl.sum(x, axis=0) / NORM_SIZE\n    if not IS_RMS_NORM:\n        # Compute variance\n        var = tl.sum((x - mean) ** 2, axis=0) / NORM_SIZE\n        rstd = 1.0 / tl.sqrt(var + 1e-5)\n    else:\n        rstd = 1.0\n\n    # Normalize\n    x_norm = (x - mean) * rstd\n\n    # Apply weights and biases\n    w = tl.load(W, mask=True)\n    b = tl.load(B, mask=True)\n    y = x_norm * w + b\n\n    # Optional element-wise operations with additional tensors\n    if X1 is not None and W1 is not None and B1 is not None:\n        x1 = tl.load(X1 + offset, mask=True)\n        w1 = tl.load(W1, mask=True)\n        b1 = tl.load(B1, mask=True)\n        y1 = x1 * w1 + b1\n        if y1_ptr is not None:\n            tl.store(y1_ptr + offset, y1, mask=True)\n\n    # Optional dropout\n    if dropout_mask_ptr is not None:\n        dropout_mask = tl.load(dropout_mask_ptr + offset, mask=True)\n        y = y * dropout_mask\n\n    # Store the results\n    tl.store(y_ptr + offset, y, mask=True)\n    if mean_ptr is not None:\n        tl.store(mean_ptr + row_id, mean, mask=True)\n    if rstd_ptr is not None:\n        tl.store(rstd_ptr + row_id, rstd, mask=True)\n    if residual_out_ptr is not None and RESIDUAL is not None:\n        residual = tl.load(RESIDUAL + offset, mask=True)\n        residual_out = y + residual\n        tl.store(residual_out_ptr + offset, residual_out, mask=True)\n\ndef layer_norm_fwd_1pass(X, W, B, RESIDUAL=None, X1=None, W1=None, B1=None, IS_RMS_NORM=False):\n    # Allocate output tensors\n    Y = triton.zeros_like(X)\n    Y1 = triton.zeros_like(X1) if X1 is not None else None\n    mean = triton.empty((X.shape[0],), dtype=triton.float32)\n    rstd = triton.empty((X.shape[0],), dtype=triton.float32)\n    residual_out = triton.zeros_like(X) if RESIDUAL is not None else None\n\n    # Launch the kernel\n    grid = (X.shape[0],)\n    _layer_norm_fwd_1pass_kernel[grid](\n        X, Y, W, B, \n        RESIDUAL, X1, W1, B1,\n        Y, Y1, mean, rstd,\n        residual_out, None, None, None,\n        NORM_SIZE=X.shape[1], IS_RMS_NORM=IS_RMS_NORM\n    )\n\n    return Y, Y1, mean, rstd, residual_out\n\n# Example usage\n# X, W, B, etc. are Triton tensors\n# Y, Y1, mean, rstd, residual_out = layer_norm_fwd_1pass(X, W, B)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"N\", \"HAS_RESIDUAL\", \"STORE_RESIDUAL_OUT\", \"IS_RMS_NORM\", \"HAS_BIAS\"],\n)\n@triton.heuristics({\"HAS_X1\": lambda args: args[\"X1\"] is not None})\n@triton.heuristics({\"HAS_W1\": lambda args: args[\"W1\"] is not None})\n@triton.heuristics({\"HAS_B1\": lambda args: args[\"B1\"] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(\n    X, Y, W, B, RESIDUAL, X1, W1, B1, Y1, RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd,\n    stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row, stride_y1_row,\n    M, N, eps, dropout_p, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr,\n    STORE_DROPOUT_MASK: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_X1: tl.constexpr,\n    HAS_W1: tl.constexpr, HAS_B1: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        x *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n            keep_mask = (\n                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n            )\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask).to(tl.float32)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)\n\ndef _layer_norm_fwd(\n    x, weight, bias, eps, residual=None, x1=None, weight1=None, bias1=None, dropout_p=0.0,\n    rowscale=None, out_dtype=None, residual_dtype=None, is_rms_norm=False, return_dropout_mask=False,\n):\n    if residual is not None:\n        residual_dtype = residual.dtype\n    M, N = x.shape\n    assert x.stride(-1) == 1\n    if residual is not None:\n        assert residual.stride(-1) == 1\n        assert residual.shape == (M, N)\n    assert weight.shape == (N,)\n    assert weight.stride(-1) == 1\n    if bias is not None:\n        assert bias.stride(-1) == 1\n        assert bias.shape == (N,)\n    if x1 is not None:\n        assert x1.shape == x.shape\n        assert rowscale is None\n        assert x1.stride(-1) == 1\n    if weight1 is not None:\n        assert weight1.shape == (N,)\n        assert weight1.stride(-1) == 1\n    if bias1 is not None:\n        assert bias1.shape == (N,)\n        assert bias1.stride(-1) == 1\n    if rowscale is not None:\n        assert rowscale.is_contiguous()\n        assert rowscale.shape == (M,)\n    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)\n    assert y.stride(-1) == 1\n    if weight1 is not None:\n        y1 = torch.empty_like(y)\n        assert y1.stride(-1) == 1\n    else:\n        y1 = None\n    if (\n        residual is not None\n        or (residual_dtype is not None and residual_dtype != x.dtype)\n        or dropout_p > 0.0\n        or rowscale is not None\n        or x1 is not None\n    ):\n        residual_out = torch.empty(\n            M, N, device=x.device, dtype=residual_dtype if residual_dtype is not None else x.dtype\n        )\n        assert residual_out.stride(-1) == 1\n    else:\n        residual_out = None\n    mean = torch.empty((M,), dtype=torch.float32, device=x.device) if not is_rms_norm else None\n    rstd = torch.empty((M,), dtype=torch.float32, device=x.device)\n    if dropout_p > 0.0:\n        seeds = torch.randint(\n            2**32, (M if x1 is None else 2 * M,), device=x.device, dtype=torch.int64\n        )\n    else:\n        seeds = None\n    if return_dropout_mask and dropout_p > 0.0:\n        dropout_mask = torch.empty(M if x1 is None else 2 * M, N, device=x.device, dtype=torch.bool)\n    else:\n        dropout_mask = None\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _layer_norm_fwd_1pass_kernel[(M,)](\n            x, y, weight, bias, residual, x1, weight1, bias1, y1, residual_out, rowscale, seeds,\n            dropout_mask, mean, rstd, x.stride(0), y.stride(0), residual.stride(0) if residual is not None else 0,\n            residual_out.stride(0) if residual_out is not None else 0, x1.stride(0) if x1 is not None else 0,\n            y1.stride(0) if y1 is not None else 0, M, N, eps, dropout_p, is_rms_norm, BLOCK_N,\n            residual is not None, residual_out is not None, bias is not None, dropout_p > 0.0,\n            dropout_mask is not None, rowscale is not None,\n        )\n    if dropout_mask is not None and x1 is not None:\n        dropout_mask, dropout_mask1 = dropout_mask.tensor_split(2, dim=0)\n    else:\n        dropout_mask1 = None\n    return (\n        y, y1, mean, rstd, residual_out if residual_out is not None else x, seeds, dropout_mask, dropout_mask1,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The provided code defines a set of Triton kernels and a PyTorch autograd function to perform chunk retention operations on tensors. The main functionality is split into several kernels to manage different parts of the computation, focusing on forward and backward passes for efficient memory usage and computation.\n\n        - `chunk_retention_fwd_kernel_h`: This kernel calculates an intermediate tensor 'h' based on inputs 'k' and 'v', and optionally uses an initial state. It updates the final state if needed.\n        - `chunk_retention_fwd_kernel_o`: Computes the output tensor 'o' using 'q', 'k', 'v', and the intermediate tensor 'h'.\n        - `chunk_retention_bwd_kernel_dh`: Part of the backward pass, this kernel computes the gradient 'dh'.\n        - `chunk_retention_bwd_kernel_dqkv`: Calculates the gradients 'dq', 'dk', and 'dv' for the backward pass.\n\n        The main Python function `chunk_retention` uses the `ChunkRetentionFunction` to perform these operations, managing both forward and backward computations with optional state inputs and outputs.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_retention_fwd_kernel_h(k_ptr, v_ptr, h_ptr, state_ptr, N, M, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n\n    k = tl.load(k_ptr + offs, mask=mask)\n    v = tl.load(v_ptr + offs, mask=mask)\n    \n    h = k * v  # Example computation, replace with the actual operation\n\n    if state_ptr is not None:\n        state = tl.load(state_ptr + offs, mask=mask)\n        h += state  # Update with initial state if provided\n\n    tl.store(h_ptr + offs, h, mask=mask)\n\n@triton.jit\ndef chunk_retention_fwd_kernel_o(q_ptr, k_ptr, v_ptr, h_ptr, o_ptr, N, M, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n\n    q = tl.load(q_ptr + offs, mask=mask)\n    k = tl.load(k_ptr + offs, mask=mask)\n    v = tl.load(v_ptr + offs, mask=mask)\n    h = tl.load(h_ptr + offs, mask=mask)\n\n    o = q * k * v + h  # Example computation, replace with the actual operation\n\n    tl.store(o_ptr + offs, o, mask=mask)\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(dout_ptr, k_ptr, v_ptr, dh_ptr, N, M, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n\n    dout = tl.load(dout_ptr + offs, mask=mask)\n    k = tl.load(k_ptr + offs, mask=mask)\n    v = tl.load(v_ptr + offs, mask=mask)\n\n    dh = dout * k * v  # Example gradient computation\n\n    tl.store(dh_ptr + offs, dh, mask=mask)\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(dout_ptr, q_ptr, k_ptr, v_ptr, dq_ptr, dk_ptr, dv_ptr, N, M, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offs < N\n\n    dout = tl.load(dout_ptr + offs, mask=mask)\n    q = tl.load(q_ptr + offs, mask=mask)\n    k = tl.load(k_ptr + offs, mask=mask)\n    v = tl.load(v_ptr + offs, mask=mask)\n\n    dq = dout * k * v  # Example gradient computation\n    dk = dout * q * v\n    dv = dout * q * k\n\n    tl.store(dq_ptr + offs, dq, mask=mask)\n    tl.store(dk_ptr + offs, dk, mask=mask)\n    tl.store(dv_ptr + offs, dv, mask=mask)\n\nimport torch\nfrom torch.autograd import Function\n\nclass ChunkRetentionFunction(Function):\n    @staticmethod\n    def forward(ctx, q, k, v, state=None):\n        N, M = q.shape\n\n        h = torch.empty_like(k)\n        o = torch.empty_like(q)\n\n        # Launch Triton kernels\n        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n        chunk_retention_fwd_kernel_h[grid](k, v, h, state, N, M, BLOCK_SIZE=1024)\n        chunk_retention_fwd_kernel_o[grid](q, k, v, h, o, N, M, BLOCK_SIZE=1024)\n\n        ctx.save_for_backward(q, k, v, h)\n\n        return o\n\n    @staticmethod\n    def backward(ctx, dout):\n        q, k, v, h = ctx.saved_tensors\n        N, M = dout.shape\n\n        dh = torch.empty_like(h)\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n\n        # Launch Triton kernels for backward pass\n        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n        chunk_retention_bwd_kernel_dh[grid](dout, k, v, dh, N, M, BLOCK_SIZE=1024)\n        chunk_retention_bwd_kernel_dqkv[grid](dout, q, k, v, dq, dk, dv, N, M, BLOCK_SIZE=1024)\n\n        return dq, dk, dv, None\n\ndef chunk_retention(q, k, v, state=None):\n    return ChunkRetentionFunction.apply(q, k, v, state)\n", "label": "from typing import Tuple\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.amp import custom_bwd, custom_fwd\nimport functools\n\ndef contiguous(fn):\n    @functools.wraps(fn)\n    def wrapper(ctx, *args, **kwargs):\n        return fn(ctx,\n                  *(i if not isinstance(i, torch.Tensor) else i.contiguous() for i in args),\n                  **{k: (v if not isinstance(v, torch.Tensor) else v.contiguous()) for k, v in kwargs.items()})\n    return wrapper\n\n@triton.jit\ndef chunk_retention_fwd_kernel_h(\n    k,\n    v,\n    h,\n    initial_state,  # initial state of the chunk [B, H, D_head_K, D_head_V]\n    final_state,  # final state of the chunk [B, H, D_head_K, D_head_V]\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    # [BK, BV]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        # [BK, BV]\n        if i_t == NT - 1 and (T % BT) != 0:\n            d_b = tl.math.exp2((T % BT) * b_b)\n            d_i = tl.math.exp2(((T % BT) - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_i[:, None]).to(b_k.dtype), allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_retention_fwd_kernel_o(\n    q,\n    k,\n    v,\n    h,\n    o,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        # [BT, BK]\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        # [BK, BT]\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        # [BK, BV]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot((b_q * d_i[:, None]).to(b_q.dtype), b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n\n    b_s *= d_s\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(\n    q,\n    do,\n    dh,\n    v,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    # [BK, BV]\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_o = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        # [BT, BV]\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n\n        b_dh += tl.dot((b_o * d_i[:, None]).to(b_o.dtype), b_v, allow_tf32=False)\n\n    b_dh *= d_b\n    p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_k * K * V, (K, V), (s_h_t, 1), (i_v * BV, i_t * BT), (BK, BV), (1, 0))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(\n    q,\n    k,\n    v,\n    h,\n    do,\n    dh,\n    dq,\n    dk,\n    dv,\n    s_qk_h,\n    s_qk_t,\n    s_qk_d,\n    s_vo_h,\n    s_vo_t,\n    s_vo_d,\n    s_h_h,\n    s_h_t,\n    scale,\n    H: tl.constexpr,\n    T: tl.constexpr,\n    K: tl.constexpr,\n    V: tl.constexpr,\n    BT: tl.constexpr,\n    BK: tl.constexpr,\n    BV: tl.constexpr,\n    NT: tl.constexpr\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    d_q = (d_q * scale).to(d_q.dtype)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        # [BT, BV]\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        # [BV, BK]\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        # [BK, BV]\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        # [BT, BT]\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        # [BT, BK]\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        # [BT, BV]\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    # [BT, BT]\n    b_ds = (b_ds * d_s).to(b_q.dtype)\n    # [BT, BK]\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n\nclass ChunkRetentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    @custom_fwd(device_type='cuda')\n    @contiguous\n    def forward(ctx, q, k, v, initial_state, output_final_state):\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        scale = K ** -0.5\n\n        final_state = None\n        if output_final_state:\n            final_state = q.new_empty(B, H, K, V, dtype=torch.float32, requires_grad=False)\n\n        h = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_retention_fwd_kernel_h[grid](\n            k, v, h, initial_state, final_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=output_final_state,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        grid = (NV, NT, B * H)\n        o = torch.empty_like(v)\n        chunk_retention_fwd_kernel_o[grid](\n            q, k, v, h, o,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            scale,\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        ctx.save_for_backward(q, k, v, h)\n        return o.to(q.dtype), final_state\n\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    @contiguous\n    def backward(ctx, do, d_ht=None):\n        q, k, v, h = ctx.saved_tensors\n\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        scale = K ** -0.5\n\n        dh = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_retention_bwd_kernel_dh[grid](\n            q, do, dh, v,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        grid = (NK, NT, B * H)\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = v.new_empty(NK, *v.shape)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        chunk_retention_bwd_kernel_dqkv[grid](\n            q, k, v, h, do, dh, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            H=H, T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        dv = dv.sum(0)\n        return dq.to(q.dtype), dk.to(k.dtype), dv.to(v.dtype), None, None\n\ndef chunk_retention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if initial_state is not None:\n        initial_state = initial_state.detach()\n    o, final_state = ChunkRetentionFunction.apply(q, k, v, initial_state, output_final_state)\n    return o, final_state\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The code defines a Triton kernel named sin_kernel and a Python wrapper function sin_triton. The kernel computes the sine of input elements. The kernel sin_kernel takes input pointers in_ptr0, out_ptr, the number of elements n_elements, and a compile-time constant BLOCK_SIZE. The program calculates the sine of each input element from the input pointer and stores the result in the output pointer. The function sin_triton is a wrapper that launches sin_kernel with the number of elements and a specific block size.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef sin_kernel(in_ptr0, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the program's unique index\n    pid = tl.program_id(0)\n    \n    # Compute the start and end index for this program\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    \n    # Mask to prevent out-of-bounds memory accesses\n    mask = offsets < n_elements\n    \n    # Load input elements from memory\n    x = tl.load(in_ptr0 + offsets, mask=mask, other=0.0)\n    \n    # Compute sine\n    y = tl.sin(x)\n    \n    # Store the result\n    tl.store(out_ptr + offsets, y, mask=mask)\n\n# Define the Python wrapper function\ndef sin_triton(input_tensor, block_size=1024):\n    # Ensure the input is a torch tensor and is on the GPU\n    assert input_tensor.is_cuda, \"Input tensor must be on the GPU\"\n    \n    # Create an output tensor of the same shape and type\n    output_tensor = torch.empty_like(input_tensor)\n    \n    # Get the number of elements\n    n_elements = input_tensor.numel()\n    \n    # Calculate the number of blocks needed\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    \n    # Launch the Triton kernel\n    sin_kernel[grid](input_tensor, output_tensor, n_elements, BLOCK_SIZE=block_size)\n    \n    return output_tensor\n\n# Example usage:\n# input_tensor = torch.randn(1024, device='cuda')\n# output_tensor = sin_triton(input_tensor)\n# print(output_tensor)\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef sin_triton(x, out):\n    n_elements = x.numel()\n    sin_kernel[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel function `decoding_fused_rotary_embedding_kernel` performs a rotary positional embedding transformation on query (`q`), key (`k`), and value (`v`) tensors, and updates key (`k_cache`) and value (`v_cache`) caches based on the transformation. The rotary embedding involves a rotation using cosine and sine values. The function handles queries and keys differently based on the head index.\n\n            The kernel is invoked from the `decoding_fused_rotary_embedding` function, which calculates the grid configuration and strides for memory accesses and calls the Triton kernel. The function also manages different layouts of the key cache.\n\n            Inputs to the Triton kernel include tensors `q`, `k`, `v`, `cos`, `sin`, `k_cache`, `v_cache`, and other parameters for indexing and transformation. The `decoding_fused_rotary_embedding` function configures the kernel launch parameters like grid size and warps based on tensor dimensions and other criteria.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef decoding_fused_rotary_embedding_kernel(\n    q_ptr, k_ptr, v_ptr,\n    cos_ptr, sin_ptr,\n    k_cache_ptr, v_cache_ptr,\n    head_idx, seq_len, n_heads, d_head,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Calculate block and thread indices\n    block_idx = tl.program_id(0)\n    thread_idx = tl.arange(0, BLOCK_SIZE)\n\n    # Compute the offsets for q, k, v based on the block index\n    q_offset = block_idx * seq_len * d_head + thread_idx\n    k_offset = block_idx * seq_len * d_head + thread_idx\n    v_offset = block_idx * seq_len * d_head + thread_idx\n\n    # Load q, k, v\n    q = tl.load(q_ptr + q_offset)\n    k = tl.load(k_ptr + k_offset)\n    v = tl.load(v_ptr + v_offset)\n\n    # Load cos and sin values\n    cos = tl.load(cos_ptr + thread_idx)\n    sin = tl.load(sin_ptr + thread_idx)\n\n    # Apply rotary embedding transformation\n    q_rot = q * cos - k * sin\n    k_rot = k * cos + q * sin\n\n    # Store the results in the cache\n    tl.store(k_cache_ptr + k_offset, k_rot)\n    tl.store(v_cache_ptr + v_offset, v)\n\n# Wrapper function\ndef decoding_fused_rotary_embedding(q, k, v, cos, sin, k_cache, v_cache, head_idx, seq_len, n_heads, d_head):\n    # Calculate grid size\n    grid_size = (n_heads, )\n\n    # Launch the Triton kernel\n    decoding_fused_rotary_embedding_kernel[grid_size](\n        q, k, v,\n        cos, sin,\n        k_cache, v_cache,\n        head_idx, seq_len, n_heads, d_head,\n        BLOCK_SIZE=128  # Example block size\n    )\n", "label": "import warnings\nfrom typing import Optional\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef decoding_fused_rotary_embedding_kernel(\n    q,\n    k,\n    v,\n    cos,\n    sin,\n    k_cache,\n    v_cache,\n    BLOCK_TABLES,\n    context_lengths,\n    x,\n    q_token_stride,\n    q_head_stride,\n    k_token_stride,\n    k_head_stride,\n    head_dim_stride,\n    cos_token_stride,\n    cos_stride,\n    kcb_stride,\n    kch_stride,\n    kcsplit_x_stride,\n    kcs_stride,\n    kcd_stride,\n    vcb_stride,\n    vch_stride,\n    vcs_stride,\n    vcd_stride,\n    bts_stride,\n    btb_stride,\n    block_size,\n    KV_GROUP_NUM: tl.constexpr,\n    HEAD_DIM: tl.constexpr,\n):\n    cur_head_idx = tl.program_id(0)\n    cur_token_idx = tl.program_id(1)\n\n    dim_range = tl.arange(0, HEAD_DIM)\n    dim_range0 = tl.arange(0, HEAD_DIM // 2)\n    dim_range1 = tl.arange(HEAD_DIM // 2, HEAD_DIM)\n\n    off_q = cur_token_idx * q_token_stride + cur_head_idx * q_head_stride\n    off_q0 = off_q + dim_range0 * head_dim_stride\n    off_q1 = off_q + dim_range1 * head_dim_stride\n\n    loaded_q0 = tl.load(q + off_q0)\n    loaded_q1 = tl.load(q + off_q1)\n    off_cos_sin = cur_token_idx * cos_token_stride + dim_range0 * cos_stride\n    loaded_cos = tl.load(cos + off_cos_sin)\n    loaded_sin = tl.load(sin + off_cos_sin)\n\n    out_q0 = loaded_q0 * loaded_cos - loaded_q1 * loaded_sin\n    out_q1 = loaded_q0 * loaded_sin + loaded_q1 * loaded_cos\n    tl.store(q + off_q0, out_q0)\n    tl.store(q + off_q1, out_q1)\n\n    handle_kv = cur_head_idx % KV_GROUP_NUM == 0\n    if handle_kv:\n        cur_k_head_idx = cur_head_idx // KV_GROUP_NUM\n        off_kv = cur_token_idx * k_token_stride + cur_k_head_idx * k_head_stride\n        off_k0 = off_kv + dim_range0 * head_dim_stride\n        off_k1 = off_kv + dim_range1 * head_dim_stride\n        loaded_k0 = tl.load(k + off_k0)\n        loaded_k1 = tl.load(k + off_k1)\n\n        out_k0 = loaded_k0 * loaded_cos - loaded_k1 * loaded_sin\n        out_k1 = loaded_k0 * loaded_sin + loaded_k1 * loaded_cos\n\n        # NOTE The precondition here is that it's only for unpadded inputs during decoding stage,\n        # and so that we could directly use the token index as the sequence index\n        past_kv_seq_len = tl.load(context_lengths + cur_token_idx) - 1\n\n        last_block_idx = past_kv_seq_len // block_size\n        block_ids = tl.load(BLOCK_TABLES + cur_token_idx * bts_stride + last_block_idx * btb_stride)\n        offsets_in_last_block = past_kv_seq_len % block_size\n        offsets_cache_base = block_ids * kcb_stride + cur_k_head_idx * kch_stride\n        k_range0 = (\n            offsets_cache_base\n            + offsets_in_last_block * kcs_stride\n            + (dim_range0 // x) * kcsplit_x_stride\n            + (dim_range0 % x) * kcd_stride\n        )\n        k_range1 = (\n            offsets_cache_base\n            + offsets_in_last_block * kcs_stride\n            + (dim_range1 // x) * kcsplit_x_stride\n            + (dim_range1 % x) * kcd_stride\n        )\n        tl.store(k_cache + k_range0, out_k0)\n        tl.store(k_cache + k_range1, out_k1)\n\n        off_v = off_kv + dim_range * head_dim_stride\n        loaded_v = tl.load(v + off_v)\n        v_range = (\n            block_ids * vcb_stride\n            + cur_k_head_idx * vch_stride\n            + offsets_in_last_block * vcs_stride\n            + dim_range * vcd_stride\n        )\n        tl.store(v_cache + v_range, loaded_v)\n\n\ndef decoding_fused_rotary_embedding(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    k_cache: Optional[torch.Tensor] = None,\n    v_cache: Optional[torch.Tensor] = None,\n    block_tables: Optional[torch.Tensor] = None,\n    kv_lengths: Optional[torch.Tensor] = None,\n    use_new_kcache_layout: bool = False,\n):\n    \"\"\"\n    Args:\n        q: query tensor, [total_tokens, head_num, head_dim]\n        k: key tensor, [total_tokens, kv_head_num, head_dim]\n        v: value tensor, [total tokens, kv_head_num, head_dim]\n        cos: cosine for rotary embedding, [max_position_len, head_dim]\n        sin: sine for rotary embedding, [max_position_len, head_dim]\n        k_cache (torch.Tensor):  Blocked key cache. [num_blocks, kv_head_num, block_size, head_dim]\n        v_cache (torch.Tensor):  Blocked value cache. [num_blocks, kv_head_num, block_size, head_dim]\n        kv_lengths, Past key/value sequence lengths plus current sequence length for each sequence. [bsz]\n        block_tables: Block tables for each sequence. [bsz, max_blocks_per_sequence]\n    \"\"\"\n    q_total_tokens, q_head_num, head_dim = q.shape\n    assert q.size(0) == k.size(0) == v.size(0)\n\n    if head_dim >= 512:\n        num_warps = 16\n    elif head_dim >= 256:\n        num_warps = 8\n    else:\n        num_warps = 4\n    k_head_num = k.size(1)\n    kv_group_num = q_head_num // k_head_num\n\n    # For KCache and VCache with the same layout\n    x = head_dim\n    kcsplit_x_stride, kcs_stride, kcd_stride = 0, k_cache.stride(2), k_cache.stride(3)\n    # For KCache layout [num_blocks, num_kv_heads, head_dim//x, block_size, x]\n    if use_new_kcache_layout:\n        assert (\n            k_cache.dim() == 5\n            and k_cache.shape[1] == v_cache.shape[1]\n            and k_cache.shape[2] * k_cache.shape[4] == v_cache.shape[3]\n        ), f\"Invalid KCache shape {k_cache.shape} and VCache shape {v_cache.shape}\"\n        x = k_cache.size(-1)\n        kcsplit_x_stride, kcs_stride, kcd_stride = k_cache.stride()[-3:]\n\n    grid = (q_head_num, q_total_tokens)\n    decoding_fused_rotary_embedding_kernel[grid](\n        q,\n        k,\n        v,\n        cos,\n        sin,\n        k_cache,\n        v_cache,\n        block_tables,\n        kv_lengths,\n        x,\n        q.stride(0),\n        q.stride(1),\n        k.stride(0),\n        k.stride(1),\n        q.stride(2),\n        cos.stride(0),\n        cos.stride(1),\n        k_cache.stride(0),\n        k_cache.stride(1),\n        kcsplit_x_stride,\n        kcs_stride,\n        kcd_stride,\n        v_cache.stride(0),\n        v_cache.stride(1),\n        v_cache.stride(2),\n        v_cache.stride(3),\n        block_tables.stride(0),\n        block_tables.stride(1),\n        k_cache.size(-2),\n        KV_GROUP_NUM=kv_group_num,\n        HEAD_DIM=head_dim,\n        num_warps=num_warps,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel is designed for linear attention operations, split into different kernels for forward and backward computations. The `chunk_linear_attn_fwd_kernel_h` computes the intermediate tensor `h` by iterating over time steps `T` and accumulating the dot product between key (`k`) and value (`v`) tensors. The `chunk_linear_attn_fwd_kernel_o` computes the output tensor `o` by using query (`q`), `k`, `v`, and `h` tensors. The backward operations are handled by `chunk_linear_attn_bwd_kernel_dh` and `chunk_linear_attn_bwd_kernel_dqkv`, which calculate gradients for intermediate and input tensors, respectively. These kernels rely on block pointers and matrix operations to efficiently compute large tensor operations with Triton's parallel execution model. The `ChunkLinearAttentionFunction` is a PyTorch `autograd.Function` that uses these kernels in its forward and backward passes, facilitating the use of Triton within PyTorch's automatic differentiation framework. ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(\n    k_ptr, v_ptr, h_ptr,\n    T, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    offset_k = pid * BLOCK_SIZE\n    offset_v = pid * BLOCK_SIZE\n    offset_h = pid * BLOCK_SIZE\n\n    h = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    for t in range(0, T, BLOCK_SIZE):\n        k = tl.load(k_ptr + offset_k + t)\n        v = tl.load(v_ptr + offset_v + t)\n        h += k * v\n\n    tl.store(h_ptr + offset_h, h)\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(\n    q_ptr, k_ptr, v_ptr, h_ptr, o_ptr,\n    T, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    offset_q = pid * BLOCK_SIZE\n    offset_k = pid * BLOCK_SIZE\n    offset_v = pid * BLOCK_SIZE\n    offset_h = pid * BLOCK_SIZE\n    offset_o = pid * BLOCK_SIZE\n\n    q = tl.load(q_ptr + offset_q)\n    h = tl.load(h_ptr + offset_h)\n\n    o = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    for t in range(0, T, BLOCK_SIZE):\n        k = tl.load(k_ptr + offset_k + t)\n        v = tl.load(v_ptr + offset_v + t)\n        o += q * (k * v / h)\n\n    tl.store(o_ptr + offset_o, o)\n\n@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(\n    grad_o_ptr, q_ptr, k_ptr, v_ptr, dh_ptr,\n    T, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    offset_q = pid * BLOCK_SIZE\n    offset_k = pid * BLOCK_SIZE\n    offset_v = pid * BLOCK_SIZE\n    offset_grad_o = pid * BLOCK_SIZE\n    offset_dh = pid * BLOCK_SIZE\n\n    dh = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    for t in range(0, T, BLOCK_SIZE):\n        grad_o = tl.load(grad_o_ptr + offset_grad_o + t)\n        q = tl.load(q_ptr + offset_q + t)\n        k = tl.load(k_ptr + offset_k + t)\n        v = tl.load(v_ptr + offset_v + t)\n        dh += grad_o * q * (k * v)\n\n    tl.store(dh_ptr + offset_dh, dh)\n\n@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(\n    grad_o_ptr, q_ptr, k_ptr, v_ptr, h_ptr, dq_ptr, dk_ptr, dv_ptr,\n    T, BLOCK_SIZE: tl.constexpr\n):\n    pid = tl.program_id(0)\n    offset_q = pid * BLOCK_SIZE\n    offset_k = pid * BLOCK_SIZE\n    offset_v = pid * BLOCK_SIZE\n    offset_grad_o = pid * BLOCK_SIZE\n    offset_h = pid * BLOCK_SIZE\n    offset_dq = pid * BLOCK_SIZE\n    offset_dk = pid * BLOCK_SIZE\n    offset_dv = pid * BLOCK_SIZE\n\n    grad_o = tl.load(grad_o_ptr + offset_grad_o)\n    h = tl.load(h_ptr + offset_h)\n\n    dq = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    dk = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    dv = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n\n    for t in range(0, T, BLOCK_SIZE):\n        q = tl.load(q_ptr + offset_q + t)\n        k = tl.load(k_ptr + offset_k + t)\n        v = tl.load(v_ptr + offset_v + t)\n\n        dq += grad_o * (k * v / h)\n        dk += grad_o * q * (v / h)\n        dv += grad_o * q * (k / h)\n\n    tl.store(dq_ptr + offset_dq, dq)\n    tl.store(dk_ptr + offset_dk, dk)\n    tl.store(dv_ptr + offset_dv, dv)\n\nimport torch\nfrom torch.autograd import Function\n\nclass ChunkLinearAttentionFunction(Function):\n    @staticmethod\n    def forward(ctx, q, k, v):\n        BLOCK_SIZE = 128  # Define block size\n        T = q.shape[0]    # Time steps\n\n        # Allocate output tensors\n        h = torch.empty_like(q)\n        o = torch.empty_like(q)\n\n        # Launch Triton kernels\n        grid = lambda meta: (triton.cdiv(T, meta['BLOCK_SIZE']),)\n        chunk_linear_attn_fwd_kernel_h[grid](k, v, h, T, BLOCK_SIZE=BLOCK_SIZE)\n        chunk_linear_attn_fwd_kernel_o[grid](q, k, v, h, o, T, BLOCK_SIZE=BLOCK_SIZE)\n\n        # Save tensors for backward pass\n        ctx.save_for_backward(q, k, v, h)\n        return o\n\n    @staticmethod\n    def backward(ctx, grad_o):\n        q, k, v, h = ctx.saved_tensors\n        BLOCK_SIZE = 128  # Define block size\n        T = q.shape[0]    # Time steps\n\n        # Allocate gradient tensors\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = torch.empty_like(v)\n        dh = torch.empty_like(h)\n\n        # Launch Triton kernels for backward pass\n        grid = lambda meta: (triton.cdiv(T, meta['BLOCK_SIZE']),)\n        chunk_linear_attn_bwd_kernel_dh[grid](grad_o, q, k, v, dh, T, BLOCK_SIZE=BLOCK_SIZE)\n        chunk_linear_attn_bwd_kernel_dqkv[grid](grad_o, q, k, v, h, dq, dk, dv, T, BLOCK_SIZE=BLOCK_SIZE)\n\n        return dq, dk, dv\n\n# Usage in PyTorch\nclass ChunkLinearAttention(torch.nn.Module):\n    def forward(self, q, k, v):\n        return ChunkLinearAttentionFunction.apply(q, k, v)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional, Tuple\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(\n    k, v, h, h0, ht,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr,\n    NT: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(\n    q, k, v, h, o,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t, scale,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr\n):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale\n\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n\n\n@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(\n    q, do, dh,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t, scale,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do.to(b_q.dtype), allow_tf32=False)\n\n\n@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(\n    q, k, v, h, do, dh, dq, dk, dv,\n    s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d,\n    s_h_h, s_h_t, scale,\n    T: tl.constexpr, K: tl.constexpr, V: tl.constexpr,\n    BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr\n):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k*n_bh+i_bh)*s_vo_h, (T, V), (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0).to(b_q.dtype)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n\n\nclass ChunkLinearAttentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, scale, initial_state, output_final_state):\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        ctx.scale = scale\n\n        final_state = None\n        if output_final_state:\n            final_state = q.new_empty(B, H, K, V, dtype=torch.float32, requires_grad=False)\n\n        h = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_linear_attn_fwd_kernel_h[grid](\n            k, v, h, initial_state, final_state,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=output_final_state,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        grid = (NV, NT, B * H)\n        o = torch.empty_like(v)\n        chunk_linear_attn_fwd_kernel_o[grid](\n            q, k, v, h, o,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            h.stride(1), h.stride(2),\n            scale,\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        ctx.save_for_backward(q, k, v, h)\n        return o.to(q.dtype), final_state\n\n    @staticmethod\n    def backward(ctx, do, dht=None):\n        q, k, v, h = ctx.saved_tensors\n\n        B, H, T, K, V = *q.shape, v.shape[-1]\n        BT = 64\n        BK, BV = min(64, triton.next_power_of_2(K)), min(32 if q.dtype == torch.float32 else 64, triton.next_power_of_2(V))\n        NT, NK, NV = triton.cdiv(T, BT), triton.cdiv(K, BK), triton.cdiv(V, BV)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        scale = ctx.scale\n\n        dh = q.new_empty(B, H, NT * K, V)\n        grid = (NK, NV, B * H)\n        chunk_linear_attn_bwd_kernel_dh[grid](\n            q, do, dh,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n\n        grid = (NK, NT, B * H)\n        dq = torch.empty_like(q)\n        dk = torch.empty_like(k)\n        dv = v.new_empty(NK, *v.shape)\n        num_stages = 1\n        num_warps = 4 if BK == 64 else 2\n        chunk_linear_attn_bwd_kernel_dqkv[grid](\n            q, k, v, h, do, dh, dq, dk, dv,\n            q.stride(1), q.stride(2), q.stride(3),\n            v.stride(1), v.stride(2), v.stride(3),\n            dh.stride(1), dh.stride(2),\n            scale,\n            T=T, K=K, V=V, BT=BT, BK=BK, BV=BV, NT=NT,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        dv = dv.sum(0)\n        return dq.to(q.dtype), dk.to(k.dtype), dv.to(v.dtype), None, None, None\n\n\ndef chunk_linear_attn(\n    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n    scale: Optional[float] = None, initial_state: torch.Tensor = None,\n    output_final_state: bool = False, normalize: bool = True\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if scale is None:\n        scale = q.shape[-1] ** -0.5\n    o, final_state = ChunkLinearAttentionFunction.apply(q, k, v, scale, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implements fused softmax and its backward operation over a 3D tensor. The softmax is applied along the last dimension of the tensor. There are two main functions: softmax and softmax_backward. \n\n            The softmax function takes 3D tensors Y (output), X (input), and an optional mask M. It applies a softmax operation along the last dimension, optionally applying a log transformation, masking based on a specified mask type, and considering causal constraints. \n\n            The softmax_backward function computes the gradient of the softmax operation. It takes the gradient output, the original softmax output, and computes the gradient input.\n\n            The @triton.jit decorator marks the kernels _softmax and _softmax_backward as JIT-compiled Triton kernels, allowing them to be highly optimized for different hardware. Autotuning with different configurations ensures the best performance by adjusting parameters like num_warps. \n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel for softmax\n@triton.jit\ndef _softmax(Y, X, M, stride_z, stride_y, stride_x, stride_m, n_cols, log, mask_type, causal, BLOCK_SIZE: tl.constexpr):\n    # Get the block index\n    z = tl.program_id(0)\n    y = tl.program_id(1)\n\n    # Compute the offset for this block\n    offset = z * stride_z + y * stride_y\n\n    # Load the input block\n    x_ptrs = X + offset + tl.arange(0, BLOCK_SIZE) * stride_x\n    x = tl.load(x_ptrs, mask=tl.arange(0, BLOCK_SIZE) < n_cols, other=-float('inf'))\n\n    # Apply mask if provided\n    if mask_type != 'none':\n        m_ptrs = M + offset + tl.arange(0, BLOCK_SIZE) * stride_m\n        mask = tl.load(m_ptrs, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n        x = tl.where(mask, x, -float('inf'))\n\n    # Causal mask\n    if causal:\n        col_idx = tl.arange(0, BLOCK_SIZE)\n        x = tl.where(col_idx[:, None] >= col_idx[None, :], x, -float('inf'))\n\n    # Compute softmax\n    max_x = tl.max(x, axis=0)\n    x_exp = tl.exp(x - max_x)\n    sum_x_exp = tl.sum(x_exp, axis=0)\n    y = x_exp / sum_x_exp\n\n    # Apply log if needed\n    if log:\n        y = tl.log(y)\n\n    # Store the result\n    y_ptrs = Y + offset + tl.arange(0, BLOCK_SIZE) * stride_y\n    tl.store(y_ptrs, y, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\n# Triton kernel for softmax backward\n@triton.jit\ndef _softmax_backward(grad_input, grad_output, softmax_output, stride_z, stride_y, stride_x, n_cols, BLOCK_SIZE: tl.constexpr):\n    # Get the block index\n    z = tl.program_id(0)\n    y = tl.program_id(1)\n\n    # Compute the offset for this block\n    offset = z * stride_z + y * stride_y\n\n    # Load the grad_output and softmax_output block\n    grad_out_ptrs = grad_output + offset + tl.arange(0, BLOCK_SIZE) * stride_x\n    grad_out = tl.load(grad_out_ptrs, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\n    softmax_out_ptrs = softmax_output + offset + tl.arange(0, BLOCK_SIZE) * stride_x\n    softmax_out = tl.load(softmax_out_ptrs, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\n    # Compute gradient of softmax\n    dot_product = tl.sum(grad_out * softmax_out, axis=0)\n    grad_in = softmax_out * (grad_out - dot_product)\n\n    # Store the result\n    grad_in_ptrs = grad_input + offset + tl.arange(0, BLOCK_SIZE) * stride_y\n    tl.store(grad_in_ptrs, grad_in, mask=tl.arange(0, BLOCK_SIZE) < n_cols)\n\n# Wrapper function for softmax\ndef softmax(Y, X, M=None, log=False, mask_type='none', causal=False):\n    # Determine the shape of the input tensor\n    Z, Y_dim, X_dim = X.shape\n\n    # Define block size and launch grid\n    BLOCK_SIZE = 128\n    grid = (Z, Y_dim)\n\n    # Launch the Triton kernel\n    _softmax[grid](Y, X, M, X.stride(0), X.stride(1), X.stride(2), M.stride(2) if M is not None else 0, X_dim, log, mask_type, causal, BLOCK_SIZE)\n\n# Wrapper function for softmax backward\ndef softmax_backward(grad_input, grad_output, softmax_output):\n    # Determine the shape of the input tensor\n    Z, Y_dim, X_dim = softmax_output.shape\n\n    # Define block size and launch grid\n    BLOCK_SIZE = 128\n    grid = (Z, Y_dim)\n\n    # Launch the Triton kernel\n    _softmax_backward[grid](grad_input, grad_output, softmax_output, softmax_output.stride(0), softmax_output.stride(1), softmax_output.stride(2), X_dim, BLOCK_SIZE)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\ndef get_depth(K):\n    return triton.next_power_of_2(K)\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"K\"],\n)\n@triton.heuristics({'DEPTH': lambda nargs: get_depth(nargs['K'])})\n@triton.heuristics({'IS_FP16': lambda nargs: nargs['Y'].dtype == torch.float16})\n@triton.jit\ndef _softmax(\n    Y, X, M,\n    stride_ym, stride_yn,\n    stride_xm, stride_xn,\n    stride_m,\n    K,\n    LOG: tl.constexpr,\n    MASK_TYPE: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    DEPTH: tl.constexpr,\n    IS_FP16: tl.constexpr,\n):\n    \"\"\"\n    Fused softmax kernel over a 3d tensor.\n    The softmax is applied over the last dimension, equivalent to torch.softmax(tensor, dim=-1)\n    \"\"\"\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, DEPTH)\n    x_ptrs = X + m * stride_xm + n * stride_xn + k\n    io_mask = k < K\n    if CAUSAL:\n        io_mask = io_mask & (k <= n)\n    x = tl.load(x_ptrs, mask=io_mask, other=float(\"-inf\"))\n    if CAUSAL:\n        off = float(\"-inf\")\n        off = off.to(x.dtype)\n        x = tl.where(k > n, off, x)\n    if MASK_TYPE is not None:\n        if MASK_TYPE == 'qk':\n            mask_ptrs = M + n * stride_m + k\n        elif MASK_TYPE == 'bk':\n            mask_ptrs = M + m * stride_m + k\n        add_mask = tl.load(mask_ptrs, io_mask, other=float(\"-inf\"))\n        x += add_mask\n    z = x - tl.max(x, axis=0)\n    if IS_FP16:\n        z = z.to(tl.float32)\n    num = tl.exp(z)\n    denom = tl.sum(num, axis=0)\n    if LOG:\n        y = z - tl.log(denom)\n    else:\n        y = num / denom\n    y_ptrs = Y + m * stride_ym + n * stride_yn + k\n    tl.store(y_ptrs, y, mask=k < K)\n\ndef softmax(Y, X, M=None, log=False, mask_type=None, causal=False):\n    assert X.ndim == 3, \"Input tensor X must be 3D\"\n    assert Y.shape == X.shape, \"Output tensor Y must have the same shape as X\"\n    M = M if M is not None else torch.empty(0, device=X.device)\n    \n    K = X.shape[-1]\n    stride_ym, stride_yn = Y.stride()[:-1]\n    stride_xm, stride_xn = X.stride()[:-1]\n    stride_m = M.stride(-1) if M.numel() > 0 else 0\n\n    grid = (X.shape[0], X.shape[1])\n    _softmax[grid](\n        Y, X, M,\n        stride_ym, stride_yn,\n        stride_xm, stride_xn,\n        stride_m,\n        K,\n        LOG=log,\n        MASK_TYPE=mask_type,\n        CAUSAL=causal\n    )\n\ndef get_depth(K):\n    return triton.next_power_of_2(K)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=1),\n        triton.Config({}, num_warps=2),\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n        triton.Config({}, num_warps=32),\n    ],\n    key=[\"K\"],\n)\n@triton.heuristics({'DEPTH': lambda nargs: get_depth(nargs['K'])})\n@triton.heuristics({'IS_FP16': lambda nargs: nargs['GradIn'].dtype == torch.float16})\n@triton.jit\ndef _softmax_backward(\n    GradIn, GradOut, Out,\n    stride_bm, stride_bn,\n    stride_gm, stride_gn,\n    stride_om, stride_on,\n    K,\n    LOG: tl.constexpr,\n    CAUSAL: tl.constexpr,\n    DEPTH: tl.constexpr,\n    IS_FP16: tl.constexpr,\n):\n    \"\"\"\n    Compute the softmax gradients.\n    \"\"\"\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, DEPTH)\n    grad_out_ptrs = GradOut + m * stride_gm + n * stride_gn + k\n    out_ptrs = Out + m * stride_om + n * stride_on + k\n    io_mask = k < K\n    if CAUSAL:\n        io_mask = io_mask & (k <= n)\n    g = tl.load(grad_out_ptrs, mask=io_mask, other=float(0))\n    o = tl.load(out_ptrs, mask=io_mask, other=float(0))\n    if CAUSAL:\n        zero = float(0)\n        zero = zero.to(g.dtype)\n        g = tl.where(k > n, zero, g)\n        o = tl.where(k > n, zero, o)\n    if LOG:\n        s = tl.sum(g, 0)\n        if IS_FP16:\n            o = o.to(tl.float32)\n        grad_in = g - tl.exp(o) * s\n    else:\n        s = tl.sum(g * o, 0)\n        grad_in = o * (g - s)\n    grad_in_ptrs = GradIn + m * stride_bm + n * stride_bn + k\n    tl.store(grad_in_ptrs, grad_in, mask=k < K)\n\n\ndef softmax_backward(GradIn, GradOut, Out, log=False, causal=False):\n    assert GradOut.shape == Out.shape, \"GradOut and Out must have the same shape\"\n    assert GradIn.shape == Out.shape, \"GradIn and Out must have the same shape\"\n    \n    K = Out.shape[-1]\n    stride_bm, stride_bn = GradIn.stride()[:-1]\n    stride_gm, stride_gn = GradOut.stride()[:-1]\n    stride_om, stride_on = Out.stride()[:-1]\n\n    grid = (Out.shape[0], Out.shape[1])\n    _softmax_backward[grid](\n        GradIn, GradOut, Out,\n        stride_bm, stride_bn,\n        stride_gm, stride_gn,\n        stride_om, stride_on,\n        K,\n        LOG=log,\n        CAUSAL=causal\n    )\n\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton-based kernel and a PyTorch module for performing RMS normalization (RMSNorm) in a more efficient manner. The kernel `rms_norm_fwd_fused` performs the normalization and scaling operations. It computes the variance of each row, calculates the reciprocal of the standard deviation, and applies the learned weights to the normalized data. The `TritonLlamaRMSNorm` class wraps this kernel into a PyTorch module, facilitating integration into neural networks. It accepts a tensor `x` and returns the normalized tensor `y`. The implementation limits the feature dimension to under 64KB and uses Triton's parallel processing to accelerate computation.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n# Define the Triton kernel for RMS normalization\n@triton.jit\ndef rms_norm_fwd_fused(x_ptr, w_ptr, y_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Pointers to data\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * n_elements\n    w_row_ptr = w_ptr + row_idx * n_elements\n    y_row_ptr = y_ptr + row_idx * n_elements\n\n    # Load data for the row\n    x = tl.load(x_row_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < n_elements, other=0.0)\n    \n    # Compute variance\n    mean_square = tl.sum(x * x, axis=0) / n_elements\n    inv_std = tl.rsqrt(mean_square + 1e-5)\n    \n    # Normalize and scale\n    y = x * inv_std * tl.load(w_row_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < n_elements, other=1.0)\n    \n    # Store the result\n    tl.store(y_row_ptr + tl.arange(0, BLOCK_SIZE), y, mask=tl.arange(0, BLOCK_SIZE) < n_elements)\n\n# PyTorch module to wrap the Triton kernel\nclass TritonLlamaRMSNorm(torch.nn.Module):\n    def __init__(self, feature_dim):\n        super(TritonLlamaRMSNorm, self).__init__()\n        assert feature_dim < 64 * 1024, \"Feature dimension must be less than 64KB\"\n        self.feature_dim = feature_dim\n        self.weight = torch.nn.Parameter(torch.ones(feature_dim))\n\n    def forward(self, x):\n        # Ensure input tensor is 2D\n        assert x.ndim == 2 and x.shape[1] == self.feature_dim, \"Input tensor must be of shape (batch_size, feature_dim)\"\n        \n        # Allocate output tensor\n        y = torch.empty_like(x)\n        \n        # Launch the Triton kernel\n        grid = (x.shape[0],)\n        rms_norm_fwd_fused[grid](\n            x_ptr=x,\n            w_ptr=self.weight,\n            y_ptr=y,\n            n_elements=self.feature_dim,\n            BLOCK_SIZE=1024  # Adjust BLOCK_SIZE based on the GPU's capacity\n        )\n        \n        return y\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size = 128\n    feature_dim = 1024  # Must be less than 64KB\n    x = torch.randn(batch_size, feature_dim, device='cuda')\n\n    rms_norm = TritonLlamaRMSNorm(feature_dim).cuda()\n    y = rms_norm(x)\n    print(y)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nimport torch.nn as nn\n\n# Kernel function for fused RMSNorm\n@triton.jit\ndef rms_norm_fwd_fused(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    W,  # pointer to the weights\n    stride,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_SIZE: tl.constexpr,\n):\n    # Map the program id to the row of X and Y it should compute.\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    # Compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n        x = tl.where(cols < N, x, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # Normalize and apply linear transformation\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        y = x_hat * w\n        # Write output\n        tl.store(Y + cols, y, mask=mask)\n\n# TritonLlamaRMSNorm class for integrating the kernel into a model\nclass TritonLlamaRMSNorm(nn.Module):\n    def __init__(self, weight, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = weight\n        self.variance_epsilon = eps\n\n    def forward(self, x):\n        y = torch.empty_like(x)\n        # reshape input data into 2D tensor\n        x_arg = x.reshape(-1, x.shape[-1])\n        M, N = x_arg.shape\n        # Less than 64KB per feature: enqueue fused kernel\n        MAX_FUSED_SIZE = 65536 // x.element_size()\n        BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n        if N > BLOCK_SIZE:\n            raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KB.\")\n        # heuristics for number of warps\n        num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n        # enqueue kernel\n        rms_norm_fwd_fused[(M,)](\n            x_arg,\n            y,\n            self.weight,\n            x_arg.stride(0),\n            N,\n            self.variance_epsilon,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n        return y\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `rmsnorm_triton` performs RMS normalization on a 3D tensor `x` using a weight vector `rms_w`. The function calculates the root mean square of elements across a specified dimension, normalizes the values, and then scales them by `rms_w`. It operates over batches and a matrix's M dimension with a fixed block size (`BLOCK_N_SIZE`). The wrapper function `rmsnorm_triton_wrapper` prepares the inputs and launches the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, output_ptr, M, N, BLOCK_N_SIZE, **meta):\n    # Compute block index\n    pid = tl.program_id(0)\n    \n    # Compute the starting position of the block\n    start_n = pid * BLOCK_N_SIZE\n    \n    # Create offsets for the current block\n    offsets_n = start_n + tl.arange(0, BLOCK_N_SIZE)\n    \n    # Load the input data\n    x = tl.load(x_ptr + offsets_n, mask=offsets_n < N, other=0.0)\n    \n    # Compute mean square\n    mean_square = tl.sum(x * x, axis=0) / N\n    \n    # Compute root mean square\n    rms = tl.sqrt(mean_square)\n    \n    # Normalize the input\n    normalized_x = x / rms\n    \n    # Load the weights\n    rms_w = tl.load(rms_w_ptr + offsets_n, mask=offsets_n < N, other=1.0)\n    \n    # Scale by weights\n    scaled_x = normalized_x * rms_w\n    \n    # Store the result\n    tl.store(output_ptr + offsets_n, scaled_x, mask=offsets_n < N)\n\n# Wrapper function\ndef rmsnorm_triton_wrapper(x, rms_w, BLOCK_N_SIZE=128):\n    # Get the shape of the input tensor\n    B, M, N = x.shape\n    \n    # Allocate output tensor\n    output = torch.empty_like(x)\n    \n    # Convert tensors to pointers\n    x_ptr = x.data_ptr()\n    rms_w_ptr = rms_w.data_ptr()\n    output_ptr = output.data_ptr()\n    \n    # Launch the kernel\n    grid = (triton.cdiv(N, BLOCK_N_SIZE),)\n    rmsnorm_triton[grid](x_ptr, rms_w_ptr, output_ptr, M, N, BLOCK_N_SIZE, num_warps=4)\n    \n    return output\n\n# Example usage\nB, M, N = 32, 128, 512  # Example dimensions\nx = torch.randn(B, M, N, device='cuda')\nrms_w = torch.randn(N, device='cuda')\n\noutput = rmsnorm_triton_wrapper(x, rms_w)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, output_ptr,\n                   stride_x_batch, stride_x_m, stride_x_k,\n                   stride_rms_w,\n                   stride_out_batch, stride_out_m, stride_out_k,\n                   N_SIZE: tl.constexpr, eps: tl.constexpr, BLOCK_N_SIZE: tl.constexpr):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n\n    offs_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_N = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        offs_n = block_n_start_idx + block_N\n        x_ptr_mask = offs_n < N_SIZE\n        x = tl.load(x_ptr + offs_m + offs_n * stride_x_k, mask=x_ptr_mask, other=0.0)\n        var += tl.extra.cuda.libdevice.pow(x.to(tl.float32), 2)\n\n    var = tl.sum(var, axis=0) / N_SIZE\n    rstd = tl.math.rsqrt(var + eps)\n\n    # multiply by weight and add bias\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        offs_n = block_n_start_idx + block_N\n        x_ptr_mask = offs_n < N_SIZE\n        rms_w = tl.load(rms_w_ptr + offs_n * stride_rms_w, mask=x_ptr_mask)\n\n        x = tl.load(x_ptr + offs_m + offs_n * stride_x_k, mask=x_ptr_mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        out = x_hat * rms_w\n        out_off = pid_batch * stride_out_batch + pid_m * stride_out_m + offs_n * stride_out_k\n        tl.store(output_ptr + out_off, out, mask=x_ptr_mask)\n\n\ndef rmsnorm_triton_wrapper(x, rms_w, eps=1e-6):\n    batch, M, K = x.shape\n    assert rms_w.shape[-1] == K\n    out = torch.empty_like(x)\n    rmsnorm_triton[(batch, M,)](x, rms_w, out,\n                                *x.stride(),\n                                *rms_w.stride(),\n                                *out.stride(),\n                                N_SIZE=K, eps=eps, BLOCK_N_SIZE=1024,\n                                )\n    return out\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `nested3` Triton kernel performs repeated memory loads and stores with a nested loop pattern. It loads data from an input pointer `in_ptr`, shifts its position by `stride_n` and stores it to an output pointer `out_ptr`. The kernel uses 2x2 tile operations with pointers calculated using provided strides, and nested loops iterate through multiple 2x2 block positions. The `wrapper_nested3` function initializes an input tensor and output tensor, configures the execution grid, and launches the kernel, printing the output tensor.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef nested3_kernel(in_ptr, out_ptr, stride_n, num_tiles, BLOCK_SIZE: tl.constexpr):\n    # Block indices\n    block_x = tl.program_id(0)\n    block_y = tl.program_id(1)\n\n    # Calculate base pointers for this block\n    base_x = block_x * BLOCK_SIZE\n    base_y = block_y * BLOCK_SIZE\n\n    # Iterate over 2x2 tiles within the block\n    for i in range(2):\n        for j in range(2):\n            # Calculate the offsets for the current tile\n            offset_x = base_x + i\n            offset_y = base_y + j\n\n            # Calculate the linear index\n            idx = offset_y * stride_n + offset_x\n\n            # Load from input\n            data = tl.load(in_ptr + idx)\n\n            # Store to output\n            tl.store(out_ptr + idx, data)\n\ndef wrapper_nested3(input_tensor, stride_n):\n    # Ensure the input tensor is on the GPU\n    input_tensor = input_tensor.to('cuda')\n    \n    # Create an output tensor of the same shape\n    output_tensor = torch.empty_like(input_tensor)\n\n    # Define the grid size\n    num_tiles_x = input_tensor.shape[1] // 2\n    num_tiles_y = input_tensor.shape[0] // 2\n    grid = (num_tiles_x, num_tiles_y)\n\n    # Launch the kernel\n    nested3_kernel[grid](\n        input_tensor,\n        output_tensor,\n        stride_n,\n        num_tiles_x * num_tiles_y,\n        BLOCK_SIZE=2\n    )\n\n    # Print the output tensor\n    print(output_tensor.cpu())\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input tensor and stride\n    input_tensor = torch.arange(16, dtype=torch.float32).reshape(4, 4)\n    stride_n = input_tensor.stride(0)\n\n    # Call the wrapper function\n    wrapper_nested3(input_tensor, stride_n)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef nested3(in_ptr, out_ptr, stride_m, stride_n):\n    offs_am = tl.arange(0, 2)\n    offs_an = tl.arange(0, 2)\n    a_ptrs = in_ptr + (offs_am[:, None] * stride_m +\n                        offs_an[None, :] * stride_n)\n\n    offs_cm = tl.arange(0, 2)\n    offs_cn = tl.arange(0, 2)\n    c_ptrs = out_ptr + stride_m * offs_cm[:, None] + stride_n * offs_cn[\n        None, :]\n\n    for i in range(0, 2):\n        a1 = tl.load(a_ptrs)\n\n        for j in range(0, 2):\n            a_ptrs += 2 * stride_n\n            a2 = tl.load(a_ptrs)\n\n            for k in range(0, 2):\n                a_ptrs += 2 * stride_n\n                a3 = tl.load(a_ptrs)\n                tl.store(c_ptrs, a1)\n                c_ptrs += 2 * stride_n\n\n                tl.store(c_ptrs, a2)\n                c_ptrs += 2 * stride_n\n                tl.store(c_ptrs, a3)\n                c_ptrs += 2 * stride_n\n\n        a_ptrs += 2 * stride_n\n\n\ndef wrapper_nested3(n_rows, n_cols):\n    x = torch.arange(0, n_rows * n_cols, device=\"cuda\", dtype=torch.int32).reshape([n_rows, n_cols])\n    output = torch.zeros([n_rows, n_cols], device=x.device, dtype=x.dtype)\n    grid = lambda meta: (n_cols // 4,)\n    nested3[grid](x, output, x.stride(0), x.stride(1))\n    print(output)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator is designed for efficient matrix multiplication with specific attention to quantized inputs, where matrix B is stored in an int4 format using GPTQ (General-Purpose Tensor Quantization).\n        \n        The `matmul4_kernel` function performs the main computation of the matrix multiplication C = A x B. Matrix A is float16, matrix B is int32 (representing int4 quantized values), and matrix C is float16. The kernel utilizes scaling and zero-point values to correctly dequantize B during multiplication. It applies a loop across the K-dimension and uses block sizes (BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K) to handle chunks of data efficiently. The function also takes advantage of Triton's features like parallel execution through `program_id` and auto-tuning with `triton.autotune`.\n\n        The `matmul_dequantize_int4_gptq` function is a wrapper to execute this kernel. It sets up grid dimensions for the execution, computes the output matrix C, and checks for memory alignment. It returns the output matrix C after computation.\n\n        The `quantize_int4` function prepares data in int4 format by packing 8 such values into an int32 word. This includes computing scales and zero points per group of features and packing them into the final format used by the kernel.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_warps=4),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul4_kernel(A, B, C, scales, zero_points, M, N, K, stride_am, stride_ak, stride_bn, stride_c, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    num_pid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M\n    num_pid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = A + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    b_ptrs = B + offs_k[:, None] * stride_bn + offs_bn[None, :]\n    c_ptrs = C + offs_am[:, None] * stride_c + offs_bn[None, :]\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs, mask=offs_am[:, None] < M)\n        b_int4 = tl.load(b_ptrs, mask=offs_bn[None, :] < N)\n\n        # Dequantize B\n        scale = tl.load(scales + offs_k)\n        zero_point = tl.load(zero_points + offs_k)\n        b = (b_int4.to(tl.float32) - zero_point) * scale\n\n        acc += tl.dot(a, b)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bn\n\n    c = acc.to(tl.float16)\n    tl.store(c_ptrs, c, mask=offs_am[:, None] < M)\n\n\ndef matmul_dequantize_int4_gptq(A, B, scales, zero_points, M, N, K):\n    assert A.shape == (M, K)\n    assert B.shape == (K, N)\n    assert scales.shape == (K,)\n    assert zero_points.shape == (K,)\n\n    # Allocate output matrix C\n    C = torch.empty((M, N), dtype=torch.float16, device='cuda')\n\n    # Define grid dimensions\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n\n    # Launch the kernel\n    matmul4_kernel[grid](\n        A, B, C, scales, zero_points, M, N, K,\n        A.stride(0), A.stride(1), B.stride(1), C.stride(0)\n    )\n\n    return C\n\nimport torch\n\ndef quantize_int4(B, group_size=8):\n    K, N = B.shape\n    scales = torch.empty(K, dtype=torch.float32, device=B.device)\n    zero_points = torch.empty(K, dtype=torch.int32, device=B.device)\n    B_int4 = torch.empty((K, N), dtype=torch.int32, device=B.device)\n\n    for k in range(0, K, group_size):\n        b_slice = B[k:k+group_size]\n        min_val, max_val = b_slice.min(), b_slice.max()\n        scale = (max_val - min_val) / 15.0\n        zero_point = min_val\n\n        scales[k:k+group_size] = scale\n        zero_points[k:k+group_size] = zero_point\n\n        b_quantized = ((b_slice - zero_point) / scale).round().to(torch.int32)\n        B_int4[k:k+group_size] = pack_int4(b_quantized)\n\n    return B_int4, scales, zero_points\n\ndef pack_int4(b_quantized):\n    packed = torch.zeros((b_quantized.shape[0], b_quantized.shape[1] // 8), dtype=torch.int32, device=b_quantized.device)\n    for i in range(8):\n        packed |= (b_quantized[:, i::8] & 0xF) << (4 * i)\n    return packed\n", "label": "import time\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.autotune(\n\tconfigs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), \n    ],\n\tkey=['M', 'N', 'K', 'NO_GROUPS'],\n)\n@triton.jit\ndef matmul4_kernel(\n\ta_ptr, b_ptr, c_ptr,\n\tscales_ptr, zeros_ptr,\n\tM, N, K,\n\tstride_am, stride_ak,\n\tstride_bk, stride_bn,\n\tstride_cm, stride_cn,\n\tstride_scales_g, stride_scales_n,\n\tstride_zeros_g, stride_zeros_n,\n\tgroupsize, NO_GROUPS: tl.constexpr,\n\tBLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n\tGROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m    \n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n    a_mask = (offs_am[:, None] < M)\n    # b_ptrs is set up such that it repeats elements along the K axis 8 times\n    b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)   # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n   # (BLOCK_SIZE_N,)\n    # zeros_ptrs is set up such that it repeats elements along the N axis 8 times\n    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)   # (BLOCK_SIZE_N,)\n    # shifter is used to extract the 4 bits of each element in the 32-bit word from B and zeros\n    shifter = (offs_k % infearure_per_bits) * bits\n    zeros_shifter = (offs_bn % infearure_per_bits) * bits\n    # If G == 1, scales and zeros are the same for all K, so we can load them once\n    if NO_GROUPS:\n        # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n        scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_N,)\n        zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n        # Unpack zeros\n        zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n        # zeros = (zeros + 1) * scales  # (BLOCK_SIZE_N,) float16\n        zeros = zeros * scales\n    # Now calculate a block of output of shape (BLOCK_SIZE_M, BLOCK_SIZE_N)\n    # M is along the batch dimension, N is along the outfeatures dimension, K is along the infeatures dimension\n    # So this loop is along the infeatures dimension (K)\n    # It's calculating BLOCK_SIZE_M batches in parallel, and for each batch, BLOCK_SIZE_N outfeatures in parallel\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)   # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n        b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)  # (BLOCK_SIZE_N,)\n            ptr = zeros_ptrs + g_id * stride_zeros_g   # (BLOCK_SIZE_N,)\n            zeros = tl.load(ptr)  # (BLOCK_SIZE_N,), each element is repeated 8 times, int32\t\n            # Unpack zeros\n            zeros = (zeros >> zeros_shifter) & 0xF  # (BLOCK_SIZE_N,) int32\n            zeros = (zeros) * scales  # (BLOCK_SIZE_N,) float16\t\n        # Now we need to unpack b (which is 4-bit values) into 32-bit values\n        b = (b >> shifter[:, None]) & 0xF  # Extract the 4-bit values\n        b = b * scales[None, :] - zeros[None, :]  # Scale and shift\n        # print(\"data type\", a, b)\n        accumulator += tl.dot(a, b.to(a.dtype))\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk  \n    c = accumulator.to(c_ptr.dtype.element_ty)  \n    # Store the result\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\n\ndef matmul_dequantize_int4_gptq(x: torch.FloatTensor, qweight: torch.IntTensor, scales: torch.FloatTensor, qzeros: torch.IntTensor, group_size, output=None) -> torch.FloatTensor:\n\t\"\"\"\n\tCompute the matrix multiplication C = A x B + bias.\n\tWhere B is quantized using GPTQ and groupsize = -1 into 4-bit values.\n\n\tA is of shape (..., K) float16\n\tqweight is of shape (K//8, N) int32\n\tscales is of shape (G, N) float16\n\tqzeros is of shape (G, N//8) int32\n\tbias is of shape (1, N) float16\n\n\tgroupsize is the number of infeatures in each group.\n\tG = K // groupsize\n\n\tReturns C of shape (..., N) float16\n\t\"\"\"\n\tassert x.shape[-1] == (qweight.shape[0] * 8), \"A must be a multiple of 8 in the last dimension\"\n\tassert x.is_contiguous(), \"A must be contiguous\"\n\n\tM, K = x.shape\n\tN = qweight.shape[1]\n\t# This is based on the possible BLOCK_SIZE_Ks\n\t# assert K % 16 == 0 and K % 32 == 0 and K % 64 == 0 and K % 128 == 0, \"K must be a multiple of 16, 32, 64, and 128\"\n\t# # This is based on the possible BLOCK_SIZE_Ns\n\t# assert N % 16 == 0 and N % 32 == 0 and N % 64 == 0 and N % 128 == 0 and N % 256 == 0, \"N must be a multiple of 16, 32, 64, 128, and 256\"\n\t# # This is based on the possible BLOCK_SIZE_Ks\n\t# assert groupsize % 32 == 0 and groupsize % 64 == 0 and groupsize % 128 == 0, \"groupsize must be a multiple of 32, 64, and 128\"\n\n\t# output = torch.empty((M, N), device='cuda', dtype=torch.float16)\n\tif output is None:\n\t\tinplace = False\n\t\toutput = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\telse:\n\t\tinplace = True\n\n\tgrid = lambda META: (\n\t\ttriton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n\t)\n\tmatmul4_kernel[grid](\n\t\tx, qweight, output,\n\t\tscales, qzeros,\n\t\tM, N, K,\n\t\tx.stride(0), x.stride(1),\n\t\tqweight.stride(0), qweight.stride(1),\n\t\toutput.stride(0), output.stride(1),\n\t\tscales.stride(0), scales.stride(1),\n\t\tqzeros.stride(0), qzeros.stride(1),\n\t\tgroup_size, group_size == K,\n    )\n\t# return output\n\tif not inplace:\n\t\treturn output\n\ndef quantize_int4(weight, group_size=128, tp_rank=0):\n    # Weight shape: [H1 // 8, H2]\n    # Scale shape: [H1 // group_size, H2]\n    # zero_pint shape: [H1 // group_size, H2 // 8]\n\n    weight = weight.transpose(1, 0)\n    h1, h2 = weight.shape\n    assert h1 % 8 == 0 and h2 % 8 == 0, \"H1 {} H2 {}\".format(h1, h2)\n    assert h2 % group_size == 0, \"H1 {} H2 {}\".format(h1, h2)\n    weight = weight.contiguous().view(-1, group_size).cuda(tp_rank)\n    weight_max = weight.amax(-1, keepdim=True)\n    weight_max = torch.where(weight_max < 0, 0, weight_max)\n    weight_min = weight.amin(-1, keepdim=True)\n    weight_min = torch.where(weight_min > 0, 0, weight_min)\n    weight_range = weight_max - weight_min \n    scale = weight_range / (2 ** 4 - 1)\n    zero_point = (-weight_min / scale).round().clamp(0, 15).to(torch.int32)\n    weight = (weight / scale + zero_point).round().clamp(0, 15).to(torch.int32).view(h1, h2)\n    int_weight = torch.empty(h1, h2 // 8).to(torch.int32).to(weight.device)\n    int_zero_point = torch.zeros(h1 // 8, h2 // group_size).to(torch.int32).to(weight.device)\n    zero_point = zero_point.view(h1, -1)\n    scale = scale.view(h1, -1)\n    # pack 8 int4 in an int32 number.\n    # Weight pack in row.\n    for pack in range(0, h2, 8):\n        for i in range(8):\n            int_weight[:, pack // 8] += weight[:, pack + i] << (i * 4)\n    # zero point pack in col.\n    for pack in range(0, h1, 8):\n        for i in range(8):\n            int_zero_point[pack // 8, :] += zero_point[pack + i, :] << (i * 4)\n    '''\n    fp_weight = torch.zeros(h1, h2).half().to(weight.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_weight[pack * 8 + i, :] = \\\n                ((int_weight[pack, :] << (28 - i * 4) >> 28) + 16) % 16\n    print((fp_weight - weight).abs().sum())\n\n    fp_zp = torch.zeros(zero_point.shape).half().to(zero_point.device)\n    for pack in range(0, h1 // 8):\n        for i in range(8):\n            fp_zp[pack * 8 + i, :] = \\\n                (int_zero_point[pack, :] >> (i * 4)) & 15\n\n    print((fp_zp - zero_point).abs().sum())\n    '''\n    weight = None\n    return int_weight.transpose(1, 0).contiguous(), scale.transpose(1, 0).contiguous(), int_zero_point.transpose(1, 0).contiguous(), group_size\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel and wrapper function provided aim to convert packed 4-bit floating-point values to scaled bf16 (bfloat16) values. The kernel is defined as `triton_f4_to_scaled_bf16_kernel` and the wrapper function is `triton_f4_to_scaled_bf16`.\n\nFunction `triton_f4_to_scaled_bf16_kernel`: \n- Purpose: Convert packed fp4 values to bf16 and apply a scale.\n- Input: `x_ptr` (fp4 packed values), `s_ptr` (scale), `output_ptr` (output storage), `n_elements_in`, and several constant parameters like masks and biases for the conversion.\n- Output: A tensor of bf16 values scaled by a given factor.\n- Key Steps: \n  - It unpacks 4-bit numbers, aligns bits, applies exponent bias adjustments, and then scales the resulting bf16 values.\n  - Utilizes Triton's parallel computation model, dividing work into blocks based on `BLOCK_SIZE_IN`.\n  - Handles special cases for zero and denormal values with conditional logic.\n\nFunction `triton_f4_to_scaled_bf16`: \n- Purpose: Prepare and launch the kernel.\n- Input: A packed fp4 tensor `x`, a scale tensor `s_e8m0`, and block size `mx_block_size`.\n- Output: Bf16 tensor with scaling applied.\n- Steps:\n  - Allocates space for the output and checks input conditions.\n  - Calculates grid size for the Triton kernel launch.\n  - Passes necessary parameters to the kernel including constants like `SIGN_MASK_F4`, `ZERO_BITS_F32`, and others.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef triton_f4_to_scaled_bf16_kernel(\n    x_ptr, s_ptr, output_ptr, n_elements_in,\n    SIGN_MASK_F4, ZERO_BITS_F32, EXPONENT_BIAS_F4, EXPONENT_BIAS_BF16,\n    BLOCK_SIZE_IN, **meta\n):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE_IN + tl.arange(0, BLOCK_SIZE_IN)\n    mask = offsets < n_elements_in\n\n    # Load packed fp4 values\n    packed_fp4 = tl.load(x_ptr + offsets, mask=mask, other=0)\n\n    # Unpack 4-bit values (assuming 2 values per byte)\n    fp4_values = (packed_fp4 & 0xF, packed_fp4 >> 4)\n\n    # Initialize bf16 output\n    bf16_output = tl.zeros([BLOCK_SIZE_IN * 2], dtype=tl.float32)\n\n    for i in range(2):\n        # Extract sign, exponent, and mantissa\n        sign = (fp4_values[i] & SIGN_MASK_F4) << 28\n        exponent = ((fp4_values[i] >> 1) & 0x7) + EXPONENT_BIAS_BF16 - EXPONENT_BIAS_F4\n        mantissa = (fp4_values[i] & 0x1) << 23\n\n        # Combine into bf16 format\n        bf16_value = sign | (exponent << 23) | mantissa\n\n        # Handle special cases (zero, denormal)\n        is_zero = fp4_values[i] == 0\n        bf16_value = tl.where(is_zero, ZERO_BITS_F32, bf16_value)\n\n        # Convert to float32 for scaling\n        bf16_value_f32 = tl.bitcast(bf16_value, tl.float32)\n\n        # Load scale and apply\n        scale = tl.load(s_ptr)\n        scaled_value = bf16_value_f32 * scale\n\n        # Store the result\n        bf16_output[i::2] = scaled_value\n\n    # Store the result\n    tl.store(output_ptr + offsets * 2, bf16_output, mask=mask)\n\nimport torch\n\ndef triton_f4_to_scaled_bf16(x, s_e8m0, mx_block_size):\n    assert x.dtype == torch.uint8, \"Input tensor must be of type uint8 for packed fp4 values.\"\n    assert s_e8m0.dtype == torch.float32, \"Scale tensor must be of type float32.\"\n\n    n_elements_in = x.numel()\n    n_elements_out = n_elements_in * 2\n\n    # Allocate output tensor\n    output = torch.empty(n_elements_out, dtype=torch.bfloat16, device=x.device)\n\n    # Constants\n    SIGN_MASK_F4 = 0x8\n    ZERO_BITS_F32 = 0x00000000\n    EXPONENT_BIAS_F4 = 7\n    EXPONENT_BIAS_BF16 = 127\n    BLOCK_SIZE_IN = mx_block_size\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(n_elements_in, BLOCK_SIZE_IN),)\n    triton_f4_to_scaled_bf16_kernel[grid](\n        x, s_e8m0, output, n_elements_in,\n        SIGN_MASK_F4, ZERO_BITS_F32, EXPONENT_BIAS_F4, EXPONENT_BIAS_BF16,\n        BLOCK_SIZE_IN\n    )\n\n    return output\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fp4_packed_to_bf16(\n    x_packed,\n    sign_mask_f4,\n    mantissa_mask_f4,\n    mbits_f4_e2m1,\n    ebits_f4_e2m1,\n    f4_e2m1_exp_bias,\n    mbits_f32,\n    ebits_f32,\n    f32_exp_bias,\n    zero_bits_f32,\n    zero_point_five_bits_f32,\n):\n    \"\"\"\n    Input: a tensor of packed fp4 values\n    Output: a tensor of bfloat16 values\n    \"\"\"\n\n    # low-bits: original location 0:3\n    # high-bits: original location 4:7\n    x_low_bits = x_packed >> 4\n    x_high_bits = x_packed & 0xF\n    x = tl.interleave(x_low_bits, x_high_bits)\n\n    # cast logic below\n    # output = x_unpacked.to(tl.float32)\n\n    # save the sign\n    sign_f4 = x & sign_mask_f4\n\n    # set everything to positive, will add sign back at the end\n    x_pos = x ^ sign_f4\n\n    # Special case zero\n    zero_mask = x_pos == 0\n\n    # There is only one denormal value in fp4: s001, which is 0.5 in f32\n    # Special case it.\n    # TODO(later): will it be faster to repeat this for all 8 positive\n    # values instead of the bit manipulations?\n    denormal_mask = x_pos == 1\n\n    # calculate the new exponent and shift it to bits 2:9 of the result\n    exp_biased_f4 = x_pos >> mbits_f4_e2m1\n    exp_biased_f32 = exp_biased_f4 - f4_e2m1_exp_bias + f32_exp_bias\n    exp_biased_f32 = exp_biased_f32.to(tl.int32) << mbits_f32\n\n    # shift the mantissa to bits 10:32 of the result\n    mantissa_f4 = x_pos & mantissa_mask_f4\n    mantissa_f32 = mantissa_f4.to(tl.int32) << (mbits_f32 - mbits_f4_e2m1)\n    output = mantissa_f32\n\n    # combine the pieces\n    result = exp_biased_f32 | mantissa_f32\n    # result[zero_mask] = ZERO_BITS_F32\n    result = tl.where(zero_mask, zero_bits_f32, result)\n    # result[denormal_mask] = ZERO_POINT_FIVE_BITS_F32\n    result = tl.where(denormal_mask, zero_point_five_bits_f32, result)\n\n    # add sign back\n    sign_f32 = sign_f4.to(tl.int32) << (\n        mbits_f32 - mbits_f4_e2m1 + ebits_f32 - ebits_f4_e2m1\n    )\n    result = result | sign_f32\n\n    # The bit shifting above is for float32, so for now we\n    # bitcast to float32 and then regular cast to bfloat16\n    # TODO(later): it should be pretty easy to cast directly to bf16, just\n    # need to adjust the mbits/ebits/special values. Perf impact is likely\n    # to be small as we would not be chaning memory access patterns.\n    output = result.to(tl.float32, bitcast=True)\n    output = output.to(tl.bfloat16)\n    return output\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE_IN\": 128}),\n        triton.Config({\"BLOCK_SIZE_IN\": 256}),\n        triton.Config({\"BLOCK_SIZE_IN\": 512}),\n        triton.Config({\"BLOCK_SIZE_IN\": 1024}),\n        triton.Config({\"BLOCK_SIZE_IN\": 2048}),\n    ],\n    key=[\"n_elements_in\"],\n)\n@triton.jit\ndef triton_f4_to_scaled_bf16_kernel(\n    x_ptr,\n    s_ptr,\n    output_ptr,\n    n_elements_in,\n    mx_block_size: tl.constexpr,\n    sign_mask_f4: tl.constexpr,\n    mantissa_mask_f4: tl.constexpr,\n    mbits_f4_e2m1: tl.constexpr,\n    ebits_f4_e2m1: tl.constexpr,\n    f4_e2m1_exp_bias: tl.constexpr,\n    mbits_f32: tl.constexpr,\n    ebits_f32: tl.constexpr,\n    f32_exp_bias: tl.constexpr,\n    zero_bits_f32: tl.constexpr,\n    zero_point_five_bits_f32: tl.constexpr,\n    e8m0_exponent_bias: tl.constexpr,\n    e8m0_exponent_nan_val: tl.constexpr,\n    BLOCK_SIZE_IN: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    n_elements_out = n_elements_in * 2\n    n_elements_s = n_elements_out // 32\n\n    BLOCK_SIZE_S: tl.constexpr = BLOCK_SIZE_IN // 16\n    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2\n\n    block_start_in = pid * BLOCK_SIZE_IN\n    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)\n    mask_in = offsets_in < n_elements_in\n    # packed uint8\n    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)\n    output = _fp4_packed_to_bf16(\n        x_packed,\n        sign_mask_f4,\n        mantissa_mask_f4,\n        mbits_f4_e2m1,\n        ebits_f4_e2m1,\n        f4_e2m1_exp_bias,\n        mbits_f32,\n        ebits_f32,\n        f32_exp_bias,\n        zero_bits_f32,\n        zero_point_five_bits_f32,\n    )\n\n    # load scale\n    block_start_s = pid * BLOCK_SIZE_S\n    offsets_s = block_start_s + tl.arange(0, BLOCK_SIZE_S)\n    mask_s = offsets_s < n_elements_s\n    s = tl.load(s_ptr + offsets_s, mask=mask_s)\n\n    # create the scale in bf16\n    s_offset = s.to(tl.int16) - e8m0_exponent_bias\n    s_fp = tl.extra.cuda.libdevice.pow(2.0, s_offset).to(tl.bfloat16)\n    s_fp = tl.where(s != e8m0_exponent_nan_val, s_fp, float(\"nan\"))\n\n    # multiply output by scale\n    # TODO(later): see if manipulating the exponent instead of fp\n    # multiplication is going to give a significant speedup\n    output = tl.reshape(\n        output, (BLOCK_SIZE_OUT // mx_block_size, mx_block_size)\n    )  # noqa: E501\n    s_fp = tl.reshape(s_fp, (BLOCK_SIZE_S // 1, 1))\n    output = output * s_fp\n    output = tl.reshape(output, (BLOCK_SIZE_OUT,))\n\n    # set up output offsets\n    block_start_out = pid * BLOCK_SIZE_OUT\n    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)\n    mask_out = offsets_out < n_elements_out\n\n    tl.store(output_ptr + offsets_out, output, mask=mask_out)\n\n\nEBITS_F32, MBITS_F32 = 8, 23\nEBITS_F4_E2M1, MBITS_F4_E2M1 = 2, 1\nEBITS_F6_E2M3, MBITS_F6_E2M3 = 2, 3\nEBITS_F6_E3M2, MBITS_F6_E3M2 = 3, 2\n\nSIGN_MASK_F4 = 0x8  # 1000\nMANTISSA_MASK_F4 = 0x1  # 0001\n\nZERO_BITS_F32 = 0x0\nZERO_POINT_FIVE_BITS_F32 = 0x3F000000\nF4_E2M1_EXP_BIAS = 1\nF32_EXP_BIAS = 127\nE8M0_EXPONENT_BIAS = 127\nE8M0_EXPONENT_NAN_VAL = 255\n\n\ndef triton_f4_to_scaled_bf16(\n    x: torch.Tensor,\n    s_e8m0: torch.Tensor,\n    mx_block_size: int,\n):\n    \"\"\"\n    Input: a tensor of packed fp4 values, and a scale in e8m0 format. The block\n        size is currently assumed to be 32.\n    Output: a tensor of bfloat16 values, multiplied by the encoded scale\n    \"\"\"\n    new_shape = (*x.shape[:-1], x.shape[-1] * 2)\n    output = torch.empty(*new_shape, device=x.device, dtype=torch.bfloat16)\n    assert x.is_contiguous()\n    assert x.is_cuda and output.is_cuda\n    n_elements_in = x.numel()\n    grid = lambda meta: (  # noqa: E731\n        triton.cdiv(n_elements_in, meta[\"BLOCK_SIZE_IN\"]),\n    )\n    triton_f4_to_scaled_bf16_kernel[grid](\n        x,\n        s_e8m0,\n        output,\n        n_elements_in,\n        mx_block_size,\n        sign_mask_f4=SIGN_MASK_F4,\n        mantissa_mask_f4=MANTISSA_MASK_F4,\n        mbits_f4_e2m1=MBITS_F4_E2M1,\n        ebits_f4_e2m1=EBITS_F4_E2M1,\n        f4_e2m1_exp_bias=F4_E2M1_EXP_BIAS,\n        mbits_f32=MBITS_F32,\n        ebits_f32=EBITS_F32,\n        f32_exp_bias=F32_EXP_BIAS,\n        zero_bits_f32=ZERO_BITS_F32,\n        zero_point_five_bits_f32=ZERO_POINT_FIVE_BITS_F32,\n        e8m0_exponent_bias=E8M0_EXPONENT_BIAS,\n        e8m0_exponent_nan_val=E8M0_EXPONENT_NAN_VAL,\n    )\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel performs matrix multiplication of two input matrices x and y, and stores the result in matrix z. \n            The kernel is named 'matmul_kernel', and it's called from the 'matmul' function.\n            The kernel uses a block-wise approach to multiply sections of the matrices. \n            It calculates the current block for both m and n dimensions based on the program's id. \n            Offsets for loading data from global memory are computed for each matrix.\n            The kernel iteratively loads sub-matrices of x and y, computes their product using `tl.dot`, and accumulates the results in z. \n            Finally, it stores the result in global memory. \n            The 'matmul' function initializes the result matrix z and calculates the grid size for launching the kernel.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef matmul_kernel(x_ptr, y_ptr, z_ptr, M, N, K, stride_xm, stride_xk, stride_yn, stride_yk, stride_zm, stride_zn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    # Program ID determines the block of the output matrix\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Compute the starting indices of the block\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Initialize accumulator\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over k dimension\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Compute offsets for x and y\n        x_offset = offs_m[:, None] * stride_xm + (offs_k[None, :] + k) * stride_xk\n        y_offset = (offs_k[:, None] + k) * stride_yk + offs_n[None, :] * stride_yn\n\n        # Load blocks from x and y\n        x = tl.load(x_ptr + x_offset, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n        y = tl.load(y_ptr + y_offset, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n\n        # Accumulate dot product\n        acc += tl.dot(x, y)\n\n    # Compute offset for z\n    z_offset = offs_m[:, None] * stride_zm + offs_n[None, :] * stride_zn\n\n    # Store the result in z\n    tl.store(z_ptr + z_offset, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\n# Wrapper function for the Triton kernel\ndef matmul(x, y):\n    # Get shapes\n    M, K = x.shape\n    K, N = y.shape\n\n    # Allocate output matrix\n    z = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n    # Define block sizes\n    BLOCK_SIZE_M = 128\n    BLOCK_SIZE_N = 128\n    BLOCK_SIZE_K = 32\n\n    # Calculate grid size\n    grid = (triton.cdiv(M, BLOCK_SIZE_M), triton.cdiv(N, BLOCK_SIZE_N))\n\n    # Launch the kernel\n    matmul_kernel[grid](\n        x, y, z,\n        M, N, K,\n        x.stride(0), x.stride(1),\n        y.stride(0), y.stride(1),\n        z.stride(0), z.stride(1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n\n    return z\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef matmul_kernel(\n    x_ptr,\n    y_ptr,\n    z_ptr,\n    m_size,\n    k_size,\n    n_size,\n    m_block_size: tl.constexpr,\n    k_block_size: tl.constexpr,\n    n_block_size: tl.constexpr,\n):\n    # Calculate the m and n blocks to compute using the program id\n    pid = tl.program_id(0)\n    num_n_blocks = tl.cdiv(n_size, n_block_size)\n    m_block = pid // num_n_blocks\n    n_block = pid % num_n_blocks\n\n    m_offsets = tl.arange(0, m_block_size) + m_block * m_block_size\n    n_offsets = tl.arange(0, n_block_size) + n_block * n_block_size\n    k_offsets = tl.arange(0, k_block_size)\n\n    x_ptrs = x_ptr + m_offsets[:, None] * k_size + k_offsets[None, :]\n    y_ptrs = y_ptr + k_offsets[:, None] * n_size + n_offsets[None, :]\n    z_ptrs = z_ptr + m_offsets[:, None] * n_size + n_offsets[None, :]\n\n    z = tl.zeros((m_block_size, n_block_size), dtype=tl.float32)\n\n    for _ in range(0, k_size, k_block_size):\n        x_sub = tl.load(x_ptrs)\n        y_sub = tl.load(y_ptrs)\n        z += tl.dot(x_sub, y_sub, allow_tf32=False)\n        x_ptrs += k_block_size\n        y_ptrs += k_block_size * n_size\n\n    tl.store(z_ptrs, z)\n\ndef matmul(x, y):\n    m_size, k_size = x.shape\n    _, n_size = y.shape\n    z = torch.empty(m_size, n_size, device='cuda:0')\n    def grid(meta):\n        return (triton.cdiv(m_size, meta['m_block_size']) * triton.cdiv(n_size, meta['n_block_size']),)\n    \n    matmul_kernel[grid](\n        x,\n        y,\n        z,\n        m_size,\n        k_size,\n        n_size,\n        m_size,\n        k_size,\n        n_size\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton operator defined is a matrix multiplication kernel with an optional activation function applied. It takes matrices A and B as inputs and computes C = A x B. The kernel function `matmul_kernel` is designed to handle matrices with dimensions M, N, and K, and the operation is optimized using blocks defined by BLOCK_SIZE_M, BLOCK_SIZE_N, and BLOCK_SIZE_K. The matrix A must be contiguous. The kernel uses a loop to iterate over the K dimension in chunks of BLOCK_SIZE_K. Optionally, a leaky ReLU activation can be applied. The result is stored in matrix C.\n    ", "predict": "import triton\nimport triton.language as tl\n\n# Define the kernel\n@triton.jit\ndef matmul_kernel(A_ptr, B_ptr, C_ptr, M, N, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, ALPHA, apply_activation, stride_am, stride_ak, stride_bn, stride_bk, stride_cm, stride_cn):\n    # Define the ranges for the block indices\n    pid = tl.program_id(axis=0)\n    num_pid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M\n    num_pid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    # Define the start positions for the blocks\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    # Create pointers to the blocks in A and B\n    A_block_ptr = A_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    B_block_ptr = B_ptr + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\n\n    # Initialize the accumulator for C\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    # Loop over K dimension in chunks of BLOCK_SIZE_K\n    for k in range(0, K, BLOCK_SIZE_K):\n        # Load the blocks of A and B\n        A_block = tl.load(A_block_ptr, mask=offs_am[:, None] < M)\n        B_block = tl.load(B_block_ptr, mask=offs_bn[None, :] < N)\n\n        # Compute the matrix multiplication for the block\n        acc += tl.dot(A_block, B_block)\n\n        # Update the pointers for the next chunk\n        A_block_ptr += BLOCK_SIZE_K * stride_ak\n        B_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    # Apply optional leaky ReLU activation\n    if apply_activation:\n        acc = tl.where(acc > 0, acc, acc * ALPHA)\n\n    # Define the output pointer for C\n    C_block_ptr = C_ptr + offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn\n\n    # Store the result in C\n    tl.store(C_block_ptr, acc, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))\n\n# Define the wrapper function\ndef matmul(A, B, BLOCK_SIZE_M=128, BLOCK_SIZE_N=128, BLOCK_SIZE_K=32, ALPHA=0.01, apply_activation=False):\n    M, K = A.shape\n    _, N = B.shape\n\n    # Ensure A is contiguous\n    assert A.is_contiguous()\n\n    # Allocate output matrix C\n    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n    # Launch the Triton kernel\n    grid = (triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),)\n    matmul_kernel[grid](\n        A, B, C,\n        M, N, K,\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K,\n        ALPHA, apply_activation,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1)\n    )\n\n    return C\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 256,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=3,\n            num_warps=8,\n        ),\n        # Additional configurations omitted for brevity\n    ],\n    key=[\"M\", \"N\", \"K\"],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,  # Pointers to matrices\n    M, N, K,  # Matrix dimensions\n    stride_am, stride_ak,  # Strides for matrix A\n    stride_bk, stride_bn,  # Strides for matrix B\n    stride_cm, stride_cn,  # Strides for matrix C\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr, ACTIVATION: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if ACTIVATION == \"leaky_relu\":\n        accumulator = leaky_relu(accumulator)\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n\ndef matmul(a, b, activation=\"\"):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n    M, K = a.shape\n    K, N = b.shape\n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    grid = lambda META: (\n        triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n    )\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        ACTIVATION=activation,\n    )\n    return c\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code implements a custom linear layer in Triton. It defines several activation functions, including tanh, ReLU, GELU, and fast GELU. The main operation is the kernel_fma function, a Triton kernel that computes a matrix multiplication with optional bias and activation. It handles matrices A, B, and C to compute C = activation(A x B + bias). The kernel is autotuned for performance. The LinearLayer class is a PyTorch function that uses kernel_fma to compute the linear transformation, optionally applying an activation function and saving activation inputs for backward pass. The linear_layer function is a wrapper around LinearLayer to perform these operations easily.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({}, num_warps=4),\n        triton.Config({}, num_warps=8),\n        triton.Config({}, num_warps=16),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef kernel_fma(\n    A, B, C, Bias, M, N, K, stride_am, stride_ak, stride_bn, stride_bk, stride_cm, stride_cn,\n    stride_bias, activation, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    # Calculate block indices\n    block_m = pid // (N // BLOCK_SIZE_N)\n    block_n = pid % (N // BLOCK_SIZE_N)\n    \n    # Define the range of the block\n    range_m = block_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    range_n = block_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    range_k = tl.arange(0, BLOCK_SIZE_K)\n    \n    # Load blocks of A and B\n    A_block = tl.load(A + range_m[:, None] * stride_am + range_k[None, :] * stride_ak, mask=range_m[:, None] < M)\n    B_block = tl.load(B + range_k[:, None] * stride_bk + range_n[None, :] * stride_bn, mask=range_n[None, :] < N)\n    \n    # Initialize accumulation\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    # Perform matrix multiplication\n    acc += tl.dot(A_block, B_block)\n    \n    # Add bias if provided\n    if Bias is not None:\n        bias = tl.load(Bias + range_n * stride_bias, mask=range_n < N)\n        acc += bias[None, :]\n    \n    # Apply activation function\n    if activation == 1:  # tanh\n        acc = tl.tanh(acc)\n    elif activation == 2:  # ReLU\n        acc = tl.maximum(acc, 0)\n    elif activation == 3:  # GELU\n        acc = 0.5 * acc * (1.0 + tl.erf(acc / tl.sqrt(2.0)))\n    elif activation == 4:  # Fast GELU\n        acc = acc * tl.sigmoid(1.702 * acc)\n    \n    # Store the result\n    tl.store(C + range_m[:, None] * stride_cm + range_n[None, :] * stride_cn, acc, mask=range_m[:, None] < M)\n\nimport torch\nfrom torch.autograd import Function\n\nclass LinearLayer(Function):\n    @staticmethod\n    def forward(ctx, A, B, bias=None, activation=0):\n        assert A.shape[1] == B.shape[0], \"Incompatible dimensions for matrix multiplication\"\n        M, K = A.shape\n        K, N = B.shape\n        \n        # Prepare output tensor\n        C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n        \n        # Call the Triton kernel\n        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n        kernel_fma[grid](\n            A, B, C, bias, M, N, K,\n            A.stride(0), A.stride(1),\n            B.stride(0), B.stride(1),\n            C.stride(0), C.stride(1),\n            bias.stride(0) if bias is not None else 0,\n            activation,\n            BLOCK_SIZE_M=64, BLOCK_SIZE_N=64, BLOCK_SIZE_K=32\n        )\n        \n        # Save for backward if needed\n        ctx.save_for_backward(A, B, bias)\n        ctx.activation = activation\n        \n        return C\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        A, B, bias = ctx.saved_tensors\n        activation = ctx.activation\n        \n        # Backpropagation logic (to be implemented as needed)\n        # ...\n        \n        return grad_A, grad_B, grad_bias, None\n\ndef linear_layer(A, B, bias=None, activation=0):\n    return LinearLayer.apply(A, B, bias, activation)\n", "label": "from typing import Optional\nimport math\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom torch.autograd.function import FunctionCtx\nfrom torch.cuda.amp import custom_fwd\nfrom triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time\n\n\nsqrt2pi = math.sqrt(2.0 / math.pi)\nsqrt2 = tl.constexpr(math.sqrt(2.0))\n\n\n@triton.jit\ndef tanh(x):\n    \"\"\"Tanh activation function\"\"\"\n    return tl.extra.cuda.libdevice.tanh(x)\n\n\n@triton.jit\ndef relu(x):\n    \"\"\"Relu activation function\"\"\"\n    return tl.maximum(0, x)\n\n\n@triton.jit\ndef fast_gelu(x):\n    \"\"\"Fast approximation of the gelu function. May slightly decrease accuracy.\"\"\"\n    return 0.5 * x * (1 + tanh(sqrt2pi * (x + 0.044715 * x * x * x)))\n\n\n@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.extra.cuda.libdevice.erf(x / sqrt2))\n\n\ndef init_to_zero(name):\n    return lambda nargs: nargs[name].zero_()\n\n\ndef get_configs_io_bound():\n    configs = []\n    for num_stages in [2, 3, 4, 5, 6]:\n        for block_m in [16, 32]:\n            for block_k in [32, 64]:\n                for block_n in [32, 64, 128, 256]:\n                    num_warps = 2 if block_n <= 64 else 4\n                    configs.append(\n                        triton.Config(\n                            {\"BLOCK_M\": block_m, \"BLOCK_N\": block_n, \"BLOCK_K\": block_k, \"SPLIT_K\": 1},\n                            num_stages=num_stages,\n                            num_warps=num_warps,\n                        )\n                    )\n                    # split_k not used\n                    # for split_k in [2, 4, 8, 16]:\n                    #     configs.append(triton.Config(\n                    #         {'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n                    #         num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n    return configs\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 32, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n        # good for int8\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=3, num_warps=8),\n        triton.Config({\"BLOCK_M\": 256, \"BLOCK_N\": 64, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 256, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 128, \"BLOCK_K\": 128, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 64, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 128, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 128, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=4, num_warps=4),\n        triton.Config({\"BLOCK_M\": 64, \"BLOCK_N\": 32, \"BLOCK_K\": 64, \"SPLIT_K\": 1}, num_stages=5, num_warps=2),\n    ]\n    + get_configs_io_bound(),\n    key=[\"CACHE_KEY_M\", \"CACHE_KEY_N\", \"CACHE_KEY_K\"],\n    prune_configs_by={\"early_config_prune\": early_config_prune, \"perf_model\": estimate_matmul_time, \"top_k\": 10},\n)\n@triton.heuristics(\n    {\n        \"K_LOAD_MASK_NEEDED\": lambda args: args[\"K\"] % (args[\"BLOCK_K\"] * args[\"SPLIT_K\"]) == 0,\n    }\n)\n@triton.jit\ndef kernel_fma(\n    C,  # Pointers to matrices\n    ACT_INPUTS,\n    A,\n    B,\n    bias,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    CACHE_KEY_M,\n    CACHE_KEY_N,\n    CACHE_KEY_K,\n    # The stride variables represent how much to increase the ptr by when moving by 1\n    # element in a particular dimension. E.g. stride_am is how much to increase a_ptr\n    # by to get the element one row down (A has M rows)\n    output_m_stride,\n    output_n_stride,\n    act_inputs_m_stride,\n    act_inputs_n_stride,\n    a_m_stride,\n    a_k_stride,\n    b_n_stride,\n    b_k_stride,\n    # Meta-parameters\n    BLOCK_M: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    # split k not used, not performant with activation, kept because early_config_prune is expecting it\n    SPLIT_K: tl.constexpr,\n    K_LOAD_MASK_NEEDED: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    SHOULD_SAVE_ACT_INPUTS: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n\n    This kernel will consolidate over K\n    \"\"\"\n    program_idx = tl.program_id(axis=0)\n\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_idx = program_idx // width\n    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)\n    block_m_idx = group_idx * GROUP_M + (program_idx % group_size)\n    block_n_idx = (program_idx % width) // group_size\n\n    # now compute the block that each program will go through\n    # m_offs (resp. n_offs) denotes a range of indices\n    # for rows (resp. col) of C\n    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    # trick to avoid masking on M and N axis\n    # m_offs_untagged and n_offs_untagged can contains addresses outside matrix boundaries\n    # modulo operation is used to wrap around the indices that go beyond the matrix boundaries\n    # The value loaded are not ok but at least we are not reading outside the A/B matrices\n    # Then, during storing in C a mask is used and the results related to these wrong values is discarded!\n    # Regarding max_contiguous and multiple_of, they are used to force the compiler to vectorize loads\n    # multiple_of indicates that the first element of rm / rn is a multiple of BLOCK_M / BLOCK_N\n    # max_contiguous indicates that the range is a block of BLOCK_M / BLOCK_N contiguous elements\n    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M), BLOCK_M)\n    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N), BLOCK_N)\n\n    k_range_offs = tl.arange(0, BLOCK_K)\n\n    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)\n    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)\n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n\n    if HAS_BIAS:\n        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0).to(tl.float32)\n        acc += bias[None, :]\n\n    for k in range(K, 0, -BLOCK_K):\n        if K_LOAD_MASK_NEEDED:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)\n            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n\n        A += BLOCK_K * a_k_stride\n        B += BLOCK_K * b_k_stride\n\n    # optional: save the activation inputs\n    if SHOULD_SAVE_ACT_INPUTS:\n        act_in_ptrs = ACT_INPUTS + m_offs[:, None] * act_inputs_m_stride + n_offs[None, :] * act_inputs_n_stride\n        tl.store(act_in_ptrs, acc)\n\n    # optional: fused activation (while the data is in shared memory)\n    if ACTIVATION == \"tanh\":\n        acc = tanh(acc)\n    if ACTIVATION == \"gelu\":\n        acc = gelu(acc)\n    if ACTIVATION == \"fast_gelu\":\n        acc = fast_gelu(acc)\n    if ACTIVATION == \"relu\":\n        acc = relu(acc)\n\n    # write back result\n    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :] * output_n_stride\n    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]\n    tl.store(C, acc, mask=c_ptr_mask)\n\n\nclass LinearLayer(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float16)\n    def forward(\n        ctx: FunctionCtx,\n        x: torch.Tensor,\n        weight: torch.Tensor,\n        bias: Optional[torch.Tensor],\n        activation: str,\n        act_inputs: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute e = activation(x @ weight + bias).\n        This wrapper kicks the `kernel_fma` Triton kernel\n        :param ctx: context for autograd\n        :param x: input tensor\n        :param weight: weight matrix\n        :param bias: an optional bias tensor\n        :param activation: Activation name. Needs to be a Triton kernel.\n        :param act_inputs: an optional tensor to save the activation inputs (for backward)\n        :return: result tensor\n        \"\"\"\n        x_ = x if x.ndim == 2 else x.flatten(0, 1)\n\n        assert x.dtype == weight.dtype, f\"Input and weight must have the same dtype, got {x.dtype} and {weight.dtype}\"\n        if bias is not None:\n            assert x.dtype == bias.dtype, f\"Input and bias must have the same dtype, got {x.dtype} and {bias.dtype}\"\n        assert x_.shape[1] == weight.shape[1], f\"Incompatible dimensions: {x_.shape} - {weight.shape}\"\n\n        assert bias is None or bias.is_contiguous()\n        assert bias is None or bias.shape[0] == weight.shape[0], \"Incompatible dimensions in between weight and bias\"\n        assert weight.is_contiguous()\n\n        M, K = x_.shape\n        N, K = weight.shape\n\n        outputs = torch.empty((M, N), device=x.device, dtype=x.dtype)\n\n        # 1D launch kernel where each block gets its own program.\n        grid = lambda META: (triton.cdiv(M, META[\"BLOCK_M\"]) * triton.cdiv(N, META[\"BLOCK_N\"]),)  # noqa\n\n        kernel_fma[grid](\n            outputs,\n            act_inputs,\n            x_,\n            weight,  # data ptrs\n            bias if bias is not None else x,  # auto skip bias if not present\n            M,  # shapes\n            N,\n            K,\n            M // 32,  # key for triton cache (limit number of compilations)\n            N // 32,\n            K // 32,\n            output_m_stride=outputs.stride(0),  # strides\n            output_n_stride=outputs.stride(1),\n            act_inputs_m_stride=act_inputs.stride(0) if act_inputs is not None else 0,\n            act_inputs_n_stride=act_inputs.stride(1) if act_inputs is not None else 0,\n            a_m_stride=x_.stride(0),\n            a_k_stride=x_.stride(1),\n            b_n_stride=weight.stride(0),\n            b_k_stride=weight.stride(1),\n            HAS_BIAS=bias is not None,  # optional fused bias\n            SHOULD_SAVE_ACT_INPUTS=act_inputs is not None,  # optional save activation inputs\n            ACTIVATION=activation if not None else x,  # optional fused activation\n            GROUP_M=8,  # speed optimization: group the programs\n        )\n\n        outputs = outputs if x.ndim == 2 else outputs.reshape(x.shape[0], -1, N)\n        ctx.save_for_backward(weight, bias, x)\n        return outputs\n\n\ndef linear_layer(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    bias: Optional[torch.Tensor],\n    activation=\"\",\n    act_inputs: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    return LinearLayer.apply(x, weight, bias, activation, act_inputs)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The provided Triton code implements a softmax function optimized for GPU execution. The main components are a kernel function named `softmax_kernel` and a wrapper function `softmax`. \n            - `softmax_kernel`: This function is a Triton JIT-compiled kernel that computes the softmax across rows of an input tensor. It takes pointers to the output and input tensors, along with other parameters such as row stride and block size. The function calculates the exponentials of the input after subtracting the maximum value for numerical stability, applies an optional mask, and normalizes the result to produce the softmax output.\n            - `softmax`: This Python function sets up and launches the Triton kernel. It handles reshaping and validation of the input tensor and optional mask tensor. Depending on the input size, it configures the grid and launches the kernel with appropriate parameters. The function returns the output tensor with computed softmax values.\n            ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, row_stride, n_cols, mask_ptr, BLOCK_SIZE: tl.constexpr):\n    # Define the row and column indices\n    row_idx = tl.program_id(0)\n    col_idx = tl.arange(0, BLOCK_SIZE)\n\n    # Compute the pointer to the current row\n    input_row_ptr = input_ptr + row_idx * row_stride\n    output_row_ptr = output_ptr + row_idx * row_stride\n\n    # Load the input data for the current row\n    input_data = tl.load(input_row_ptr + col_idx, mask=col_idx < n_cols, other=-float('inf'))\n\n    # Compute the maximum value for numerical stability\n    row_max = tl.max(input_data, axis=0)\n    input_data = input_data - row_max\n\n    # Compute the exponentials\n    exp_data = tl.exp(input_data)\n\n    # Apply the mask if provided\n    if mask_ptr is not None:\n        mask_data = tl.load(mask_ptr + row_idx * row_stride + col_idx, mask=col_idx < n_cols, other=0.0)\n        exp_data = exp_data * mask_data\n\n    # Compute the sum of exponentials\n    sum_exp = tl.sum(exp_data, axis=0)\n\n    # Normalize to get the softmax\n    softmax_result = exp_data / sum_exp\n\n    # Store the result\n    tl.store(output_row_ptr + col_idx, softmax_result, mask=col_idx < n_cols)\n\nimport torch\n\ndef softmax(input_tensor, mask_tensor=None):\n    # Validate input tensor\n    assert input_tensor.ndim == 2, \"Input tensor must be 2D\"\n    n_rows, n_cols = input_tensor.shape\n\n    # Create an output tensor\n    output_tensor = torch.empty_like(input_tensor)\n\n    # Define block size (this can be tuned based on your hardware)\n    BLOCK_SIZE = 128\n\n    # Configure the grid\n    grid = (n_rows,)\n\n    # Launch the Triton kernel\n    softmax_kernel[grid](\n        output_tensor,\n        input_tensor,\n        input_tensor.stride(0),\n        n_cols,\n        mask_tensor if mask_tensor is not None else 0,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return output_tensor\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, row_stride, n_cols, mask_ptr, BLOCK_SIZE: tl.constexpr):\n    r\"\"\"the kernel function for implementing softmax operator\n    Args:\n        output_ptr: the output after finishing softmax operation, (N, hidden_dim)\n        input_ptr: the tensor of input, shape should be (N, hidden_dim)\n        n_cols(tl.constexpr): the number of cols of input\n        BLOCK_SIZE(tl.constexpr): the block_size of your hidden_dim dimension, typically BLOCK_SIZE >= hidden_dim\n    \"\"\"\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float(\"inf\")).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n\n    if mask_ptr is not None:\n        # load mask into SRAM\n        mask_ptrs = (mask_ptr + (row_idx * row_stride)) + col_offsets\n        mask = tl.load(mask_ptrs, mask=col_offsets < n_cols, other=0).to(tl.float32)\n\n        # update\n        row_minus_max = row_minus_max + mask\n\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    # Write back output to DRAM\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\ndef softmax(input: torch.Tensor, mask: torch.Tensor = None, dim=-1) -> torch.Tensor:\n    if mask is not None:\n        # assert input[-1] == mask[-1], \"the last dimentions should be the same for input and mask\"\n        assert input.shape[-1] == mask.shape[-1], \"the last dimensions should be the same for input and mask\"\n    assert dim == -1 or dim == len(input.shape) - 1, \"currently softmax layer only support last dimention\"\n\n    hidden_dim = input.shape[-1]\n    output = torch.empty_like(input)\n    input = input.view(-1, hidden_dim)\n    if mask is not None:\n        mask = mask.view(-1, hidden_dim)\n        assert input.shape[0] == mask.shape[0], \"the fist dimention of mask and input should be the same\"\n\n    num_rows, num_cols = input.shape\n    block_size = max(triton.next_power_of_2(num_cols), 2)\n    num_warps = 16\n    if block_size >= 4096:\n        num_warps = 16\n    elif block_size >= 2048:\n        num_warps = 8\n    else:\n        num_warps = 4\n\n    if num_rows <= 350000:\n        grid = (num_rows,)\n        softmax_kernel[grid](\n            output, input, input.stride(0), num_cols, mask, BLOCK_SIZE=block_size, num_warps=num_warps\n        )\n    else:\n        grid = lambda meta: (triton.cdiv(num_rows, meta[\"BLOCK_M\"]),)\n\n        softmax_kernel[grid](\n            output_ptr=output,\n            input_ptr=input,\n            row_stride=input.stride(0),\n            n_rows=num_rows,\n            n_cols=num_cols,\n            mask_ptr=mask,\n            # currently manually setting up size\n            BLOCK_M=32,\n            BLOCK_SIZE=block_size,\n        )\n\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton operator is designed to implement an attention mechanism commonly used in transformer architectures. The primary function, `attention_fwd_kernel`, takes in query (q), key (k), and value (v) tensors and performs scaled dot-product attention. The input tensors are batched and represent different heads of attention. The kernel computes attention scores using the formula: scores = softmax(q * k^T), and then it applies these scores to the value tensor to produce the output.\n\n            The function `attention_fwd_kernel` is decorated with `@triton.jit` to enable just-in-time compilation. It processes data in blocks, leveraging Triton's capabilities to efficiently handle tensor computations. The kernel utilizes the variables BT (block size for the sequence dimension), BD (block size for the head dimension), and NT (number of blocks), ensuring that tensor operations are performed in parallel.\n\n            The kernel has several key operations: loading the blocks of the q, k, and v tensors, computing the dot products to get attention scores, applying these scores to get the output values, and optionally updating an intermediate tensor `h` that can be stored or used conditionally.\n\n            The `AttentionFunction` class acts as a wrapper around the kernel and provides the `forward` method for executing the kernel on input tensors. This class also sets up necessary parameters and configurations for the kernel, like the grid dimensions for parallel execution and scaling factors for the attention mechanism.\n\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Kernel for the forward pass of the attention mechanism\n@triton.jit\ndef attention_fwd_kernel(\n    q_ptr, k_ptr, v_ptr, out_ptr, h_ptr, \n    scale, n_heads, seq_len, head_dim, \n    BLOCK_SIZE: tl.constexpr, \n    BLOCK_DIM: tl.constexpr\n):\n    # Get program ids\n    pid = tl.program_id(0)\n    bid = tl.program_id(1)\n    \n    # Calculate offsets\n    head_offset = bid * head_dim\n    seq_offset = pid * BLOCK_SIZE\n    \n    # Pointers to the blocks of q, k, v\n    q_block_ptr = q_ptr + head_offset + seq_offset * head_dim\n    k_block_ptr = k_ptr + head_offset + seq_offset * head_dim\n    v_block_ptr = v_ptr + head_offset + seq_offset * head_dim\n    \n    # Load q and k blocks\n    q_block = tl.load(q_block_ptr, mask=(seq_offset < seq_len))\n    k_block = tl.load(k_block_ptr, mask=(seq_offset < seq_len))\n    \n    # Compute dot product q * k^T\n    qk_dot = tl.dot(q_block, tl.trans(k_block))\n    \n    # Scale the scores\n    qk_dot = qk_dot * scale\n    \n    # Apply softmax\n    scores = tl.softmax(qk_dot, axis=1)\n    \n    # Load v block\n    v_block = tl.load(v_block_ptr, mask=(seq_offset < seq_len))\n    \n    # Compute the output\n    out_block = tl.dot(scores, v_block)\n    \n    # Store the output\n    out_ptr += head_offset + seq_offset * head_dim\n    tl.store(out_ptr, out_block, mask=(seq_offset < seq_len))\n    \n    # Optionally store intermediate results in h\n    if h_ptr is not None:\n        h_ptr += head_offset + seq_offset * head_dim\n        tl.store(h_ptr, scores, mask=(seq_offset < seq_len))\n\n\n# Wrapper class for the attention kernel\nclass AttentionFunction:\n    def __init__(self, scale, n_heads, seq_len, head_dim, block_size=128):\n        self.scale = scale\n        self.n_heads = n_heads\n        self.seq_len = seq_len\n        self.head_dim = head_dim\n        self.block_size = block_size\n\n    def forward(self, q, k, v, h=None):\n        # Allocate output tensor\n        out = torch.empty_like(q)\n        \n        # Define grid dimensions\n        grid = (self.seq_len // self.block_size, self.n_heads)\n        \n        # Launch the kernel\n        attention_fwd_kernel[grid](\n            q_ptr=q, k_ptr=k, v_ptr=v, out_ptr=out, h_ptr=h,\n            scale=self.scale, n_heads=self.n_heads, \n            seq_len=self.seq_len, head_dim=self.head_dim,\n            BLOCK_SIZE=self.block_size, BLOCK_DIM=self.head_dim\n        )\n        \n        return out\n\n# Example usage\n# Define parameters\nn_heads = 8\nseq_len = 512\nhead_dim = 64\nscale = 1.0 / (head_dim ** 0.5)\n\n# Create random input tensors\nq = torch.randn((n_heads, seq_len, head_dim), device='cuda', dtype=torch.float32)\nk = torch.randn((n_heads, seq_len, head_dim), device='cuda', dtype=torch.float32)\nv = torch.randn((n_heads, seq_len, head_dim), device='cuda', dtype=torch.float32)\n\n# Instantiate the attention function\nattention = AttentionFunction(scale=scale, n_heads=n_heads, seq_len=seq_len, head_dim=head_dim)\n\n# Compute the attention output\noutput = attention.forward(q, k, v)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef attention_fwd_kernel(\n    q,\n    k,\n    v,\n    h,\n    o,\n    s_qh,\n    s_qt,\n    s_qd,\n    s_hh,\n    s_ht,\n    T,\n    scale,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NT: tl.constexpr,\n    STORE: tl.constexpr,\n    IFCOND: tl.constexpr\n):\n    i_bh = tl.program_id(0)\n\n    # [BD, BD]\n    b_h = tl.zeros([BD, BD], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i * BT, 0), (BT, BD), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qh, (BD, T), (s_qd, s_qt), (0, i * BT), (BD, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i * BT, 0), (BT, BD), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (NT * BD, BD), (s_ht, s_qd), (i * BD, 0), (BD, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i * BT, 0), (BT, BD), (1, 0))\n\n        if STORE:\n            tl.store(p_h, b_h.to(p_h.dtype.element_ty))\n        # [BT, BD]\n        b_q = tl.load(p_q)\n        b_q = (b_q * scale).to(b_q.dtype)\n        # [BD, BT]\n        b_k = tl.load(p_k)\n        # [BT, BD]\n        b_v = tl.load(p_v)\n\n        # [BT, BT]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        # [BT, BD]\n        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n        if IFCOND:\n            if i == 0:\n                b_h = tl.dot(b_k, b_v, allow_tf32=False)\n            else:\n                b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n                b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n            b_h += tl.dot(b_k, b_v, allow_tf32=False)\n\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty))\n\n\nclass AttentionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, store=False, ifcond=False):\n        batch_size, n_heads, seq_len, d_head = q.shape\n        scale = d_head ** -0.5\n        BD = q.shape[-1]\n        BT = 32\n        NT = triton.cdiv(seq_len, BT)\n        num_stages = 3 if d_head <= 64 else 2\n        num_warps = 4\n\n        h = q.new_empty(batch_size, n_heads, NT * BD, BD)\n        o = torch.empty_like(q)\n        grid = (batch_size * n_heads,)\n        attention_fwd_kernel[grid](\n            q, k, v, h, o,\n            q.stride(1), q.stride(2), q.stride(3), h.stride(1), h.stride(2),\n            seq_len, scale,\n            BT=BT, BD=BD, NT=NT, STORE=store, IFCOND=ifcond,\n            num_warps=num_warps,\n            num_stages=num_stages\n        )\n        return o\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This Triton kernel implements a forward attention mechanism. The main function, `_attn_fwd`, takes query (Q), key (K), value (V) matrices and scaling factors as inputs, and computes attention scores stored in the `Out` tensor. The function uses a block-wise approach to process input tensors in chunks of size `BLOCK_M` and `BLOCK_N`. The kernel uses the Triton JIT decorator for compiling and executing on GPUs.\n\n            The `_attn_fwd_inner` function is a helper that performs the main attention computation in the loop. It calculates the scaled dot-product of the query and key matrices, applies the exponential function, and updates the accumulated attention scores `acc` and normalization factor `l_i`. The loop iterates over the context size `N_CTX` in increments of `BLOCK_N`.\n\n            Key inputs:\n            - `Q`, `K`, `V`: Query, key, and value matrices.\n            - `Q_scale`, `K_scale`: Scaling factors for Q and K.\n            - `Out`: The output tensor for storing attention scores.\n\n            Outputs:\n            - Updated `Out` tensor containing attention scores.\n\n            Key function:\n            - `_attn_fwd_inner`: Handles the block-wise attention score computation.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Constants defining block sizes\nBLOCK_M = 128\nBLOCK_N = 128\n\n@triton.jit\ndef _attn_fwd_inner(Q, K, V, Out, Q_scale, K_scale, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Define program ids\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n\n    # Define offsets for the blocks\n    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    # Load Q and K blocks\n    Q_block = tl.load(Q + offs_m[:, None] * N_CTX + offs_n[None, :])\n    K_block = tl.load(K + offs_n[:, None] * N_CTX + offs_m[None, :])\n\n    # Scale Q and K\n    Q_block_scaled = Q_block * Q_scale\n    K_block_scaled = K_block * K_scale\n\n    # Compute scaled dot-product attention\n    scores = tl.dot(Q_block_scaled, K_block_scaled)\n\n    # Apply exponential function\n    exp_scores = tl.exp(scores)\n\n    # Initialize accumulators\n    acc = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n\n    # Accumulate scores and normalization factor\n    acc += exp_scores\n    l_i += tl.sum(exp_scores, axis=1)\n\n    # Normalize and store the result in the output tensor\n    Out_block = acc / l_i[:, None]\n    tl.store(Out + offs_m[:, None] * N_CTX + offs_n[None, :], Out_block)\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Out, Q_scale, K_scale, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    # Launch the kernel over blocks\n    grid = (triton.cdiv(Q.shape[0], BLOCK_M), triton.cdiv(N_CTX, BLOCK_N))\n    _attn_fwd_inner[grid](Q, K, V, Out, Q_scale, K_scale, N_CTX, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n\n# Wrapper function to call the kernel\ndef attention_forward(Q, K, V, Q_scale, K_scale):\n    # Assuming Q, K, V are Triton tensors\n    N_CTX = Q.shape[1]\n    Out = tl.zeros_like(Q)  # Output tensor\n\n    # Call the kernel\n    _attn_fwd(Q, K, V, Out, Q_scale, K_scale, N_CTX, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n\n    return Out\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, \n                    K_ptrs, K_scale_ptr, V_ptrs,  \n                    start_m,  \n                    BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  \n                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  \n                    N_CTX: tl.constexpr):\n    lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = (offs_n[None, :] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[:, None])\n        k = tl.load(K_ptrs, mask = k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask = (offs_n[:, None] < (N_CTX - start_n)) & ((tl.arange(0, 128) < 96)[None, :]))\n        p = p.to(tl.float16)\n        \n        acc += tl.dot(p, v, out_dtype=tl.float16)  \n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i\n\n@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  \n              stride_qz, stride_qh, stride_qm, stride_qk,  \n              stride_kz, stride_kh, stride_kn, stride_kk,  \n              stride_vz, stride_vh, stride_vk, stride_vn,  \n              stride_oz, stride_oh, stride_om, stride_on,  \n              Z, H, N_CTX,  \n              HEAD_DIM: tl.constexpr,  \n              BLOCK_M: tl.constexpr,  \n              BLOCK_N: tl.constexpr,  \n              STAGE: tl.constexpr  \n              ):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    \n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N) \n    \n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, 128)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, 128], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,  \n                                    start_m,  \n                                    BLOCK_M, HEAD_DIM, BLOCK_N,  \n                                    4 - STAGE, offs_m, offs_n, N_CTX \n                                    )\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < N_CTX) & ((tl.arange(0, 128) < 96)[None, :]))\n\ndef forward(q, k, v, q_scale, k_scale):\n    BLOCK_M = 128\n    BLOCK_N = 64\n    HEAD_DIM_Q, HEAD_DIM_K = q.shape[-1], k.shape[-1]\n    HEAD_DIM_V = v.shape[-1]\n    assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n    o = torch.empty_like(q, dtype=torch.float16)\n    stage = 1\n\n    grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n    M = torch.empty((q.shape[0], q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n    _attn_fwd[grid](\n        q, k, v, q_scale, k_scale, o,  \n        q.stride(0), q.stride(1), q.stride(2), q.stride(3),  \n        k.stride(0), k.stride(1), k.stride(2), k.stride(3),  \n        v.stride(0), v.stride(1), v.stride(2), v.stride(3),  \n        o.stride(0), o.stride(1), o.stride(2), o.stride(3),  \n        q.shape[0], q.shape[1],  \n        N_CTX=q.shape[2],  \n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  \n        STAGE=stage,  \n        num_warps=8,  \n        num_stages=4)\n    return o\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The code defines a matrix multiplication kernel using Triton, a Python library for writing custom GPU kernels. \n        The kernel `matmul_kernel` is decorated with `@triton.autotune`, which enables auto-tuning of configurations for optimal performance based on input sizes M, N, and K.\n        The kernel function computes the product of matrices A and B and stores the result in matrix C. \n        It uses a block-wise approach where each block computes a partial result of C. \n        Key variables: \n        - BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K: Dimensions of the blocks used in matrix multiplication.\n        - GROUP_SIZE_M: Number of blocks processed together for load balancing. \n        A wrapper function `triton_matmul` is defined to facilitate calling the kernel. It takes matrices A and B as inputs, asserts compatible dimensions, sets up an output matrix C, calculates the grid size based on input dimensions, and launches the kernel.\n        ", "predict": "import triton\nimport triton.language as tl\n\n# Define the matrix multiplication kernel\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 4}),\n        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 2}),\n    ],\n    key=['M', 'N', 'K']\n)\n@triton.jit\ndef matmul_kernel(\n    A_ptr, B_ptr, C_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_SIZE_M - 1) // BLOCK_SIZE_M\n    grid_n = (N + BLOCK_SIZE_N - 1) // BLOCK_SIZE_N\n\n    group_id = pid // GROUP_SIZE_M\n    within_group_id = pid % GROUP_SIZE_M\n\n    block_m = group_id * GROUP_SIZE_M + within_group_id\n    block_n = tl.program_id(1)\n\n    offs_m = block_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = block_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    A_block_ptr = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    B_block_ptr = B_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)\n\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, K, BLOCK_SIZE_K):\n        A_block = tl.load(A_block_ptr, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K), other=0.0)\n        B_block = tl.load(B_block_ptr, mask=(offs_k[:, None] < K) & (offs_n[None, :] < N), other=0.0)\n        acc += tl.dot(A_block, B_block)\n\n        A_block_ptr += BLOCK_SIZE_K * stride_ak\n        B_block_ptr += BLOCK_SIZE_K * stride_bk\n\n    C_block_ptr = C_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)\n    tl.store(C_block_ptr, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n\n# Wrapper function to facilitate calling the kernel\ndef triton_matmul(A, B):\n    assert A.shape[1] == B.shape[0], \"Incompatible dimensions for matrix multiplication\"\n    M, K = A.shape\n    _, N = B.shape\n\n    C = torch.empty((M, N), device=A.device, dtype=A.dtype)\n\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * META['GROUP_SIZE_M'],\n                         triton.cdiv(N, META['BLOCK_SIZE_N']))\n\n    matmul_kernel[grid](\n        A, B, C,\n        M, N, K,\n        A.stride(0), A.stride(1),\n        B.stride(0), B.stride(1),\n        C.stride(0), C.stride(1)\n    )\n\n    return C\n", "label": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr\n):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n\ndef triton_matmul(a, b):\n    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n    M, K = a.shape\n    K, N = b.shape\n\n    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n    matmul_kernel[grid](\n        a, b, c, M, N, K,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1)\n    )\n    return c\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code defines a Triton kernel named '_copy_to_kcache_seqlen_n_kernel' and a Python function 'copy_k_to_blocked_cache' that manage the process of copying keys or values (K or V) into a key-value cache during the decoding stage of a sequence model. The kernel operates on data structures with specific layouts that differ based on a boolean parameter. The main task of the Triton kernel is to transfer tokens from an input tensor 'K' to a pre-allocated cache 'KCache' based on sequence lengths and a block table structure. Each sequence can contain multiple tokens, defined by 'n_tokens'. The kernel computes the correct position in the cache to store the token and handles the data transfer using Triton\u2019s load and store operations. The function 'copy_k_to_blocked_cache' prepares the necessary parameters and calls this kernel. It ensures that input dimensions are compatible and selects an appropriate number of execution warps based on the dimension size.\n            ", "predict": "import triton\nimport triton.language as tl\n\n# Triton kernel\n@triton.jit\ndef _copy_to_kcache_seqlen_n_kernel(K, KCache, seq_lens, block_table, n_tokens, BLOCK_SIZE: tl.constexpr):\n    # Define the program id and size\n    pid = tl.program_id(axis=0)\n    \n    # Compute the start index in the cache for this program\n    start_idx = seq_lens[pid] * n_tokens\n\n    # Iterate over the block size\n    for i in range(0, BLOCK_SIZE):\n        # Compute the position in K\n        k_pos = pid * BLOCK_SIZE + i\n\n        # Compute the position in KCache\n        cache_pos = start_idx + block_table[pid] * BLOCK_SIZE + i\n\n        # Load the token from K\n        token = tl.load(K + k_pos)\n\n        # Store the token in KCache\n        tl.store(KCache + cache_pos, token)\n\n# Python wrapper\ndef copy_k_to_blocked_cache(K, KCache, seq_lens, block_table, n_tokens, block_size):\n    # Ensure input dimensions are compatible\n    assert K.shape[0] == len(seq_lens), \"Incompatible input dimensions\"\n    assert len(block_table) == len(seq_lens), \"Block table size must match sequence length size\"\n\n    # Determine the number of execution warps\n    num_warps = min(4, (K.shape[0] + block_size - 1) // block_size)\n\n    # Launch the Triton kernel\n    grid = (K.shape[0],)\n    _copy_to_kcache_seqlen_n_kernel[grid](K, KCache, seq_lens, block_table, n_tokens, BLOCK_SIZE=block_size, num_warps=num_warps)\n\n# Example usage\nimport torch\n\n# Example tensor sizes and parameters\nK = torch.randn(1024, dtype=torch.float32).cuda()\nKCache = torch.zeros_like(K).cuda()\nseq_lens = torch.tensor([0, 1, 2, 3], dtype=torch.int32).cuda()\nblock_table = torch.tensor([0, 1, 2, 3], dtype=torch.int32).cuda()\nn_tokens = 4\nblock_size = 256\n\n# Call the function\ncopy_k_to_blocked_cache(K, KCache, seq_lens, block_table, n_tokens, block_size)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n\n# supports two types of cache layouts\n# 1. [num_blocks, num_kv_heads, block_size, head_dim]\n# 2. [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n@triton.jit\ndef _copy_to_kcache_seqlen_n_kernel(\n    K,  # K or V\n    KCache,  # [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n    BLOCK_TABLES,\n    seq_lengths,\n    stride_kt,\n    stride_kh,\n    stride_kd,\n    stride_kcb,\n    stride_kch,\n    stride_kcsplit_x,\n    stride_kcs,\n    stride_kcx,\n    stride_bts,\n    stride_btb,\n    block_size,\n    n_tokens,\n    HEAD_DIM: tl.constexpr,\n    KCACHE_X: tl.constexpr,\n):\n    # `n_tokens` is used to specify the number of tokens to copy for each sequence\n    # When n_tokens > 1, tokens from different sequences are packed into the first dimension of the grid,\n    #   `seq_lengths` must be the lengths of sequences counting the number of tokens to copy\n    #   E.g. if n_tokens = 5, seq_lengths = [12, 15], then the already-copied position ids are [0-6, 0-9]\n    #   for the two sequences, respectively. And the position ids to be copied are [7-11, 9-14].\n    # When n_tokens = 1, consider token idx as the sequence idx, since it's only used during regular decoding stage\n    cur_token_idx = tl.program_id(0)\n    cur_seq_idx = cur_token_idx // n_tokens\n    # `cur_token_shift` is only valid and functional when `n_tokens` > 1\n    cur_token_shift = cur_token_idx - (n_tokens * (cur_seq_idx + 1))\n    cur_kv_head_idx = tl.program_id(1)\n    split_x_idx = tl.program_id(2)\n\n    past_kv_seq_len = tl.load(seq_lengths + cur_seq_idx) + cur_token_shift\n    last_bt_block_idx = past_kv_seq_len // block_size\n    block_table_ptr = BLOCK_TABLES + cur_seq_idx * stride_bts\n    block_id = tl.load(block_table_ptr + last_bt_block_idx * stride_btb)\n    offset_last_block = past_kv_seq_len % block_size\n    offsets_dmodel = split_x_idx * KCACHE_X + tl.arange(0, KCACHE_X)\n    offsets_k = cur_token_idx * stride_kt + cur_kv_head_idx * stride_kh + offsets_dmodel * stride_kd\n    k = tl.load(K + offsets_k)\n    offsets_kcache = (\n        block_id * stride_kcb\n        + cur_kv_head_idx * stride_kch\n        + split_x_idx * stride_kcsplit_x\n        + offset_last_block * stride_kcs\n        + tl.arange(0, KCACHE_X)\n    )\n    tl.store(KCache + offsets_kcache, k)\n    return\n\n\ndef copy_k_to_blocked_cache(\n    k: torch.Tensor,\n    k_cache: torch.Tensor,\n    kv_lengths: torch.Tensor,\n    block_tables: torch.Tensor,\n    n: int = 1,\n    use_new_kcache_layout: bool = False,\n):\n    \"\"\"\n    Copy keys or values to the blocked key/value cache during decoding stage.\n\n    Args:\n        k (torch.Tensor): [bsz, 1, num_kv_heads, head_dim]/[bsz, num_kv_heads, head_dim] - Keys or values during decoding with seq len 1.\n            [bsz * n, num_kv_heads, head_dim] - Keys or values with seq len n\n        k_cache (torch.Tensor): [num_blocks, num_kv_heads, block_size, head_dim] - Blocked key or value cache.\n            new KCache Layout [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n        kv_lengths (torch.Tensor): [bsz] - Past key/value sequence lengths plus current sequence length for each sequence.\n        block_tables (torch.Tensor): [bsz, max_blocks_per_sequence] - Block tables for each sequence.\n        n (int): Number of tokens to copy for each sequence. Default to 1.\n        use_new_kcache_layout (bool): Whether to use the new layout for kcache. Default to False.\n    \"\"\"\n    assert k.dtype == k_cache.dtype, \"Expected consistent dtype for tensor and cache.\"\n    if k.dim() == 4:\n        k = k.reshape(-1, k.size(-2), k.size(-1))\n    k_shape = k.shape\n    bsz, num_kv_heads, head_dim = k_shape\n    # NOTE when n > 1, the shape of k is [bsz * n, num_kv_heads, head_dim]\n    if n > 1:\n        assert bsz % n == 0, \"Each sequence should have the same number of tokens to be copied\"\n        bsz = bsz // n\n\n    assert kv_lengths.shape[0] == block_tables.shape[0] == bsz, (\n        f\"Got incompatible batch size (number of seqs):\\n\"\n        f\"  Past kv sequence lengths bsz {kv_lengths.shape[0]}; \"\n        f\" block tables bsz {block_tables.shape[0]}, input k batch size {bsz}\"\n    )\n\n    k_cache_shape = k_cache.shape\n    # Modify if the shape of kv cahce is changed.\n    block_size = k_cache_shape[-2]\n\n    x = head_dim\n    stride_kcsplit_x, stride_kcs, stride_kcd = 0, k_cache.stride(2), k_cache.stride(3)\n    if use_new_kcache_layout:\n        # when using kcache layout [num_blocks, num_kv_heads, head_dim // x, block_size, x]\n        assert (\n            len(k_cache_shape) == 5\n            and k_cache_shape[1] == k_shape[1]\n            and k_cache_shape[2] * k_cache_shape[4] == k_shape[2]\n        ), f\"Incompatible k_cache shape {k_cache_shape} with k shape {k_shape}\"\n        x = k_cache.size(-1)\n        stride_kcsplit_x, stride_kcs, stride_kcd = k_cache.stride()[2:]\n\n    num_warps = 8 if head_dim > 128 else 4\n    grid = (bsz * n, num_kv_heads, head_dim // x)\n    _copy_to_kcache_seqlen_n_kernel[grid](\n        k,\n        k_cache,\n        block_tables,\n        kv_lengths,\n        k.stride(0),\n        k.stride(1),\n        k.stride(2),\n        k_cache.stride(0),\n        k_cache.stride(1),\n        stride_kcsplit_x,\n        stride_kcs,\n        stride_kcd,\n        block_tables.stride(0),\n        block_tables.stride(1),\n        block_size,\n        n_tokens=n,\n        HEAD_DIM=head_dim,\n        KCACHE_X=x,\n        num_warps=num_warps,\n    )\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel `cos_func` computes the cosine of each element in the input tensor `a` and stores the result in tensor `b`. \n            The kernel processes the tensor in blocks, using `BLOCK_SIZE` to determine the number of elements each block processes. \n            `offset` calculates the starting point of each block, and `mask` ensures computations are only performed on valid elements. \n            The `cos` operation is performed using `tl.cos` on the input values loaded from `a`, and results are stored back in `b`.\n            The `cos` function is a wrapper that prepares the output tensor `B`, calculates the required block and grid sizes, and launches the kernel to process the input tensor `A`.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the block size\nBLOCK_SIZE = 1024\n\n@triton.jit\ndef cos_func(a_ptr, b_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Compute the block index\n    block_idx = tl.program_id(axis=0)\n    \n    # Compute the starting offset for this block\n    offset = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Create a mask to ensure we only process valid elements\n    mask = offset < n_elements\n    \n    # Load elements from the input tensor 'a'\n    a = tl.load(a_ptr + offset, mask=mask, other=0.0)\n    \n    # Compute the cosine of each element\n    b = tl.cos(a)\n    \n    # Store the results in the output tensor 'b'\n    tl.store(b_ptr + offset, b, mask=mask)\n\ndef cos(A):\n    # Ensure input is a contiguous tensor\n    A = A.contiguous()\n    \n    # Create the output tensor B with the same shape as A\n    B = torch.empty_like(A)\n    \n    # Calculate the number of elements in the input tensor\n    n_elements = A.numel()\n    \n    # Calculate the number of blocks needed\n    grid_size = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n    \n    # Launch the Triton kernel\n    cos_func[grid_size](A, B, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return B\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random input tensor\n    A = torch.randn(2048, device='cuda')\n    \n    # Compute the cosine using the Triton kernel\n    B = cos(A)\n    \n    # Verify the result with PyTorch's cosine function\n    assert torch.allclose(B, torch.cos(A), atol=1e-6), \"Results do not match!\"\n", "label": "import torch\nimport triton\nimport triton.language as tl\nimport math\n\n# Kernel function: Computes the cosine of each element in the input tensor.\n@triton.jit\ndef cos_func(a, b, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Calculate the offset for each block and thread\n    offset = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    # Determine which elements are valid within the block\n    mask = offset < n_elements\n    # Load elements from tensor `a` into `a_value`\n    a_value = tl.load(a + offset, mask=mask)\n    # Compute the cosine of each element in `a_value`\n    b_value = tl.cos(a_value.to(tl.float32))\n    # Store the result back to tensor `b`\n    tl.store(b + offset, b_value, mask=mask)  \n\n# Function to invoke the Triton kernel and perform the computation\ndef cos(A):\n    # Prepare output tensor `B` with the same shape and type as `A`\n    B = torch.empty_like(A)\n    # Determine the total number of elements in the input tensor `A`\n    n_elements = A.numel()\n    # Calculate the optimal block size\n    block_size = triton.next_power_of_2(math.ceil(math.sqrt(n_elements)))\n    # Determine the grid size needed to cover all elements\n    grid_size = triton.cdiv(n_elements, block_size)\n    # Launch the Triton kernel\n    cos_func[(grid_size, 1, 1)](A, B, n_elements, block_size)\n    return B\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code provides a fast cross-entropy loss function using Triton kernels for efficient GPU computation. The `Fast_CrossEntropyLoss` class extends PyTorch's autograd functionality to implement the forward and backward passes of cross-entropy computation. \n            \n            The main kernel `_cross_entropy_forward` computes the log-sum-exp trick to stabilize the exponentials for calculating the loss and normalizes the logits. It supports optional features like softcapping and logit scaling. \n            \n            `_cross_entropy_backward` computes the gradient of the loss with respect to the logits for backpropagation. It uses similar optional features as the forward kernel. \n            \n            The kernels are launched in parallel over the number of input rows and chunks if necessary. \n            \n            `fast_cross_entropy_loss` is the user-facing function that reshapes inputs, calls the `Fast_CrossEntropyLoss` class, and averages the loss over the valid labels.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\nclass Fast_CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    @triton.jit\n    def _cross_entropy_forward(logits, labels, loss, num_classes, softcap=None, logit_scale=None):\n        row_idx = tl.program_id(0)\n        col_idx = tl.arange(0, num_classes)\n        \n        logits_row = logits + row_idx * num_classes\n        logits_row = tl.load(logits_row + col_idx)\n        \n        if logit_scale is not None:\n            logits_row *= logit_scale\n        \n        max_logits = tl.max(logits_row, axis=0)\n        logits_row = logits_row - max_logits\n        \n        exp_logits = tl.exp(logits_row)\n        sum_exp_logits = tl.sum(exp_logits, axis=0)\n        \n        log_sum_exp = tl.log(sum_exp_logits)\n        normalized_logits = logits_row - log_sum_exp\n        \n        if softcap is not None:\n            normalized_logits = tl.minimum(normalized_logits, softcap)\n        \n        label = tl.load(labels + row_idx)\n        loss_val = -normalized_logits[label]\n        \n        tl.store(loss + row_idx, loss_val)\n\n    @staticmethod\n    @triton.jit\n    def _cross_entropy_backward(logits, labels, grad_output, grad_logits, num_classes, softcap=None, logit_scale=None):\n        row_idx = tl.program_id(0)\n        col_idx = tl.arange(0, num_classes)\n        \n        logits_row = logits + row_idx * num_classes\n        logits_row = tl.load(logits_row + col_idx)\n        \n        if logit_scale is not None:\n            logits_row *= logit_scale\n        \n        max_logits = tl.max(logits_row, axis=0)\n        logits_row = logits_row - max_logits\n        \n        exp_logits = tl.exp(logits_row)\n        sum_exp_logits = tl.sum(exp_logits, axis=0)\n        \n        prob = exp_logits / sum_exp_logits\n        \n        label = tl.load(labels + row_idx)\n        grad = prob\n        grad[label] -= 1.0\n        \n        if softcap is not None:\n            grad = tl.minimum(grad, softcap)\n        \n        grad *= grad_output[row_idx]\n        \n        tl.store(grad_logits + row_idx * num_classes + col_idx, grad)\n\n    @staticmethod\n    def forward(ctx, logits, labels, softcap=None, logit_scale=None):\n        num_classes = logits.shape[1]\n        loss = torch.empty(logits.shape[0], device=logits.device, dtype=logits.dtype)\n        \n        Fast_CrossEntropyLoss._cross_entropy_forward[(logits.shape[0],)](logits, labels, loss, num_classes, softcap, logit_scale)\n        \n        ctx.save_for_backward(logits, labels)\n        ctx.num_classes = num_classes\n        ctx.softcap = softcap\n        ctx.logit_scale = logit_scale\n        \n        return loss.mean()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        logits, labels = ctx.saved_tensors\n        num_classes = ctx.num_classes\n        softcap = ctx.softcap\n        logit_scale = ctx.logit_scale\n        \n        grad_logits = torch.empty_like(logits)\n        \n        Fast_CrossEntropyLoss._cross_entropy_backward[(logits.shape[0],)](logits, labels, grad_output, grad_logits, num_classes, softcap, logit_scale)\n        \n        return grad_logits, None, None, None\n\ndef fast_cross_entropy_loss(logits, labels, softcap=None, logit_scale=None):\n    return Fast_CrossEntropyLoss.apply(logits, labels, softcap, logit_scale)\n\n# Example usage:\nlogits = torch.randn(32, 10, device='cuda', requires_grad=True)\nlabels = torch.randint(0, 10, (32,), device='cuda')\nloss = fast_cross_entropy_loss(logits, labels)\nloss.backward()\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n# from .utils import triton_tanh\nfrom triton.language.extra import libdevice\n\ntriton_tanh = libdevice.tanh\nnext_power_of_2 = triton.next_power_of_2\nMAX_FUSED_SIZE : int = 65536\n\ndef calculate_settings(n : int) -> (int, int):\n    BLOCK_SIZE : int = next_power_of_2(n)\n    if BLOCK_SIZE > MAX_FUSED_SIZE:\n        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n    num_warps : int = 4\n    if   BLOCK_SIZE >= 32768: num_warps = 32\n    elif BLOCK_SIZE >=  8192: num_warps = 16\n    elif BLOCK_SIZE >=  2048: num_warps = 8\n    return BLOCK_SIZE, num_warps\n\n@triton.heuristics({\n    \"DO_SOFTCAPPING\": lambda args: args[\"DO_SOFTCAPPING\"],\n    \"DO_LOGIT_SCALING\": lambda args: args[\"DO_LOGIT_SCALING\"],\n})\n@triton.jit\ndef _cross_entropy_forward(\n    logits_ptr, logits_row_stride,\n    loss_ptr,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n\n    logits = logits.to(tl.float32)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx)\n        if DO_LOGIT_SCALING: x = LOGIT_SCALE * x\n        if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)\n        loss = logsumexp - x.to(tl.float32)\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)\n\n@triton.heuristics({\n    \"DO_SOFTCAPPING\": lambda args: args[\"DO_SOFTCAPPING\"],\n    \"DO_LOGIT_SCALING\": lambda args: args[\"DO_LOGIT_SCALING\"],\n})\n@triton.jit\ndef _chunked_cross_entropy_forward(\n    logits_ptr, logits_row_stride,\n    loss_ptr,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    N_CHUNKS: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n\n    col_offsets = chunk_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n\n    label_idx = tl.load(labels_ptr).to(tl.int32)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING: logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING: logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n\n    logits = logits.to(tl.float32)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx).to(tl.float32)\n            if DO_LOGIT_SCALING: x = LOGIT_SCALE * x\n            if DO_SOFTCAPPING: x = SOFTCAP * triton_tanh(x / SOFTCAP)\n            loss = -1.0 * x.to(tl.float32)\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n        tl.store(logsumexp_ptr, logsumexp)\n\n\n@triton.heuristics({\n    \"DO_SOFTCAPPING\": lambda args: args[\"DO_SOFTCAPPING\"],\n    \"DO_LOGIT_SCALING\": lambda args: args[\"DO_LOGIT_SCALING\"],\n})\n@triton.jit\ndef _cross_entropy_backward(\n    logits_ptr, logits_row_stride,\n    dloss_ptr, dloss_row_stride,\n    logsumexp_ptr,\n    labels_ptr,\n    VOCAB_SIZE: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n    DO_SOFTCAPPING: tl.constexpr,\n    SOFTCAP: tl.constexpr,\n    DO_LOGIT_SCALING: tl.constexpr,\n    LOGIT_SCALE: tl.constexpr,\n):\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n\n    logits_ptr += row_idx * logits_row_stride.to(tl.int64)\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx*BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)\n\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))\n\n    if DO_LOGIT_SCALING:\n        x = x * LOGIT_SCALE\n    \n    if DO_SOFTCAPPING:\n        partial = triton_tanh(x / SOFTCAP)\n        x = SOFTCAP * partial\n    \n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x.to(tl.float32) - logsumexp)\n    y = tl.where(\n        col_offsets == label_idx,\n        y - 1.0,\n        y,\n    )\n\n    if DO_LOGIT_SCALING:\n        y = y * LOGIT_SCALE\n    \n    if DO_SOFTCAPPING:\n        y = y * (1.0 - partial*partial)\n    \n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)\n\nMAX_FUSED_SIZE = 65536\n\nclass Fast_CrossEntropyLoss(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, labels, logit_softcapping=0, logit_scaling=0):\n        n_rows, vocab_size = logits.shape\n\n        div, mod = divmod(vocab_size, MAX_FUSED_SIZE)\n        n_chunks = div + (mod != 0)\n        losses = torch.empty(n_rows, dtype=torch.float32, device=\"cuda:0\")\n\n        DO_SOFTCAPPING = (logit_softcapping != 0)\n        DO_LOGIT_SCALING = (logit_scaling != 0)\n\n        if n_chunks == 1:\n            BLOCK_SIZE, num_warps = calculate_settings(vocab_size)\n            logsumexp = torch.empty(n_rows, dtype=torch.float32, device=\"cuda:0\")\n\n            _cross_entropy_forward[(n_rows,)](\n                logits, logits.stride(0),\n                losses,\n                logsumexp,\n                labels,\n                VOCAB_SIZE=vocab_size,\n                BLOCK_SIZE=BLOCK_SIZE,\n                DO_SOFTCAPPING=DO_SOFTCAPPING,\n                SOFTCAP=logit_softcapping,\n                DO_LOGIT_SCALING=DO_LOGIT_SCALING,\n                LOGIT_SCALE=logit_scaling,\n                num_warps=num_warps,\n            )\n        else:\n            logsumexp = torch.empty((n_rows, n_chunks,), dtype=torch.float32, device=\"cuda:0\")\n\n            _chunked_cross_entropy_forward[(n_rows, n_chunks,)](\n                logits, logits.stride(0),\n                losses,\n                logsumexp,\n                labels,\n                VOCAB_SIZE=vocab_size,\n                N_CHUNKS=n_chunks,\n                BLOCK_SIZE=MAX_FUSED_SIZE,\n                DO_SOFTCAPPING=DO_SOFTCAPPING,\n                SOFTCAP=logit_softcapping,\n                DO_LOGIT_SCALING=DO_LOGIT_SCALING,\n                LOGIT_SCALE=logit_scaling,\n                num_warps=32,\n            )\n            logsumexp = torch.logsumexp(logsumexp, dim=1)\n            losses += logsumexp\n            losses.masked_fill_(labels == -100, 0)\n        \n        ctx.save_for_backward(logits, logsumexp, labels)\n        ctx.DO_SOFTCAPPING = DO_SOFTCAPPING\n        ctx.logit_softcapping = logit_softcapping\n        ctx.DO_LOGIT_SCALING = DO_LOGIT_SCALING\n        ctx.logit_scaling = logit_scaling\n        return losses\n    \n    @staticmethod\n    def backward(ctx, dlosses):\n        logits, logsumexp, labels = ctx.saved_tensors\n        n_rows, vocab_size = logits.shape\n\n        BLOCK_SIZE = 4096\n        div, mod = divmod(vocab_size, BLOCK_SIZE)\n        n_blocks = div + (mod != 0)\n\n        _cross_entropy_backward[(n_rows, n_blocks,)](\n            logits, logits.stride(0),\n            dlosses, dlosses.stride(0),\n            logsumexp,\n            labels,\n            VOCAB_SIZE=vocab_size,\n            BLOCK_SIZE=BLOCK_SIZE,\n            DO_SOFTCAPPING=ctx.DO_SOFTCAPPING,\n            SOFTCAP=ctx.logit_softcapping,\n            DO_LOGIT_SCALING=ctx.DO_LOGIT_SCALING,\n            LOGIT_SCALE=ctx.logit_scaling,\n            num_warps=8,\n        )\n        return logits, None, None, None,\n    \ndef fast_cross_entropy_loss(\n    logits,\n    labels,\n    logit_softcapping=0,\n    logit_scaling=0,\n    n_items=None,\n):\n    batch, seq_len, d = logits.shape\n    assert(labels.shape == (batch, seq_len))\n\n    loss = Fast_CrossEntropyLoss.apply(\n        logits.view(batch*seq_len, d),\n        labels.view(-1),\n        logit_softcapping,\n        logit_scaling,\n    )\n    if n_items is None:\n        n_items = torch.count_nonzero(labels != -100)\n    return loss.sum() / n_items\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n        The Triton kernel named 'log_softmax_kernel' implements the log softmax operation over a matrix. It takes as input pointers to the input and output data, along with dimensions M, N, and K, where M is the number of rows, N is the number of columns, and K is an auxiliary dimension. The kernel loads data from the input pointer, computes the log softmax by first normalizing the input, and stores the result back in the output pointer. The kernel supports automatic tuning of its block sizes (BLOCK_M and BLOCK_N) and number of warps via heuristic functions to optimize performance based on the input size.\n        \n        The 'LogSoftmax' class is an autograd function for PyTorch, wrapping the Triton kernel for forward and backward passes. The 'forward' method sets up the kernel execution, calculates grid dimensions, and invokes the log softmax kernel. It saves necessary data for backward computation. The 'backward' method calculates the gradient using another Triton kernel, 'log_softmax_backward_kernel', which computes the gradient of the log softmax operation.\n\n        The 'log_softmax' function is a user-facing API that leverages 'LogSoftmax' to perform the log softmax operation on an input tensor, specifying the dimension and optionally the data type. It ensures the input is contiguous and provides an output tensor of the same shape.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=4),\n        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=8),\n    ],\n    key=['M', 'N']\n)\n@triton.jit\ndef log_softmax_kernel(input_ptr, output_ptr, M, N, K, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start_m = pid * BLOCK_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_M)\n    offsets_n = tl.arange(0, BLOCK_N)\n\n    mask_m = offsets_m < M\n    mask_n = offsets_n < N\n\n    input_ptrs = input_ptr + offsets_m[:, None] * N + offsets_n[None, :]\n    output_ptrs = output_ptr + offsets_m[:, None] * N + offsets_n[None, :]\n\n    # Load input data\n    input_data = tl.load(input_ptrs, mask=mask_m[:, None] & mask_n[None, :], other=-float('inf'))\n\n    # Compute max for numerical stability\n    row_max = tl.max(input_data, axis=1, keepdim=True)\n    input_data = input_data - row_max\n\n    # Compute exp and sum\n    exp_data = tl.exp(input_data)\n    row_sum = tl.sum(exp_data, axis=1, keepdim=True)\n\n    # Compute log softmax\n    log_softmax_data = input_data - tl.log(row_sum)\n\n    # Store result\n    tl.store(output_ptrs, log_softmax_data, mask=mask_m[:, None] & mask_n[None, :])\n\nimport torch\n\nclass LogSoftmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, dim):\n        M, N = input.shape\n        output = torch.empty_like(input)\n        \n        grid = lambda meta: (triton.cdiv(M, meta['BLOCK_M']),)\n        log_softmax_kernel[grid](input, output, M, N, input.stride(0))\n\n        ctx.save_for_backward(output)\n        ctx.dim = dim\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output, = ctx.saved_tensors\n        grad_input = torch.empty_like(grad_output)\n\n        # Implement backward kernel call here (not provided)\n        # log_softmax_backward_kernel(...)\n\n        return grad_input, None\n\ndef log_softmax(input, dim=-1, dtype=None):\n    if dtype is not None:\n        input = input.to(dtype)\n    return LogSoftmax.apply(input, dim)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n\ndef heur_block_n(args):\n    return triton.next_power_of_2(args[\"N\"])\n\n\ndef heur_num_warps(args):\n    if args[\"N\"] <= 1024:\n        return 4\n    elif args[\"N\"] <= 2048:\n        return 8\n    else:\n        return 16\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 1}),\n        triton.Config({\"BLOCK_M\": 2}),\n        triton.Config({\"BLOCK_M\": 4}),\n        triton.Config({\"BLOCK_M\": 8}),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"BLOCK_N\": heur_block_n,\n        \"num_warps\": heur_num_warps,\n    }\n)\n@triton.jit\ndef log_softmax_kernel(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    input_ptrs = input_ptr + offset\n    inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(tl.float32)\n    row_minus_max = inp - tl.max(inp, axis=1)[:, None]\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    softmax_output = tl.log(numerator / denominator)\n    output_ptrs = output_ptr + offset\n    tl.store(output_ptrs, softmax_output, mask=mask)\n\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_M\": 1}),\n        triton.Config({\"BLOCK_M\": 2}),\n        triton.Config({\"BLOCK_M\": 4}),\n        triton.Config({\"BLOCK_M\": 8}),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"BLOCK_N\": heur_block_n,\n        \"num_warps\": heur_num_warps,\n    }\n)\n@triton.jit\ndef log_softmax_backward_kernel(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offset = tl.arange(0, BLOCK_N)\n\n    offsets = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    out_ptrs = out_ptr + offsets\n    out = tl.load(out_ptrs, mask=mask).to(tl.float32)\n    out_grad_ptrs = out_grad_ptr + offsets\n    out_grad = tl.load(out_grad_ptrs, mask=mask).to(tl.float32)\n\n    scale = tl.sum(out_grad, 1)\n    in_grad = out_grad - tl.exp(out.to(tl.float32)) * scale[:, None]\n\n    in_grad_ptrs = in_grad_ptr + offsets\n    tl.store(in_grad_ptrs, in_grad, mask=mask)\n\n\nclass LogSoftmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, dim, dtype):\n        logging.debug(\"GEMS LOG_SOFTMAX\")\n\n        assert dim >= -x.ndim and dim < x.ndim, \"Invalid dim\"\n        dim = dim % x.ndim\n        M = 1\n        N = x.shape[dim]\n        for i in range(dim):\n            M *= x.shape[i]\n        inp = x.contiguous()\n        if dtype is None:\n            dtype = x.dtype\n        out = torch.empty_like(inp, dtype=dtype)\n        K = inp.numel() // M // N\n\n        grid = lambda meta: (\n            triton.cdiv(M, meta[\"BLOCK_M\"]),\n            K,\n        )\n        with torch.cuda.device(inp.device):\n            log_softmax_kernel[grid](\n                out,\n                inp,\n                M,\n                N,\n                K,\n            )\n        ctx.save_for_backward(out)\n        ctx.dim = dim\n        return out\n\n    @staticmethod\n    def backward(ctx, out_grad):\n        logging.debug(\"GEMS LOG_SOFTMAX VJP\")\n\n        dim = ctx.dim\n        (out,) = ctx.saved_tensors\n\n        assert dim >= -out.ndim and dim < out.ndim, \"Invalid dim\"\n        dim = dim % out.ndim\n        M = 1\n        N = out.shape[dim]\n        for i in range(dim):\n            M *= out.shape[i]\n\n        out_grad = out_grad.contiguous()\n        in_grad = torch.empty_like(out)\n        K = out.numel() // M // N\n\n        grid = lambda meta: (\n            triton.cdiv(M, meta[\"BLOCK_M\"]),\n            K,\n        )\n        with torch.cuda.device(in_grad.device):\n            log_softmax_backward_kernel[grid](\n                out,\n                out_grad,\n                in_grad,\n                M,\n                N,\n                K,\n            )\n        return in_grad, None, None\n\n\ndef log_softmax(x, dim=-1, dtype=None):\n    return LogSoftmax.apply(x, dim, dtype)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.This Triton code defines a function `index_select_cat_fwd` that selects rows from a 2D CUDA tensor `source` based on indices from a 1D CUDA tensor `index` and stores the result in `output`. The kernel `index_select_cat_fwd_kernel` is launched in a 2D grid. Each block processes a block of indices (`BLOCK_SIZE_INDEX`) and a block of columns (`BLOCK_SIZE_COL`). Inside the kernel, it calculates offsets for source and output tensors based on indices and columns, loads the source data, and stores it into the output tensor. The `grid` function calculates the grid size based on the number of indices and columns.", "predict": "import triton\nimport triton.language as tl\n\n# Define the Triton kernel\n@triton.jit\ndef index_select_cat_fwd_kernel(\n    source_ptr, index_ptr, output_ptr,\n    num_indices, num_cols,\n    BLOCK_SIZE_INDEX: tl.constexpr, BLOCK_SIZE_COL: tl.constexpr\n):\n    # Define the block indices\n    block_idx = tl.program_id(0)\n    block_col = tl.program_id(1)\n\n    # Calculate start index for this block\n    start_idx = block_idx * BLOCK_SIZE_INDEX\n    start_col = block_col * BLOCK_SIZE_COL\n\n    # Create a range for this block\n    range_idx = tl.arange(0, BLOCK_SIZE_INDEX) + start_idx\n    range_col = tl.arange(0, BLOCK_SIZE_COL) + start_col\n\n    # Mask to ensure we don't go out of bounds\n    mask_idx = range_idx < num_indices\n    mask_col = range_col < num_cols\n\n    # Load indices from the index tensor\n    indices = tl.load(index_ptr + range_idx, mask=mask_idx)\n\n    # Calculate offsets for the source and output\n    source_offsets = indices[:, None] * num_cols + range_col[None, :]\n    output_offsets = range_idx[:, None] * num_cols + range_col[None, :]\n\n    # Load data from the source tensor\n    source_data = tl.load(source_ptr + source_offsets, mask=mask_idx[:, None] & mask_col[None, :])\n\n    # Store the selected data into the output tensor\n    tl.store(output_ptr + output_offsets, source_data, mask=mask_idx[:, None] & mask_col[None, :])\n\n# Wrapper function to launch the kernel\ndef index_select_cat_fwd(source, index, output, BLOCK_SIZE_INDEX=128, BLOCK_SIZE_COL=128):\n    num_indices = index.shape[0]\n    num_cols = source.shape[1]\n\n    # Calculate grid size\n    grid = (\n        (num_indices + BLOCK_SIZE_INDEX - 1) // BLOCK_SIZE_INDEX,\n        (num_cols + BLOCK_SIZE_COL - 1) // BLOCK_SIZE_COL\n    )\n\n    # Launch the kernel\n    index_select_cat_fwd_kernel[grid](\n        source, index, output,\n        num_indices, num_cols,\n        BLOCK_SIZE_INDEX=BLOCK_SIZE_INDEX,\n        BLOCK_SIZE_COL=BLOCK_SIZE_COL\n    )\n\n# Example usage:\n# Assuming source is a 2D CUDA tensor, index is a 1D CUDA tensor, and output is a pre-allocated 2D CUDA tensor\n# index_select_cat_fwd(source, index, output)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef index_select_cat_fwd_kernel(\n    output_ptr,  # *Pointer* to output tensor.\n    source_ptr,  # *Pointer* to source tensor.\n    index_ptr,  # *Pointer* to index tensor.\n    num_indices,\n    num_cols,\n    stride0,  # Stride information of source tensor.\n    stride1,\n    BLOCK_SIZE_INDEX: tl.constexpr,  # Number of indices each program should process.\n    BLOCK_SIZE_COL: tl.constexpr,  # Number of cols each program should process.\n):\n    pid0 = tl.program_id(axis=0)  # We use 2D launch grid\n    pid1 = tl.program_id(axis=1)\n\n    indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    rows = tl.load(index_ptr + indices, mask=(indices < num_indices))\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n\n    source_offsets = source_ptr + rows[:, None] * stride0 + cols[None, :] * stride1\n    mask = (indices[:, None] < num_indices) & (cols[None, :] < num_cols)\n    output = tl.load(source_offsets, mask=mask)\n\n    output_offsets = output_ptr + indices[:, None] * stride0 + cols[None, :] * stride1\n    tl.store(output_offsets, output, mask=mask)\n\n\ndef index_select_cat_fwd(\n    output: torch.Tensor,\n    source: torch.Tensor,\n    index: torch.Tensor,\n):\n    if not (source.is_cuda and index.is_cuda):\n        raise ValueError(\"The index tensor and the source tensor must be of type CUDA!\")\n\n    if not source.ndim == 2:\n        raise ValueError(f\"Expected 2-dimensional tensor, got {source.ndim}.\")\n    if not index.ndim == 1:\n        raise ValueError(f\"Expected 1-dimensional tensor, got {index.ndim}.\")\n\n    num_rows, num_cols = source.shape\n    num_indices = index.shape[0]\n\n    if num_indices > num_rows:\n        print(f\"Warning: The number of indices exceeds the number of rows in the source tensor. Truncating indices.\")\n        num_indices = num_rows\n        index = index[:num_rows]\n\n    stride0, stride1 = source.stride(0), source.stride(1)\n\n    def grid(meta):\n        return (\n            triton.cdiv(num_indices, meta[\"BLOCK_SIZE_INDEX\"]),\n            triton.cdiv(num_cols, meta[\"BLOCK_SIZE_COL\"]),\n        )\n\n    index_select_cat_fwd_kernel[grid](\n        output,\n        source,\n        index,\n        num_indices,\n        num_cols,\n        stride0,\n        stride1,\n        BLOCK_SIZE_INDEX=1,\n        BLOCK_SIZE_COL=512,\n    )\n\n    return output\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernel '_fwd_kernel_token_att1' computes the scaled dot-product attention for given query and key tensors. The function 'token_att_fwd' is a wrapper around the Triton kernel, handling configurations and launching the kernel execution. Inputs include query (q), key (k), location information (B_Loc), sequence start locations (B_Start_Loc), sequence lengths (B_Seqlen), maximum input length (max_input_len), and the attention output buffer (att_out). The kernel calculates attention scores by performing a dot product between the query and key vectors, scaling the result, and storing it into 'att_out'. Key computation involves loading query and key elements, calculating the attention value by summing the element-wise product of q and k, and storing these results. Key tensor dimensions and execution configurations are defined and determined by input parameters and tensor shapes.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _fwd_kernel_token_att1(\n    q_ptr, k_ptr, att_out_ptr,\n    B_Loc_ptr, B_Start_Loc_ptr, B_Seqlen_ptr,\n    max_input_len, scale, num_heads, head_dim,\n    stride_q_batch, stride_q_head, stride_q_seq, stride_q_dim,\n    stride_k_batch, stride_k_head, stride_k_seq, stride_k_dim,\n    stride_out_batch, stride_out_head, stride_out_seq, stride_out_dim,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Compute the block indices\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    seq_idx = tl.program_id(2)\n\n    # Compute the start location for the current sequence\n    start_loc = tl.load(B_Start_Loc_ptr + batch_idx)\n    seq_len = tl.load(B_Seqlen_ptr + batch_idx)\n\n    # Calculate the offset in the q, k, and att_out tensors\n    q_offset = (batch_idx * stride_q_batch +\n                head_idx * stride_q_head +\n                seq_idx * stride_q_seq)\n    k_offset = (batch_idx * stride_k_batch +\n                head_idx * stride_k_head)\n    out_offset = (batch_idx * stride_out_batch +\n                  head_idx * stride_out_head +\n                  seq_idx * stride_out_seq)\n\n    # Load query vector\n    q = tl.load(q_ptr + q_offset + tl.arange(0, BLOCK_SIZE) * stride_q_dim)\n\n    # Initialize attention output\n    att_out = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n\n    # Iterate over key vectors\n    for k_seq_idx in range(seq_len):\n        k = tl.load(k_ptr + k_offset + k_seq_idx * stride_k_seq + tl.arange(0, BLOCK_SIZE) * stride_k_dim)\n        # Compute dot product and scale\n        dot_product = tl.dot(q, k) * scale\n        # Store the result\n        tl.store(att_out_ptr + out_offset + k_seq_idx * stride_out_dim, dot_product)\n\n### Wrapper Function: `token_att_fwd`\n\nThis function sets up the execution configuration and launches the Triton kernel.\n\n", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_token_att1(\n    Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    Att_Out,\n    stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd,\n    att_stride_h, att_stride_bs,\n    kv_group_num,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    \n    cur_kv_head = cur_head // kv_group_num\n\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s * offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] < cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index + offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new < cur_batch_end_index)\n    return\n\n@torch.no_grad()\ndef token_att_fwd(q, k, att_out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len):\n    BLOCK = 32\n    # shape constraints\n    Lq, Lk = q.shape[-1], k.shape[-1]\n    assert Lq == Lk\n    assert Lk in {16, 32, 64, 128}\n    sm_scale = 1.0 / (Lk ** 0.5)\n\n    batch, head_num = B_Loc.shape[0], q.shape[1]\n\n    grid = (batch, head_num, triton.cdiv(max_input_len, BLOCK))\n    kv_group_num = q.shape[1] // k.shape[1]\n\n    num_warps = 4 if Lk <= 64 else 8\n    num_warps = 2\n\n    _fwd_kernel_token_att1[grid](\n        q, k, sm_scale, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n        att_out,\n        B_Loc.stride(0), B_Loc.stride(1),\n        q.stride(0), q.stride(1), q.stride(2),\n        k.stride(0), k.stride(1), k.stride(2),\n        att_out.stride(0), att_out.stride(1),\n        kv_group_num=kv_group_num,\n        BLOCK_DMODEL=Lk,\n        BLOCK_N=BLOCK,\n        num_warps=num_warps,\n        num_stages=1,\n    )\n    return\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The Triton kernel function `_l2_norm_bwd_kernel` computes the backward pass for the L2 normalization operation. It processes each row of the input matrix `X` individually to calculate gradients (`DX`) based on the gradients of the output (`DY`). The function loads slices of `X` and `DY`, computes the variance, and applies the gradient formula incorporating the variance to store the computed gradient in `DX`. The outer function `_l2_norm_bwd` reshapes inputs for batch processing, configures block sizes, and calls the kernel function. The inputs are `x`, `dy` and an optional `eps` for numerical stability, while the output is `dx`.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_bwd_kernel(X_ptr, DY_ptr, DX_ptr, N, eps, BLOCK_SIZE: tl.constexpr):\n    # Get the row index\n    row_idx = tl.program_id(0)\n    \n    # Compute the offset for the row\n    offset = row_idx * N\n    \n    # Load a slice of X and DY for the current row\n    X = tl.load(X_ptr + offset + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < N, other=0.0)\n    DY = tl.load(DY_ptr + offset + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < N, other=0.0)\n    \n    # Compute the squared norm of the row\n    squared_norm = tl.sum(X * X, axis=0)\n    \n    # Compute the variance with epsilon for numerical stability\n    variance = tl.sqrt(squared_norm + eps)\n    \n    # Compute the gradient\n    grad = (DY - tl.sum(DY * X, axis=0) * X / (variance * variance)) / variance\n    \n    # Store the result in DX\n    tl.store(DX_ptr + offset + tl.arange(0, BLOCK_SIZE), grad, mask=tl.arange(0, BLOCK_SIZE) < N)\n\ndef _l2_norm_bwd(x, dy, eps=1e-5):\n    # Get the shape of the input\n    B, N = x.shape\n    \n    # Allocate memory for the output gradient\n    dx = torch.empty_like(x)\n    \n    # Configure block size\n    BLOCK_SIZE = triton.next_power_of_2(N)\n    \n    # Launch the kernel\n    _l2_norm_bwd_kernel[(B,)](x, dy, dx, N, eps, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return dx\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_bwd_kernel(\n    X,  # pointer to the input\n    DY,  # pointer to the output gradient\n    DX,  # pointer to the input gradient\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x) \n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0).to(tl.float32)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var+eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n\ndef _l2_norm_bwd(\n    x, dy, eps=1e-5,\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, dy.shape[-1])\n    dy = dy.reshape(-1, dy.shape[-1])\n    if dy.stride(-1) != 1:\n        dy = dy.contiguous()\n    dx = torch.empty_like(x)\n    N = x.shape[-1]\n    M = x.shape[0]\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _l2_norm_bwd_kernel[(M,)](\n            x,\n            dy,\n            dx,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return dx.reshape(x_shape_og)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            This code implements a custom softmax operation using Triton, designed to handle varying sequence lengths efficiently. The main function, `token_softmax_fwd`, prepares and launches the Triton kernel `_fwd_kernel_token_softmax`. It calculates the softmax for sequences of logits with different lengths in parallel, for batches and heads in attention mechanisms. Inputs include `Logics` (logits), `B_Start_Loc` (start indices of sequences), `B_Seqlen` (sequence lengths), and `Prob_Out` (output tensor for probabilities). The kernel computes the softmax along sequence elements by normalizing logits, subtracting the max for stability, exponentiating, and dividing by the sum. It handles masked values by using `-inf` for padding positions.\n            ", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    logits_ptr, b_start_loc_ptr, b_seqlen_ptr, prob_out_ptr,\n    stride_logits_m, stride_logits_h, stride_logits_n,\n    stride_prob_m, stride_prob_h, stride_prob_n,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Get the program id\n    pid_m = tl.program_id(0)\n    pid_h = tl.program_id(1)\n\n    # Calculate the start index and sequence length for the current batch and head\n    start_loc = tl.load(b_start_loc_ptr + pid_m)\n    seqlen = tl.load(b_seqlen_ptr + pid_m)\n\n    # Create offsets for logits and output probabilities\n    offsets = start_loc + tl.arange(0, BLOCK_SIZE)\n    offsets = tl.where(offsets < start_loc + seqlen, offsets, start_loc + seqlen - 1)\n\n    # Load logits for the current sequence\n    logits = tl.load(logits_ptr + pid_m * stride_logits_m + pid_h * stride_logits_h + offsets * stride_logits_n, mask=offsets < start_loc + seqlen, other=-float('inf'))\n\n    # Compute the max logits for numerical stability\n    max_logits = tl.max(logits, axis=0)\n\n    # Subtract max_logits, exponentiate and sum for normalization\n    logits = logits - max_logits\n    exp_logits = tl.exp(logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=0)\n\n    # Compute probabilities\n    probs = exp_logits / sum_exp_logits\n\n    # Store the probabilities\n    tl.store(prob_out_ptr + pid_m * stride_prob_m + pid_h * stride_prob_h + offsets * stride_prob_n, probs, mask=offsets < start_loc + seqlen)\n\ndef token_softmax_fwd(logits, b_start_loc, b_seqlen, prob_out):\n    # Ensure input tensors are contiguous\n    logits = logits.contiguous()\n    b_start_loc = b_start_loc.contiguous()\n    b_seqlen = b_seqlen.contiguous()\n    prob_out = prob_out.contiguous()\n\n    # Get the shapes and strides\n    batch_size, num_heads, max_seqlen = logits.shape\n    stride_logits_m, stride_logits_h, stride_logits_n = logits.stride()\n    stride_prob_m, stride_prob_h, stride_prob_n = prob_out.stride()\n\n    # Launch the Triton kernel\n    grid = (batch_size, num_heads)\n    BLOCK_SIZE = 128  # Adjust based on your hardware's capabilities\n\n    _fwd_kernel_token_softmax[grid](\n        logits, b_start_loc, b_seqlen, prob_out,\n        stride_logits_m, stride_logits_h, stride_logits_n,\n        stride_prob_m, stride_prob_h, stride_prob_n,\n        BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage\nbatch_size = 2\nnum_heads = 4\nmax_seqlen = 128\n\nlogits = torch.randn((batch_size, num_heads, max_seqlen), device='cuda', dtype=torch.float32)\nb_start_loc = torch.tensor([0, 64], device='cuda', dtype=torch.int32)\nb_seqlen = torch.tensor([64, 64], device='cuda', dtype=torch.int32)\nprob_out = torch.empty_like(logits)\n\ntoken_softmax_fwd(logits, b_start_loc, b_seqlen, prob_out)\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel_token_softmax(\n    Logics, B_Start_Loc, B_Seqlen,\n    Prob_Out,\n    stride_logic_h, stride_logic_bs,\n    stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: tl.constexpr\n):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n\n    row = tl.load(Logics + cur_head * stride_logic_h + (cur_batch_in_all_start_index + col_offsets) * stride_logic_bs,\n                  mask=col_offsets < cur_batch_seq_len, other=-float('inf')).to(tl.float32)\n\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    tl.store(Prob_Out + cur_head * stride_prob_h + (cur_batch_in_all_start_index + col_offsets)\n             * stride_prob_bs, softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n\n\n@torch.no_grad()\ndef token_softmax_fwd(Logics, B_Start_Loc, B_Seqlen, Prob_Out, max_input_len):\n    BLOCK_SIZE = triton.next_power_of_2(max_input_len)\n    batch, head_num = B_Start_Loc.shape[0], Logics.shape[0]\n\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n\n    _fwd_kernel_token_softmax[(batch, head_num)](\n        Logics, B_Start_Loc, B_Seqlen,\n        Prob_Out,\n        Logics.stride(0), Logics.stride(1),\n        Prob_Out.stride(0), Prob_Out.stride(1),\n        num_warps=num_warps,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n    return\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The provided code defines a Triton kernel and a wrapper function for computing a chunk-based cumulative sum across a vector in a tensor. The kernel `chunk_global_cumsum_vector_kernel` operates on 2D slices (blocks) of the input tensor `s` and computes the cumulative sum along a specified dimension. The wrapper function `chunk_global_cumsum_vector` sets up this kernel by configuring grid sizes and prepares input/output tensors.\n\n    The main kernel logic involves loading blocks of data, computing a cumulative sum using matrix multiplication with a lower triangular mask (to accumulate values), and storing the result back. The kernel is optimized with Triton's `autotune` feature, exploring different configurations of block sizes (`BT`) and the number of warps.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 32, 'num_warps': 1}),\n        triton.Config({'BT': 64, 'num_warps': 2}),\n        triton.Config({'BT': 128, 'num_warps': 4}),\n    ],\n    key=['N']\n)\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(s_ptr, out_ptr, N, stride, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n\n    # Offsets for loading the block\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n    # Mask to ensure we don't go out of bounds\n    mask = offsets < N\n\n    # Load data from the input tensor\n    s = tl.load(s_ptr + offsets, mask=mask, other=0.0)\n\n    # Create a lower triangular mask for cumulative sum computation\n    lt_mask = tl.arange(0, BLOCK_SIZE)[:, None] <= tl.arange(0, BLOCK_SIZE)[None, :]\n\n    # Compute the cumulative sum using matrix multiplication with the lower triangular mask\n    cumsum = tl.dot(lt_mask.to(tl.float32), s.to(tl.float32))\n\n    # Store the result back to the output tensor\n    tl.store(out_ptr + offsets, cumsum, mask=mask)\n\n\ndef chunk_global_cumsum_vector(s, out, BLOCK_SIZE=64):\n    assert s.ndim == 2, \"Input tensor must be 2D\"\n    N, stride = s.shape\n\n    # Launch the kernel\n    grid = (triton.cdiv(N, BLOCK_SIZE),)\n    chunk_global_cumsum_vector_kernel[grid](\n        s, out, N, stride, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n# Example usage\nimport torch\n\n# Create a sample 2D tensor\ns = torch.rand((128, 128), dtype=torch.float32).cuda()\nout = torch.empty_like(s)\n\n# Compute the chunk-based cumulative sum\nchunk_global_cumsum_vector(s, out)\n\n# Print the result\nprint(out)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Optional\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': 16}, num_warps=2),\n        triton.Config({'BT': 16}, num_warps=4),\n        triton.Config({'BT': 16}, num_warps=8),\n        triton.Config({'BT': 32}, num_warps=2),\n        triton.Config({'BT': 32}, num_warps=4),\n        triton.Config({'BT': 32}, num_warps=8),\n        triton.Config({'BT': 64}, num_warps=2),\n        triton.Config({'BT': 64}, num_warps=4),\n        triton.Config({'BT': 64}, num_warps=8),\n    ],\n    key=['S']\n)\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(\n    s,\n    z,\n    s_s_h,\n    s_s_t,\n    s_s_d,\n    T: tl.constexpr,\n    S: tl.constexpr,\n    BT: tl.constexpr,\n    BS: tl.constexpr\n):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1., 0.)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        # [BT, BS]\n        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n\ndef chunk_global_cumsum_vector(\n    s: torch.Tensor,\n    dtype: Optional[torch.dtype] = None,\n) -> torch.Tensor:\n    B, H, T, S = s.shape\n    BS = 32\n    dtype = dtype or s.dtype\n    grid = (triton.cdiv(S, BS), B * H)\n    z = torch.empty_like(s, dtype=dtype)\n    chunk_global_cumsum_vector_kernel[grid](\n        s, z,\n        s.stride(1), s.stride(2), s.stride(3),\n        T=T, S=S, BS=BS\n    )\n    return z\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    This Triton kernel implementation involves forward and backward pass functions for state-space models (SSM). The primary functions are `diag_ssm_forward_kernel` and `diag_ssm_forward_kernel_complex` for real and complex forward passes, respectively. The `diag_ssm_backward_kernel` and `diag_ssm_backward_kernel_complex` handle the backward pass.\n\n    Each function processes a 3D tensor input representing sequences of data, computes the state-space model output or gradients, and writes the results into a specified output tensor. The kernels are optimized to handle batch processing.\n\n    The forward pass functions compute output sequences by iteratively applying a diagonal state-space transformation matrix. The backward pass functions calculate gradients by propagating errors backward through the sequence.\n\n    The Python class `_ssm_forward` wraps these Triton kernels to integrate with PyTorch's autograd system, handling both real and complex cases appropriately.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef diag_ssm_forward_kernel(\n    input_ptr, output_ptr, state_ptr, matrix_ptr,\n    BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Get program ID\n    pid = tl.program_id(axis=0)\n    \n    # Compute batch index and sequence length\n    batch_idx = pid // SEQ_LEN\n    seq_idx = pid % SEQ_LEN\n    \n    # Compute pointer offsets\n    input_offset = batch_idx * SEQ_LEN * HIDDEN_SIZE + seq_idx * HIDDEN_SIZE\n    output_offset = input_offset\n    state_offset = batch_idx * HIDDEN_SIZE\n    \n    # Load input and state\n    input = tl.load(input_ptr + input_offset)\n    state = tl.load(state_ptr + state_offset)\n    \n    # Load diagonal matrix\n    diag_matrix = tl.load(matrix_ptr + seq_idx * HIDDEN_SIZE)\n    \n    # Compute new state\n    new_state = state * diag_matrix + input\n    \n    # Store new state and output\n    tl.store(state_ptr + state_offset, new_state)\n    tl.store(output_ptr + output_offset, new_state)\n\n@triton.jit\ndef diag_ssm_backward_kernel(\n    grad_output_ptr, grad_input_ptr, grad_state_ptr, matrix_ptr,\n    BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE,\n    BLOCK_SIZE: tl.constexpr\n):\n    # Get program ID\n    pid = tl.program_id(axis=0)\n    \n    # Compute batch index and sequence length\n    batch_idx = pid // SEQ_LEN\n    seq_idx = pid % SEQ_LEN\n    \n    # Compute pointer offsets\n    grad_output_offset = batch_idx * SEQ_LEN * HIDDEN_SIZE + seq_idx * HIDDEN_SIZE\n    grad_input_offset = grad_output_offset\n    grad_state_offset = batch_idx * HIDDEN_SIZE\n    \n    # Load grad output and state\n    grad_output = tl.load(grad_output_ptr + grad_output_offset)\n    grad_state = tl.load(grad_state_ptr + grad_state_offset)\n    \n    # Load diagonal matrix\n    diag_matrix = tl.load(matrix_ptr + seq_idx * HIDDEN_SIZE)\n    \n    # Compute grad input and new grad state\n    grad_input = grad_output\n    new_grad_state = grad_state * diag_matrix + grad_output\n    \n    # Store grad input and new grad state\n    tl.store(grad_input_ptr + grad_input_offset, grad_input)\n    tl.store(grad_state_ptr + grad_state_offset, new_grad_state)\n\n# Complex versions of these kernels would involve handling real and imaginary parts separately.\n\nimport torch\nfrom torch.autograd import Function\n\nclass _SSMForward(Function):\n    @staticmethod\n    def forward(ctx, input, state, matrix):\n        # Allocate output tensor\n        output = torch.empty_like(input)\n        \n        # Define grid and block sizes\n        BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE = input.shape\n        BLOCK_SIZE = 128  # Example block size\n        \n        # Launch Triton kernel\n        diag_ssm_forward_kernel[(BATCH_SIZE * SEQ_LEN,)](\n            input, output, state, matrix,\n            BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        # Save for backward\n        ctx.save_for_backward(input, state, matrix)\n        \n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, state, matrix = ctx.saved_tensors\n        \n        # Allocate gradient tensors\n        grad_input = torch.empty_like(input)\n        grad_state = torch.empty_like(state)\n        \n        # Define grid and block sizes\n        BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE = grad_output.shape\n        BLOCK_SIZE = 128  # Example block size\n        \n        # Launch Triton kernel for backward pass\n        diag_ssm_backward_kernel[(BATCH_SIZE * SEQ_LEN,)](\n            grad_output, grad_input, grad_state, matrix,\n            BATCH_SIZE, SEQ_LEN, HIDDEN_SIZE,\n            BLOCK_SIZE=BLOCK_SIZE\n        )\n        \n        return grad_input, grad_state, None  # No gradient for matrix\n\n# Example usage:\n# input = torch.randn(batch_size, seq_len, hidden_size, device='cuda')\n# state = torch.zeros(batch_size, hidden_size, device='cuda')\n# matrix = torch.randn(seq_len, hidden_size, device='cuda')\n# output = _SSMForward.apply(input, state, matrix)\n", "label": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef diag_ssm_forward_kernel(s_ptr, x_ptr, lambda_ptr, y_ptr, length,\n                            batch_size, dim, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u524d\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u5b9e\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim]\n        x_ptr: [length, batch_size, dim]\n        lambda_ptr: [dim]\n        y_ptr: [length, batch_size, dim]\n    \"\"\"\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n    s = tl.load(s_ptr + col_offsets, mask=mask, other=0)\n    Lambda = tl.load(lambda_ptr + col_offsets % dim, mask=mask, other=0)\n    for t in range(length):\n        offsets = t * batch_size * dim + col_offsets\n        x = tl.load(x_ptr + offsets, mask=mask, other=0)\n        s = s * Lambda + x\n        tl.store(y_ptr + offsets, s, mask=mask)\n\n@triton.jit\ndef diag_ssm_backward_kernel(\n        s_ptr, lambda_ptr, y_ptr, grad_s_ptr, grad_x_ptr, grad_lambda_ptr,\n        grad_y_ptr, length, batch_size, dim, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u53cd\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u5b9e\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim]\n        lambda_ptr: [dim]\n        y_ptr: [length, batch_size, dim]\n        grad_s_ptr: [batch_size, dim]\n        grad_x_ptr: [length, batch_size, dim]\n        grad_lambda_ptr: [batch_size, dim]\n        grad_y_ptr: [length, batch_size, dim]\n    \"\"\"\n\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n\n    Lambda = tl.load(lambda_ptr + col_offsets % dim, mask=mask, other=0)\n\n    # \u521d\u59cb\u5316\u68af\u5ea6\u4e3a\u96f6\n    grad_s = tl.zeros_like(Lambda)\n    grad_Lambda = tl.zeros_like(Lambda)\n\n    for i in range(length):\n        # Triton \u4e0d\u652f\u6301 range(length - 1, -1, -1)\n        t = length - 1 - i\n        offsets = t * batch_size * dim + col_offsets\n\n        grad_y = tl.load(grad_y_ptr + offsets, mask=mask, other=0)\n        if t > 0:\n            s = tl.load(\n                y_ptr + offsets - batch_size * dim, mask=mask, other=0)\n        else:\n            s = tl.load(s_ptr + col_offsets, mask=mask, other=0)\n\n        grad_s = grad_y + grad_s\n        grad_x = grad_s\n        grad_Lambda += grad_s * s\n        grad_s = grad_s * Lambda\n\n        tl.store(grad_x_ptr + offsets, grad_x, mask=mask)\n\n    tl.store(grad_s_ptr + col_offsets, grad_s, mask=mask)\n    tl.store(grad_lambda_ptr + col_offsets, grad_Lambda, mask=mask)\n\n@triton.jit\ndef diag_ssm_forward_kernel_complex(s_ptr, x_ptr, y_ptr, lambda_ptr,\n                                    length, batch_size, dim,\n                                    BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u524d\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u590d\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim, 2]\n        x_ptr: [length, batch_size, dim, 2]\n        lambda_ptr: [dim, 2]\n        y_ptr: [length, batch_size, dim, 2]\n    \"\"\"\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n\n    # \u52a0\u8f7d's'\u548c'Lambda'\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n    s_real = tl.load(s_ptr + col_offsets * 2, mask=mask, other=0)\n    s_imag = tl.load(s_ptr + col_offsets * 2 + 1, mask=mask, other=0)\n    lambda_real = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2, mask=mask, other=0)\n    lambda_imag = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2 + 1, mask=mask, other=0)\n\n    for t in range(length):\n        offsets = (t * batch_size * dim + col_offsets) * 2\n        # \u52a0\u8f7d'x'\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n        x_real = tl.load(x_ptr + offsets, mask=mask, other=0)\n        x_imag = tl.load(x_ptr + offsets + 1, mask=mask, other=0)\n\n        # \u590d\u6570\u7684\u4e58\u6cd5\u548c\u52a0\u6cd5\n        new_s_real = s_real * lambda_real - s_imag * lambda_imag + x_real\n        new_s_imag = s_real * lambda_imag + s_imag * lambda_real + x_imag\n\n        # \u5b58\u50a8\u66f4\u65b0\u540e\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n        tl.store(y_ptr + offsets, new_s_real, mask=mask)\n        tl.store(y_ptr + offsets + 1, new_s_imag, mask=mask)\n\n        # \u66f4\u65b0's'\u4ee5\u8fdb\u884c\u4e0b\u4e00\u6b21\u8fed\u4ee3\n        s_real, s_imag = new_s_real, new_s_imag\n\n@triton.jit\ndef diag_ssm_backward_kernel_complex(\n        s_ptr, lambda_ptr, y_ptr, grad_s_ptr, grad_x_ptr, grad_lambda_ptr,\n        grad_y_ptr, length, batch_size, dim, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    \u53cd\u5411\u4f20\u64ad\u6838\u51fd\u6570\uff08\u590d\u6570\u7248\u672c\uff09\n\n    \u53c2\u6570:\n        s_ptr: [batch_size, dim, 2]\n        lambda_ptr: [dim, 2]\n        y_ptr: [length, batch_size, dim, 2]\n        grad_s_ptr: [batch_size, dim, 2]\n        grad_x_ptr: [length, batch_size, dim, 2]\n        grad_lambda_ptr: [batch_size, dim, 2]\n        grad_y_ptr: [length, batch_size, dim, 2]\n    \"\"\"\n\n    # \u590d\u6570\u81ea\u5bfc\u6570\u8ba1\u7b97 \\partial f / \\partial z^*\n    # \u56e0\u6b64\u5728\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u9700\u8981\u53d6\u5171\u8f6d\n    # \u53c2\u8003\uff1ahttps://pytorch.org/docs/stable/notes/autograd.html#autograd-for-complex-numbers\n    # \u6240\u4ee5\u5728\u52a0\u8f7d/\u5b58\u50a8\u68af\u5ea6\u7684\u865a\u90e8\u65f6\uff0c\u9700\u8981\u53d6\u53cd\n\n    col_idx = tl.program_id(0) * BLOCK_SIZE\n    col_offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < batch_size * dim\n\n    # \u52a0\u8f7d'Lambda'\u7684\u5b9e\u90e8\u548c\u865a\u90e8\n    lambda_real = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2, mask=mask, other=0)\n    lambda_imag = tl.load(\n        lambda_ptr + (col_offsets % dim) * 2 + 1, mask=mask, other=0)\n\n    # \u521d\u59cb\u5316\u68af\u5ea6\u4e3a\u96f6\n    grad_s_real = tl.zeros_like(lambda_real)\n    grad_s_imag = tl.zeros_like(lambda_imag)\n    grad_lambda_real = tl.zeros_like(lambda_real)\n    grad_lambda_imag = tl.zeros_like(lambda_imag)\n\n    for i in range(length):\n        # Triton \u4e0d\u652f\u6301 range(length - 1, -1, -1)\n        t = length - 1 - i\n        offsets = (t * batch_size * dim + col_offsets) * 2\n\n        grad_y_real = tl.load(grad_y_ptr + offsets, mask=mask, other=0)\n        grad_y_imag = -tl.load(\n            grad_y_ptr + offsets + 1, mask=mask, other=0)\n        if t > 0:\n            s_real = tl.load(\n                y_ptr + offsets - 2 * batch_size * dim, mask=mask, other=0)\n            s_imag = tl.load(\n                y_ptr + offsets - 2 * batch_size * dim + 1,\n                mask=mask,\n                other=0)\n        else:\n            s_real = tl.load(s_ptr + 2 * col_offsets, mask=mask, other=0)\n            s_imag = tl.load(\n                s_ptr + 2 * col_offsets + 1, mask=mask, other=0)\n\n        grad_s_real = grad_y_real + grad_s_real\n        grad_s_imag = grad_y_imag + grad_s_imag\n        grad_x_real = grad_s_real\n        grad_x_imag = grad_s_imag\n        grad_lambda_real += grad_s_real * s_real - grad_s_imag * s_imag\n        grad_lambda_imag += grad_s_real * s_imag + grad_s_imag * s_real\n        grad_s_real = grad_x_real * lambda_real - grad_x_imag * lambda_imag\n        grad_s_imag = grad_x_real * lambda_imag + grad_x_imag * lambda_real\n\n        tl.store(grad_x_ptr + offsets, grad_x_real, mask=mask)\n        tl.store(grad_x_ptr + offsets + 1, -grad_x_imag, mask=mask)\n\n    # \u5b58\u50a8\u6700\u7ec8\u7684\u68af\u5ea6\n    tl.store(grad_s_ptr + col_offsets * 2, grad_s_real, mask=mask)\n    tl.store(grad_s_ptr + col_offsets * 2 + 1, -grad_s_imag, mask=mask)\n    tl.store(\n        grad_lambda_ptr + col_offsets * 2, grad_lambda_real, mask=mask)\n    tl.store(\n        grad_lambda_ptr + col_offsets * 2 + 1,\n        -grad_lambda_imag,\n        mask=mask)\n\nclass _ssm_forward(torch.autograd.Function):\n    # TODO \u4f7f\u7528 @triton.autotune \u9009\u62e9\u6700\u4f73\u7684 BLOCK_SIZE\n    # \u5bf9\u4e8e3090\uff0cBLOCK_SIZE = 128\u4f3c\u4e4e\u6548\u679c\u826f\u597d\n    BLOCK_SIZE = 128\n\n    @staticmethod\n    def forward(ctx, s, x, Lambda):\n        assert s.is_contiguous() and x.is_contiguous() and Lambda.is_contiguous()\n        length, batch_size, dim = x.shape\n        n = batch_size * dim\n        y = torch.zeros_like(x)\n        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']), )\n\n        if Lambda.dtype == torch.complex64:\n            # \u786e\u4fdds\u548cx\u662f\u590d\u6570\u5f20\u91cf\n            if not torch.is_complex(s):\n                raise ValueError(\"\u5f53Lambda\u4e3a\u590d\u6570\u65f6\uff0cs\u5fc5\u987b\u662f\u590d\u6570\u5f20\u91cf\")\n            if not torch.is_complex(x):\n                raise ValueError(\"\u5f53Lambda\u4e3a\u590d\u6570\u65f6\uff0cx\u5fc5\u987b\u662f\u590d\u6570\u5f20\u91cf\")\n            diag_ssm_forward_kernel_complex[grid](\n                torch.view_as_real(s), torch.view_as_real(x),\n                torch.view_as_real(y), torch.view_as_real(Lambda), length,\n                batch_size, dim, _ssm_forward.BLOCK_SIZE)\n        elif Lambda.dtype.is_floating_point:\n            diag_ssm_forward_kernel[grid](s, x, Lambda, y, length,\n                                          batch_size, dim,\n                                          _ssm_forward.BLOCK_SIZE)\n        else:\n            raise ValueError(\"\u4e0d\u652f\u6301\u7684 dtype: %s\" % Lambda.dtype)\n        ctx.save_for_backward(s, y, Lambda)\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_y):\n        s, y, Lambda = ctx.saved_tensors\n        length, batch_size, dim = y.shape\n        grad_y = grad_y.contiguous()\n        n = batch_size * dim\n        grad_s = torch.empty_like(s)\n        grad_x = torch.empty_like(grad_y)\n        # grad_lambda \u5b58\u50a8\u6bcf\u4e2a\u6279\u6b21\u4e2d Lambda \u7684\u68af\u5ea6\n        # \u6211\u4eec\u5c06\u5728\u5185\u6838\u5b8c\u6210\u540e\u8fdb\u884c\u6c42\u548c\n        grad_lambda = torch.empty_like(s)\n        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']), )\n        if Lambda.dtype == torch.complex64:\n            diag_ssm_backward_kernel_complex[grid](\n                torch.view_as_real(s), torch.view_as_real(Lambda),\n                torch.view_as_real(y), torch.view_as_real(grad_s),\n                torch.view_as_real(grad_x),\n                torch.view_as_real(grad_lambda),\n                torch.view_as_real(grad_y), length, batch_size, dim,\n                _ssm_forward.BLOCK_SIZE)\n        else:\n            diag_ssm_backward_kernel[grid](\n                s, Lambda, y, grad_s, grad_x, grad_lambda, grad_y, length,\n                batch_size, dim, _ssm_forward.BLOCK_SIZE)\n        return grad_s, grad_x, grad_lambda.sum(dim=0)\n\ndiag_ssm_forward_triton = _ssm_forward.apply\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThis Triton kernel `_quantize_global` is designed to perform global quantization on a CUDA tensor `x`. It takes input tensor `x`, computes its inverse maximum absolute value (`absmax_inv`), and uses this to scale the values of `x` into an 8-bit integer range, storing the results in the `output` tensor. The main parameters are the pointers `x_ptr`, `absmax_inv_ptr`, and `output_ptr` which point to the input, the scaling factor, and the output memory respectively. `n_elements` denotes the number of elements to process, and `BLOCK_SIZE` is a compile-time constant that dictates the number of elements each block handles. The kernel reads a block of elements, applies the quantization formula, and writes the results back. The `quantize_global` function orchestrates this by preparing necessary inputs, launching the kernel with the appropriate grid size, and finally returning the quantized output along with the maximum absolute value `absmax`.\n", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Define the Triton kernel\n@triton.jit\ndef _quantize_global(x_ptr, absmax_inv_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    # Define the block index\n    pid = tl.program_id(axis=0)\n    \n    # Calculate the start position for this block\n    block_start = pid * BLOCK_SIZE\n    \n    # Load the input data\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    \n    # Load the inverse of the maximum absolute value\n    absmax_inv = tl.load(absmax_inv_ptr)\n    \n    # Quantize the data\n    quantized = x * absmax_inv * 127.0\n    quantized = tl.clamp(quantized, -127.0, 127.0)\n    \n    # Convert to int8\n    quantized_int8 = quantized.to(tl.int8)\n    \n    # Store the result\n    tl.store(output_ptr + offsets, quantized_int8, mask=mask)\n\n# Define the wrapper function\ndef quantize_global(x, BLOCK_SIZE=1024):\n    # Ensure input is a CUDA tensor\n    assert x.is_cuda, \"Input tensor must be on CUDA\"\n    \n    # Compute the maximum absolute value\n    absmax = torch.max(torch.abs(x))\n    \n    # Compute the inverse of the maximum absolute value\n    absmax_inv = 1.0 / absmax if absmax != 0 else 0.0\n    \n    # Prepare output tensor\n    output = torch.empty_like(x, dtype=torch.int8)\n    \n    # Launch the Triton kernel\n    n_elements = x.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    \n    _quantize_global[grid](x, absmax_inv, output, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n    \n    return output, absmax\n\n# Example usage\nx = torch.randn(10000, device='cuda')\nquantized_output, absmax = quantize_global(x)\nprint(quantized_output)\nprint(absmax)\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n# global quantize\n@triton.autotune(\n    configs=[\n        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=4),\n        triton.Config({\"BLOCK_SIZE\": 2048}, num_stages=1),\n    ],\n    key=[\"n_elements\"],\n)\n@triton.jit\ndef _quantize_global(\n    x_ptr,\n    absmax_inv_ptr,\n    output_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    absmax_inv = tl.load(absmax_inv_ptr)\n    output = tl.extra.cuda.libdevice.llrint(127.0 * (x * absmax_inv))\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef quantize_global(x: torch.Tensor):\n    absmax = x.abs().max().unsqueeze(0)\n    absmax_inv = 1.0 / absmax\n    output = torch.empty(*x.shape, device=\"cuda\", dtype=torch.int8)\n    assert x.is_cuda and output.is_cuda\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n    _quantize_global[grid](x, absmax_inv, output, n_elements)\n    return output, absmax\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The `_l2_norm_fwd_1pass_kernel` is a Triton kernel function designed to compute the L2 normalization of input tensor `X`. It reads the data in blocks of size `BLOCK_N`, computes the variance, and applies the normalization. The output is stored in tensor `Y`. It takes several inputs including pointers to input and output data, stride for the row (`stride_x_row`), the number of columns (`N`), a small epsilon value (`eps`), and a block size (`BLOCK_N`). The function iterates over the rows using the program ID (`row`), loads the relevant section of `X`, calculates the L2 norm, and stores the result in `Y`.\n\n            The `_l2_norm_fwd` function serves as a wrapper for the kernel. It ensures that `x` is in a contiguous format if needed, prepares an output tensor `y`, and checks constraints on the maximum allowable block size. The function then launches the Triton kernel with appropriate arguments and returns the result in the original shape of `x`.\n            ", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(X_ptr, Y_ptr, stride_x_row, N, eps, BLOCK_N, **meta):\n    # Get the row index for this program instance\n    row = tl.program_id(0)\n\n    # Compute the start index for this row in the input and output tensors\n    offset = row * stride_x_row\n\n    # Initialize accumulators for variance calculation\n    sum_sq = tl.zeros((BLOCK_N,), dtype=tl.float32)\n\n    # Iterate over the columns in blocks of BLOCK_N\n    for start in range(0, N, BLOCK_N):\n        # Load a block of data from X\n        block_offsets = tl.arange(0, BLOCK_N) + start\n        mask = block_offsets < N  # Mask to handle boundaries\n        x = tl.load(X_ptr + offset + block_offsets, mask=mask, other=0.0)\n\n        # Accumulate the sum of squares\n        sum_sq += x * x\n\n    # Compute the L2 norm\n    norm = tl.sqrt(sum_sq + eps)\n\n    # Normalize the input data and store in Y\n    for start in range(0, N, BLOCK_N):\n        block_offsets = tl.arange(0, BLOCK_N) + start\n        mask = block_offsets < N\n        x = tl.load(X_ptr + offset + block_offsets, mask=mask, other=0.0)\n        y = x / norm\n        tl.store(Y_ptr + offset + block_offsets, y, mask=mask)\n\ndef _l2_norm_fwd(x, eps=1e-5, BLOCK_N=128):\n    # Ensure input is contiguous\n    if not x.is_contiguous():\n        x = x.contiguous()\n\n    # Get input dimensions\n    num_rows, num_cols = x.shape\n\n    # Prepare output tensor\n    y = torch.empty_like(x)\n\n    # Launch the Triton kernel\n    grid = (num_rows,)  # One program instance per row\n    stride_x_row = x.stride(0)\n\n    _l2_norm_fwd_1pass_kernel[grid](\n        x, y, stride_x_row, num_cols, eps, BLOCK_N,\n        num_warps=4,  # Set an appropriate number of warps\n        num_stages=2  # Set an appropriate number of stages\n    )\n\n    return y\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(\n    X,  # pointer to the input\n    Y,  # pointer to the output\n    stride_x_row,  # how much to increase the pointer when moving by 1 row\n    N,  # number of columns in X\n    eps,  # epsilon to avoid division by zero\n    BLOCK_N: tl.constexpr,\n):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) \n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    y = x * rstd\n    tl.store(Y + cols, y, mask=mask)\n\ndef _l2_norm_fwd(\n    x, eps=1e-6\n):\n    x_shape_og = x.shape\n    x = x.reshape(-1, x.shape[-1])\n    if x.stride(-1) != 1:\n        x = x.contiguous()\n    y = torch.empty_like(x)\n    N = x.shape[-1]\n    M = x.shape[0]\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    if N > BLOCK_N:\n        raise RuntimeError(\n            \"This layer norm doesn't support feature dim >= 64KB.\")\n    with torch.cuda.device(x.device.index):\n        _l2_norm_fwd_1pass_kernel[(M,)](\n            x,\n            y,\n            x.stride(0),\n            N,\n            eps,\n            BLOCK_N,\n        )\n    return y.reshape(x_shape_og)\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThe Triton kernels perform a specialized matrix multiplication with RMS normalization and optional rotary embeddings. The main kernel `rms_matmul_rbe` takes in input matrix pointers and performs the operation c = (rms(x) * rms_w) @ w, where rms denotes root mean square normalization. It supports operations in different precisions (FP16 and FP8) and has an optional rotary embeddings epilogue. The wrapper `rms_matmul_rbe_qkv_wrapper` prepares and launches kernels for processing Q, K, V matrices for attention mechanisms.\n", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef rms_matmul_rbe(x_ptr, w_ptr, c_ptr, rms_w_ptr, rotary_ptr, \n                   M, N, K, stride_xm, stride_xk, stride_wk, stride_wn, \n                   stride_cm, stride_cn, apply_rotary, BLOCK_SIZE: tl.constexpr):\n    # Define the block indices\n    pid = tl.program_id(axis=0)\n    m_offset = pid * BLOCK_SIZE\n\n    # Offsets for x, w, and c\n    x_offset = m_offset * stride_xm\n    c_offset = m_offset * stride_cm\n\n    # Load the block of x\n    x = tl.load(x_ptr + x_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_xk, mask=tl.arange(0, BLOCK_SIZE)[:, None] < M - m_offset, other=0.0)\n\n    # Compute RMS normalization for x\n    rms_x = tl.sqrt(tl.sum(x * x, axis=1) / K)\n\n    # Load the block of w\n    w = tl.load(w_ptr + tl.arange(0, BLOCK_SIZE)[:, None] * stride_wn, mask=tl.arange(0, BLOCK_SIZE)[:, None] < N, other=0.0)\n\n    # Compute RMS normalization for w\n    rms_w = tl.load(rms_w_ptr + tl.arange(0, BLOCK_SIZE), mask=tl.arange(0, BLOCK_SIZE) < N, other=1.0)\n\n    # Optionally apply rotary embeddings\n    if apply_rotary:\n        rotary = tl.load(rotary_ptr + tl.arange(0, BLOCK_SIZE)[:, None], mask=tl.arange(0, BLOCK_SIZE)[:, None] < N, other=0.0)\n        x = x * rotary\n\n    # Perform matrix multiplication with RMS normalization\n    x_normalized = x / rms_x[:, None]\n    w_normalized = w * rms_w[None, :]\n    c = tl.dot(x_normalized, w_normalized)\n\n    # Store the result\n    tl.store(c_ptr + c_offset + tl.arange(0, BLOCK_SIZE)[:, None] * stride_cn, c, mask=tl.arange(0, BLOCK_SIZE)[:, None] < M - m_offset)\n\n\ndef rms_matmul_rbe_qkv_wrapper(Q, K, V, W_q, W_k, W_v, rms_w_q, rms_w_k, rms_w_v, rotary_embeddings=None, apply_rotary=False):\n    # Extract dimensions\n    M, K = Q.shape\n    _, N = W_q.shape\n\n    # Define strides\n    stride_qm, stride_qk = Q.stride()\n    stride_wqk, stride_wqn = W_q.stride()\n    stride_cm, stride_cn = M, N\n\n    # Allocate output\n    C_q = torch.empty((M, N), device=Q.device, dtype=Q.dtype)\n    C_k = torch.empty((M, N), device=K.device, dtype=K.dtype)\n    C_v = torch.empty((M, N), device=V.device, dtype=V.dtype)\n\n    # Launch kernel for Q\n    triton.launch(kernel=rms_matmul_rbe, grid=(M // BLOCK_SIZE,),\n                  args=[Q, W_q, C_q, rms_w_q, rotary_embeddings, M, N, K, stride_qm, stride_qk, stride_wqk, stride_wqn, stride_cm, stride_cn, apply_rotary],\n                  num_warps=4, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Launch kernel for K\n    triton.launch(kernel=rms_matmul_rbe, grid=(M // BLOCK_SIZE,),\n                  args=[K, W_k, C_k, rms_w_k, rotary_embeddings, M, N, K, stride_qm, stride_qk, stride_wqk, stride_wqn, stride_cm, stride_cn, apply_rotary],\n                  num_warps=4, BLOCK_SIZE=BLOCK_SIZE)\n\n    # Launch kernel for V\n    triton.launch(kernel=rms_matmul_rbe, grid=(M // BLOCK_SIZE,),\n                  args=[V, W_v, C_v, rms_w_v, rotary_embeddings, M, N, K, stride_qm, stride_qk, stride_wqk, stride_wqn, stride_cm, stride_cn, apply_rotary],\n                  num_warps=4, BLOCK_SIZE=BLOCK_SIZE)\n\n    return C_q, C_k, C_v\n", "label": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rms_matmul_rbe(\n        x_ptr, w_ptr, rms_w_ptr, out_ptr,\n        M, N, K,\n        stride_x_batch, stride_x_m, stride_x_k,\n        stride_w_k, stride_w_n,\n        stride_rms_w,\n        stride_out_batch, stride_out_m, stride_out_n,\n        start_token_position,\n        USE_FP8: tl.constexpr,\n        RBE_EPILOGUE: tl.constexpr,\n        THETA: tl.constexpr,\n        EPS: tl.constexpr,\n        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\n    Prologue: RMS\n    Epilogue: nothing or Rotary embeddings\n    c = ROBE((rms(a) * rms_w) @ b)\n    \"\"\"\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + offs_m[:, None] * stride_x_m + offs_k[None, :] * stride_x_k)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_w_k + offs_n[None, :] * stride_w_n)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n    x_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        x = tl.load(x_ptrs)\n        x_sum += tl.extra.cuda.libdevice.pow(x.to(tl.float32), 2)\n        rms_w = tl.load(rms_w_ptrs)  # TODO add an assert that rms_w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n            rms_w = rms_w.to(tl.float16)\n        x = x * rms_w\n        w = tl.load(w_ptrs)  # TODO add an assert that w is a multiple of BLOCK SIZE K\n        if USE_FP8:\n            w = w.to(tl.float8e5, bitcast=True)\n            w = w.to(tl.float32)\n            w = w.to(tl.float16)\n        accumulator += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x_k\n        w_ptrs += BLOCK_SIZE_K * stride_w_k\n        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n    x_mean = tl.sum(x_sum, axis=1) / K + EPS\n    x_norm = tl.math.rsqrt(x_mean)\n    accumulator = accumulator * x_norm[:, None]\n\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (\n                pid_batch * stride_out_batch + offs_m[:, None] * stride_out_m + offs_n[None, :] * stride_out_n)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n\n\n@triton.jit\ndef rms_matmul_rbe_qkv(x_ptr,\n                       q_weight_ptr, k_weight_ptr, v_weight_ptr,\n                       rms_w_ptr,\n                       q_ptr, k_ptr, v_ptr,\n                       M, N, K,\n                       stride_x_batch, stride_x_m, stride_x_k,\n                       stride_q_w_k, stride_q_w_n,\n                       stride_k_w_k, stride_k_w_n,\n                       stride_v_w_k, stride_v_w_n,\n                       stride_rms_w,\n                       stride_q_batch, stride_q_m, stride_q_n,\n                       stride_k_batch, stride_k_m, stride_k_n,\n                       stride_v_batch, stride_v_m, stride_v_n,\n                       start_token_position,\n                       USE_FP8: tl.constexpr,\n                       THETA: tl.constexpr,\n                       EPS: tl.constexpr,\n                       BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):\n    # q\n    rms_matmul_rbe(\n        x_ptr=x_ptr,\n        w_ptr=q_weight_ptr, rms_w_ptr=rms_w_ptr, out_ptr=q_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=stride_x_k,\n        stride_w_k=stride_q_w_k, stride_w_n=stride_q_w_n,\n        stride_rms_w=stride_rms_w,\n        stride_out_batch=stride_q_batch, stride_out_m=stride_q_m, stride_out_n=stride_q_n,\n        start_token_position=start_token_position,\n        USE_FP8=USE_FP8,\n        RBE_EPILOGUE=True,\n        THETA=THETA,\n        EPS=EPS,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    # k\n    rms_matmul_rbe(\n        x_ptr=x_ptr,\n        w_ptr=k_weight_ptr, rms_w_ptr=rms_w_ptr, out_ptr=k_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=stride_x_k,\n        stride_w_k=stride_k_w_k, stride_w_n=stride_k_w_n,\n        stride_rms_w=stride_rms_w,\n        stride_out_batch=stride_k_batch, stride_out_m=stride_k_m, stride_out_n=stride_k_n,\n        start_token_position=start_token_position,\n        USE_FP8=USE_FP8,\n        RBE_EPILOGUE=True,\n        THETA=THETA,\n        EPS=EPS,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n    # v\n    rms_matmul_rbe(\n        x_ptr=x_ptr,\n        w_ptr=v_weight_ptr, rms_w_ptr=rms_w_ptr, out_ptr=v_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=stride_x_k,\n        stride_w_k=stride_v_w_k, stride_w_n=stride_v_w_n,\n        stride_rms_w=stride_rms_w,\n        stride_out_batch=stride_v_batch, stride_out_m=stride_v_m, stride_out_n=stride_v_n,\n        start_token_position=start_token_position,\n        USE_FP8=USE_FP8,\n        RBE_EPILOGUE=False,\n        THETA=THETA,\n        EPS=EPS,\n        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n\ndef rms_matmul_rbe_qkv_wrapper(x: torch.Tensor,\n                               start_pos: int,\n                               q_weight: torch.Tensor, k_weight: torch.Tensor, v_weight: torch.Tensor,\n                               rms_w: torch.Tensor,\n                               n_heads: int, head_dim: int,\n                               k: torch.Tensor,\n                               v: torch.Tensor,\n                               eps: float = 1e-6, theta=10000.):\n    assert q_weight.shape == k_weight.shape == v_weight.shape\n    assert q_weight.dtype == k_weight.dtype == v_weight.dtype == rms_w.dtype\n    assert q_weight.dtype in [torch.float16, torch.int8]\n    batch, M, K = x.shape\n\n    assert K == rms_w.shape[0]\n\n    q_weight_t = q_weight.t()\n    k_weight_t = k_weight.t()\n    v_weight_t = v_weight.t()\n    K_W, N = q_weight_t.shape\n    assert K == K_W\n    q = torch.empty((batch, M, N), dtype=torch.float16, device=q_weight_t.device)\n\n    k = k.view((batch, M, N))\n    v = v.view((batch, M, N))\n    assert k.dtype == k_weight.dtype\n    assert v.dtype == v_weight.dtype\n\n    q_ptr = triton.reinterpret(q, tl.float16)\n    k_ptr = triton.reinterpret(k, tl.float8e5 if k.dtype == torch.int8 else tl.float16)\n    v_ptr = triton.reinterpret(v, tl.float8e5 if v.dtype == torch.int8 else tl.float16)\n\n    grid = lambda META: (\n    batch, triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]))\n\n    rms_matmul_rbe_qkv[grid](\n        x_ptr=x,\n        q_weight_ptr=q_weight_t, k_weight_ptr=k_weight_t, v_weight_ptr=v_weight_t,\n        rms_w_ptr=rms_w,\n        q_ptr=q_ptr, k_ptr=k_ptr, v_ptr=v_ptr,\n        M=M, N=N, K=K,\n        stride_x_batch=x.stride(0), stride_x_m=x.stride(1), stride_x_k=x.stride(2),\n        stride_q_w_k=q_weight_t.stride(0), stride_q_w_n=q_weight_t.stride(1),\n        stride_k_w_k=k_weight_t.stride(0), stride_k_w_n=k_weight_t.stride(1),\n        stride_v_w_k=v_weight_t.stride(0), stride_v_w_n=v_weight_t.stride(1),\n        stride_rms_w=rms_w.stride(0),\n        stride_q_batch=q.stride(0), stride_q_m=q.stride(1), stride_q_n=q.stride(2),\n        stride_k_batch=k.stride(0), stride_k_m=k.stride(1), stride_k_n=k.stride(2),\n        stride_v_batch=v.stride(0), stride_v_m=v.stride(1), stride_v_n=v.stride(2),\n        start_token_position=start_pos,\n        USE_FP8=q_weight.dtype == torch.int8,\n        THETA=theta,\n        EPS=eps,\n        BLOCK_SIZE_M=16, BLOCK_SIZE_N=64, BLOCK_SIZE_K=64,\n        num_stages=4, num_warps=4\n    )\n    q = q.view(batch, M, n_heads, head_dim)\n    k = k.view(batch, M, n_heads, head_dim)\n    v = v.view(batch, M, n_heads, head_dim)\n    return q, k, v\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n    The code defines a Triton kernel for a fused recurrent hierarchical gated recurrent network (HGRN) and its PyTorch wrapper. The `fused_recurrent_hgrn_fwd_kernel` performs forward computation on input tensors `x` and `g`, with optional initial state `h0`, and stores the output in tensor `o`. It computes `o_t = g_t * h_t + x_t` iteratively over time `T`. The `fused_recurrent_hgrn_bwd_kernel` handles the backward pass, calculating gradients for `x` and `g`. The `FusedRecurrentHGRNFunction` is a PyTorch `Function` that wraps these kernels for autograd compatibility. The function `fused_recurrent_hgrn` provides an easy-to-use interface for the operation, taking tensors `x`, `g`, and an optional `initial_state`, and returning the result and optionally the final state.\n    ", "predict": "import triton\nimport triton.language as tl\n\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(x_ptr, g_ptr, h0_ptr, o_ptr, T, N, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    \n    # Load initial state\n    h_t = tl.load(h0_ptr + offset, mask=offset < N, other=0.0)\n    \n    for t in range(T):\n        # Compute the index for time step t\n        x_idx = t * N + offset\n        g_idx = t * N + offset\n        \n        # Load x_t and g_t\n        x_t = tl.load(x_ptr + x_idx, mask=offset < N, other=0.0)\n        g_t = tl.load(g_ptr + g_idx, mask=offset < N, other=0.0)\n        \n        # Compute h_t and o_t\n        h_t = g_t * h_t + x_t\n        tl.store(o_ptr + x_idx, h_t, mask=offset < N)\n\n#### Backward Kernel\n\n\nimport torch\nfrom torch.autograd import Function\n\nclass FusedRecurrentHGRNFunction(Function):\n    @staticmethod\n    def forward(ctx, x, g, h0=None):\n        # Allocate output tensor\n        o = torch.empty_like(x)\n        \n        # Get dimensions\n        T, N = x.shape\n        \n        # Launch forward kernel\n        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n        fused_recurrent_hgrn_fwd_kernel[grid](x, g, h0, o, T, N, BLOCK_SIZE=1024)\n        \n        # Save tensors for backward\n        ctx.save_for_backward(x, g, h0, o)\n        return o\n\n    @staticmethod\n    def backward(ctx, grad_o):\n        x, g, h0, o = ctx.saved_tensors\n        \n        # Allocate gradient tensors\n        grad_x = torch.empty_like(x)\n        grad_g = torch.empty_like(g)\n        \n        # Get dimensions\n        T, N = x.shape\n        \n        # Launch backward kernel\n        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)\n        fused_recurrent_hgrn_bwd_kernel[grid](grad_o, x, g, grad_x, grad_g, h0, T, N, BLOCK_SIZE=1024)\n        \n        return grad_x, grad_g, None\n\ndef fused_recurrent_hgrn(x, g, initial_state=None):\n    return FusedRecurrentHGRNFunction.apply(x, g, initial_state)\n\n### Usage\n\nYou can now use `fused_recurrent_hgrn` in your PyTorch models. Here is an example:\n\n", "label": "\nimport torch\nimport triton\nimport triton.language as tl\nfrom typing import Tuple\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BD': 32}, num_warps=1),\n        triton.Config({'BD': 32}, num_warps=2),\n        triton.Config({'BD': 32}, num_warps=4),\n        triton.Config({'BD': 32}, num_warps=8),\n        triton.Config({'BD': 64}, num_warps=1),\n        triton.Config({'BD': 64}, num_warps=2),\n        triton.Config({'BD': 64}, num_warps=4),\n        triton.Config({'BD': 64}, num_warps=8),\n        triton.Config({'BD': 128}, num_warps=1),\n        triton.Config({'BD': 128}, num_warps=2),\n        triton.Config({'BD': 128}, num_warps=4),\n        triton.Config({'BD': 128}, num_warps=8),\n    ],\n    key=['D']\n)\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(\n    x,\n    g,\n    o,\n    h0,\n    ht,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    STORE_FINAL_STATE: tl.constexpr\n):\n    i_d, i_bh = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_x = x + i_bh * T * D + o_d\n    p_g = g + i_bh * T * D + o_d\n    p_o = o + i_bh * T * D + o_d\n\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_h = b_g * b_h + b_x\n        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)\n\n        p_x += D\n        p_g += D\n        p_o += D\n\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * D + o_d\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BD': 32}, num_warps=1),\n        triton.Config({'BD': 32}, num_warps=2),\n        triton.Config({'BD': 32}, num_warps=4),\n        triton.Config({'BD': 32}, num_warps=8),\n        triton.Config({'BD': 64}, num_warps=1),\n        triton.Config({'BD': 64}, num_warps=2),\n        triton.Config({'BD': 64}, num_warps=4),\n        triton.Config({'BD': 64}, num_warps=8),\n        triton.Config({'BD': 128}, num_warps=1),\n        triton.Config({'BD': 128}, num_warps=2),\n        triton.Config({'BD': 128}, num_warps=4),\n        triton.Config({'BD': 128}, num_warps=8),\n    ],\n    key=['D']\n)\n@triton.jit\ndef fused_recurrent_hgrn_bwd_kernel(\n    g,\n    o,\n    dx,\n    dg,\n    do,\n    h0,\n    T: tl.constexpr,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr\n):\n    i_d, i_bh = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n\n    p_g = g + (i_bh * T + T - 1) * D + o_d\n    p_o = o + (i_bh * T + T - 2) * D + o_d\n    p_dx = dx + (i_bh * T + T - 1) * D + o_d\n    p_dg = dg + (i_bh * T + T - 1) * D + o_d\n    p_do = do + (i_bh * T + T - 1) * D + o_d\n\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_bh * D + o_d, mask=mask, other=0).to(tl.float32)\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dg = b_dh * b_o\n        b_dh = b_dh * b_g\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)\n\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D\n\n\nclass FusedRecurrentHGRNFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, g, initial_state=None, output_final_state=False):\n        B, H, T, D = x.shape\n\n        final_state = None\n        if output_final_state:\n            final_state = x.new_empty(B, H, D)\n\n        o = torch.empty_like(x)\n        def grid(meta): return (triton.cdiv(D, meta['BD']), B * H)\n        fused_recurrent_hgrn_fwd_kernel[grid](\n            x, g, o, initial_state, final_state,\n            T, D,\n            USE_INITIAL_STATE=initial_state is not None,\n            STORE_FINAL_STATE=final_state is not None\n        )\n        ctx.save_for_backward(g, o, initial_state)\n        return o, final_state\n\n    @staticmethod\n    def backward(ctx, do, dht=None):\n        g, o, initial_state = ctx.saved_tensors\n        B, H, T, D = do.shape\n\n        dx = torch.empty_like(o)\n        dg = torch.empty_like(g)\n        def grid(meta): return (triton.cdiv(D, meta['BD']), B * H)\n        fused_recurrent_hgrn_bwd_kernel[grid](\n            g, o, dx, dg, do, initial_state,\n            T, D,\n            USE_INITIAL_STATE=initial_state is not None,\n        )\n\n        return dx, dg, None, None\n\n\ndef fused_recurrent_hgrn(\n    x: torch.Tensor,\n    g: torch.Tensor,\n    initial_state: torch.Tensor = None,\n    output_final_state: bool = False\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    if initial_state is not None:\n        initial_state = initial_state.detach()\n    o, final_state = FusedRecurrentHGRNFunction.apply(x, g, initial_state, output_final_state)\n    return o, final_state\n\n\n\n\n"}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\nThis Triton code defines a custom softmax operation for GPU acceleration using Triton language. There are forward and backward kernel functions for both 'inner' and 'non-inner' cases based on dimensions.\n\nKey functions:\n- `softmax_kernel_non_inner`: Performs softmax for non-inner dimensions. It computes the softmax on selected tiles and is optimized by heuristically determined tile sizes (TILE_K, TILE_N) and the number of warps.\n- `softmax_kernel_inner`: Optimized for inner dimensions, handling data directly within each CTA (cooperative thread array). Operates on M (outer dimension) and N (inner dimension) sizes.\n- `softmax_backward_kernel_non_inner` & `softmax_backward_kernel_inner`: These functions compute gradients for the forward pass, again considering different dimensions.\n\nHelper functions (`heur_`) compute optimal configurations for execution based on input matrix dimensions M, N, K, and return TILE sizes or CTA (cooperative thread array) strategies.\n\nThe `Softmax` class wraps these kernels in an autograd-compatible format for use in PyTorch with a static forward and backward method. The forward method computes the softmax and saves the result for backward calculations.\n\nMain input parameters:\n- `output_ptr`, `input_ptr`: Device pointers for reading/writing data.\n- `M`, `N`, `K`: Dimensions for data partitioning.\n- `TILE_N`, `TILE_K`, `ONE_TILE_PER_CTA`: Compile-time constants determined by heuristics.\n\nThe softmax operation is efficiently performed by splitting the task into parallel segments processed by Triton kernels, benefiting from GPU parallelism.\n\n", "predict": "import triton\nimport triton.language as tl\nimport torch\n\n# Helper function to determine optimal tile sizes and warp count\ndef heur_optimal_tile_sizes(M, N, K):\n    TILE_N = 128 if N > 128 else N\n    TILE_K = 32 if K > 32 else K\n    num_warps = 4 if N > 128 else 1\n    return TILE_N, TILE_K, num_warps\n\n# Kernel for softmax on non-inner dimensions\n@triton.jit\ndef softmax_kernel_non_inner(output_ptr, input_ptr, M, N, K, TILE_N: tl.constexpr, TILE_K: tl.constexpr, num_warps: tl.constexpr):\n    pid = tl.program_id(0)\n    # Compute start and end indices for this program\n    row_start = pid * TILE_N\n    row_end = min(row_start + TILE_N, N)\n    # Initialize pointers for input and output\n    input_ptrs = input_ptr + row_start\n    output_ptrs = output_ptr + row_start\n    # Load data\n    data = tl.load(input_ptrs)\n    # Compute softmax\n    max_val = tl.max(data, axis=0)\n    data = data - max_val\n    exp_data = tl.exp(data)\n    sum_exp = tl.sum(exp_data, axis=0)\n    softmax_result = exp_data / sum_exp\n    # Store results\n    tl.store(output_ptrs, softmax_result)\n\n# Kernel for softmax on inner dimensions\n@triton.jit\ndef softmax_kernel_inner(output_ptr, input_ptr, M, N, TILE_N: tl.constexpr, num_warps: tl.constexpr):\n    pid = tl.program_id(0)\n    # Compute start and end indices for this program\n    col_start = pid * TILE_N\n    col_end = min(col_start + TILE_N, N)\n    # Initialize pointers for input and output\n    input_ptrs = input_ptr + col_start\n    output_ptrs = output_ptr + col_start\n    # Load data\n    data = tl.load(input_ptrs)\n    # Compute softmax\n    max_val = tl.max(data, axis=1)\n    data = data - max_val\n    exp_data = tl.exp(data)\n    sum_exp = tl.sum(exp_data, axis=1)\n    softmax_result = exp_data / sum_exp\n    # Store results\n    tl.store(output_ptrs, softmax_result)\n\n# Kernel for softmax backward pass on non-inner dimensions\n@triton.jit\ndef softmax_backward_kernel_non_inner(grad_output_ptr, grad_input_ptr, softmax_output_ptr, M, N, K, TILE_N: tl.constexpr, TILE_K: tl.constexpr, num_warps: tl.constexpr):\n    pid = tl.program_id(0)\n    # Compute start and end indices for this program\n    row_start = pid * TILE_N\n    row_end = min(row_start + TILE_N, N)\n    # Initialize pointers\n    grad_output_ptrs = grad_output_ptr + row_start\n    grad_input_ptrs = grad_input_ptr + row_start\n    softmax_output_ptrs = softmax_output_ptr + row_start\n    # Load data\n    grad_output = tl.load(grad_output_ptrs)\n    softmax_output = tl.load(softmax_output_ptrs)\n    # Compute gradient\n    dot_product = tl.sum(grad_output * softmax_output, axis=0)\n    grad_input = softmax_output * (grad_output - dot_product)\n    # Store results\n    tl.store(grad_input_ptrs, grad_input)\n\n# Kernel for softmax backward pass on inner dimensions\n@triton.jit\ndef softmax_backward_kernel_inner(grad_output_ptr, grad_input_ptr, softmax_output_ptr, M, N, TILE_N: tl.constexpr, num_warps: tl.constexpr):\n    pid = tl.program_id(0)\n    # Compute start and end indices for this program\n    col_start = pid * TILE_N\n    col_end = min(col_start + TILE_N, N)\n    # Initialize pointers\n    grad_output_ptrs = grad_output_ptr + col_start\n    grad_input_ptrs = grad_input_ptr + col_start\n    softmax_output_ptrs = softmax_output_ptr + col_start\n    # Load data\n    grad_output = tl.load(grad_output_ptrs)\n    softmax_output = tl.load(softmax_output_ptrs)\n    # Compute gradient\n    dot_product = tl.sum(grad_output * softmax_output, axis=1)\n    grad_input = softmax_output * (grad_output - dot_product)\n    # Store results\n    tl.store(grad_input_ptrs, grad_input)\n\n# PyTorch wrapper for the Triton kernels\nclass Softmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, inner=False):\n        M, N = input.shape\n        TILE_N, TILE_K, num_warps = heur_optimal_tile_sizes(M, N, N)\n        output = torch.empty_like(input)\n        \n        if inner:\n            softmax_kernel_inner[(M,)](output, input, M, N, TILE_N, num_warps)\n        else:\n            softmax_kernel_non_inner[(M,)](output, input, M, N, N, TILE_N, TILE_K, num_warps)\n        \n        ctx.save_for_backward(output)\n        ctx.inner = inner\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        softmax_output, = ctx.saved_tensors\n        M, N = grad_output.shape\n        grad_input = torch.empty_like(grad_output)\n        TILE_N, TILE_K, num_warps = heur_optimal_tile_sizes(M, N, N)\n        \n        if ctx.inner:\n            softmax_backward_kernel_inner[(M,)](grad_output, grad_input, softmax_output, M, N, TILE_N, num_warps)\n        else:\n            softmax_backward_kernel_non_inner[(M,)](grad_output, grad_input, softmax_output, M, N, N, TILE_N, TILE_K, num_warps)\n        \n        return grad_input, None\n\n# Usage\ninput_tensor = torch.randn(128, 128, device='cuda')\nsoftmax_result = Softmax.apply(input_tensor, inner=False)\n", "label": "import logging\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# from ..utils import libentry\n\nMAX_TILE_K = 8192\nNUM_SMS = torch.cuda.get_device_properties(\n    torch.cuda.current_device()\n).multi_processor_count\n\n\ndef heur_tile_k(args):\n    tile_k = 1\n    upper_bound = min(args[\"K\"], MAX_TILE_K)\n    while tile_k <= upper_bound:\n        num_blocks = args[\"M\"] * triton.cdiv(args[\"K\"], tile_k)\n        num_waves = num_blocks / NUM_SMS\n        if (num_waves > 1) and (tile_k * 2 <= upper_bound):\n            tile_k *= 2\n        else:\n            break\n    return tile_k\n\n\ndef heur_tile_n_non_inner(args):\n    return triton.cdiv(8192, args[\"TILE_K\"])\n\n\ndef heur_one_tile_per_cta(args):\n    return args[\"TILE_N\"] >= args[\"N\"]\n\n\ndef heur_num_warps_non_inner(args):\n    tile_size = args[\"TILE_N\"] * args[\"TILE_K\"]\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n\n\n@triton.heuristics(\n    {\n        \"TILE_K\": heur_tile_k,\n        \"TILE_N\": heur_tile_n_non_inner,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n        \"num_warps\": heur_num_warps_non_inner,\n    }\n)\n@triton.jit\ndef softmax_kernel_non_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_k = tl.program_id(1)\n    pid_m = tl.program_id(0)\n\n    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n        mask = (n_offsets[:, None] < N) & (k_offsets < K)\n        input_ptrs = input_ptr + offset\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m[None, :])\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N, TILE_K], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N, TILE_K], value=0.0, dtype=tl.float32)\n\n        # specialization does not improve performance inn this example, as tested\n        for start_n in range(0, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            alpha = tl.exp(m - m_new)\n            z = z * alpha + tl.exp(inp - m_new)\n            m = m_new\n\n        m_reduced = tl.max(m, 0)  # (TILE_K,)\n        z = tl.sum(z * tl.exp(m - m_reduced[None, :]), 0)  # (TILE_K, )\n        m = m_reduced\n\n        # specialization does not improve performance inn this example, as tested\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets\n            mask = (n_offsets[:, None] < N) & (k_offsets[None, :] < K)\n            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))\n            o = tl.exp(inp - m[None, :]) / z[None, :]\n            tl.store(output_ptr + offsets, o, mask=mask)\n\n\n@triton.jit\ndef next_multiple_of(a, b):\n    # the smallest x>=a that x%b ==0\n    return tl.cidv(a, b) * b\n\n\n@triton.jit\ndef prev_multiple_of(a, b):\n    # the largest x<a that x%b ==0\n    return tl.cdiv(a, b) * b - b\n\n\ndef heur_tile_n_inner(args):\n    if args[\"N\"] <= (32 * 1024):\n        return triton.next_power_of_2(args[\"N\"])\n    else:\n        return 4096\n\n\ndef heur_num_warps_inner(args):\n    tile_size = args[\"TILE_N\"]\n    if tile_size < 2048:\n        return 4\n    elif tile_size < 4096:\n        return 8\n    else:\n        return 16\n\n\n@triton.heuristics(\n    {\n        \"TILE_N\": heur_tile_n_inner,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n        \"num_warps\": heur_num_warps_inner,\n    }\n)\n@triton.jit\ndef softmax_kernel_inner(\n    output_ptr,\n    input_ptr,\n    M,\n    N,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offset = pid_m * N + n_offsets\n        input_ptrs = input_ptr + offset\n        mask = n_offsets < N\n        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(\n            output_ptr.dtype.element_ty\n        )\n        m = tl.max(inp, 0)\n        e = tl.exp(inp - m)\n        z = tl.sum(e, 0)\n        out = e / z\n        output_ptrs = output_ptr + offset\n        tl.store(output_ptrs, out, mask=mask)\n    else:\n        m = tl.full([TILE_N], value=float(\"-inf\"), dtype=tl.float32)\n        z = tl.full([TILE_N], value=0.0, dtype=tl.float32)\n        input_ptr += pid_m * N\n        output_ptr += pid_m * N\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        for start_n in range(0, previous_multiple, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets)\n            m_new = tl.maximum(m, inp)\n            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)\n            m = m_new\n        # specialize the last iteration\n        for start_n in range(previous_multiple, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(input_ptr + n_offsets, mask=mask, other=-float(\"inf\"))\n            m_new = tl.maximum(m, inp)\n            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)\n            m = m_new\n\n        m_reduced = tl.max(m, 0)\n        z = tl.sum(z * tl.exp(m - m_reduced), 0)\n        m = m_reduced\n\n        previous_multiple = prev_multiple_of(N, TILE_N)\n        # specialize the first iteration\n        for start_n in range(0, TILE_N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            mask = n_offsets < N\n            inp = tl.load(\n                input_ptr + n_offsets,\n                mask=mask,\n                other=-float(\"inf\"),\n                eviction_policy=\"evict_first\",\n            )\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o, mask=mask)\n        for start_n in range(TILE_N, N, TILE_N):\n            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)\n            inp = tl.load(input_ptr + n_offsets, eviction_policy=\"evict_first\")\n            o = tl.exp(inp - m) / z\n            tl.store(output_ptr + n_offsets, o)\n\n\ndef heur_tile_n_bwd_non_inner(args):\n    return max(1, 1024 // args[\"TILE_K\"])\n\n\n# ------------------------  backward -------------------------------\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"TILE_K\": 32}),\n        triton.Config({\"TILE_K\": 64}),\n        triton.Config({\"TILE_K\": 128}),\n        triton.Config({\"TILE_K\": 256}),\n        triton.Config({\"TILE_K\": 512}),\n        triton.Config({\"TILE_K\": 1024}),\n    ],\n    key=[\n        \"M\",\n        \"N\",\n        \"K\",\n    ],\n)\n@triton.heuristics(\n    {\n        \"TILE_N\": heur_tile_n_bwd_non_inner,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n    },\n)\n@triton.jit\ndef softmax_backward_kernel_non_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    K,\n    TILE_N: tl.constexpr,\n    TILE_K: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    offsets_k = pid_k * TILE_K + tl.arange(0, TILE_K)\n\n    if ONE_TILE_PER_CTA:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        mask = (offsets_n < N)[:, None] & (offsets_k < K)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, axis=0)\n        in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        scale = tl.zeros([TILE_N, TILE_K], dtype=tl.float32)\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            offsets_n += TILE_N\n            offsets += TILE_N * K\n        scale = tl.sum(scale, axis=0)  # (TILE_K)\n\n        offsets_n = tl.arange(0, TILE_N)\n        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k\n        for _ in range(0, N, TILE_N):\n            mask = (offsets_n < N)[:, None] & (offsets_k < K)\n            out_tile = tl.load(out_ptr + offsets, mask=mask)\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[None, :])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            offsets_n += TILE_N\n            offsets += TILE_N * K\n\n\ndef heru_tile_m(args):\n    return max(1, 1024 // args[\"TILE_N\"])\n\n\n\n@triton.autotune(\n    configs=[\n        triton.Config({\"TILE_N\": 32}),\n        triton.Config({\"TILE_N\": 64}),\n        triton.Config({\"TILE_N\": 128}),\n        triton.Config({\"TILE_N\": 256}),\n        triton.Config({\"TILE_N\": 512}),\n        triton.Config({\"TILE_N\": 1024}),\n    ],\n    key=[\"M\", \"N\"],\n)\n@triton.heuristics(\n    values={\n        \"TILE_M\": heru_tile_m,\n        \"ONE_TILE_PER_CTA\": heur_one_tile_per_cta,\n    },\n)\n@triton.jit\ndef softmax_backward_kernel_inner(\n    out_ptr,\n    out_grad_ptr,\n    in_grad_ptr,\n    M,\n    N,\n    TILE_M: tl.constexpr,\n    TILE_N: tl.constexpr,\n    ONE_TILE_PER_CTA: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    m_offsets = pid_m * TILE_M + tl.arange(0, TILE_M)\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        mask = (m_offsets[:, None] < M) & (n_offsets < N)\n        out_tile = tl.load(out_ptr + offsets, mask=mask)\n        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n        scale = tl.sum(out_tile * out_grad_tile, 1)\n        in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n    else:\n        scale = tl.zeros([TILE_M, TILE_N], dtype=tl.float32)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_last\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            scale += out_tile * out_grad_tile\n            n_offsets += TILE_N\n            offsets += TILE_N\n        scale = tl.sum(scale, 1)  # (TILE_M,)\n\n        n_offsets = tl.arange(0, TILE_N)\n        offsets = m_offsets[:, None] * N + n_offsets\n        for _ in range(0, N, TILE_N):\n            mask = (m_offsets[:, None] < M) & (n_offsets < N)\n            out_tile = tl.load(\n                out_ptr + offsets, mask=mask, eviction_policy=\"evict_first\"\n            )\n            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)\n            in_grad_tile = out_tile * (out_grad_tile - scale[:, None])\n            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)\n            n_offsets += TILE_N\n            offsets += TILE_N\n\n\nclass Softmax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, dim, dtype):\n        logging.debug(\"GEMS SOFTMAX\")\n\n        assert dim >= -x.ndim and dim < x.ndim, \"Invalid dim\"\n        dim = dim % x.ndim\n        M = 1\n        N = x.shape[dim]\n        for i in range(dim):\n            M *= x.shape[i]  # pre_dim\n        inp = x.contiguous()\n        if dtype is None:\n            dtype = x.dtype\n        out = torch.empty_like(inp, dtype=dtype)\n        K = inp.numel() // M // N  # post_dim\n\n        with torch.cuda.device(inp.device):\n            if K > 1:\n                grid = lambda meta: (M, triton.cdiv(K, meta[\"TILE_K\"]), 1)\n                softmax_kernel_non_inner[grid](\n                    out,\n                    inp,\n                    M,\n                    N,\n                    K,\n                )\n            else:\n                grid = (M, 1, 1)\n                softmax_kernel_inner[grid](\n                    out,\n                    inp,\n                    M,\n                    N,\n                )\n        ctx.save_for_backward(out)\n        ctx.dim = dim\n        return out\n\n    @staticmethod\n    def backward(ctx, out_grad):\n        logging.debug(\"GEMS SOFTMAX VJP\")\n        dim = ctx.dim\n        (out,) = ctx.saved_tensors\n\n        assert dim >= -out.ndim and dim < out.ndim, \"Invalid dim\"\n        dim = dim % out.ndim\n        M = 1\n        N = out.shape[dim]\n        for i in range(dim):\n            M *= out.shape[i]\n\n        out_grad = out_grad.contiguous()\n        in_grad = torch.empty_like(out)\n        K = out.numel() // M // N\n\n        with torch.cuda.device(in_grad.device):\n            if K > 1:\n                grid = lambda meta: (M, triton.cdiv(K, meta[\"TILE_K\"]), 1)\n                softmax_backward_kernel_non_inner[grid](\n                    out,\n                    out_grad,\n                    in_grad,\n                    M,\n                    N,\n                    K,\n                )\n            else:\n                grid = lambda meta: (triton.cdiv(M, meta[\"TILE_M\"]), 1, 1)\n                softmax_backward_kernel_inner[grid](\n                    out,\n                    out_grad,\n                    in_grad,\n                    M,\n                    N,\n                )\n        return in_grad, None, None\n\n\ndef softmax(x, dim=-1, dtype=None):\n    return Softmax.apply(x, dim, dtype)\n\n\n\n\n"}
