# TritonBench

TritonBench features two distinct channels: **TritonBench-G** and **TritonBench-T**, each with its own evaluation framework. For detailed information, refer to the paper [TRITONBENCH: Benchmarking Large Language Model Capabilities for Generating Triton Operators](https://arxiv.org/pdf/2502.14752).

## Data
- **TritonBench-G** offers two versions of Alpaca-format instructions: 
  - Simple instruction: `TritonBench_G_simp_alpac_v1.json`
  - Complex instruction: `TritonBench_G_comp_alpac_v1.json`
- It also includes executable folders (`TritonBench_G_v1`) and associated statistics (`TritonBench_G_v1.json`).
- **TritonBench-T** offers two versions of Alpaca-format instructions: 
  - Simple instruction: `TritonBench_T_simp_alpac_v1.json`
  - Complex instruction: `TritonBench_T_comp_alpac_v1.json`
- It also includes executable folders (`TritonBench_T_v1`) and associated statistics (`TritonBench_T_v1.json`).
- Additionally, there are two sets of filtered GitHub data:
  - `train_crawl.json` (4024 entries) ‚Äì de-duplicated using BERT score similarity.
  - `train_synth.json` (4133 entries) ‚Äì data synthesized using Jiuci.
- The combined 8k dataset can be used for **RAG** (Retrieval-Augmented Generation).

## LLM Generated
We also provide the output results from all major models used in the paper. These are located in the `LLM_generated` directory.

## Python Environment
- `triton = 3.1.0`
- `torch >= 2.5.1`
- After installation, update the `py_interpreter` path in scripts within `EVAL/eval_G/` and `EVAL/eval_T/` if your environment differs from the hardcoded `/workspace/TritonBench/.venv/bin/python`.

## Evaluation Process
**Note**: All commands should be run from the root of the `TritonBench` project. Please create the result directories before running the commands. For example: 
```bash
mkdir -p results/G_call_acc results/G_perf results/T_call_acc results/T_perf
```

### TritonBench-G
The evaluation process for `TritonBench-G` is sequential. The output of one step is the input for the next.

1. **Code Similarity Evaluation**: First, use **CodeBLEU** to evaluate code similarity. For detailed instructions, refer to `EVAL/CodeBLEU/readme_4similarity.md`.

2. **Call Accuracy**: This step checks if the generated code is syntactically correct and callable. Run `0_call_acc.py` on the LLM-generated output files. 
    
    For example, to evaluate files in `LLM_generated/Bench_G_general_purpose`:
    ```bash
    python EVAL/eval_G/0_call_acc.py \
      --source LLM_generated/Bench_G_general_purpose/ \
      --target results/G_call_acc/Bench_G_general_purpose/ \
      --GPUs [0,1,2,3]
    ```
    This generates a directory for each `.jsonl` file from the source, containing runnable Python scripts.

3. **Execution Accuracy**: This step compares the output of the generated code with the golden reference. Run `1_exe_acc.py` on the folders generated in the previous step.
    ```bash
    python EVAL/eval_G/1_exe_acc.py \
      --folder results/G_call_acc/Bench_G_general_purpose/ \
      --GPUs [0,1,2,3]
    ```
    This script will remove any python files that do not produce the correct output. The remaining files are those that passed execution accuracy check.

4. **Efficiency**: This step measures the performance of the correctly executed operators. This needs to be run for each model output directory generated by the call accuracy step.

    For each model output (e.g., for each subdirectory in `results/G_call_acc/Bench_G_general_purpose/`):
    ```bash
    # Identify the specific model output directory to evaluate
    export MODEL_DIR=YOUR_MODEL_OUTPUT.jsonl # e.g., results_modified_output_claude-3-5-sonnet-20240620_comp.jsonl
    
    # cd to the performance metrics directory
    cd performance_metrics/perf_G
    
    # Run the benchmark
    python run_bench/write_file.py \
      --input_folder_path ../../results/G_call_acc/Bench_G_general_purpose/${MODEL_DIR}/ \
      --results_path ../../results/G_perf/Bench_G_general_purpose/${MODEL_DIR}/
    
    # This script runs the benchmark based on the files created by write_file.py
    python run_bench/multiprocess_gpu_run.py
    
    # Return to the project root
    cd ../../

    # Finally, calculate the efficiency score
    python EVAL/eval_G/2_efficiency.py \
      --gen_folder results/G_perf/Bench_G_general_purpose/${MODEL_DIR}/
    ```

### TritonBench-T
For **TritonBench-T**, there is no code similarity evaluation. Only call accuracy, execution accuracy, and speedup are assessed. The process is similar to `TritonBench-G`.

1. **Call Accuracy**:
    ```bash
    python EVAL/eval_T/0_call_acc.py \
      --source LLM_generated/Bench_T_general_purpose/ \
      --target results/T_call_acc/Bench_T_general_purpose/ \
      --GPUs [0,1,2,3]
    ```

2. **Execution Accuracy**:
    ```bash
    python EVAL/eval_T/1_exe_acc.py \
      --folder results/T_call_acc/Bench_T_general_purpose/ \
      --GPUs [0,1,2,3]
    ```

3. **Efficiency and Speedup**: This needs to be run for each model output directory.
    
    For each model output (e.g., for each subdirectory in `results/T_call_acc/Bench_T_general_purpose/`):
    ```bash
    # Identify the specific model output directory to evaluate
    export MODEL_DIR=YOUR_MODEL_OUTPUT.jsonl # e.g., results_modified_output_claude-3-5-sonnet-20240620_comp.jsonl

    cd performance_metrics/perf_T

    python run_bench/write_file.py \
      --input_folder_path ../../results/T_call_acc/Bench_T_general_purpose/${MODEL_DIR}/ \
      --results_path ../../results/T_perf/Bench_T_general_purpose/${MODEL_DIR}/

    python run_bench/multiprocess_gpu_run.py

    cd ../../

    python EVAL/eval_T/2_efficiency.py \
      --gen_folder results/T_perf/Bench_T_general_purpose/${MODEL_DIR}/
    ```

**Note**: Ensure that accuracy and efficiency evaluations are performed sequentially, as each step depends on the output of the previous one.

## Hugging face
We have published our dataset on [Hugging Face](https://huggingface.co/collections/LiShangZ/tritonbench-67c0016bc8a8654cfd612a1a).

## üì© Contact Us
If you have any questions, feel free to reach out to us at:  
**‚úâÔ∏è Email:** [qshi9510@gmail.com]
