[
    {
        "name": "fused_bmm_rmsnorm_gelu_dropout_sub",
        "func_inputs": "fused_bmm_rmsnorm_gelu_dropout_sub(input1, input2, other, normalized_shape, dropout_p=0.5, training=True, approximate='none', eps=1e-5, *, out=None) -> Tensor. Args: input1 (Tensor): First input tensor for batch matrix multiplication, of shape (B, N, M), where B is the batch size. input2 (Tensor): Second input tensor for batch matrix multiplication, of shape (B, M, P). other (Tensor): Tensor to subtract from the result after dropout, must be broadcastable to the shape of the output. normalized_shape (int or list or torch.Size): Shape over which RMS normalization is applied, typically the size of the last dimension P. dropout_p (float, optional): Probability of an element to be zeroed in the dropout layer. Default: 0.5. training (bool, optional): Apply dropout if True. Default: True. approximate (str, optional): Can be 'none' or 'tanh'. The approximation to use for GELU. Default: 'none'. eps (float, optional): A value added to the denominator for numerical stability in RMS normalization. Default: 1e-5. out (Tensor, optional): Output tensor. Ignored if None. Default: None. Shape: - Input1: (B, N, M), Input2: (B, M, P), Other: broadcastable to (B, N, P). Output: (B, N, P).",
        "description": "Performs a fused operation combining batch matrix multiplication, RMS normalization, GELU activation, dropout, and subtraction. The function takes three input tensors, performs batch matrix multiplication on the first two, applies RMS normalization, GELU activation, and dropout, and finally subtracts the third tensor from the result.",
        "math": "Given input tensors X, Y, and O, this function computes:\n\n\\[\n\\begin{align*}\nZ &= \\text{bmm}(X, Y) \\\\\nZ_{\\text{norm}} &= \\text{RMSNorm}(Z, \\epsilon) \\\\\nG &= \\text{GELU}(Z_{\\text{norm}}) \\\\\nD &= \\text{Dropout}(G, p) \\\\\nY &= D - O\n\\end{align*}\n\\]\n\nwhere:\n\n- \\text{bmm}(X, Y) performs batch matrix multiplication.\n- \\text{RMSNorm}(Z, \\epsilon) = \\frac{Z}{\\sqrt{\\text{mean}(Z^2, \\text{dim}=-1) + \\epsilon}} applies Root Mean Square Layer Normalization over the last dimension.\n- \\text{GELU}(Z_{\\text{norm}}) applies the Gaussian Error Linear Unit activation function element-wise.\n- \\text{Dropout}(G, p) randomly zeroes elements of G with probability p.\n- D - O subtracts tensor O from D, where O must be broadcastable to the shape of D.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_bmm_rmsnorm_gelu_dropout_sub(input1, input2, other, normalized_shape, dropout_p=0.5, training=True, approximate='none', eps=1e-5, *, out=None):\n    Z = torch.bmm(input1, input2)\n    Z_norm = torch.nn.functional.rms_norm(Z, normalized_shape, eps=eps)\n    G = torch.nn.functional.gelu(Z_norm, approximate=approximate)\n    D = torch.nn.functional.dropout(G, p=dropout_p, training=training)\n    Y = torch.sub(D, other)\n    if out is not None:\n        out.copy_(Y)\n        return out\n    return Y\n\n# Example usage\nB, N, M, P = 2, 3, 4, 5  # Batch size and dimensions\ninput1 = torch.randn(B, N, M)\ninput2 = torch.randn(B, M, P)\nother = torch.randn(B, N, P)\nnormalized_shape = P  # RMSNorm over the last dimension\n\noutput = fused_bmm_rmsnorm_gelu_dropout_sub(\n    input1, input2, other, normalized_shape, dropout_p=0.2, training=True\n)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([2, 3, 5])\n```",
        "torch_code": "Z = torch.bmm(input1, input2)\nZ_norm = torch.nn.functional.rms_norm(Z, normalized_shape, eps=eps)\nG = torch.nn.functional.gelu(Z_norm, approximate=approximate)\nD = torch.nn.functional.dropout(G, p=dropout_p, training=training)\nY = torch.sub(D, other)\nif out is not None:\n    out.copy_(Y)\n    return out\nreturn Y",
        "torch_cnt": 5,
        "other": "- The shapes of `input1` and `input2` must be compatible for batch matrix multiplication: `input1` of shape `(B, N, M)` and `input2` of shape `(B, M, P)` result in an output of shape `(B, N, P)`.\n- The `normalized_shape` argument for RMS normalization should match the dimensions over which to compute the RMS. For an output of shape `(B, N, P)`, setting `normalized_shape=P` applies normalization over the last dimension.\n- The `other` tensor must be broadcastable to the shape of the output tensor after dropout.\n- The `dropout` is applied during training when `training=True`. Set `training=False` to disable dropout during evaluation.\n- The `GELU` activation is applied element-wise to the normalized output.\n- All operations are differentiable and support autograd.",
        "difficulty": 4,
        "params_cnt": 9,
        "file": "fused_bmm_rmsnorm_gelu_dropout_sub.py"
    },
    {
        "name": "torch.div",
        "func_inputs": "div(input, other, *, rounding_mode=None, out=None) -> Tensor; input (Tensor): the dividend; other (Tensor or Number): the divisor; rounding_mode (str, optional): Type of rounding applied to the result; out (Tensor, optional): the output tensor",
        "description": "Divides each element of the input tensor by the corresponding element of the other tensor, supporting broadcasting, type promotion, and handling integer, float, and complex inputs. Rounding behavior can be controlled with the rounding_mode parameter.",
        "math": "\\text{out}_i = \\frac{\\text{input}_i}{\\text{other}_i}",
        "example": ">>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])",
        "torch_code": "torch.div(x, 0.5); torch.div(a, b); torch.div(a, b, rounding_mode='trunc'); torch.div(a, b, rounding_mode='floor')",
        "torch_cnt": 4,
        "other": "By default, performs a 'true' division like Python 3. Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "div.py"
    },
    {
        "name": "sigmoid_conv2d",
        "func_inputs": "sigmoid_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, out=None) -> Tensor\nArgs:\ninput (Tensor): The input tensor of shape `(minibatch, in_channels, iH, iW)`.\nweight (Tensor): The convolution filters of shape `(out_channels, in_channels / groups, kH, kW)`.\nbias (Tensor, optional): Optional bias tensor of shape `(out_channels)`. Default: None.\nstride (int or tuple, optional): The stride of the convolution kernel. Can be a single number or a tuple `(sH, sW)`. Default: 1.\npadding (int, tuple, or string, optional): Padding on both sides of the input. Can be 'valid', 'same', single number, or tuple `(padH, padW)`. Default: 0.\ndilation (int or tuple, optional): The spacing between kernel elements. Default: 1.\ngroups (int, optional): Number of groups to split the input into. Default: 1.\nout (Tensor, optional): The output tensor.",
        "description": "Applies a 2D convolution over an input tensor with specified filters, followed by applying the sigmoid activation function element-wise to the result. This ensures that the convolutional output values are scaled between 0 and 1.",
        "math": "\\text{out} = \\sigma(\\text{conv2d}(\\text{input}, \\text{weight})) where \\sigma(x) = \\frac{1}{1 + e^{-x}} is the sigmoid function.",
        "example": ">>> import torch >>> import torch.nn.functional as F >>> # Define inputs and filters >>> inputs = torch.randn(1, 3, 5, 5) >>> filters = torch.randn(2, 3, 3, 3) >>> bias = torch.randn(2) >>> # Apply convolution with bias, then sigmoid >>> result = sigmoid_conv2d(inputs, filters, bias=bias, padding=1) >>> result.shape torch.Size([1, 2, 5, 5]) >>> # Applying convolution without bias >>> result = sigmoid_conv2d(inputs, filters, padding=1) >>> result.shape torch.Size([1, 2, 5, 5])",
        "torch_code": "torch.nn.functional.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups); torch.sigmoid(conv_result, out=out)",
        "torch_cnt": 2,
        "other": "The function combines 2D convolution and sigmoid activation, ensuring output values are between 0 and 1.",
        "difficulty": 4,
        "params_cnt": 8,
        "file": "sigmoid_conv2d.py"
    },
    {
        "name": "solve_multiple_lu",
        "func_inputs": "def solve_multiple_lu(A, Bs, *, pivot=True, out=None) -> Tensor\n\n    - **A** (Tensor): Coefficient matrix of shape `(*, n, n)`, where `*` is zero or more batch dimensions.\n    - **Bs** (Tensor): Right-hand side tensor of shape `(*, n, k)`, where `k` is the number of right-hand sides.\n    - **pivot** (bool, optional): Controls whether to compute the LU decomposition with partial pivoting (`True`) or without pivoting (`False`). Default: `True`.\n    - **out** (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Solves multiple linear systems with the same coefficient matrix using LU decomposition. Given a square matrix A and multiple right-hand side vectors B, this function computes the solutions X to the linear systems A X = B by performing the LU decomposition of A and reusing it to solve for multiple right-hand sides efficiently. Supports batch dimensions.",
        "math": "LU Decomposition:\nA = P L U\n- P is a permutation matrix.\n- L is a lower triangular matrix with unit diagonal elements.\n- U is an upper triangular matrix.\nSolving Linear Systems:\nFor each b_i in B, compute x_i by:\n- Apply permutation: b'_i = P^T b_i\n- Solve L y_i = b'_i\n- Solve U x_i = y_i",
        "example": "```python\nimport torch\n\nA = torch.tensor([[3., 1., 2.],\n                  [6., 3., 4.],\n                  [3., 1., 5.]])\nb1 = torch.tensor([0., -1., 2.])\nb2 = torch.tensor([1., 0., 3.])\nBs = torch.stack([b1, b2], dim=1)  # Shape (3, 2)\nX = solve_multiple_lu(A, Bs)\nx1 = X[:, 0]\nx2 = X[:, 1]\nprint(\"Solution x1:\", x1)\nprint(\"Solution x2:\", x2)\n# Verify the solutions\nprint(\"Residual for x1:\", A @ x1 - b1)\nprint(\"Residual for x2:\", A @ x2 - b2)\n```",
        "torch_code": "P, L, U = torch.linalg.lu(A, pivot=pivot)\nBs_perm = torch.matmul(P.transpose(-2, -1), Bs)\nY = torch.linalg.solve_triangular(L, Bs_perm, upper=False, unitriangular=True)\nX = torch.linalg.solve_triangular(U, Y, upper=True)",
        "torch_cnt": 4,
        "other": "This function efficiently reuses the LU decomposition of A to solve multiple linear systems with different right-hand sides. If `pivot=False`, no permutation is applied. Supports batch dimensions.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "solve_multiple_lu.py"
    },
    {
        "name": "torch.tanh",
        "func_inputs": "tanh(input, *, out=None) -> Tensor Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the hyperbolic tangent of the elements of the input tensor.",
        "math": "\\text{out}_{i} = \\tanh(\\text{input}_{i})",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n>>> torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])",
        "torch_code": "torch.tanh(a)",
        "torch_cnt": 1,
        "other": "",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "tanh.py"
    },
    {
        "name": "relu_sqrt",
        "func_inputs": "def relu_sqrt(input, inplace=False, out=None) -> Tensor: input (Tensor): The input tensor. inplace (bool, optional): If True, modifies input in-place (if possible). Default is False. out (Tensor, optional): The output tensor.",
        "description": "Applies the rectified linear unit (ReLU) function to each element in input, and then computes the square root of the result. This function ensures all negative values in input are set to zero before applying the square root.",
        "math": "\\text{out}_i = \\sqrt{\\max(0, \\text{input}_i)}",
        "example": ">>> import torch >>> a = torch.tensor([-1.0, 0.0, 4.0, 9.0]) >>> result = relu_sqrt(a) >>> result tensor([0.0000, 0.0000, 2.0000, 3.0000]) >>> result = relu_sqrt(a, inplace=True) >>> result tensor([0.0000, 0.0000, 2.0000, 3.0000])",
        "torch_code": "torch.nn.functional.relu(input, inplace=inplace) torch.sqrt(relu_result, out=out)",
        "torch_cnt": 2,
        "other": "The function modifies input in-place if inplace is set to True.",
        "difficulty": 2,
        "params_cnt": 3,
        "file": "relu_sqrt.py"
    },
    {
        "name": "torch.sqrt",
        "func_inputs": "sqrt(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the square-root of the elements of the input tensor. It computes the square root element-wise.",
        "math": "\\text{out}_{i} = \\sqrt{\\text{input}_{i}}",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])",
        "torch_code": "torch.sqrt(a)",
        "torch_cnt": 1,
        "other": "The function can handle negative inputs, resulting in NaN for those elements.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "sqrt.py"
    },
    {
        "name": "sigmoid_argmax",
        "func_inputs": "sigmoid_argmax(input, dim=None, keepdim=False) -> LongTensor: input (Tensor): The input tensor. dim (int, optional): The dimension to reduce. Default is None, which computes the argmax over all elements. keepdim (bool, optional): Whether the output tensor has :attr:`dim` retained or not. Default is False.",
        "description": "Applies the sigmoid (logistic) function to each element in the input and then computes the indices of the maximum values along the specified dimension or over all elements if no dimension is specified. If dim is not specified, it returns the index of the maximum value in the flattened tensor.",
        "math": "sigmoid(x) = 1 / (1 + e^{-x})",
        "example": "import torch\na = torch.tensor([[0.8, -1.5, 2.5], [0.7, 3.1, -2.4]])\n# Apply sigmoid and then find the index of the maximum value in the entire tensor\nresult = sigmoid_argmax(a)\nresult\ntensor(4)\n\n# Apply sigmoid and find the indices of the maximum values along dimension 1\nresult = sigmoid_argmax(a, dim=1)\nresult\ntensor([2, 1])",
        "torch_code": "torch.sigmoid(input)\ntorch.argmax(sigmoid_result, dim=dim, keepdim=keepdim)",
        "torch_cnt": 2,
        "other": "The function uses PyTorch tensor operations and returns a LongTensor containing indices.",
        "difficulty": 1,
        "params_cnt": 3,
        "file": "sigmoid_argmax.py"
    },
    {
        "name": "sub",
        "func_inputs": "sub(input, other, *, alpha=1, out=None) -> Tensor; input (Tensor): the input tensor.; other (Tensor or Number): the tensor or number to subtract from input.; alpha (Number): the multiplier for other.; out (Tensor, optional): the output tensor.",
        "description": "Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`. The operation is defined as: out_i = input_i - alpha * other_i. Supports broadcasting to a common shape, type promotion, and works with integer, float, and complex inputs.",
        "math": "out_i = input_i - alpha * other_i",
        "example": ">>> a = torch.tensor((1, 2))\n>>> b = torch.tensor((0, 1))\n>>> torch.sub(a, b, alpha=2)\ntensor([1, 0])",
        "torch_code": "torch.sub(a, b, alpha=2)",
        "torch_cnt": 1,
        "other": "Supports broadcasting, type promotion, and works with integer, float, and complex inputs.",
        "difficulty": 2,
        "params_cnt": 4,
        "file": "sub.py"
    },
    {
        "name": "torch.nn.functional.grid_sample",
        "func_inputs": "def grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=False) -> Tensor",
        "description": "Computes output using input values and pixel locations from grid, supporting spatial (4-D) and volumetric (5-D) input. Interpolates output value at specified grid positions using nearest or bilinear interpolation. Grid values are normalized within [-1, 1] range, and values outside are handled by padding_mode. Often used with affine_grid to build Spatial Transformer Networks.",
        "math": "",
        "example": "",
        "torch_code": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=False)",
        "torch_cnt": 1,
        "other": "Note: NaN values in grid are interpreted as -1. align_corners=True changes sampled grid positions with image resolution. Default for align_corners changed to False since version 1.2.0. bicubic mode implemented using cubic convolution algorithm with alpha=-0.75; other packages might use different alpha values.",
        "difficulty": 4,
        "params_cnt": 5,
        "file": "grid_sample.py"
    },
    {
        "name": "torch.linalg.svd",
        "func_inputs": "def linalg.svd(A, full_matrices=True, *, driver=None, out=None) -> (Tensor, Tensor, Tensor)\n\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    full_matrices (bool, optional): controls whether to compute the full or reduced SVD, and consequently, the shape of the returned tensors `U` and `Vh`. Default: `True`.\n\nKeyword args:\n    driver (str, optional): name of the cuSOLVER method to be used. This keyword argument only works on CUDA inputs. Available options are: `None`, `gesvd`, `gesvdj`, and `gesvda`. Default: `None`.\n    out (tuple, optional): output tuple of three tensors. Ignored if `None`.",
        "description": "Computes the singular value decomposition (SVD) of a matrix. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions. The returned decomposition is a named tuple (U, S, Vh) which corresponds to U, S, V^{H} above. The singular values are returned in descending order. The parameter full_matrices chooses between the full (default) and reduced SVD. The driver kwarg may be used in CUDA with a cuSOLVER backend to choose the algorithm used to compute the SVD. The choice of a driver is a trade-off between accuracy and speed.",
        "math": "A = U \\operatorname{diag}(S) V^{\\text{H}} \\mathrlap{\\qquad U \\in \\mathbb{K}^{m \\times m}, S \\in \\mathbb{R}^k, V \\in \\mathbb{K}^{n \\times n}}",
        "example": ">>> A = torch.randn(5, 3)\n>>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n>>> U.shape, S.shape, Vh.shape\n(torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(A, U @ torch.diag(S) @ Vh)\ntensor(1.0486e-06)\n\n>>> U, S, Vh = torch.linalg.svd(A)\n>>> U.shape, S.shape, Vh.shape\n(torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(A, U[:, :3] @ torch.diag(S) @ Vh)\ntensor(1.0486e-06)\n\n>>> A = torch.randn(7, 5, 3)\n>>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n>>> torch.dist(A, U @ torch.diag_embed(S) @ Vh)\ntensor(3.0957e-06)",
        "torch_code": "torch.linalg.svd(A, full_matrices=True, driver=None, out=None)",
        "torch_cnt": 1,
        "other": "Differences with numpy.linalg.svd: Unlike numpy.linalg.svd, this function always returns a tuple of three tensors and it doesn't support compute_uv argument. Please use torch.linalg.svdvals, which computes only the singular values, instead of compute_uv=False. When full_matrices=True, the gradients with respect to U[..., :, min(m, n):] and Vh[..., min(m, n):, :] will be ignored, as those vectors can be arbitrary bases of the corresponding subspaces. The returned tensors U and V are not unique, nor are they continuous with respect to A. Gradients computed using U or Vh will only be finite when A does not have repeated singular values.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "svd.py"
    },
    {
        "name": "torch.special.i0",
        "func_inputs": "i0(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor; Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the zeroth order modified Bessel function of the first kind for each element of the input tensor.",
        "math": "\\text{out}_{i} = I_0(\\text{input}_{i}) = \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2}",
        "example": ">>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])",
        "torch_code": "torch.i0(torch.arange(5, dtype=torch.float32))",
        "torch_cnt": 1,
        "other": "The function calculates the zeroth order modified Bessel function of the first kind, which is a special mathematical function.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "i0.py"
    },
    {
        "name": "torch.rsqrt",
        "func_inputs": "rsqrt(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor.; Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the reciprocal of the square-root of each of the elements of the input tensor.",
        "math": "\\text{out}_{i} = \\frac{1}{\\sqrt{\\text{input}_{i}}}",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n>>> torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])",
        "torch_code": "torch.rsqrt(a)",
        "torch_cnt": 1,
        "other": "Note: The function will return 'nan' for negative input values.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "rsqrt.py"
    },
    {
        "name": "dropout_relu_batch_norm_conv2d",
        "func_inputs": "dropout_relu_batch_norm_conv2d(input: torch.Tensor, weight: torch.Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1, p=0.5, training=True, inplace=False) -> torch.Tensor; Args: input (Tensor): Input tensor of shape \\(N, C_{in}, H, W\\). weight (Tensor): Convolution filters of shape \\(C_{out}, C_{in} / \\text{groups}, kH, kW\\). bias (Tensor, optional): Bias tensor of shape \\(C_{out}\\). Default is None. stride (int or tuple, optional): Stride of the convolution. Default: 1 padding (int, tuple, or str, optional): Implicit padding on both sides of the input. Default: 0 dilation (int or tuple, optional): Spacing between kernel elements. Default: 1 groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 p (float, optional): Probability of an element to be zeroed in dropout. Default: 0.5 training (bool, optional): If True, applies dropout during training. Default: True inplace (bool, optional): If True, performs the operation in-place. Default: False",
        "description": "Applies a 2D convolution followed by batch normalization, ReLU activation, and dropout. Sequentially applies conv2d, batch normalization for stabilizing training and reducing internal covariate shift, ReLU activation function, and dropout where some elements of the tensor are randomly zeroed with probability `p`.",
        "math": "",
        "example": ">>> input = torch.randn(1, 4, 8, 8)\n>>> weight = torch.randn(8, 4, 3, 3)\n>>> output = dropout_relu_batch_norm_conv2d(input, weight, stride=1, padding=1, p=0.5, training=True)\n>>> output.shape\ntorch.Size([1, 8, 8, 8])",
        "torch_code": "x = torch.nn.functional.conv2d(input, weight, bias, stride, padding, dilation, groups)\nx = torch.nn.functional.batch_norm(x, running_mean=None, running_var=None, training=training)\nx = torch.nn.functional.relu(x, inplace=inplace)\nx = torch.nn.functional.dropout(x, p=p, training=training, inplace=inplace)",
        "torch_cnt": 4,
        "other": "Output tensor is returned after applying conv2d, batch normalization, ReLU, and dropout.",
        "difficulty": 3,
        "params_cnt": 9,
        "file": "dropout_relu_batch_norm_conv2d.py"
    },
    {
        "name": "fused_mv_logsoftmax_dropout",
        "func_inputs": "fused_mv_logsoftmax_dropout(input, vec, p=0.5, training=True, inplace=False, dim=0, *, out=None) -> Tensor",
        "description": "Performs a fused operation combining matrix-vector multiplication, log-softmax activation, and dropout. The function first performs matrix-vector multiplication on the input matrix and vector. The result is then passed through a log-softmax activation function along the specified dimension. Finally, dropout is applied to the output of the log-softmax operation.",
        "math": "Given an input matrix A ∈ ℝ^(n × m) and a vector v ∈ ℝ^m, the function computes:\n\nz = A * v\ns = log(exp(z) / ∑_j exp(z_j))\ny = Dropout(s, p)\n\nwhere log(exp(z) / ∑_j exp(z_j)) is the log-softmax function applied along dimension `dim`, and Dropout(s, p) randomly zeroes elements of s with probability p.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_mv_logsoftmax_dropout(input, vec, p=0.5, training=True, inplace=False, dim=0, *, out=None):\n    z = torch.mv(input, vec)\n    s = torch.nn.functional.log_softmax(z, dim=dim)\n    y = torch.nn.functional.dropout(s, p=p, training=training, inplace=inplace)\n    if out is not None:\n        out.copy_(y)\n        return out\n    return y\n\n# Example usage\ninput = torch.tensor([[1.0, 2.0, 3.0],\n                      [4.0, 5.0, 6.0],\n                      [7.0, 8.0, 9.0],\n                      [10.0, 11.0, 12.0],\n                      [13.0, 14.0, 15.0]])\nvec = torch.tensor([0.1, 0.2, 0.3])\noutput = fused_mv_logsoftmax_dropout(input, vec, p=0.2, training=True, dim=0)\nprint(\"Output:\", output)\n# Output: tensor([-1.4076, -1.3030, -1.1985, -1.0939, -0.9893])\n```",
        "torch_code": "z = torch.mv(input, vec)\ns = torch.nn.functional.log_softmax(z, dim=dim)\ny = torch.nn.functional.dropout(s, p=p, training=training, inplace=inplace)\nif out is not None:\n    out.copy_(y)\n    return out\nreturn y",
        "torch_cnt": 3,
        "other": "- The shapes of `input` and `vec` must be compatible for matrix-vector multiplication: the number of columns in `input` must match the size of `vec`.\n- The `dim` argument in `log_softmax` specifies the dimension along which the log-softmax is computed. Since `z` is a 1-D tensor of shape `(n,)`, `dim` should be `0` or `-1`.\n- The `dropout` is applied during training when `training=True`. Set `training=False` to disable dropout during evaluation.\n- This function supports autograd for gradient computation.\n- All operations are differentiable and support backpropagation.",
        "difficulty": 2,
        "params_cnt": 7,
        "file": "fused_mv_logsoftmax_dropout.py"
    },
    {
        "name": "torch.add",
        "func_inputs": "add(input, other, *, alpha=1, out=None) -> Tensor; input (Tensor): the input tensor.; other (Tensor or Number): the tensor or number to add to input.; alpha (Number): the multiplier for other.; out (Tensor, optional): the output tensor.",
        "description": "Adds the tensor or number 'other', scaled by 'alpha', to the 'input' tensor. Supports broadcasting to a common shape, type promotion, and accepts integer, float, and complex inputs.",
        "math": "\\text{{out}}_i = \\text{{input}}_i + \\text{{alpha}} \\times \\text{{other}}_i",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n\n>>> b = torch.randn(4)\n>>> b\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> c = torch.randn(4, 1)\n>>> c\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(b, c, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])",
        "torch_code": "torch.add(a, 20); torch.add(b, c, alpha=10)",
        "torch_cnt": 2,
        "other": "Supports broadcasting and type promotion.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "add.py"
    },
    {
        "name": "fused_silu_layer_norm_conv2d",
        "func_inputs": "fused_silu_layer_norm_conv2d(x: torch.Tensor, weight: torch.Tensor, conv_weight: torch.Tensor, conv_bias: torch.Tensor = None, conv_stride: int = 1, conv_padding: int = 0, conv_dilation: int = 1, conv_groups: int = 1, ln_eps: float = 1e-5) -> torch.Tensor\n\nArgs:\n    x (Tensor): Input tensor for convolution, normalization, and activation.\n    weight (Tensor): Learnable weight of size matching normalized output dimensions for LayerNorm.\n    conv_weight (Tensor): Convolution kernel tensor of appropriate dimensions.\n    conv_bias (Tensor, optional): Convolution bias tensor. Default: ``None``.\n    conv_stride (int, optional): Stride of convolution. Default: 1.\n    conv_padding (int, optional): Padding added to both sides of input. Default: 0.\n    conv_dilation (int, optional): Dilation of convolution kernel. Default: 1.\n    conv_groups (int, optional): Number of groups for convolution. Default: 1.\n    ln_eps (float, optional): Epsilon value for Layer Normalization. Default: 1e-5.\n\nExample:\n    >>> x = torch.randn(4, 3, 32, 32)\n    >>> conv_weight = torch.randn(8, 3, 3, 3)\n    >>> conv_bias = torch.zeros(8)\n    >>> weight = torch.ones(8)\n    >>> output = fused_silu_layer_norm_conv2d(x, weight, conv_weight, conv_bias, conv_stride=1, conv_padding=1)\n    >>> print(output.shape)\n    torch.Size([4, 8, 32, 32])",
        "description": "Applies 2D Convolution, followed by Layer Normalization and SiLU activation to the input tensor `x`. Sequentially performs convolution on `x`, then applies layer normalization on the convolution output, followed by SiLU activation applied element-wise.",
        "math": "",
        "example": ">>> x = torch.randn(4, 3, 32, 32)\n>>> conv_weight = torch.randn(8, 3, 3, 3)\n>>> conv_bias = torch.zeros(8)\n>>> weight = torch.ones(8)\n>>> output = fused_silu_layer_norm_conv2d(x, weight, conv_weight, conv_bias, conv_stride=1, conv_padding=1)\n>>> print(output.shape)\ntorch.Size([4, 8, 32, 32])",
        "torch_code": "conv_out = torch.nn.functional.conv2d(x, conv_weight, bias=conv_bias, stride=conv_stride, padding=conv_padding, dilation=conv_dilation, groups=conv_groups)\nnormalized_out = torch.nn.functional.layer_norm(conv_out, conv_out.shape[1:], eps=ln_eps)\noutput = torch.nn.functional.silu(normalized_out)",
        "torch_cnt": 3,
        "other": "Convolution operation parameters include stride, padding, dilation, and groups. Layer Normalization uses an epsilon value. Default values are provided for optional parameters.",
        "difficulty": 4,
        "params_cnt": 8,
        "file": "fused_silu_layer_norm_conv2d.py"
    },
    {
        "name": "fused_index_select_eq",
        "func_inputs": "fused_index_select_eq(input, dim, index, other, *, out=None) -> Tensor. Args: input (Tensor): The input tensor X. dim (int): The dimension along which to index. index (IntTensor or LongTensor): The indices to select along dimension dim. other (Tensor or float): The tensor or value Y to compare with the selected tensor. out (Tensor, optional): Output tensor. Ignored if None. Default: None",
        "description": "Performs a fused operation combining index selection and element-wise equality comparison. It selects elements from the input tensor along a specified dimension using provided indices and then performs an element-wise equality comparison between the selected elements and another tensor or scalar. The result is a boolean tensor of the same shape as the selected elements, indicating where the comparisons are true.",
        "math": "Given an input tensor X, dimension \text{dim}, index tensor I, and another tensor or scalar Y, the function computes:\n\n1. **Index Selection:**\n\nSelect elements from X along dimension \text{dim} using indices I:\n\n\\[\nS = \\text{index\\_select}(X, \\text{dim}, I)\n\\]\n\n2. **Element-wise Equality Comparison:**\n\nCompare the selected tensor S with Y element-wise:\n\n\\[\nO = (S == Y)\n\\]\n\nThe output tensor O is a boolean tensor of the same shape as S.",
        "example": "```python\nimport torch\n\ndef fused_index_select_eq(input, dim, index, other, *, out=None):\n    selected = torch.index_select(input, dim, index)\n    output = torch.eq(selected, other)\n    if out is not None:\n        out.copy_(output)\n        return out\n    return output\n\n# Example usage\ninput = torch.tensor([[1, 2], [3, 4], [5, 6]])\nother = torch.tensor([[1, 2], [5, 6]])\nindex = torch.tensor([0, 2])  # Select first and third rows\ndim = 0\n\noutput = fused_index_select_eq(input, dim=dim, index=index, other=other)\nprint(\"Output:\")\nprint(output)\n# Output:\n# tensor([[ True,  True],\n#         [ True,  True]])\n```",
        "torch_code": "selected = torch.index_select(input, dim, index)\noutput = torch.eq(selected, other)",
        "torch_cnt": 2,
        "other": "- The shapes of the selected tensor S and other must be broadcastable for the element-wise comparison.\n- If other is a scalar, it is broadcasted to the shape of S.\n- The function supports autograd for gradient computation, although the output is a boolean tensor.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "fused_index_select_eq.py"
    },
    {
        "name": "torch.argmax",
        "func_inputs": "argmax(input, dim, keepdim=False) -> LongTensor\nArgs:\n    input (Tensor): the input tensor.\n    dim (int): the dimension to reduce. If ``None``, the argmax of the flattened input is returned.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.",
        "description": "Returns the indices of the maximum values of a tensor across a specified dimension. If the dimension is None, it returns the index of the maximum value in the flattened input tensor. The output tensor can retain the reduced dimension if keepdim is set to True.",
        "math": "",
        "example": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])",
        "torch_code": "torch.argmax(a, dim=1)",
        "torch_cnt": 1,
        "other": "This is the second value returned by torch.max. See its documentation for the exact semantics of this method.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "argmax.py"
    },
    {
        "name": "fused_lu_solve",
        "func_inputs": "def fused_lu_solve(A: Tensor, b: Tensor) -> Tensor: A: The input matrix `A` of shape `(n, n)`. b: The right-hand side tensor `b` of shape `(n,)`.",
        "description": "Computes the solution `x` to the equation `Ax = b` using LU decomposition. Given matrix `A`, this function performs LU decomposition and then solves for `x` in `L @ U @ x = b`, where `P`, `L`, and `U` are derived from the LU decomposition.",
        "math": "Solves `Ax = b` using LU decomposition, where `A = P @ L @ U` and `L @ U @ x = b`.",
        "example": ">>> A = torch.tensor([[3., 1., 2.], [6., 3., 4.], [3., 1., 5.]]) >>> b = torch.tensor([1., 2., 3.]) >>> x = fused_lu_solve(A, b) >>> print(x)",
        "torch_code": "P, L, U = torch.linalg.lu(A) x = torch.linalg.solve(L @ U, b)",
        "torch_cnt": 2,
        "other": "The function uses LU decomposition to solve linear equations.",
        "difficulty": 5,
        "params_cnt": 2,
        "file": "fused_lu_solve.py"
    },
    {
        "name": "normalize_pairwise_distance",
        "func_inputs": "normalize_pairwise_distance(x1, x2, p_distance=2.0, eps_distance=1e-6, keepdim=False, p_norm=2, dim_norm=1, eps_norm=1e-12) -> Tensor; x1 (Tensor): The first input tensor; x2 (Tensor): The second input tensor, must have the same shape as `x1`; p_distance (float): The norm degree for computing the pairwise distance. Default: 2.0; eps_distance (float): Small value to avoid division by zero in pairwise distance calculation. Default: 1e-6; keepdim (bool): Whether to keep the reduced dimensions in the output. Default: False; p_norm (float): The exponent value in the norm formulation for normalization. Default: 2; dim_norm (int): The dimension along which normalization is applied. Default: 1; eps_norm (float): Small value to avoid division by zero in normalization. Default: 1e-12",
        "description": "Computes the pairwise distance between `x1` and `x2` using the specified norm, then normalizes the resulting distances along the specified dimension. This combined operation is useful for obtaining normalized distance values between two sets of vectors.",
        "math": "\\text{distance} = \\frac{\\text{pairwise\\_distance}(x1, x2)}{\\max(\\lVert \\text{pairwise\\_distance}(x1, x2) \\rVert_p, \\epsilon)}",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define two input tensors\n>>> x1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n>>> x2 = torch.tensor([[1.0, 2.5], [2.5, 4.0]])\n>>> # Compute the normalized pairwise distance\n>>> result = normalize_pairwise_distance(x1, x2, p_distance=2.0, dim_norm=0)\n>>> result\ntensor([0.0000, 0.4472])\n\n>>> # Normalize along a different dimension\n>>> result = normalize_pairwise_distance(x1, x2, p_distance=1.0, dim_norm=0)\n>>> result\ntensor([0.0000, 0.5000])",
        "torch_code": "torch.nn.functional.pairwise_distance(x1, x2, p=p_distance, eps=eps_distance, keepdim=keepdim)\ntorch.nn.functional.normalize(distance, p=p_norm, dim=dim_norm, eps=eps_norm)",
        "torch_cnt": 2,
        "other": "The combined operation is useful for obtaining normalized distance values between two sets of vectors.",
        "difficulty": 3,
        "params_cnt": 7,
        "file": "normalize_pairwise_distance.py"
    },
    {
        "name": "torch.max",
        "func_inputs": "max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor) input (Tensor): the input tensor. dim (int): the dimension to reduce. keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``. out (tuple, optional): the result tuple of two output tensors (max, max_indices).",
        "description": "Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. Indices is the index location of each maximum value found (argmax). If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed, resulting in the output tensors having 1 fewer dimension than input. If there are multiple maximal values in a reduced row, the indices of the first maximal value are returned.",
        "math": "",
        "example": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))",
        "torch_code": "torch.max(a, 1)",
        "torch_cnt": 1,
        "other": "If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.",
        "difficulty": 1,
        "params_cnt": 4,
        "file": "max.py"
    },
    {
        "name": "log_softmax_linear",
        "func_inputs": "log_softmax_linear(input, weight, bias=None, dim=-1, dtype=None) -> Tensor: input (Tensor): The input tensor of shape `(*, in_features)`, where `*` represents any number of additional dimensions. weight (Tensor): The weight matrix of shape `(out_features, in_features)`. bias (Tensor, optional): The optional bias tensor of shape `(out_features)`. Default: None. dim (int): The dimension along which log_softmax will be computed. Default: -1. dtype (:class:`torch.dtype`, optional): The desired data type of the returned tensor. If specified, the input tensor is cast to :attr:`dtype` before the operation. Default: None.",
        "description": "Applies a linear transformation to the input tensor followed by the log_softmax activation function. This combined operation is optimized to be numerically stable and efficient, applying both a linear transformation and log-softmax in one step.",
        "math": "\\text{out} = \\log\\left(\\frac{\\exp(\\text{linear}(\\text{input}))}{\\sum_j \\exp(\\text{linear}(\\text{input})_j)}\\right) y = xA^T + b",
        "example": "import torch\nimport torch.nn.functional as F\ninput = torch.randn(5, 10)\nweight = torch.randn(6, 10)\nbias = torch.randn(6)\nresult = log_softmax_linear(input, weight, bias, dim=-1)\nresult\nresult = log_softmax_linear(input, weight, dim=1)\nresult.shape",
        "torch_code": "torch.nn.functional.linear(input, weight, bias)\ntorch.nn.functional.log_softmax(linear_result, dim=dim, dtype=dtype)",
        "torch_cnt": 2,
        "other": "The values along the specified dimension represent log probabilities and sum to 1.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "log_softmax_linear.py"
    },
    {
        "name": "torch.nn.functional.relu",
        "func_inputs": "relu(input, inplace=False) -> Tensor\n\nArgs:\n    inplace: can optionally do the operation in-place. Default: False\n\nShape:\n    - Input: (*), where * means any number of dimensions.\n    - Output: (*), same shape as the input.",
        "description": "Applies the rectified linear unit function element-wise. This operation compares each element in the input tensor to zero and returns the element itself if it is greater than zero or zero otherwise. The operation can be performed in-place, modifying the input tensor directly if inplace=True.",
        "math": "ReLU(x) = (x)^+ = max(0, x)",
        "example": ">>> m = nn.ReLU()\n>>> input = torch.randn(2)\n>>> output = m(input)\nAn implementation of CReLU - https://arxiv.org/abs/1603.05201\n>>> m = nn.ReLU()\n>>> input = torch.randn(2).unsqueeze(0)\n>>> output = torch.cat((m(input), m(-input)))",
        "torch_code": "torch.nn.functional.relu(input, inplace=False)",
        "torch_cnt": 1,
        "other": "See torch.nn.ReLU for more details.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "relu.py"
    },
    {
        "name": "least_squares_qr",
        "func_inputs": "def least_squares_qr(A, b, *, mode='reduced', out=None) -> Tensor: A (Tensor): Coefficient matrix of shape (*, m, n), where * is zero or more batch dimensions. b (Tensor): Right-hand side vector or matrix of shape (*, m) or (*, m, k), where k is the number of right-hand sides. mode (str, optional): Determines the type of QR decomposition to use. One of 'reduced' (default) or 'complete'. See torch.linalg.qr for details. out (Tensor, optional): Output tensor. Ignored if None. Default: None.",
        "description": "Solves the least squares problem for an overdetermined system of linear equations using QR decomposition. It computes the least squares solution x that minimizes the Euclidean 2-norm |Ax - b|_2, where A is the coefficient matrix and b is the right-hand side vector or matrix.",
        "math": "The QR decomposition of A is given by A = QR, where Q is a matrix with orthonormal columns and R is an upper triangular matrix. The least squares solution is x = R^{-1} Q^H b.",
        "example": "import torch\n\n# Define an overdetermined system Ax = b\nA = torch.tensor([[1.0, 1.0],[1.0, 2.0],[1.0, 3.0]])\nb = torch.tensor([1.0, 2.0, 2.0])\n\n# Solve for x using least_squares_qr\nx = least_squares_qr(A, b)\n\nprint(\"Least squares solution x:\")\nprint(x)\n# Output:\n# tensor([0.3333, 0.6667])\n\n# Verify that the residual norm is minimized\nresidual = torch.norm(A @ x - b)\nprint(\"Residual norm:\", residual.item())\n# Output:\n# Residual norm: 0.4714045524597168\n\n# Compare with torch.linalg.lstsq\nlstsq_solution = torch.linalg.lstsq(A, b.unsqueeze(-1)).solution\nprint(\"Solution from torch.linalg.lstsq:\")\nprint(lstsq_solution.squeeze())\n# Output:\n# tensor([0.3333, 0.6667])",
        "torch_code": "Q, R = torch.linalg.qr(A, mode=mode)\nQTb = torch.matmul(Q.transpose(-2, -1).conj(), b)\nx = torch.linalg.solve(R, QTb)",
        "torch_cnt": 3,
        "other": "The function utilizes QR decomposition to efficiently solve overdetermined linear systems by finding the least squares solution.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "least_squares_qr.py"
    },
    {
        "name": "determinant_via_qr",
        "func_inputs": "determinant_via_qr(A, *, mode='reduced', out=None) -> Tensor",
        "description": "Computes the determinant of a square matrix using QR decomposition. It performs QR decomposition of a square matrix A in \\mathbb{K}^{n \times n} (where \\mathbb{K} is either \\mathbb{R} or \\mathbb{C}) and computes the determinant by taking the product of the diagonal elements of R.",
        "math": "The QR decomposition of A is: A = Q R, where Q is an orthogonal/unitary matrix, R is an upper triangular matrix. The determinant is given by: \\det(A) = \\det(Q)\\cdot \\prod_{i=1}^{n} R_{ii}. For real matrices, \\det(Q) = \\pm 1. For complex matrices, |\\det(Q)| = 1.",
        "example": "```python\nimport torch\n\ndef determinant_via_qr(A, *, mode='reduced', out=None):\n    Q, R = torch.linalg.qr(A, mode=mode)\n    # Compute determinant of Q\n    det_Q = torch.det(Q)\n    # Compute product of diagonal elements of R\n    diag_R = torch.diagonal(R, dim1=-2, dim2=-1)\n    prod_diag_R = torch.prod(diag_R, dim=-1)\n    # Compute determinant\n    determinant = det_Q * prod_diag_R\n    if out is not None:\n        out.copy_(determinant)\n        return out\n    return determinant\n\n# Example usage\nA = torch.tensor([[2., -2., 18.],\n                  [2., 1., 0.],\n                  [1., 2., 0.]])\ndet = determinant_via_qr(A)\nprint(\"Determinant:\", det.item())\n# Verify with torch.det\ndet_builtin = torch.det(A)\nprint(\"Determinant from torch.det:\", det_builtin.item())\n# Output should be the same\n\n# Batch example\nA_batch = torch.randn(3, 4, 4)\ndet_batch = determinant_via_qr(A_batch)\ndet_builtin_batch = torch.det(A_batch)\nprint(\"Determinant difference:\", torch.allclose(det_batch, det_builtin_batch))\n```",
        "torch_code": "torch.linalg.qr(A, mode=mode); torch.det(Q); torch.diagonal(R, dim1=-2, dim2=-1); torch.prod(diag_R, dim=-1); torch.det(A); torch.allclose(det_batch, det_builtin_batch)",
        "torch_cnt": 6,
        "other": "Numerical stability considerations are important, especially for ill-conditioned matrices. The function explicitly computes \\det(Q) to account for the sign. For complex matrices, the result may be complex.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "determinant_via_qr.py"
    },
    {
        "name": "fused_tile_exp",
        "func_inputs": "fused_tile_exp(input, dims, *, out=None) -> Tensor; input (Tensor): The input tensor X whose elements are to be repeated and exponentiated.; dims (tuple of int): The number of repetitions for each dimension. If `dims` has fewer dimensions than `input`, ones are prepended to `dims` until all dimensions are specified.; out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Performs a fused operation combining tiling (repeating elements) and the exponential function. The input tensor is first repeated along each dimension according to the specified `dims` using the tiling operation, then the exponential function is applied element-wise to the resulting tensor.",
        "math": "Given an input tensor X and a tuple of dimensions \text{dims}, the function computes:\n1. **Tiling:**\nThe input tensor is repeated along each dimension according to the specified number of times in `dims`:\nY = tile(X, dims)\n2. **Exponential Function:**\nThe exponential function is applied element-wise to the tiled tensor:\nZ = exp(Y)",
        "example": "import torch\n\nx = torch.tensor([1.0, 2.0, 3.0])\ndims = (2,)\noutput = fused_tile_exp(x, dims)\nprint('Output:')\nprint(output)\n# Output:\n# tensor([  2.7183,   7.3891,  20.0855,   2.7183,   7.3891,  20.0855])",
        "torch_code": "tiled = torch.tile(input, dims)\noutput = torch.exp(tiled)\nif out is not None:\n    out.copy_(output)\n    return out\nreturn output",
        "torch_cnt": 2,
        "other": "The `dims` parameter controls how many times the input tensor is repeated along each dimension.\nIf `dims` specifies fewer dimensions than `input`, ones are prepended to `dims` until all dimensions are specified.\nThe function supports autograd for gradient computation.\nAll operations are differentiable and support backpropagation.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "fused_tile_exp.py"
    },
    {
        "name": "sqrt_tanh",
        "func_inputs": "def sqrt_tanh(input, out=None) -> Tensor: input (Tensor): The input tensor. out (Tensor, optional): The output tensor.",
        "description": "Computes the square root of each element in the input tensor, and then applies the hyperbolic tangent (tanh) function to the square-rooted values. The function returns a tensor where each element is the result of applying sqrt followed by tanh to each element of the input.",
        "math": "\\text{out}_{i} = \\tanh(\\sqrt{\\text{input}_{i}})",
        "example": ">>> import torch\n>>> a = torch.tensor([0.25, 1.0, 4.0, 9.0])\n>>> result = sqrt_tanh(a)\n>>> result\ntensor([0.4805, 0.7616, 0.9640, 0.9951])\n\n>>> a = torch.tensor([0.25, -1.0, 4.0, -9.0])\n>>> result = sqrt_tanh(a)\n>>> result\ntensor([ 0.4805,     nan,  0.9640,     nan])",
        "torch_code": "torch.sqrt(input); torch.tanh(sqrt_result, out=out)",
        "torch_cnt": 2,
        "other": "Using a tensor with some negative values results in NaN for those elements.",
        "difficulty": 3,
        "params_cnt": 2,
        "file": "sqrt_tanh.py"
    },
    {
        "name": "silu_batch_norm",
        "func_inputs": "silu_batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-5) -> Tensor; input (Tensor): The input tensor for Batch Normalization.; running_mean (Tensor): The running mean tensor (used during evaluation).; running_var (Tensor): The running variance tensor (used during evaluation).; weight (Tensor, optional): The weight tensor for Batch Normalization scaling. Default: None.; bias (Tensor, optional): The bias tensor for Batch Normalization. Default: None.; training (bool, optional): Whether the module is in training mode. Default: False.; momentum (float, optional): Value used for the running mean and variance computation. Default: 0.1.; eps (float, optional): A small value added to the denominator for numerical stability. Default: 1e-5.",
        "description": "Applies Batch Normalization over an input tensor across channels, followed by the Sigmoid Linear Unit (SiLU) activation function applied element-wise. This combined operation normalizes the input tensor and then applies a non-linear SiLU activation.",
        "math": "The combined operation is defined as: \\text{out} = \\text{silu}(\\text{BatchNorm}(x)), where the SiLU function is defined as: \\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) = \\frac{1}{1 + \\exp(-x)}",
        "example": "Example::\n\n    >>> import torch\n    >>> import torch.nn.functional as F\n    >>> # Define input tensor\n    >>> input = torch.randn(20, 10)  # (batch_size, num_features)\n    >>> # Running mean and variance tensors\n    >>> running_mean = torch.zeros(10)\n    >>> running_var = torch.ones(10)\n    >>> # Apply silu_batch_norm in training mode\n    >>> output = silu_batch_norm(input, running_mean, running_var, training=True)\n    >>> output.shape\n    torch.Size([20, 10])\n\n    >>> # Apply silu_batch_norm with specific weight and bias\n    >>> weight = torch.ones(10)\n    >>> bias = torch.zeros(10)\n    >>> output = silu_batch_norm(input, running_mean, running_var, weight, bias, training=False)\n    >>> output.shape\n    torch.Size([20, 10])",
        "torch_code": "batch_norm_result = torch.nn.functional.batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\nreturn torch.nn.functional.silu(batch_norm_result)",
        "torch_cnt": 2,
        "other": "Returns: A tensor that has undergone batch normalization and SiLU activation.",
        "difficulty": 3,
        "params_cnt": 8,
        "file": "silu_batch_norm.py"
    },
    {
        "name": "torch.Tensor.index_fill_",
        "func_inputs": "index_fill_(dim, index, value) -> Tensor\n\nArgs:\n    dim (int): dimension along which to index\n    index (LongTensor): indices of :attr:`self` tensor to fill in\n    value (float): the value to fill with\n\nExample::\n    >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n    >>> index = torch.tensor([0, 2])\n    >>> x.index_fill_(1, index, -1)\n    tensor([[-1.,  2., -1.],\n            [-1.,  5., -1.],\n            [-1.,  8., -1.]])",
        "description": "Fills the elements of the self tensor with a specified value by selecting the indices in the order given in the index tensor. The operation is performed along a specified dimension.",
        "math": "",
        "example": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 2])\n>>> x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n        [-1.,  5., -1.],\n        [-1.,  8., -1.]])",
        "torch_code": "x.index_fill_(1, index, -1)",
        "torch_cnt": 1,
        "other": "The function modifies the tensor in-place.",
        "difficulty": 2,
        "params_cnt": 3,
        "file": "index_fill_.py"
    },
    {
        "name": "fused_cross_entropy_softmax_layernorm",
        "func_inputs": "fused_cross_entropy_softmax_layernorm(logits, targets, normalized_shape, weight=None, ignore_index=-100, reduction='mean', label_smoothing=0.0, eps=1e-5, *, out=None) -> Tuple[Tensor, Tensor] - logits (Tensor): Input logits of shape (N, C) or (N, C, *), where N is the batch size and C is the number of classes. - targets (Tensor): Ground truth class indices or class probabilities. If containing class indices: shape (N) or (N, *) with values 0 <= targets_i < C. If containing class probabilities: same shape as logits and values in [0, 1]. - normalized_shape (int or list or torch.Size): Input shape over which layer normalization is applied. - weight (Tensor, optional): A manual rescaling weight given to each class. If provided, must be of size C. - ignore_index (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. Default: -100. - reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. Default: 'mean'. - label_smoothing (float, optional): A float in [0.0, 1.0] specifying the amount of smoothing when computing the loss. Default: 0.0. - eps (float, optional): A value added to the denominator for numerical stability in layer normalization. Default: 1e-5. - out (Tensor, optional): Output tensor for the normalized probabilities. Ignored if None. Default: None.",
        "description": "Performs a fused operation combining cross-entropy loss computation, softmax activation, and layer normalization. It computes the cross-entropy loss for given logits and targets, applies softmax activation to the logits, and then applies layer normalization to the resulting probabilities.",
        "math": "Given input logits \\mathbf{z} and target labels \\mathbf{y}, the function computes:\n\n1. **Cross-Entropy Loss:**\n\nFor target class indices:\n\n\\[\nL = -\\sum_{i=1}^N \\log\\left( \\frac{e^{z_{i, y_i}}}{\\sum_{j=1}^C e^{z_{i, j}}} \\right)\n\\]\n\nFor target class probabilities:\n\n\\[\nL = -\\sum_{i=1}^N \\sum_{j=1}^C y_{i, j} \\log\\left( \\frac{e^{z_{i, j}}}{\\sum_{k=1}^C e^{z_{i, k}}} \\right)\n\\]\n\n2. **Softmax Activation:**\n\n\\[\n\\mathbf{p}_{i, j} = \\text{Softmax}(z_{i, j}) = \\frac{e^{z_{i, j}}}{\\sum_{k=1}^C e^{z_{i, k}}}\n\\]\n\n3. **Layer Normalization:**\n\n\\[\n\\mathbf{o}_i = \\frac{\\mathbf{p}_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} \\times \\gamma + \\beta\n\\]\n\nwhere:\n\n- \\mu_i and \\sigma_i^2 are the mean and variance of \\mathbf{p}_i over the dimensions specified by `normalized_shape`.\n- \\gamma and \\beta are learnable parameters (omitted if `elementwise_affine=False`).\n- \\epsilon is a small value added for numerical stability.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_cross_entropy_softmax_layernorm(logits, targets, normalized_shape, weight=None, ignore_index=-100, reduction='mean', label_smoothing=0.0, eps=1e-5, *, out=None):\n    # Compute cross-entropy loss\n    loss = torch.nn.functional.cross_entropy(\n        logits,\n        targets,\n        weight=weight,\n        ignore_index=ignore_index,\n        reduction=reduction,\n        label_smoothing=label_smoothing\n    )\n    # Compute softmax probabilities\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    # Apply layer normalization\n    output = torch.nn.functional.layer_norm(\n        probabilities,\n        normalized_shape=normalized_shape,\n        weight=None,\n        bias=None,\n        eps=eps\n    )\n    if out is not None:\n        out.copy_(output)\n        return loss, out\n    return loss, output\n\n# Example usage\nN, C = 4, 5  # Batch size and number of classes\nlogits = torch.randn(N, C, requires_grad=True)\ntargets = torch.tensor([1, 0, 3, 2])  # Class indices\nnormalized_shape = C  # Apply LayerNorm over the class dimension\n\nloss, output = fused_cross_entropy_softmax_layernorm(\n    logits,\n    targets,\n    normalized_shape=normalized_shape\n)\nprint(\"Loss:\", loss.item())\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([4, 5])\n```",
        "torch_code": "loss = torch.nn.functional.cross_entropy(\n    logits,\n    targets,\n    weight=weight,\n    ignore_index=ignore_index,\n    reduction=reduction,\n    label_smoothing=label_smoothing\n)\nprobabilities = torch.nn.functional.softmax(logits, dim=-1)\noutput = torch.nn.functional.layer_norm(\n    probabilities,\n    normalized_shape=normalized_shape,\n    weight=None,\n    bias=None,\n    eps=eps\n)",
        "torch_cnt": 3,
        "other": "- The `logits` tensor should contain raw, unnormalized scores for each class.\n- The `targets` can be class indices or class probabilities matching the shape of `logits`.\n- The `normalized_shape` argument in `layer_norm` should correspond to the dimensions over which you want to apply normalization.\n- If `elementwise_affine` parameters (`weight` and `bias`) are needed in `layer_norm`, they can be defined and passed accordingly.\n- All operations support autograd for gradient computation.",
        "difficulty": 3,
        "params_cnt": 8,
        "file": "fused_cross_entropy_softmax_layernorm.py"
    },
    {
        "name": "torch.mean",
        "func_inputs": "input (Tensor): the input tensor. dim (int or tuple of ints): the dimension or dimensions to reduce. keepdim (bool): whether the output tensor has dim retained or not. dtype (torch.dtype, optional): the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. out (Tensor, optional): the output tensor.",
        "description": "Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed, resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "math": "",
        "example": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.3841,  0.6320,  0.4254, -0.7384],\n        [-0.9644,  1.0131, -0.6549, -1.4279],\n        [-0.2951, -1.3350, -0.7694,  0.5600],\n        [ 1.0842, -0.9580,  0.3623,  0.2343]])\n>>> torch.mean(a, 1)\ntensor([-0.0163, -0.5085, -0.4599,  0.1807])\n>>> torch.mean(a, 1, True)\ntensor([[-0.0163],\n        [-0.5085],\n        [-0.4599],\n        [ 0.1807]])",
        "torch_code": "torch.mean(a, 1); torch.mean(a, 1, True)",
        "torch_cnt": 2,
        "other": "See also torch.nanmean which computes the mean value of non-NaN elements.",
        "difficulty": 1,
        "params_cnt": 5,
        "file": "mean.py"
    },
    {
        "name": "torch.linalg.eig",
        "func_inputs": "def linalg.eig(A, *, out=None) -> (Tensor, Tensor) Args: A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions consisting of diagonalizable matrices. Keyword args: out (tuple, optional): output tuple of two tensors. Ignored if `None`. Default: `None`.",
        "description": "Computes the eigenvalue decomposition of a square matrix if it exists. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions. The returned eigenvalues are not guaranteed to be in any specific order. The eigenvalues and eigenvectors of a real matrix may be complex. When inputs are on a CUDA device, this function synchronizes that device with the CPU. Assumes that A is diagonalizable. The returned eigenvectors are normalized to have norm 1. The eigenvectors of a matrix are not unique, nor are they continuous with respect to A. Gradients computed using the eigenvectors tensor will only be finite when A has distinct eigenvalues.",
        "math": "A = V \\operatorname{diag}(\\Lambda) V^{-1}\\mathrlap{\\qquad V \\in \\mathbb{C}^{n \\times n}, \\Lambda \\in \\mathbb{C}^n}",
        "example": ">>> A = torch.randn(2, 2, dtype=torch.complex128)\n>>> A\ntensor([[ 0.9828+0.3889j, -0.4617+0.3010j],\n        [ 0.1662-0.7435j, -0.6139+0.0562j]], dtype=torch.complex128)\n>>> L, V = torch.linalg.eig(A)\n>>> L\ntensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)\n>>> V\ntensor([[ 0.9218+0.0000j,  0.1882-0.2220j],\n        [-0.0270-0.3867j,  0.9567+0.0000j]], dtype=torch.complex128)\n>>> torch.dist(V @ torch.diag(L) @ torch.linalg.inv(V), A)\ntensor(7.7119e-16, dtype=torch.float64)\n\n>>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> L, V = torch.linalg.eig(A)\n>>> torch.dist(V @ torch.diag_embed(L) @ torch.linalg.inv(V), A)\ntensor(3.2841e-16, dtype=torch.float64)",
        "torch_code": "torch.linalg.eig(A)",
        "torch_cnt": 5,
        "other": "The eigenvalues and eigenvectors of a real matrix may be complex. When inputs are on a CUDA device, this function synchronizes that device with the CPU. Assumes that A is diagonalizable. The returned eigenvectors are normalized to have norm 1. The eigenvectors of a matrix are not unique, nor are they continuous with respect to A. Gradients computed using the eigenvectors tensor will only be finite when A has distinct eigenvalues.",
        "difficulty": 3,
        "params_cnt": 2,
        "file": "eig.py"
    },
    {
        "name": "torch.special.logsumexp",
        "func_inputs": "def logsumexp(input, dim, keepdim=False, *, out=None) -> Tensor",
        "description": "This function computes the logarithm of the sum of exponentials of input elements along the specified dimension. It is useful for numerical stability when computing log probabilities.",
        "math": "logsumexp(x) = log(sum(exp(x)))",
        "example": "",
        "torch_code": "torch.logsumexp(input, dim, keepdim=False, out=None)",
        "torch_cnt": 1,
        "other": "Alias for torch.logsumexp.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "logsumexp.py"
    },
    {
        "name": "fused_embedding_add_tanh",
        "func_inputs": "fused_embedding_add_tanh(input_indices, weight, other, *, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, out=None) -> Tensor; input_indices (LongTensor): Tensor containing indices into the embedding matrix, of arbitrary shape (*); weight (Tensor): The embedding matrix of shape (V, D), where V is the number of embeddings (vocabulary size), and D is the embedding dimension; other (Tensor): Tensor to be added to the embeddings, must be broadcastable to the shape of E; padding_idx (int, optional): If specified, the entries at `padding_idx` do not contribute to the gradient; max_norm (float, optional): If given, each embedding vector with norm larger than `max_norm` is renormalized to have norm `max_norm`; norm_type (float, optional): The p-norm to compute for the `max_norm` option. Default: `2.0`; scale_grad_by_freq (bool, optional): If `True`, scale gradients by the inverse of frequency of the words in the mini-batch. Default: `False`; sparse (bool, optional): If `True`, gradient w.r.t. `weight` will be a sparse tensor. Default: `False`; out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`",
        "description": "Performs a fused operation combining embedding lookup, element-wise addition, and tanh activation. The function retrieves embeddings from an embedding matrix using input indices, adds another tensor to these embeddings, and applies a tanh activation function to the result. It supports options for padding indices, max norm for embeddings, scaling gradients by frequency, and sparse gradients.",
        "math": "Given input indices \\mathbf{i}, embedding weight matrix W, and tensor O, the function computes:\n\\[\n\\begin{align*}\nE &= \\text{Embedding}(\\mathbf{i}, W) \\\\\nS &= E + O \\\\\nY &= \\tanh(S)\n\\end{align*}\n\\]",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\n# Example usage\n# Embedding matrix of size (vocab_size, embedding_dim)\nvocab_size = 10\nembedding_dim = 3\nweight = torch.randn(vocab_size, embedding_dim)\n\n# Input indices (arbitrary shape)\ninput_indices = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n\n# Other tensor to add, broadcastable to embedding output\nother = torch.randn_like(torch.nn.functional.embedding(input_indices, weight))\n\n# Call the fused function\noutput = fused_embedding_add_tanh(input_indices, weight, other)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([2, 4, 3])\n```",
        "torch_code": "E = torch.nn.functional.embedding(input_indices, weight, padding_idx=padding_idx, max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq, sparse=sparse)\nS = torch.add(E, other)\nY = torch.nn.functional.tanh(S)\nif out is not None:\n    out.copy_(Y)\n    return out\nreturn Y",
        "torch_cnt": 3,
        "other": "- The `other` tensor must be broadcastable to the shape of the embeddings retrieved by `torch.nn.functional.embedding`.\n- All parameters related to `torch.nn.functional.embedding` are passed through to allow for options like `padding_idx`, `max_norm`, etc.\n- This function supports autograd for gradient computation.\n- All operations are differentiable and support backpropagation.",
        "difficulty": 3,
        "params_cnt": 9,
        "file": "fused_embedding_add_tanh.py"
    },
    {
        "name": "fused_mv_sigmoid_sub",
        "func_inputs": "fused_mv_sigmoid_sub(input, vec, other, alpha=1, *, out=None) -> Tensor; input (Tensor): Input matrix A of shape (n, m); vec (Tensor): Input vector \\mathbf{v} of shape (m); other (Tensor or Number): Tensor or scalar b to subtract from the sigmoid output, scaled by \\alpha; alpha (Number, optional): Scalar multiplier for other. Default: `1`; out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`",
        "description": "Performs a fused operation combining matrix-vector multiplication, sigmoid activation, and subtraction.",
        "math": "Given an input matrix A, a vector \\mathbf{v}, and another tensor or scalar b, the function computes:\n\\[\n\\begin{align*}\n\\mathbf{z} &= A \\mathbf{v} \\\\\n\\mathbf{s} &= \\sigma(\\mathbf{z}) = \\frac{1}{1 + \\exp(-\\mathbf{z})} \\\\\n\\mathbf{y} &= \\mathbf{s} - \\alpha b\n\\end{align*}\n\\]",
        "example": "import torch\nimport torch.nn.functional as F\n\ndef fused_mv_sigmoid_sub(input, vec, other, alpha=1, *, out=None):\n    z = torch.mv(input, vec)\n    s = torch.nn.functional.sigmoid(z)\n    y = torch.sub(s, other, alpha=alpha)\n    if out is not None:\n        out.copy_(y)\n        return out\n    return y\n\n# Example usage\ninput = torch.tensor([[1.0, 2.0, 3.0],\n                      [4.0, 5.0, 6.0]])\nvec = torch.tensor([0.1, 0.2, 0.3])\nother = torch.tensor([0.5, 0.5])\nalpha = 1.0\n\noutput = fused_mv_sigmoid_sub(input, vec, other, alpha=alpha)\nprint(\"Output:\", output)\n# Output: tensor([-0.4256, -0.4013])",
        "torch_code": "z = torch.mv(input, vec)\ns = torch.nn.functional.sigmoid(z)\ny = torch.sub(s, other, alpha=alpha)",
        "torch_cnt": 3,
        "other": "- The shapes of `input` and `vec` must be compatible for matrix-vector multiplication.\n- The `other` tensor must be broadcastable to the shape of the output from the sigmoid function.\n- The function supports autograd for gradient computation.\n- All operations are differentiable and support backpropagation.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "fused_mv_sigmoid_sub.py"
    },
    {
        "name": "add_gelu",
        "func_inputs": "def add_gelu(input, other, alpha=1, approximate='none', out=None) -> Tensor: input (Tensor): The input tensor. other (Tensor or Number): The tensor or number to add to input. alpha (Number, optional): The multiplier for other. Default is 1. approximate (str, optional): The approximation method for GELU. Default is 'none'. out (Tensor, optional): The output tensor.",
        "description": "Adds the tensor or number `other`, scaled by the multiplier `alpha`, to the input tensor `input`, and then applies the Gaussian Error Linear Units (GELU) activation function to the result.",
        "math": "\\text{out}_i = \\text{GELU}(\\text{input}_i + \\text{alpha} \\times \\text{other}_i) where GELU is defined as: - \\text{GELU}(x) = x * \\Phi(x) when approximate is 'none', - \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3))) when approximate is 'tanh'.",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.tensor([1.0, -2.0, 3.0])\n>>> b = torch.tensor([0.5, 1.5, -1.0])\n>>> result = add_gelu(a, b, alpha=2)\n>>> result\n>>> result = add_gelu(a, b, alpha=2, approximate='tanh')\n>>> result",
        "torch_code": "add_result = torch.add(input, other, alpha=alpha)\nreturn torch.nn.functional.gelu(add_result, approximate=approximate, out=out)",
        "torch_cnt": 2,
        "other": "The GELU function is defined with two methods: an exact method using the Cumulative Distribution Function for Gaussian Distribution, and an approximate method using a tanh-based formula.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "add_gelu.py"
    },
    {
        "name": "fused_cosine_embedding_loss_with_normalization",
        "func_inputs": "def fused_cosine_embedding_loss_with_normalization(input1: torch.Tensor, input2: torch.Tensor, target: torch.Tensor, margin: float = 0, reduction: str = 'mean') -> torch.Tensor: input1 (Tensor): First input tensor to be normalized and compared. input2 (Tensor): Second input tensor to be normalized and compared. target (Tensor): Tensor label with values 1 or -1, where 1 encourages similarity and -1 encourages dissimilarity. margin (float, optional): Margin for dissimilarity. Default: 0. reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. Default: 'mean'.",
        "description": "Computes cosine embedding loss between two normalized tensors. This function first normalizes the inputs along the specified dimension using L2 normalization and then calculates the cosine embedding loss. The loss encourages similarity when the target is 1 and dissimilarity when the target is -1. It accepts optional parameters margin for dissimilarity control and reduction method for output aggregation.",
        "math": "",
        "example": ">>> input1 = torch.randn(3, 5, requires_grad=True)\n>>> input2 = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.tensor([1, -1, 1])  # Example labels for similarity/dissimilarity\n>>> loss = fused_cosine_embedding_loss_with_normalization(input1, input2, target)\n>>> print(loss)\n>>> loss.backward()",
        "torch_code": "torch.nn.functional.normalize\n,torch.nn.functional.normalize,torch.nn.functional.cosine_embedding_loss",
        "torch_cnt": 3,
        "other": "The inputs are first L2 normalized along dimension 1 before loss calculation. The reduction parameter can be 'none', 'mean', or 'sum', with default as 'mean'.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "fused_cosine_embedding_loss_with_normalization.py"
    },
    {
        "name": "fused_transformer_block",
        "func_inputs": "fused_transformer_block(input, weight1, weight2, residual, dropout_p=0.1, eps=1e-5, *, out=None) -> Tensor; input (Tensor): Input tensor of shape (*, N, D_in), where * denotes any number of batch dimensions.; weight1 (Tensor): Weight matrix of shape (D_in, D_k).; weight2 (Tensor): Weight matrix of shape (D_k, D_out).; residual (Tensor): Residual tensor to be added before layer normalization, must be broadcastable to the shape of Z_4.; dropout_p (float, optional): Probability of an element to be zeroed in the dropout layer. Default: 0.1.; eps (float, optional): A value added to the denominator for numerical stability in layer normalization. Default: 1e-5.; out (Tensor, optional): Output tensor. Ignored if None. Default: None.",
        "description": "Performs a sequence of operations commonly used in transformer models, combining matrix multiplication, softmax, dropout, another matrix multiplication, layer normalization, and addition (residual connection).",
        "math": "Given an input tensor X, weight matrices W_1 and W_2, and a residual tensor R, the function computes:\n\n    \\[\n    \\begin{align*}\n    Z_1 &= X W_1 \\\\\n    Z_2 &= \\text{softmax}(Z_1) \\\\\n    Z_3 &= \\text{dropout}(Z_2, p) \\\\\n    Z_4 &= Z_3 W_2 \\\\\n    Y &= \\text{LayerNorm}(Z_4 + R, \\gamma, \\beta, \\epsilon)\n    \\end{align*}\n    \\]\n\n    where:\n    - \\text{softmax}(Z) is applied along the last dimension.\n    - \\text{dropout}(Z, p) randomly zeroes elements of Z with probability p.\n    - \\text{LayerNorm} applies layer normalization with learnable parameters \\gamma and \\beta, and epsilon \\epsilon for numerical stability.\n    - R is the residual tensor added to Z_4 before layer normalization.",
        "example": "import torch\n    import torch.nn.functional as F\n\n    def fused_transformer_block(input, weight1, weight2, residual, dropout_p=0.1, eps=1e-5, *, out=None):\n        z1 = input @ weight1\n        z2 = F.softmax(z1, dim=-1)\n        z3 = F.dropout(z2, p=dropout_p, training=True)\n        z4 = z3 @ weight2\n        y = F.layer_norm(z4 + residual, normalized_shape=(z4.size(-1),), eps=eps)\n        if out is not None:\n            out.copy_(y)\n            return out\n        return y\n\n    # Example usage\n    N, D_in, D_k, D_out = 5, 10, 20, 10\n    input = torch.randn(N, D_in)\n    weight1 = torch.randn(D_in, D_k)\n    weight2 = torch.randn(D_k, D_out)\n    residual = torch.randn(N, D_out)\n\n    output = fused_transformer_block(input, weight1, weight2, residual)\n    print('Output shape:', output.shape)\n    # Output shape: torch.Size([5, 10])",
        "torch_code": "z1 = input @ weight1\n    z2 = F.softmax(z1, dim=-1)\n    z3 = F.dropout(z2, p=dropout_p, training=True)\n    z4 = z3 @ weight2\n    y = F.layer_norm(z4 + residual, normalized_shape=(z4.size(-1),), eps=eps)\n    if out is not None:\n        out.copy_(y)\n        return out\n    return y",
        "torch_cnt": 4,
        "other": "- The dimensions of `input` and `weight1` must be compatible for matrix multiplication: the last dimension of `input` must match the first dimension of `weight1`.\n    - The output of the first matrix multiplication has shape `(*, N, D_k)`.\n    - The `softmax` is applied along the last dimension (`dim=-1`).\n    - The `dropout` is applied during training. Set `training=False` to disable dropout during evaluation.\n    - The `layer_norm` is applied over the last dimension of the input tensor.\n    - The `residual` tensor must be broadcastable to the shape of `z4`.",
        "difficulty": 3,
        "params_cnt": 7,
        "file": "fused_transformer_block.py"
    },
    {
        "name": "torch.log1p",
        "func_inputs": "log1p(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the natural logarithm of (1 + input). This function is more accurate than torch.log for small values of input.",
        "math": "y_i = \\log_{e} (x_i + 1)",
        "example": ">>> a = torch.randn(5)\n>>> a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n>>> torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])",
        "torch_code": "torch.log1p(a)",
        "torch_cnt": 1,
        "other": "This function is more accurate than torch.log for small values of input.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "log1p.py"
    },
    {
        "name": "sigmoid_batch_norm",
        "func_inputs": "def sigmoid_batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-5) -> Tensor\nArgs:\n    input (Tensor): The input tensor of shape `(N, C)` or `(N, C, L)`, where `N` is batch size, `C` is the number of features or channels, and `L` is the sequence length.\n    running_mean (Tensor): The running mean of the input channels.\n    running_var (Tensor): The running variance of the input channels.\n    weight (Tensor, optional): Learnable scaling factor for each channel, typically represented as `γ`. Default: None.\n    bias (Tensor, optional): Learnable shift for each channel, typically represented as `β`. Default: None.\n    training (bool, optional): If `True`, updates running statistics; if `False`, uses them for normalization. Default: False.\n    momentum (float, optional): Value for updating the running mean and variance. Default: 0.1.\n    eps (float, optional): A small value added for numerical stability. Default: 1e-5.",
        "description": "Applies Batch Normalization over the input tensor across each channel, followed by applying the sigmoid activation function element-wise to the normalized result. This is useful for scaling the output to a range between 0 and 1 after normalization.",
        "math": "\\text{out} = \\sigma\\left(\\frac{\\text{input} - \\text{mean}}{\\sqrt{\\text{var} + \\epsilon}} * \\gamma + \\beta \\right) where \\sigma(x) = \\frac{1}{1 + \\exp(-x)} is the sigmoid function.",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define input tensor and batch normalization parameters\n>>> input = torch.randn(20, 10)\n>>> running_mean = torch.zeros(10)\n>>> running_var = torch.ones(10)\n>>> weight = torch.ones(10)\n>>> bias = torch.zeros(10)\n>>> # Apply batch normalization followed by sigmoid activation\n>>> result = sigmoid_batch_norm(input, running_mean, running_var, weight, bias, training=True)\n>>> result.shape\ntorch.Size([20, 10])",
        "torch_code": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight, bias, training=training, momentum=momentum, eps=eps)\ntorch.sigmoid(bn_result)",
        "torch_cnt": 2,
        "other": "The function normalizes the input tensor using batch normalization and then applies the sigmoid activation function to scale the output between 0 and 1.",
        "difficulty": 2,
        "params_cnt": 8,
        "file": "sigmoid_batch_norm.py"
    },
    {
        "name": "fused_hardsigmoid_batch_norm",
        "func_inputs": "fused_hardsigmoid_batch_norm(x: torch.Tensor, running_mean: torch.Tensor, running_var: torch.Tensor, weight: torch.Tensor = None, bias: torch.Tensor = None, training: bool = False, momentum: float = 0.1, eps: float = 1e-5, inplace: bool = False) -> torch.Tensor: Args: x (Tensor): Input tensor for batch normalization and activation. running_mean (Tensor): The running mean buffer (persistent). running_var (Tensor): The running variance buffer (persistent). weight (Tensor, optional): Learnable weight of size C for the normalized tensor. Default: ``None`` bias (Tensor, optional): Learnable bias of size C for the normalized tensor. Default: ``None`` training (bool, optional): Flag for training mode, used to update running estimates. Default: ``False`` momentum (float, optional): The value for the running mean and variance momentum. Default: ``0.1`` eps (float, optional): Small constant added to variance to improve numerical stability. Default: ``1e-5`` inplace (bool, optional): If ``True``, perform Hardsigmoid in-place. Default: ``False``",
        "description": "Applies Batch Normalization followed by the Hardsigmoid activation function on the input tensor `x`. This function performs batch normalization on `x` using the specified parameters and then applies Hardsigmoid activation element-wise on the normalized output.",
        "math": "",
        "example": ">>> x = torch.randn(4, 3, 32, 32) >>> running_mean = torch.zeros(3) >>> running_var = torch.ones(3) >>> weight = torch.ones(3) >>> bias = torch.zeros(3) >>> output = fused_hardsigmoid_batch_norm(x, running_mean, running_var, weight, bias, training=True) >>> print(output.shape) torch.Size([4, 3, 32, 32])",
        "torch_code": "torch.nn.functional.batch_norm(x, running_mean, running_var, weight, bias, training, momentum, eps); torch.nn.functional.hardsigmoid(normalized_x, inplace=inplace)",
        "torch_cnt": 2,
        "other": "The function includes optional parameters for learnable weight and bias, a training flag to update running estimates, momentum for running mean and variance, a small constant `eps` for numerical stability, and an `inplace` option for Hardsigmoid.",
        "difficulty": 2,
        "params_cnt": 8,
        "file": "fused_hardsigmoid_batch_norm.py"
    },
    {
        "name": "torch.special.zeta",
        "func_inputs": "zeta(input, other, *, out=None) -> Tensor; Args: input (Tensor): the input tensor corresponding to `x`. other (Tensor): the input tensor corresponding to `q`. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the Hurwitz zeta function, elementwise. The function calculates the sum of the series for each element in the input tensors, which represent the parameters x and q of the Hurwitz zeta function. The Riemann zeta function is a special case when q equals 1.",
        "math": "\\zeta(x, q) = \\sum_{k=0}^{\\infty} \\frac{1}{(k + q)^x}",
        "example": ">>> x = torch.tensor([2., 4.])\n>>> torch.special.zeta(x, 1)\ntensor([1.6449, 1.0823])\n>>> torch.special.zeta(x, torch.tensor([1., 2.]))\ntensor([1.6449, 0.0823])\n>>> torch.special.zeta(2, torch.tensor([1., 2.]))\ntensor([1.6449, 0.6449])",
        "torch_code": "torch.special.zeta(x, 1)\ntorch.special.zeta(x, torch.tensor([1., 2.]))\ntorch.special.zeta(2, torch.tensor([1., 2.]))",
        "torch_cnt": 3,
        "other": "The Riemann zeta function corresponds to the case when `q = 1`",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "zeta.py"
    },
    {
        "name": "symmetric_matrix_vector_norm",
        "func_inputs": "def symmetric_matrix_vector_norm(A: torch.Tensor, x: torch.Tensor, alpha: float, beta: float, p: float = 2.0) -> torch.Tensor: A (Tensor): A symmetric matrix of shape `(n, n)`. x (Tensor): A vector of shape `(n,)`. alpha (float): Scalar multiplier for the matrix-vector product. beta (float): Scalar multiplier added to `y`. p (float, optional): Order of the norm. Default is 2.0 (Euclidean norm).",
        "description": "Computes the matrix-vector product for a symmetric matrix `A` and a vector `x`, with scaling factors `alpha` and `beta`. Then calculates the norm of the resulting vector `y`. The operation performed is: 1. `y = alpha * torch.mv(A, x) + beta * y`, assuming `A` is symmetric. 2. `norm = torch.norm(y, p)`.",
        "math": "y = alpha * torch.mv(A, x) + beta * y\nnorm = torch.norm(y, p)",
        "example": ">>> A = torch.tensor([[2.0, -1.0], [-1.0, 2.0]])\n>>> x = torch.tensor([1.0, 1.0])\n>>> alpha = 1.5\n>>> beta = 0.5\n>>> result = symmetric_matrix_vector_norm(A, x, alpha, beta)\n>>> print(result)",
        "torch_code": "y = alpha * torch.mv(A, x) + beta * x\nnorm = torch.norm(y, p)",
        "torch_cnt": 2,
        "other": "Assumes `A` is symmetric.",
        "difficulty": 2,
        "params_cnt": 4,
        "file": "symmetric_matrix_vector_norm.py"
    },
    {
        "name": "softplus_linear",
        "func_inputs": "softplus_linear(input, weight, bias=None, beta=1, threshold=20) -> Tensor",
        "description": "Applies a linear transformation to the input tensor, followed by the Softplus activation function applied element-wise. This combined operation first performs a linear transformation and then introduces non-linearity with Softplus, which is smoother than ReLU and approximates it for large values. The function is particularly designed to improve numerical stability by reverting to a linear function for values above a specified threshold.",
        "math": "The combined operation is defined as: out = Softplus(Linear(x)), where the Softplus function is defined as: Softplus(x) = (1/β) * log(1 + exp(β * x))",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> input = torch.randn(2, 3)   # (batch_size, in_features)\n>>> weight = torch.randn(4, 3)  # (out_features, in_features)\n>>> bias = torch.randn(4)       # (out_features)\n>>> result = softplus_linear(input, weight, bias)\n>>> result.shape\ntorch.Size([2, 4])\n>>> result = softplus_linear(input, weight, bias, beta=2, threshold=10)\n>>> result.shape\ntorch.Size([2, 4])",
        "torch_code": "linear_result = torch.nn.functional.linear(input, weight, bias)\nreturn torch.nn.functional.softplus(linear_result, beta=beta, threshold=threshold)",
        "torch_cnt": 2,
        "other": "For values exceeding the threshold, the function helps maintain numerical stability by approximating a linear function, which enhances stability and prevents potential overflow.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "softplus_linear.py"
    },
    {
        "name": "fused_svd_reconstruct",
        "func_inputs": "fused_svd_reconstruct(A: Tensor) -> Tensor: The input matrix `A` of shape `(m, n)`.",
        "description": "Reconstructs the input matrix `A` using its Singular Value Decomposition (SVD). This function combines the Singular Value Decomposition (SVD) with matrix reconstruction. Given a matrix `A`, it performs the following operations: 1. Compute the SVD of `A`: A = U Σ V^H, where `U` and `Vh` are unitary matrices and `S` contains the singular values of `A`. 2. Reconstruct `A` as A_reconstructed = U Σ V^H.",
        "math": "A = U Σ V^H\nA_reconstructed = U diag(S) V^H",
        "example": ">>> A = torch.tensor([[3.0, 2.0, 2.0], [2.0, 3.0, -2.0]])\n>>> A_reconstructed = fused_svd_reconstruct(A)\n>>> print(A_reconstructed)",
        "torch_code": "U, S, Vh = torch.linalg.svd(A, full_matrices=False)\nA_reconstructed = U @ torch.diag(S) @ Vh",
        "torch_cnt": 2,
        "other": "The function returns the reconstructed matrix `A` of shape `(m, n)`, approximating the original matrix.",
        "difficulty": 5,
        "params_cnt": 1,
        "file": "fused_svd_reconstruct.py"
    },
    {
        "name": "fused_mul_add_logsoftmax_dropout_bmm",
        "func_inputs": "fused_mul_add_logsoftmax_dropout_bmm(input1, input2, other, mat2, p=0.5, training=True, inplace=False, dim=-1, *, out=None) -> Tensor",
        "description": "Performs a fused operation combining element-wise multiplication, addition, log-softmax activation, dropout, and batch matrix multiplication.",
        "math": "Given input tensors X_1, X_2, O, and M, the function computes:\n\n\\[\n\\begin{align*}\nZ &= X_1 \\odot X_2 \\\\\nS &= Z + O \\\\\nL &= \\log\\left( \\frac{\\exp(S)}{\\sum_j \\exp(S_j)} \\right) \\\\\nD &= \\text{Dropout}(L, p) \\\\\nY &= \\text{bmm}(D, M)\n\\end{align*}\n\\]\n\nwhere:\n\n- X_1 and X_2 are input tensors for element-wise multiplication.\n- \\odot denotes element-wise multiplication.\n- O is a tensor or scalar to be added to Z, must be broadcastable to the shape of Z.\n- \\log\\left( \\frac{\\exp(S)}{\\sum_j \\exp(S_j)} \\right) is the log-softmax function applied along dimension `dim`.\n- \\text{Dropout}(L, p) randomly zeroes elements of L with probability p.\n- \\text{bmm}(D, M) performs batch matrix multiplication.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_mul_add_logsoftmax_dropout_bmm(input1, input2, other, mat2, p=0.5, training=True, inplace=False, dim=-1, *, out=None):\n    Z = torch.mul(input1, input2)\n    S = torch.add(Z, other)\n    L = torch.nn.functional.log_softmax(S, dim=dim)\n    D = torch.nn.functional.dropout(L, p=p, training=training, inplace=inplace)\n    Y = torch.bmm(D, mat2)\n    if out is not None:\n        out.copy_(Y)\n        return out\n    return Y\n\n# Example usage\nB, N, D_in, D_out = 2, 3, 4, 5  # Batch size, sequence length, input dimension, output dimension\ninput1 = torch.randn(B, N, D_in)\ninput2 = torch.randn(B, N, D_in)\nother = torch.randn(B, N, D_in)\nmat2 = torch.randn(B, D_in, D_out)\n\noutput = fused_mul_add_logsoftmax_dropout_bmm(input1, input2, other, mat2, p=0.2, training=True, dim=-1)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([2, 3, 5])\n```",
        "torch_code": "Z = torch.mul(input1, input2)\nS = torch.add(Z, other)\nL = torch.nn.functional.log_softmax(S, dim=dim)\nD = torch.nn.functional.dropout(L, p=p, training=training, inplace=inplace)\nY = torch.bmm(D, mat2)\nif out is not None:\n    out.copy_(Y)\n    return out\nreturn Y",
        "torch_cnt": 5,
        "other": "- The shapes of `input1`, `input2`, and `other` must be broadcastable to each other.\n- The `mat2` tensor must have a shape compatible with the output of the dropout layer for batch matrix multiplication, i.e., `mat2` should have shape `(B, D_in, D_out)` if the dropout output has shape `(B, N, D_in)`.\n- The `log_softmax` function is applied along dimension `dim`, which should be the dimension of the features (typically `-1` for the last dimension).\n- The `dropout` is applied during training when `training=True`. Set `training=False` to disable dropout during evaluation.\n- All operations are differentiable and support autograd.",
        "difficulty": 3,
        "params_cnt": 8,
        "file": "fused_mul_add_logsoftmax_dropout_bmm.py"
    },
    {
        "name": "torch.nn.functional.selu",
        "func_inputs": "selu(input, inplace=False) -> Tensor",
        "description": "Applies the element-wise SELU (Scaled Exponential Linear Unit) function to the input tensor. The SELU function is defined as scale * (max(0, x) + min(0, alpha * (exp(x) - 1))), where the constants alpha and scale are fixed values with alpha approximately 1.673 and scale approximately 1.051.",
        "math": "SELU(x) = scale * (max(0,x) + min(0, alpha * (exp(x) - 1))), with alpha=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946.",
        "example": "",
        "torch_code": "torch.nn.functional.selu",
        "torch_cnt": 1,
        "other": "See torch.nn.SELU for more details.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "selu.py"
    },
    {
        "name": "scaled_add_norm",
        "func_inputs": "scaled_add_norm(y: Tensor, x: Tensor, alpha: float) -> Tensor: y (Tensor): The target tensor to be modified, of shape `(n,)`. x (Tensor): The tensor to be scaled and added to `y`, of shape `(n,)`. alpha (float): The scalar multiplier for `x`.",
        "description": "Computes `y += alpha * x` and returns the 2-norm of the modified `y`. The function takes a target tensor `y`, a tensor `x` to be scaled by a scalar `alpha`, and adds the scaled `x` to `y`. It then calculates and returns the 2-norm of the updated `y`.",
        "math": "y += alpha * x\nnorm = ||y||_2",
        "example": ">>> y = torch.tensor([1.0, 2.0, 3.0])\n>>> x = torch.tensor([0.5, -1.0, 2.0])\n>>> alpha = 2.0\n>>> result = scaled_add_norm(y, x, alpha)\n>>> print(result)  # Outputs the 2-norm of `y + alpha * x`",
        "torch_code": "y += alpha * x\nnorm = torch.norm(y)",
        "torch_cnt": 2,
        "other": "The function modifies the input tensor `y` in place and calculates the 2-norm using `torch.norm`.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "scaled_add_norm.py"
    },
    {
        "name": "leaky_relu_conv2d",
        "func_inputs": "def leaky_relu_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, negative_slope=0.01, inplace=False) -> Tensor",
        "description": "Applies a 2D convolution over the input tensor, followed by applying the Leaky ReLU activation function element-wise to the result. This allows for both feature extraction and non-linear activation in one step.",
        "math": "The combined operation is defined as:\n\n.. math::\n    \\text{out} = \\text{LeakyReLU}(\\text{conv2d}(\\text{input}))\n\nwhere the Leaky ReLU function is applied element-wise as:\n\n.. math::\n    \\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} \\times \\min(0, x)",
        "example": "Example::\n\n    >>> import torch\n    >>> import torch.nn.functional as F\n    >>> # Define inputs, weights, and optional bias\n    >>> inputs = torch.randn(1, 3, 5, 5)\n    >>> filters = torch.randn(6, 3, 3, 3)\n    >>> bias = torch.randn(6)\n    >>> # Apply leaky_relu_conv2d with padding and bias\n    >>> result = leaky_relu_conv2d(inputs, filters, bias=bias, padding=1, negative_slope=0.1)\n    >>> result.shape\n    torch.Size([1, 6, 5, 5])\n\n    >>> # Apply leaky_relu_conv2d without bias and with stride 2\n    >>> result = leaky_relu_conv2d(inputs, filters, stride=2)\n    >>> result.shape\n    torch.Size([1, 6, 2, 2])",
        "torch_code": "conv_result = torch.nn.functional.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\nreturn torch.nn.functional.leaky_relu(conv_result, negative_slope=negative_slope, inplace=inplace)",
        "torch_cnt": 2,
        "other": "The function combines 2D convolution and Leaky ReLU activation in one step, allowing for efficient computation.",
        "difficulty": 3,
        "params_cnt": 9,
        "file": "leaky_relu_conv2d.py"
    },
    {
        "name": "sqrt_exp",
        "func_inputs": "def sqrt_exp(input, out=None) -> Tensor: input (Tensor): The input tensor. out (Tensor, optional): The output tensor.",
        "description": "Computes the square root of each element in :attr:`input`, and then applies the exponential function to the square-rooted values. The combined operation is defined as: out_i = e^(sqrt(input_i))",
        "math": "out_i = e^(sqrt(input_i))",
        "example": ">>> import torch\n>>> a = torch.tensor([0.25, 1.0, 4.0, 9.0])\n>>> result = sqrt_exp(a)\n>>> result\ntensor([ 1.2840,  2.7183,  7.3891, 20.0855])\n>>> out = torch.empty(4)\n>>> sqrt_exp(a, out=out)\n>>> out\ntensor([ 1.2840,  2.7183,  7.3891, 20.0855])",
        "torch_code": "torch.sqrt(input)\ntorch.exp(sqrt_result, out=out)",
        "torch_cnt": 2,
        "other": "N/A",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "sqrt_exp.py"
    },
    {
        "name": "cos_avg_pool1d",
        "func_inputs": "def cos_avg_pool1d(input: torch.Tensor, kernel_size: int, stride: int = None, padding: int = 0, ceil_mode: bool = False, count_include_pad: bool = True) -> torch.Tensor\n    \n    input (Tensor): The input tensor of shape (minibatch, in_channels, iW).\n    kernel_size (int): Size of the pooling window.\n    stride (int, optional): Stride of the pooling window. Defaults to `kernel_size`.\n    padding (int, optional): Zero-padding added to both sides of the input. Default is 0.\n    ceil_mode (bool, optional): If True, uses ceil instead of floor to compute the output shape. Default is False.\n    count_include_pad (bool, optional): If True, includes the zero-padding in the averaging calculation. Default is True.",
        "description": "Applies the cosine function element-wise to the input tensor, followed by a 1D average pooling. The function first computes the cosine of each element in the input tensor, then applies 1D average pooling over the resulting tensor with the specified kernel size, stride, padding, ceil mode, and padding inclusion.",
        "math": "\\text{output} = \\text{avg\\_pool1d}(\\cos(\\text{input}))",
        "example": ">>> input = torch.tensor([[[1.0, 2.0, 3.0, 4.0, 5.0]]])\n>>> cos_avg_pool1d(input, kernel_size=2, stride=1)\ntensor([[[-0.4161, -0.8135, -0.8576, -0.2751]]])",
        "torch_code": "cos_result = torch.cos(input)\nreturn torch.nn.functional.avg_pool1d(cos_result, kernel_size=kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode, count_include_pad=count_include_pad)",
        "torch_cnt": 2,
        "other": "The function involves computing the cosine transformation followed by pooling, and handles parameters like stride, padding, and ceil mode.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "cos_avg_pool1d.py"
    },
    {
        "name": "sum_std",
        "func_inputs": "def sum_std(input, dim=None, keepdim=False, dtype=None, correction=1, out=None) -> Tensor: input (Tensor): The input tensor. dim (int or tuple of ints, optional): The dimension(s) to reduce. If None, all dimensions are reduced. keepdim (bool, optional): Whether the output tensor has dim retained or not. Default is False. dtype (torch.dtype, optional): The desired data type of the returned tensor. If specified, the input tensor is cast to dtype before the operation. Default: None. correction (int, optional): Difference between the sample size and sample degrees of freedom. Default is 1 (Bessel's correction). out (Tensor, optional): The output tensor.",
        "description": "Computes the sum of elements in the input tensor along the specified dimension(s), followed by calculating the standard deviation of the summed values.",
        "math": "\\text{sum} = \\sum_{i=0}^{N-1} x_i\n\n\\sigma = \\sqrt{\\frac{1}{\\max(0,~N - \\delta N)}\\sum_{i=0}^{N-1}(x_i-\\bar{x})^2}",
        "example": ">>> import torch\n>>> a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n>>> # Compute the sum of all elements, then compute the std of this sum\n>>> result = sum_std(a)\n>>> result\ntensor(0.)\n\n>>> # Compute the sum along dimension 1, then compute the std of each row's sum\n>>> result = sum_std(a, dim=1)\n>>> result\ntensor(1.5000)\n\n>>> # Compute the sum along dimension 1 with keepdim=True, then compute std\n>>> result = sum_std(a, dim=1, keepdim=True)\n>>> result\ntensor([[1.5000],\n        [1.5000]])",
        "torch_code": "sum_result = torch.sum(input, dim=dim, keepdim=keepdim, dtype=dtype)\nreturn torch.std(sum_result, correction=correction, keepdim=keepdim, out=out)",
        "torch_cnt": 2,
        "other": "The function uses Bessel's correction by default with a correction value of 1.",
        "difficulty": 4,
        "params_cnt": 6,
        "file": "sum_std.py"
    },
    {
        "name": "mul_relu",
        "func_inputs": "def mul_relu(input, other, inplace=False, out=None) -> Tensor: input (Tensor): The input tensor to be multiplied. other (Tensor or Number): The tensor or number to multiply with `input`. inplace (bool, optional): If True, modifies `input` in-place, if possible. Default is False. out (Tensor, optional): The output tensor.",
        "description": "This function performs element-wise multiplication of two inputs, input and other, and then applies the Rectified Linear Unit (ReLU) function to the result, which replaces all negative values with zero.",
        "math": "ReLU(x) = max(0, x); out_i = ReLU(input_i * other_i)",
        "example": ">>> import torch; >>> import torch.nn.functional as F; >>> a = torch.tensor([1.0, -2.0, 3.0]); >>> b = torch.tensor([0.5, 1.5, -1.0]); >>> result = mul_relu(a, b); >>> result; tensor([0.5000, 0.0000, 0.0000]); >>> result = mul_relu(a, 2); >>> result; tensor([2.0000, 0.0000, 6.0000])",
        "torch_code": "mul_result = torch.mul(input, other, out=out); return F.relu(mul_result, inplace=inplace)",
        "torch_cnt": 2,
        "other": "The function uses torch.mul for multiplication and F.relu for the ReLU operation.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "mul_relu.py"
    },
    {
        "name": "gelu_conv2d",
        "func_inputs": "def gelu_conv2d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None, stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int], str] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, approximate: str = 'none', out: Optional[Tensor] = None) -> Tensor\nArgs:\n    input (Tensor): The input tensor of shape `(minibatch, in_channels, iH, iW)`.\n    weight (Tensor): The convolution filters of shape `(out_channels, in_channels / groups, kH, kW)`.\n    bias (Tensor, optional): Optional bias tensor of shape `(out_channels)`. Default: ``None``.\n    stride (int or tuple, optional): The stride of the convolution kernel. Can be a single number or a tuple `(sH, sW)`. Default: 1.\n    padding (int, tuple, or string, optional): Padding on both sides of the input. Can be 'valid', 'same', single number, or tuple `(padH, padW)`. Default: 0.\n    dilation (int or tuple, optional): The spacing between kernel elements. Default: 1.\n    groups (int, optional): Number of groups to split the input into. Default: 1.\n    approximate (str, optional): The approximation method for GELU. Default is 'none'.\n    out (Tensor, optional): The output tensor.",
        "description": "Applies a 2D convolution over an input tensor with specified filters, followed by applying the Gaussian Error Linear Units (GELU) activation function element-wise to the result. This helps introduce non-linearity after the convolution operation.",
        "math": "The combined operation is defined as:\n\n.. math::\n    \\text{out} = \\text{GELU}(\\text{conv2d}(\\text{input}, \\text{weight}))\n\nwhere GELU is computed as:\n\n- If :attr:`approximate` is 'none', GELU is computed as:\n\n  .. math::\n      \\text{GELU}(x) = x * \\Phi(x)\n\n  where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\n\n- If :attr:`approximate` is 'tanh', GELU is approximated as:\n\n  .. math::\n      \\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))",
        "example": "Example::\n\n    >>> import torch\n    >>> import torch.nn.functional as F\n    >>> # Define inputs and filters\n    >>> inputs = torch.randn(1, 3, 5, 5)\n    >>> filters = torch.randn(2, 3, 3, 3)\n    >>> bias = torch.randn(2)\n    >>> # Apply convolution with bias, then GELU activation\n    >>> result = gelu_conv2d(inputs, filters, bias=bias, padding=1)\n    >>> result.shape\n    torch.Size([1, 2, 5, 5])\n\n    >>> # Applying convolution without bias and using GELU with 'tanh' approximation\n    >>> result = gelu_conv2d(inputs, filters, padding=1, approximate='tanh')\n    >>> result.shape\n    torch.Size([1, 2, 5, 5])",
        "torch_code": "conv_result = torch.nn.functional.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\nreturn torch.nn.functional.gelu(conv_result, approximate=approximate, out=out)",
        "torch_cnt": 2,
        "other": "The function combines 2D convolution and GELU activation, with options for approximation methods for GELU.",
        "difficulty": 4,
        "params_cnt": 9,
        "file": "gelu_conv2d.py"
    },
    {
        "name": "fused_instance_norm_selu_conv2d",
        "func_inputs": "fused_instance_norm_selu_conv2d(input: Tensor, weight: Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1, num_features=None, eps=1e-5, momentum=0.1, affine=False, track_running_stats=False) -> Tensor: input (Tensor): Input tensor of shape (minibatch, in_channels, iH, iW). weight (Tensor): Weights for the convolution, shape (out_channels, in_channels / groups, kH, kW). bias (Tensor, optional): Bias for the convolution layer, shape (out_channels). stride (int or tuple, optional): Stride of the convolution. Default is 1. padding (int or tuple, optional): Padding for the convolution. Default is 0. dilation (int or tuple, optional): Spacing between kernel elements. Default is 1. groups (int, optional): Number of blocked connections from input channels to output channels. Default is 1. num_features (int, optional): Number of features or channels in the input for instance normalization. eps (float, optional): A value added to the denominator for numerical stability in instance normalization. Default is 1e-5. momentum (float, optional): Momentum for updating running statistics in instance normalization. Default is 0.1. affine (bool, optional): If True, instance normalization has learnable affine parameters. Default is False. track_running_stats (bool, optional): If True, tracks running mean and variance for instance normalization. Default is False.",
        "description": "Applies a fused operation consisting of a 2D convolution followed by SELU activation and instance normalization on the input tensor.",
        "math": "",
        "example": ">>> input = torch.randn(1, 4, 5, 5)  # Example input tensor >>> weight = torch.randn(8, 4, 3, 3)  # Convolution weights >>> output = fused_instance_norm_selu_conv2d(input, weight, num_features=8) >>> print(output.shape)  # Expected output shape: (1, 8, 5, 5)",
        "torch_code": "conv_output = torch.nn.functional.conv2d(input, weight, bias, stride, padding, dilation, groups) selu_output = torch.nn.functional.selu(conv_output) normalized_output = torch.nn.functional.instance_norm(selu_output, num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)",
        "torch_cnt": 3,
        "other": "The function combines convolution, SELU activation, and instance normalization in a single operation.",
        "difficulty": 4,
        "params_cnt": 11,
        "file": "fused_instance_norm_selu_conv2d.py"
    },
    {
        "name": "fused_fractional_max_pool2d_with_relu",
        "func_inputs": "def fused_fractional_max_pool2d_with_relu(input: torch.Tensor, kernel_size, output_size=None, output_ratio=None, return_indices=False) -> torch.Tensor: Input (Tensor): Input tensor. kernel_size (int or Tuple[int, int]): Size of the pooling window. output_size (Tuple[int, int], optional): Target output size (height, width). output_ratio (Tuple[float, float], optional): If set, output size is scaled as a ratio of the input size. return_indices (bool, optional): If `True`, return the max pooling indices along with the output.",
        "description": "Applies a ReLU activation followed by 2D fractional max pooling over an input signal composed of multiple planes. The input is first rectified (non-negative) and then pooled using fractional max pooling.",
        "math": "",
        "example": ">>> x = torch.randn(1, 3, 8, 8)  # Example input with shape (batch, channels, height, width)\n>>> # Apply ReLU followed by fractional max pooling with kernel size 3 and output size (2, 2)\n>>> output = fused_fractional_max_pool2d_with_relu(x, kernel_size=3, output_size=(2, 2))\n>>> print(output.shape)  # Expected output shape: (1, 3, 2, 2)",
        "torch_code": "torch.nn.functional.relu(input)\ntorch.nn.functional.fractional_max_pool2d(relu_output, kernel_size=kernel_size, output_size=output_size, output_ratio=output_ratio, return_indices=return_indices)",
        "torch_cnt": 2,
        "other": "The function combines ReLU activation with fractional max pooling, allowing for optional output size or ratio specification and the option to return pooling indices.",
        "difficulty": 4,
        "params_cnt": 5,
        "file": "fused_fractional_max_pool2d_with_relu.py"
    },
    {
        "name": "torch.special.chebyshev_polynomial_t",
        "func_inputs": "chebyshev_polynomial_t(input, n, *, out=None) -> Tensor; Args: input (Tensor): the input tensor. n (Tensor): Degree of the polynomial. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the Chebyshev polynomial of the first kind T_n(input). If n = 0, returns 1. If n = 1, returns input. For n < 6 or |input| > 1, uses a recursive formula. Otherwise, uses an explicit trigonometric formula.",
        "math": "T_{n + 1}(input) = 2 \\times input \\times T_{n}(input) - T_{n - 1}(input)\nT_{n}(input) = \\text{cos}(n \\times \\text{arccos}(x))",
        "example": "",
        "torch_code": "torch.special.chebyshev_polynomial_t(input, n, out=None)",
        "torch_cnt": 1,
        "other": "If n = 0, returns 1. If n = 1, returns input. Uses recursion for n < 6 or |input| > 1, otherwise uses trigonometric formula.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "chebyshev_polynomial_t.py"
    },
    {
        "name": "torch.special.logit",
        "func_inputs": "logit(input, eps=None, *, out=None) -> Tensor; input (Tensor): the input tensor.; eps (float, optional): the epsilon for input clamp bound. Default: None; out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the logit of the elements of input. The input is clamped to [eps, 1 - eps] when eps is not None. When eps is None and input < 0 or input > 1, the function yields NaN.",
        "math": "y_{i} = \\ln(\\frac{z_{i}}{1 - z_{i}}); z_{i} = \\begin{cases} x_{i} & \\text{if eps is None} \\\\ \\text{eps} & \\text{if } x_{i} < \\text{eps} \\\\ x_{i} & \\text{if } \\text{eps} \\leq x_{i} \\leq 1 - \\text{eps} \\\\ 1 - \\text{eps} & \\text{if } x_{i} > 1 - \\text{eps} \\end{cases}",
        "example": ">>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])",
        "torch_code": "torch.special.logit(a, eps=1e-6)",
        "torch_cnt": 1,
        "other": "input is clamped to [eps, 1 - eps] when eps is not None. When eps is None and input < 0 or input > 1, the function yields NaN.",
        "difficulty": 2,
        "params_cnt": 3,
        "file": "logit.py"
    },
    {
        "name": "solve_symmetric_ldl",
        "func_inputs": "solve_symmetric_ldl(A, b, *, hermitian=False, out=None) -> Tensor A (Tensor): 形状为 (*, n, n) 的对称（或 Hermitian）矩阵，其中 * 是零个或多个批次维度。 b (Tensor): 形状为 (*, n) 或 (*, n, k) 的右端项张量。 hermitian (bool, 可选): 是否将 A 视为 Hermitian 矩阵。默认值：False。 out (Tensor, 可选): 输出张量。如果为 None，则忽略。默认值：None。",
        "description": "Solves a symmetric (or Hermitian) linear system A x = b using LDL decomposition. The function first decomposes A into L and D through LDL decomposition, reconstructs matrix A, and then uses `torch.linalg.solve` to solve the linear system.",
        "math": "Given a symmetric (or Hermitian) matrix A in \\mathbb{K}^{n \\times n} (where \\mathbb{K} is the real field \\mathbb{R} or complex field \\mathbb{C}), the LDL decomposition of A is represented as: A = L D L^{\\mathrm{T}} or A = L D L^{\\mathrm{H}}.",
        "example": "A = torch.tensor([[4., 1., 1.], [1., 3., 0.], [1., 0., 2.]]); b = torch.tensor([1., 2., 3.]); x = solve_symmetric_ldl(A, b); print(\"Solution x:\", x); residual = A @ x - b; print(\"Residual:\", residual)",
        "torch_code": "L, D, _ = torch.linalg.ldl_factor(A, hermitian=hermitian); D_mat = torch.diag_embed(D); if hermitian: A_reconstructed = L @ D_mat @ L.conj().transpose(-2, -1); else: A_reconstructed = L @ D_mat @ L.transpose(-2, -1); x = torch.linalg.solve(A_reconstructed, b); if out is not None: out.copy_(x); return out; return x",
        "torch_cnt": 4,
        "other": "This function supports batch processing; all computations are performed across batch dimensions.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "solve_symmetric_ldl.py"
    },
    {
        "name": "exp_sqrt",
        "func_inputs": "def exp_sqrt(input, out=None) -> Tensor; input (Tensor): The input tensor.; out (Tensor, optional): The output tensor.",
        "description": "Computes the exponential of each element in the input tensor, followed by calculating the square root of the result. Returns a tensor where each element is the result of applying exponential followed by square root to each element of input.",
        "math": "\\text{out}_i = \\sqrt{e^{\\text{input}_i}}",
        "example": ">>> import torch\n>>> a = torch.tensor([0.0, 1.0, 2.0, 3.0])\n>>> result = exp_sqrt(a)\n>>> result\ntensor([1.0000, 1.6487, 2.7183, 4.4817])\n\n>>> a = torch.tensor([-1.0, 0.5, 5.0, -3.0])\n>>> result = exp_sqrt(a)\n>>> result\ntensor([1.6487, 1.2840, 148.4132, 0.3679])",
        "torch_code": "torch.exp(input)\ntorch.sqrt(exp_result, out=out)",
        "torch_cnt": 2,
        "other": "This function will return NaN for input elements that result in negative values after `exp` and `sqrt` due to overflow.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "exp_sqrt.py"
    },
    {
        "name": "combined_activation",
        "func_inputs": "combined_activation(input, weight1, weight2, bias, *, out=None) -> Tensor; input (Tensor): Input tensor of shape (*, N, D_{in}), where * denotes any number of batch dimensions.; weight1 (Tensor): Weight matrix of shape (D_{in}, D_{out}).; weight2 (Tensor): Weight tensor for element-wise multiplication, must be broadcastable to the shape of the intermediate activation.; bias (Tensor): Bias tensor, must be broadcastable to the shape of the output.; out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Performs a sequence of operations combining matrix multiplication, sigmoid, tanh, element-wise multiplication, and addition. It supports batches of inputs, where any leading batch dimensions in `input` will be preserved in the output. The function's operations are differentiable and support autograd. The function ensures the dimensions of `input` and `weight1` are compatible for matrix multiplication, and that `weight2` and `bias` are broadcastable to the shape of the output tensor.",
        "math": "Given an input tensor X, weight matrices W_1 and W_2, and a bias b, the function computes: Y = (tanh(sigmoid(X W_1)) ⊙ W_2) + b\n\n- σ(z) = 1 / (1 + exp(-z)) is the sigmoid function applied element-wise.\n- tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z)) is the hyperbolic tangent function applied element-wise.\n- ⊙ denotes element-wise multiplication.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef combined_activation(input, weight1, weight2, bias, *, out=None):\n    z = torch.mm(input, weight1)\n    s = torch.sigmoid(z)\n    t = torch.tanh(s)\n    m = t * weight2\n    y = m + bias\n    if out is not None:\n        out.copy_(y)\n        return out\n    return y\n\n# Example usage\nN, D_in, D_out = 5, 10, 3\ninput = torch.randn(N, D_in)\nweight1 = torch.randn(D_in, D_out)\nweight2 = torch.randn(D_out)        # Must be broadcastable to (N, D_out)\nbias = torch.randn(D_out)           # Must be broadcastable to (N, D_out)\n\noutput = combined_activation(input, weight1, weight2, bias)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([5, 3])\n```",
        "torch_code": "torch.mm, torch.sigmoid, torch.tanh, torch.mul, torch.add",
        "torch_cnt": 5,
        "other": "The function supports differentiable operations and autograd. It requires compatibility in dimensions for matrix multiplication and broadcasting for element-wise operations.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "combined_activation.py"
    },
    {
        "name": "scaled_add_dot",
        "func_inputs": "def scaled_add_dot(y: Tensor, x: Tensor, alpha: float) -> Tensor: y (Tensor): The target tensor to be modified, of shape (n,). x (Tensor): The tensor to be scaled and added to y, of shape (n,). alpha (float): The scalar multiplier for x.",
        "description": "Computes `y += alpha * x` and returns the dot product of the modified `y` with itself. This fused function performs two operations: 1. Scales `x` by a factor of `alpha` and adds the result to `y`. 2. Computes the dot product of the modified `y` with itself.",
        "math": "y += alpha * x\ndot_product = torch.dot(y, y)",
        "example": ">>> y = torch.tensor([1.0, 2.0, 3.0])\n>>> x = torch.tensor([0.5, -1.0, 2.0])\n>>> alpha = 2.0\n>>> result = scaled_add_dot(y, x, alpha)\n>>> print(result)  # Outputs the dot product of `y + alpha * x` with itself",
        "torch_code": "y += alpha * x\ndot_product = torch.dot(y, y)",
        "torch_cnt": 2,
        "other": "The function modifies the input tensor `y` in place.",
        "difficulty": 1,
        "params_cnt": 3,
        "file": "scaled_add_dot.py"
    },
    {
        "name": "torch.tensordot",
        "func_inputs": "def tensordot(a: Tensor, b: Tensor, dims: Union[int, Tuple[List[int], List[int]], List[List[int]]]) -> Tensor: \nArgs:\n    a (Tensor): Left tensor to contract\n    b (Tensor): Right tensor to contract\n    dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to contract or explicit lists of dimensions for a and b respectively",
        "description": "Returns a contraction of a and b over multiple dimensions. It implements a generalized matrix product.",
        "math": "r_{i_0,...,i_{m-d}, i_d,...,i_n} = \\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}.",
        "example": ">>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n[4532., 4874.],\n[4664., 5018.],\n[4796., 5162.],\n[4928., 5306.]])\n\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436, 6.2922, 2.7556, -1.0732, 3.2741],\n[ 3.3161, 0.0704, 5.0187, -0.4079, -4.3126, 4.8744],\n[ 0.8223, 3.9445, 3.2168, -0.2400, 3.4117, 1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[ 7.7193, -2.4867, -10.3204],\n[ 1.5513, -14.4737, -6.5113],\n[ -0.2850, 4.2573, -3.5997]])",
        "torch_code": "torch.tensordot(a, b, dims)",
        "torch_cnt": 1,
        "other": "The sizes in the contracted dimensions must match, but broadcasted dimensions are handled.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "tensordot.py"
    },
    {
        "name": "torch.linalg.qr",
        "func_inputs": "qr(A, mode='reduced', *, out=None) -> (Tensor, Tensor) A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions. mode (str, optional): one of `'reduced'`, `'complete'`, `'r'`. Controls the shape of the returned tensors. Default: `'reduced'`. out (tuple, optional): output tuple of two tensors. Ignored if `None`. Default: `None`.",
        "description": "Computes the QR decomposition of a matrix. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions. The parameter mode chooses between the full and reduced QR decomposition. It is always differentiable for 'reduced' mode, differentiable for 'complete' mode when m <= n, and never differentiable for 'r' mode.",
        "math": "A = QR where Q is orthogonal in the real case and unitary in the complex case, and R is upper triangular with real diagonal. For tall matrices (m > n), the reduced QR decomposition is A = QR with Q in K^{m x n} and R in K^{n x n}.",
        "example": "Examples::\n\n    >>> A = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n    >>> Q, R = torch.linalg.qr(A)\n    >>> Q\n    tensor([[-0.8571,  0.3943,  0.3314],\n            [-0.4286, -0.9029, -0.0343],\n            [ 0.2857, -0.1714,  0.9429]])\n    >>> R\n    tensor([[ -14.0000,  -21.0000,   14.0000],\n            [   0.0000, -175.0000,   70.0000],\n            [   0.0000,    0.0000,  -35.0000]])\n    >>> (Q @ R).round()\n    tensor([[  12.,  -51.,    4.],\n            [   6.,  167.,  -68.],\n            [  -4.,   24.,  -41.]])\n    >>> (Q.T @ Q).round()\n    tensor([[ 1.,  0.,  0.],\n            [ 0.,  1., -0.],\n            [ 0., -0.,  1.]])\n    >>> Q2, R2 = torch.linalg.qr(A, mode='r')\n    >>> Q2\n    tensor([])\n    >>> torch.equal(R, R2)\n    True\n    >>> A = torch.randn(3, 4, 5)\n    >>> Q, R = torch.linalg.qr(A, mode='complete')\n    >>> torch.dist(Q @ R, A)\n    tensor(1.6099e-06)\n    >>> torch.dist(Q.mT @ Q, torch.eye(4))\n    tensor(6.2158e-07)",
        "torch_code": "torch.linalg.qr(A)\ntorch.linalg.qr(A, mode='r')\ntorch.linalg.qr(A, mode='complete')",
        "torch_cnt": 3,
        "other": "Differences with numpy.linalg.qr: mode='raw' is not implemented. Unlike numpy.linalg.qr, this function always returns a tuple of two tensors. When mode='r', the Q tensor is an empty tensor. The elements in the diagonal of R are not necessarily positive, making the QR decomposition unique only up to the sign of the diagonal of R. The QR decomposition is only well-defined if the first k = min(m, n) columns of every matrix in A are linearly independent.",
        "difficulty": 5,
        "params_cnt": 3,
        "file": "qr.py"
    },
    {
        "name": "torch.asin",
        "func_inputs": "asin(input, *, out=None) -> Tensor: input (Tensor): the input tensor. out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the arcsine of the elements of the input tensor. The function computes the inverse sine (arcsine) for each element in the input tensor.",
        "math": "\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i})",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])",
        "torch_code": "torch.asin(a)",
        "torch_cnt": 1,
        "other": "The function returns NaN for input values outside the range [-1, 1] as arcsine is not defined for those values.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "asin.py"
    },
    {
        "name": "fused_masked_select_add_gelu",
        "func_inputs": "fused_masked_select_add_gelu(input, mask, other, *, alpha=1, approximate='none', out=None) -> Tensor",
        "description": "This function performs a fused operation combining masked selection, addition, and GELU activation. It first selects elements from the input tensor based on a boolean mask, then adds a scalar or tensor (scaled by alpha) to the selected values, and finally applies the GELU (Gaussian Error Linear Unit) activation function element-wise to the result.",
        "math": "Z = masked_select(X, M)\nS = Z + alpha * O\nY = GELU(S)",
        "example": "import torch\nimport torch.nn.functional as F\n\ninput = torch.randn(3, 4)\nprint('Input tensor:')\nprint(input)\n\nmask = input.ge(0)\nprint('\\nMask tensor:')\nprint(mask)\n\nother = torch.tensor(1.0)\noutput = fused_masked_select_add_gelu(input, mask, other)\nprint('\\nOutput tensor after fused operation:')\nprint(output)",
        "torch_code": "Z = torch.masked_select(input, mask)\nS = torch.add(Z, other, alpha=alpha)\nY = torch.nn.functional.gelu(S, approximate=approximate)",
        "torch_cnt": 3,
        "other": "The function is differentiable and supports autograd. The mask and other tensor must be broadcastable to the shape of the selected elements. The 'approximate' parameter can be set to 'tanh' for a faster, approximate GELU computation.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "fused_masked_select_add_gelu.py"
    },
    {
        "name": "fused_pairwise_distance_adaptive_avg_pool2d",
        "func_inputs": "def fused_pairwise_distance_adaptive_avg_pool2d(x1: torch.Tensor, x2: torch.Tensor, output_size: int or tuple, p: float = 2.0, eps: float = 1e-6, keepdim: bool = False) -> torch.Tensor: x1 (Tensor): First input tensor for adaptive average pooling and distance calculation. x2 (Tensor): Second input tensor for adaptive average pooling and distance calculation. output_size (int or tuple): The target output size for the adaptive average pooling. p (float, optional): The norm degree for pairwise distance calculation. Default: 2.0 eps (float, optional): Small value to avoid division by zero in pairwise distance. Default: 1e-6 keepdim (bool, optional): Whether to keep the reduced dimension. Default: False",
        "description": "This function applies adaptive average pooling to the input tensors `x1` and `x2` to resize them to the specified `output_size`, and then computes the pairwise distance between the pooled outputs. The function first applies `adaptive_avg_pool2d` to each input tensor, and then calculates the pairwise distance using the specified norm `p`. A small value `eps` is added to avoid division by zero during distance calculation. The function can also retain the reduced dimension of the output via the `keepdim` parameter.",
        "math": "No explicit formula provided. The function applies adaptive average pooling followed by pairwise distance calculation with norm p and epsilon to avoid division by zero.",
        "example": ">>> x1 = torch.randn(4, 3, 32, 32)\n>>> x2 = torch.randn(4, 3, 32, 32)\n>>> output = fused_pairwise_distance_adaptive_avg_pool2d(x1, x2, output_size=(8, 8))\n>>> print(output.shape)\ntorch.Size([4, 8, 8])",
        "torch_code": "pooled_x1 = torch.nn.functional.adaptive_avg_pool2d(x1, output_size)\npooled_x2 = torch.nn.functional.adaptive_avg_pool2d(x2, output_size)\noutput = torch.nn.functional.pairwise_distance(pooled_x1, pooled_x2, p=p, eps=eps, keepdim=keepdim)",
        "torch_cnt": 3,
        "other": "The function combines adaptive average pooling and pairwise distance calculation in a sequential manner.",
        "difficulty": 4,
        "params_cnt": 6,
        "file": "fused_pairwise_distance_adaptive_avg_pool2d.py"
    },
    {
        "name": "add_mean",
        "func_inputs": "def add_mean(input, other, dim=None, alpha=1, keepdim=False, dtype=None, out=None) -> Tensor: input (Tensor): The input tensor. other (Tensor or Number): The tensor or number to add to input. dim (int or tuple of ints, optional): The dimension(s) to reduce. Default: None. alpha (Number, optional): The multiplier for other. Default: 1. keepdim (bool, optional): Whether the output tensor has dim retained or not. Default: False. dtype (torch.dtype, optional): The desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation. This is useful for preventing data type overflows. Default: None. out (Tensor, optional): The output tensor.",
        "description": "Adds the `other` tensor, scaled by `alpha`, to the `input` tensor and computes the mean value along the specified dimension. If no dimension is specified, it computes the mean over all elements. Supports broadcasting, type promotion, and works with integer, float, and complex inputs.",
        "math": "\\text{out}_i = \\text{mean}(\\text{input}_i + \\text{alpha} \\times \\text{other}_i)",
        "example": ">>> import torch\n>>> a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n>>> b = torch.tensor([[0.5, 0.5], [0.5, 0.5]])\n>>> # Add b to a, then compute the mean of the result across all elements\n>>> result = add_mean(a, b)\n>>> result\ntensor(2.5)\n\n>>> # Add b to a, then compute the mean along dimension 1\n>>> result = add_mean(a, b, dim=1)\n>>> result\ntensor([1.75, 3.75])",
        "torch_code": "add_result = torch.add(input, other, alpha=alpha)\nreturn torch.mean(add_result, dim=dim, keepdim=keepdim, dtype=dtype, out=out)",
        "torch_cnt": 2,
        "other": "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.",
        "difficulty": 4,
        "params_cnt": 7,
        "file": "add_mean.py"
    },
    {
        "name": "fused_layer_norm_relu_linear",
        "func_inputs": "fused_layer_norm_relu_linear(input: Tensor, weight: Tensor, bias=None, normalized_shape=None, eps=1e-5, elementwise_affine=True) -> Tensor: Input (Tensor): Input tensor with shape (*, in_features). Weight (Tensor): Weights for the linear transformation, shape (out_features, in_features). Bias (Tensor, optional): Bias for the linear transformation, shape (out_features). Normalized_shape (int or list or torch.Size, optional): Shape of the dimensions to normalize. Eps (float, optional): A value added to the denominator for numerical stability. Default is 1e-5. Elementwise_affine (bool, optional): If True, layer normalization has learnable parameters. Default is True. Returns: Tensor: Result after applying the linear transformation, ReLU, and layer normalization. Example: >>> input = torch.randn(4, 5) # Example input tensor >>> weight = torch.randn(3, 5) # Linear transformation weights >>> bias = torch.randn(3) # Bias for linear layer >>> normalized_shape = 3 >>> # Apply fused operation >>> output = fused_layer_norm_relu_linear(input, weight, bias, normalized_shape) >>> print(output.shape) # Expected output shape: (4, 3)",
        "description": "Applies a fused operation consisting of a linear transformation followed by ReLU activation and layer normalization on the input tensor.",
        "math": "",
        "example": ">>> input = torch.randn(4, 5)  # Example input tensor\n>>> weight = torch.randn(3, 5)  # Linear transformation weights\n>>> bias = torch.randn(3)  # Bias for linear layer\n>>> normalized_shape = 3\n>>> # Apply fused operation\n>>> output = fused_layer_norm_relu_linear(input, weight, bias, normalized_shape)\n>>> print(output.shape)  # Expected output shape: (4, 3)",
        "torch_code": "torch.nn.functional.linear(input, weight, bias)\ntorch.nn.functional.relu(linear_output)\ntorch.nn.functional.layer_norm(relu_output, normalized_shape, eps=eps, elementwise_affine=elementwise_affine)",
        "torch_cnt": 3,
        "other": "The function performs a sequence of operations: linear transformation, ReLU activation, and layer normalization. It supports optional bias and learnable parameters for layer normalization.",
        "difficulty": 4,
        "params_cnt": 6,
        "file": "fused_layer_norm_relu_linear.py"
    },
    {
        "name": "fused_add_mul_groupnorm",
        "func_inputs": "fused_add_mul_groupnorm(input1, input2, weight, bias, num_groups, eps=1e-5, *, out=None) -> Tensor; input1 (Tensor): The first input tensor X; input2 (Tensor): The second input tensor Y, must be broadcastable to the shape of X; weight (Tensor): Learnable weight parameter \\gamma of shape (C,), where C is the number of channels; bias (Tensor): Learnable bias parameter \\beta of shape (C,); num_groups (int): Number of groups to separate the channels into for group normalization; eps (float, optional): A value added to the denominator for numerical stability in group normalization. Default: `1e-5`; out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`",
        "description": "Performs a fused operation combining element-wise addition, element-wise multiplication, and group normalization. It takes two input tensors, adds them element-wise, multiplies the result with the second tensor, and then applies group normalization using learnable parameters for scaling and shifting. The function supports autograd for gradient computation and all operations are differentiable.",
        "math": "Given two input tensors X and Y, and learnable parameters \\gamma and \\beta for group normalization, the function computes:\n\n\\[\n\\begin{align*}\nZ &= X + Y \\\\\nM &= Z \\odot Y \\\\\nO &= \\text{GroupNorm}(M, \\gamma, \\beta, \\text{num\\_groups}, \\epsilon)\n\\end{align*}\n\\]\n\nwhere:\n- X and Y are input tensors.\n- \\odot denotes element-wise multiplication.\n- \\text{GroupNorm}(M, \\gamma, \\beta, \\text{num\\_groups}, \\epsilon) applies group normalization to M with \\gamma and \\beta as affine parameters, over the specified number of groups, and \\epsilon for numerical stability.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_add_mul_groupnorm(input1, input2, weight, bias, num_groups, eps=1e-5, *, out=None):\n    z = torch.add(input1, input2)\n    m = torch.mul(z, input2)\n    o = torch.nn.functional.group_norm(m, num_groups=num_groups, weight=weight, bias=bias, eps=eps)\n    if out is not None:\n        out.copy_(o)\n        return out\n    return o\n\n# Example usage\nN, C, H, W = 2, 4, 5, 5  # Batch size, channels, height, width\ninput1 = torch.randn(N, C, H, W)\ninput2 = torch.randn(N, C, H, W)\nweight = torch.ones(C)\nbias = torch.zeros(C)\nnum_groups = 2  # Number of groups for GroupNorm\n\noutput = fused_add_mul_groupnorm(input1, input2, weight, bias, num_groups)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([2, 4, 5, 5])\n```",
        "torch_code": "z = torch.add(input1, input2)\nm = torch.mul(z, input2)\no = torch.nn.functional.group_norm(m, num_groups=num_groups, weight=weight, bias=bias, eps=eps)",
        "torch_cnt": 3,
        "other": "- The shapes of `input1` and `input2` must be broadcastable to each other.\n- The `weight` and `bias` parameters must have shape `(C,)`, where `C` is the number of channels in the input tensors.\n- The `num_groups` parameter must divide the number of channels `C` evenly.\n- This function supports autograd for gradient computation.\n- All operations are differentiable and support backpropagation.",
        "difficulty": 2,
        "params_cnt": 6,
        "file": "fused_add_mul_groupnorm.py"
    },
    {
        "name": "torch.optim.SGD",
        "func_inputs": "def SGD(params, lr=1e-3, momentum=0, weight_decay=0, dampening=0, nesterov=False, maximize=False, foreach=None, differentiable=False, fused=None)",
        "description": "Implements stochastic gradient descent, optionally with momentum, weight decay, dampening, and Nesterov momentum. It can maximize or minimize an objective function and supports different optimization algorithms for performance.",
        "math": "\\begin{aligned} &g_t \\leftarrow \\nabla_{\\theta} f_t (\\theta_{t-1}) \\\\\\ &\\text{if} \\: \\lambda \\neq 0 \\\\\\ &g_t \\leftarrow g_t + \\lambda \\theta_{t-1} \\\\\\ &\\text{if} \\: \\mu \\neq 0 \\\\\\ &\\text{if} \\: t > 1 \\\\\\ &\\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t \\\\\\ &\\text{else} \\\\\\ &\\textbf{b}_t \\leftarrow g_t \\\\\\ &\\text{if} \\: \\textit{nesterov} \\\\\\ &g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t \\\\\\ &\\text{else} \\\\\\ &g_t  \\leftarrow  \\textbf{b}_t \\\\\\ &\\text{if} \\: \\textit{maximize} \\\\\\ &\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t \\\\\\ &\\text{else} \\\\\\ &\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t \\end{aligned}",
        "example": ">>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> optimizer.zero_grad()\n>>> loss_fn(model(input), target).backward()\n>>> optimizer.step()",
        "torch_code": "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)",
        "torch_cnt": 4,
        "other": "Nesterov momentum is based on a research paper. The algorithm prioritizes different implementations based on performance. It differs from some traditional frameworks in its handling of momentum. The initial momentum buffer is set to the gradient value at the first step.",
        "difficulty": 3,
        "params_cnt": 10,
        "file": "SGD.py"
    },
    {
        "name": "relu_batch_norm_conv2d",
        "func_inputs": "def relu_batch_norm_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, running_mean=None, running_var=None, bn_weight=None, bn_bias=None, training=False, momentum=0.1, eps=1e-5, inplace=False) -> Tensor\n\nArgs:\n    input (Tensor): The input tensor of shape (minibatch, in_channels, iH, iW).\n    weight (Tensor): The convolution filters of shape (out_channels, in_channels / groups, kH, kW).\n    bias (Tensor, optional): Optional bias tensor of shape (out_channels). Default: None.\n    stride (int or tuple, optional): The stride of the convolution kernel. Default: 1.\n    padding (int, tuple, or string, optional): Padding added to all sides of the input. Default: 0.\n    dilation (int or tuple, optional): The spacing between kernel elements. Default: 1.\n    groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n    running_mean (Tensor, optional): The running mean for batch normalization. Default: None.\n    running_var (Tensor, optional): The running variance for batch normalization. Default: None.\n    bn_weight (Tensor, optional): Learnable scaling factor for batch normalization (gamma). Default: None.\n    bn_bias (Tensor, optional): Learnable shift factor for batch normalization (beta). Default: None.\n    training (bool, optional): If True, updates running statistics for batch normalization. Default: False.\n    momentum (float, optional): Value for updating the running mean and variance in batch normalization. Default: 0.1.\n    eps (float, optional): A small value added for numerical stability in batch normalization. Default: 1e-5.\n    inplace (bool, optional): If True, performs ReLU in-place. Default: False.",
        "description": "Applies a 2D convolution over the input tensor, followed by batch normalization and then applies the ReLU activation function element-wise to the normalized result. This combined operation is useful for applying feature extraction, normalization, and non-linearity in one step, commonly used in convolutional neural networks (CNNs).",
        "math": "out = ReLU(BatchNorm(conv2d(input)))\nReLU(x) = max(0, x)\ny = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define input tensor, filters, and batch normalization parameters\n>>> inputs = torch.randn(1, 3, 5, 5)\n>>> filters = torch.randn(6, 3, 3, 3)\n>>> running_mean = torch.zeros(6)\n>>> running_var = torch.ones(6)\n>>> bn_weight = torch.ones(6)\n>>> bn_bias = torch.zeros(6)\n>>> # Apply relu_batch_norm_conv2d with padding\n>>> result = relu_batch_norm_conv2d(inputs, filters, running_mean=running_mean, running_var=running_var, bn_weight=bn_weight, bn_bias=bn_bias, padding=1, training=True)\n>>> result.shape\ntorch.Size([1, 6, 5, 5])\n\n>>> # Apply relu_batch_norm_conv2d without batch norm weights and with stride 2\n>>> result = relu_batch_norm_conv2d(inputs, filters, running_mean=running_mean, running_var=running_var, padding=1, stride=2, training=True)\n>>> result.shape\ntorch.Size([1, 6, 3, 3])",
        "torch_code": "conv_result = torch.nn.functional.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\nbn_result = torch.nn.functional.batch_norm(conv_result, running_mean, running_var, bn_weight, bn_bias, training=training, momentum=momentum, eps=eps)\nreturn torch.nn.functional.relu(bn_result, inplace=inplace)",
        "torch_cnt": 3,
        "other": "The function combines convolution, batch normalization, and ReLU activation in a single step, which is a common pattern in CNNs for efficient computation.",
        "difficulty": 4,
        "params_cnt": 15,
        "file": "relu_batch_norm_conv2d.py"
    },
    {
        "name": "torch.nn.functional.conv2d",
        "func_inputs": "conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor Args: input: input tensor of shape (minibatch , in_channels , iH , iW) weight: filters of shape (out_channels , in_channels/groups , kH , kW) bias: optional bias tensor of shape (out_channels). Default: None stride: the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'}, single number or a tuple (padH, padW). Default: 0 dilation: the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 groups: split input into groups, both in_channels and out_channels should be divisible by the number of groups. Default: 1",
        "description": "Applies a 2D convolution over an input image composed of several input planes. Supports TensorFloat32. May select a nondeterministic algorithm on CUDA with CuDNN for performance. Supports complex data types.",
        "math": "",
        "example": ">>> # With square kernels and equal stride\n>>> filters = torch.randn(8, 4, 3, 3)\n>>> inputs = torch.randn(1, 4, 5, 5)\n>>> F.conv2d(inputs, filters, padding=1)",
        "torch_code": "F.conv2d(inputs, filters, padding=1)",
        "torch_cnt": 1,
        "other": "Supports TensorFloat32. May select a nondeterministic algorithm on CUDA with CuDNN. Supports complex data types.",
        "difficulty": 4,
        "params_cnt": 7,
        "file": "conv2d.py"
    },
    {
        "name": "normalized_cosine_similarity",
        "func_inputs": "def normalized_cosine_similarity(x1: Tensor, x2: Tensor, dim: int = 1, eps_similarity: float = 1e-8, p_norm: float = 2, eps_norm: float = 1e-12) -> Tensor",
        "description": "Computes the cosine similarity between two normalized input tensors `x1` and `x2`. This function normalizes `x1` and `x2` along a specified dimension using L_p normalization, and subsequently calculates the cosine similarity between these normalized tensors along the specified dimension. This involves ensuring vectors are scaled to avoid division by zero by introducing small epsilon values both during normalization and similarity computation.",
        "math": "The operation is defined as:\n\nsimilarity = \\frac{\\text{normalize}(x1) \\cdot \\text{normalize}(x2)}{\\max(\\lVert \\text{normalize}(x1) \\Vert _2, \\epsilon) \\cdot \\max(\\lVert \\text{normalize}(x2) \\Vert _2, \\epsilon)}\n\nwhere the `normalize` function is defined as:\n\nv = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.",
        "example": "import torch\nimport torch.nn.functional as F\n# Define two input tensors\nx1 = torch.randn(100, 128)\nx2 = torch.randn(100, 128)\n# Compute normalized cosine similarity\nresult = normalized_cosine_similarity(x1, x2)\nresult.shape\n# Adjust norms and epsilon for cosine similarity calculation\nresult = normalized_cosine_similarity(x1, x2, dim=0, p_norm=1, eps_similarity=1e-5)\nresult.shape",
        "torch_code": "torch.nn.functional.normalize(x1, p=p_norm, dim=dim, eps=eps_norm)\ntorch.nn.functional.normalize(x2, p=p_norm, dim=dim, eps=eps_norm)\ntorch.nn.functional.cosine_similarity(x1_normalized, x2_normalized, dim=dim, eps=eps_similarity)",
        "torch_cnt": 3,
        "other": "The function allows broadcasting x2 to match x1's shape. Default values are provided for dimension, normalization, and similarity thresholds to enhance robustness against division by zero.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "normalized_cosine_similarity.py"
    },
    {
        "name": "fused_cholesky_solve",
        "func_inputs": "def fused_cholesky_solve(A: Tensor, b: Tensor) -> Tensor: A: The symmetric positive-definite matrix `A` of shape `(n, n)`. b: The right-hand side tensor `b` of shape `(n, k)`.",
        "description": "Computes the solution `x` to the equation `Ax = b` using the Cholesky decomposition. It first performs Cholesky decomposition on a symmetric positive-definite matrix `A` to obtain a lower triangular matrix `L` such that `A = L * L.T`, then solves for `x` in `Ax = b` using the Cholesky factorization.",
        "math": "Cholesky decomposition: A = L * L.T, Solve: Ax = b",
        "example": ">>> A = torch.tensor([[4.0, 1.0], [1.0, 3.0]])\n>>> b = torch.tensor([[1.0], [2.0]])\n>>> x = fused_cholesky_solve(A, b)\n>>> print(x)",
        "torch_code": "L = torch.linalg.cholesky(A)\nx = torch.cholesky_solve(b, L)",
        "torch_cnt": 2,
        "other": "The function assumes that the input matrix `A` is symmetric positive-definite.",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "fused_cholesky_solve.py"
    },
    {
        "name": "torch.matmul",
        "func_inputs": "matmul(input, other, *, out=None) -> Tensor\nArguments:\n    input (Tensor): the first tensor to be multiplied\n    other (Tensor): the second tensor to be multiplied\nKeyword args:\n    out (Tensor, optional): the output tensor.",
        "description": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors: 1D tensors return a dot product; 2D tensors return a matrix-matrix product; 1D and 2D tensors return a matrix-vector product; N-dimensional tensors (N > 2) return a batched matrix multiply with broadcasting support. Sparse layouts are supported for 2D matrix-matrix products. TensorFloat32 is supported. On certain ROCm devices, float16 inputs use different precision for backward. The 1D dot product version does not support an out parameter.",
        "math": "",
        "example": ">>> # vector x vector\n>>> tensor1 = torch.randn(3)\n>>> tensor2 = torch.randn(3)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n>>> # matrix x vector\n>>> tensor1 = torch.randn(3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n>>> # batched matrix x broadcasted vector\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n>>> # batched matrix x batched matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(10, 4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n>>> # batched matrix x broadcasted matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])",
        "torch_code": "torch.matmul(tensor1, tensor2)",
        "torch_cnt": 1,
        "other": "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "matmul.py"
    },
    {
        "name": "fused_gather_masked_fill",
        "func_inputs": "fused_gather_masked_fill(input, dim, index, mask, value, *, sparse_grad=False, out=None) -> Tensor; input (Tensor): The input tensor X.; dim (int): The dimension along which to index.; index (LongTensor): The indices of elements to gather, of the same dimensionality as `input`.; mask (BoolTensor): A boolean mask tensor, broadcastable to the shape of the output tensor Y.; value (float): The value to fill in where `mask` is True.; sparse_grad (bool, optional): If True, gradient w.r.t. `input` will be a sparse tensor. Default: `False`.; out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Performs a fused operation combining torch.gather and torch.Tensor.masked_fill. It first gathers values from the input tensor along a specified dimension using provided indices, and then replaces the gathered elements with a specified value where the mask is True.",
        "math": "Y = \\text{gather}(X, \\text{dim}, I)\nY[M] = \\text{value}",
        "example": "import torch\n\ndef fused_gather_masked_fill(input, dim, index, mask, value, *, sparse_grad=False, out=None):\n    gathered = torch.gather(input, dim, index, sparse_grad=sparse_grad)\n    output = gathered.masked_fill(mask, value)\n    if out is not None:\n        out.copy_(output)\n        return out\n    return output\n\n# Example usage\ninput = torch.tensor([[1, 2], [3, 4]])\nindex = torch.tensor([[0, 0], [1, 0]])\nmask = torch.tensor([[True, False], [False, True]])\nvalue = -1\n\noutput = fused_gather_masked_fill(input, 1, index, mask, value)\nprint(\"Output:\")\nprint(output)\n# Output:\n# tensor([[-1,  1],\n#         [ 4, -1]])",
        "torch_code": "gathered = torch.gather(input, dim, index, sparse_grad=sparse_grad)\noutput = gathered.masked_fill(mask, value)",
        "torch_cnt": 2,
        "other": "- The input and index tensors must have the same number of dimensions.\n- The size of index at each dimension d must not exceed the size of input at that dimension, except at dimension dim.\n- The mask tensor must be broadcastable to the shape of the gathered output.\n- The function supports autograd for gradient computation.\n- All operations are differentiable and support backpropagation.",
        "difficulty": 4,
        "params_cnt": 6,
        "file": "fused_gather_masked_fill.py"
    },
    {
        "name": "fused_cross_entropy_log_softmax",
        "func_inputs": "def fused_cross_entropy_log_softmax(input: torch.Tensor, target: torch.Tensor, dim: int = 1, weight: torch.Tensor = None, ignore_index: int = -100, reduction: str = 'mean', label_smoothing: float = 0.0) -> torch.Tensor\nArgs:\n    input (Tensor): Input tensor of logits, where softmax will be computed along `dim`.\n    target (Tensor): Ground truth class indices or probabilities.\n    dim (int, optional): Dimension along which to compute log softmax. Default is 1.\n    weight (Tensor, optional): Manual rescaling weight for each class.\n    ignore_index (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. Default: -100.\n    reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. Default: 'mean'.\n    label_smoothing (float, optional): Specifies the amount of smoothing to be applied, where 0.0 means no smoothing. Default: 0.0.",
        "description": "This function computes the cross entropy loss with log softmax applied to the input logits. It combines log softmax activation and cross entropy loss calculation in a numerically stable way. The log softmax is applied to the input logits, and the cross entropy loss is computed between the normalized logits and the target. The function allows customization with options such as which dimension to apply the log softmax, manual rescaling weights for each class, handling of ignored targets, reduction method for loss aggregation, and label smoothing to modify the target distribution.",
        "math": "log_softmax(x_i) = log(exp(x_i) / sum(exp(x)))\nCE(y, p) = -sum(y * log(p))",
        "example": ">>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.randint(5, (3,), dtype=torch.int64)\n>>> loss = fused_cross_entropy_log_softmax(input, target)\n>>> print(loss)\n>>> loss.backward()",
        "torch_code": "log_probs = torch.nn.functional.log_softmax(input, dim=dim)\nloss = torch.nn.functional.cross_entropy(log_probs, target, weight=weight, ignore_index=ignore_index, reduction=reduction, label_smoothing=label_smoothing)",
        "torch_cnt": 2,
        "other": "The function integrates the log softmax and cross entropy loss computation into a single operation for numerical stability. The input and target tensors must be of compatible shapes, where the input is expected to have logits of size (N, C) and target should have size (N,) for class indices.",
        "difficulty": 4,
        "params_cnt": 7,
        "file": "fused_cross_entropy_log_softmax.py"
    },
    {
        "name": "torch.addmm",
        "func_inputs": "addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor; input (Tensor): matrix to be added; mat1 (Tensor): the first matrix to be matrix multiplied; mat2 (Tensor): the second matrix to be matrix multiplied; beta (Number, optional): multiplier for input (β); alpha (Number, optional): multiplier for mat1 @ mat2 (α); out (Tensor, optional): the output tensor.",
        "description": "Performs a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result. If mat1 is a (n x m) tensor, mat2 is a (m x p) tensor, then input must be broadcastable with a (n x p) tensor and out will be a (n x p) tensor. Alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively. If beta is 0, then input will be ignored, and nan and inf in it will not be propagated. This operation supports sparse layouts. If input is sparse the result will have the same layout and if out is provided it must have the same layout as input. Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. This operator supports TensorFloat32. On certain ROCm devices, when using float16 inputs this module will use different precision for backward.",
        "math": "out = β * input + α * (mat1 @ mat2)",
        "example": ">>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])",
        "torch_code": "torch.addmm(M, mat1, mat2)",
        "torch_cnt": 1,
        "other": "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. This operator supports TensorFloat32. On certain ROCm devices, when using float16 inputs this module will use different precision for backward.",
        "difficulty": 4,
        "params_cnt": 6,
        "file": "addmm.py"
    },
    {
        "name": "fused_qr_solve",
        "func_inputs": "def fused_qr_solve(A: Tensor, b: Tensor) -> Tensor: A: The matrix `A` of shape `(m, n)` where `m >= n`. b: The right-hand side tensor `b` of shape `(m, k)`.",
        "description": "Solves the linear system `Ax = b` using QR decomposition. This function combines the QR decomposition with solving a linear system. Given a matrix `A` and a vector (or matrix) `b`, it performs the QR decomposition of `A` and computes the solution `x` using the formula `x = R^{-1} (Q^T b)`.",
        "math": "x = R^{-1} Q^T b",
        "example": ">>> A = torch.tensor([[12.0, -51.0, 4.0], [6.0, 167.0, -68.0], [-4.0, 24.0, -41.0]])\n>>> b = torch.tensor([[1.0], [2.0], [3.0]])\n>>> x = fused_qr_solve(A, b)\n>>> print(x)",
        "torch_code": "Q, R = torch.linalg.qr(A, mode='reduced')\nx = torch.linalg.solve(R, Q.T @ b)",
        "torch_cnt": 2,
        "other": "The function assumes `m >= n` for the matrix `A`.",
        "difficulty": 3,
        "params_cnt": 2,
        "file": "fused_qr_solve.py"
    },
    {
        "name": "sigmoid_adaptive_avg_pool2d",
        "func_inputs": "def sigmoid_adaptive_avg_pool2d(input: Tensor, output_size: Union[int, Tuple[int, int]]) -> Tensor",
        "description": "Applies a 2D adaptive average pooling over an input tensor, followed by the sigmoid activation function applied element-wise. This is used for downsampling a feature map to a specified output size and then normalizing the result with the sigmoid function.",
        "math": "out = σ(AdaptiveAvgPool2D(input))\nSigmoid(x) = 1 / (1 + exp(-x))",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define input tensor\n>>> input = torch.randn(1, 3, 64, 64)  # (batch, channels, height, width)\n>>> # Apply sigmoid_adaptive_avg_pool2d to downsample to 32x32\n>>> result = sigmoid_adaptive_avg_pool2d(input, output_size=(32, 32))\n>>> result.shape\ntorch.Size([1, 3, 32, 32])\n\n>>> # Apply sigmoid_adaptive_avg_pool2d with output size 1x1\n>>> result = sigmoid_adaptive_avg_pool2d(input, output_size=(1, 1))\n>>> result.shape\ntorch.Size([1, 3, 1, 1])",
        "torch_code": "torch.nn.functional.adaptive_avg_pool2d(input, output_size)\ntorch.nn.functional.sigmoid(pooled_result)",
        "torch_cnt": 2,
        "other": "Each element in the resulting tensor is scaled to the range (0, 1) by the sigmoid activation.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "sigmoid_adaptive_avg_pool2d.py"
    },
    {
        "name": "torch.cos",
        "func_inputs": "cos(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor.; Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the cosine of the elements of the input tensor.",
        "math": "\\text{out}_{i} = \\cos(\\text{input}_{i})",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])",
        "torch_code": "torch.cos(a)",
        "torch_cnt": 1,
        "other": "The function computes the cosine of each element in the input tensor and returns a new tensor with these values.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "cos.py"
    },
    {
        "name": "fused_bmm_dropout_gelu",
        "func_inputs": "fused_bmm_dropout_gelu(input1, input2, p=0.5, training=True, inplace=False, approximate='none', *, out=None) -> Tensor\n- **input1** (Tensor): First input tensor for batch matrix multiplication, of shape (B, N, M), where B is the batch size.\n- **input2** (Tensor): Second input tensor for batch matrix multiplication, of shape (B, M, P).\n- **p** (float, optional): Probability of an element to be zeroed in the dropout layer. Default: `0.5`.\n- **training** (bool, optional): Apply dropout if `True`. Default: `True`.\n- **inplace** (bool, optional): If set to `True`, will perform the dropout operation in-place. Default: `False`.\n- **approximate** (str, optional): Can be `'none'` or `'tanh'`. The approximation to use for GELU. Default: `'none'`.\n- **out** (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Performs a fused operation combining batch matrix multiplication, dropout, and GELU activation. It computes the batch matrix multiplication of two input tensors, applies dropout to the result, and then applies the GELU activation function.",
        "math": "Given two input tensors X and Y, this function computes:\n\n\\[\n\\begin{align*}\nZ &= \\text{bmm}(X, Y) \\\\\nD &= \\text{Dropout}(Z, p) \\\\\nO &= \\text{GELU}(D)\n\\end{align*}\n\\]\n\nwhere:\n\n- \\text{bmm}(X, Y) performs batch matrix multiplication.\n- \\text{Dropout}(Z, p) randomly zeroes elements of Z with probability p.\n- \\text{GELU}(D) applies the Gaussian Error Linear Unit activation function element-wise to D.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_bmm_dropout_gelu(input1, input2, p=0.5, training=True, inplace=False, approximate='none', *, out=None):\n    Z = torch.bmm(input1, input2)\n    D = torch.nn.functional.dropout(Z, p=p, training=training, inplace=inplace)\n    O = torch.nn.functional.gelu(D, approximate=approximate)\n    if out is not None:\n        out.copy_(O)\n        return out\n    return O\n\n# Example usage\nB, N, M, P = 2, 3, 4, 5  # Batch size, dimensions\ninput1 = torch.randn(B, N, M)\ninput2 = torch.randn(B, M, P)\n\noutput = fused_bmm_dropout_gelu(input1, input2, p=0.2, training=True)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([2, 3, 5])\n```",
        "torch_code": "Z = torch.bmm(input1, input2)\nD = torch.nn.functional.dropout(Z, p=p, training=training, inplace=inplace)\nO = torch.nn.functional.gelu(D, approximate=approximate)",
        "torch_cnt": 3,
        "other": "- The shapes of `input1` and `input2` must be compatible for batch matrix multiplication: `input1` of shape `(B, N, M)` and `input2` of shape `(B, M, P)` result in an output of shape `(B, N, P)`.\n- The `dropout` is applied during training when `training=True`. Set `training=False` to disable dropout during evaluation.\n- The `GELU` activation is applied element-wise to the output of dropout.\n- All operations are differentiable and support autograd.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "fused_bmm_dropout_gelu.py"
    },
    {
        "name": "torch.trunc",
        "func_inputs": "trunc(input, *, out=None) -> Tensor\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the truncated integer values of the elements of the input tensor. For integer inputs, it follows the array-api convention of returning a copy of the input tensor.",
        "math": "",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])",
        "torch_code": "torch.trunc(a)",
        "torch_cnt": 1,
        "other": "For integer inputs, follows the array-api convention of returning a copy of the input tensor.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "trunc.py"
    },
    {
        "name": "matrix_power_eig",
        "func_inputs": "def matrix_power_eig(A, k, *, out=None) -> Tensor\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions consisting of square matrices.\n    k (float or complex): the exponent to which the matrix :attr:`A` is to be raised.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.",
        "description": "Computes the matrix power A^k of a square matrix A using eigendecomposition. It relies on A being diagonalizable and computes the power through the equation A^k = V diag(Λ^k) V^(-1), where Λ and V are the eigenvalues and eigenvectors of A. It allows for fractional powers of matrices and supports real or complex exponents. If A is not diagonalizable, the result may not be accurate.",
        "math": "A^k = V diag(Λ^k) V^{-1}, where A = V diag(Λ) V^{-1}, and Λ^k denotes the element-wise power of the eigenvalues.",
        "example": ">>> import torch\n>>> A = torch.tensor([[2.0, 0.0], [0.0, 3.0]])\n>>> k = 2\n>>> matrix_power_eig(A, k)\ntensor([[4.+0.j, 0.+0.j],\n        [0.+0.j, 9.+0.j]])\n>>> # Compare with torch.matrix_power\n>>> torch.matrix_power(A, k)\ntensor([[4., 0.],\n        [0., 9.]])\n>>> # Non-integer exponent\n>>> k = 0.5\n>>> matrix_power_eig(A, k)\ntensor([[1.4142+0.j, 0.0000+0.j],\n        [0.0000+0.j, 1.7321+0.j]])\n>>> # Example with real matrix but complex eigenvalues\n>>> A = torch.tensor([[0.0, -1.0], [1.0, 0.0]])\n>>> k = 0.5\n>>> matrix_power_eig(A, k)\ntensor([[0.7071+0.0000j, -0.7071+0.0000j],\n        [0.7071+0.0000j,  0.7071+0.0000j]])",
        "torch_code": "torch.linalg.eig, torch.diag_embed, torch.linalg.inv, torch.copy_",
        "torch_cnt": 4,
        "other": "Supports input of float, double, cfloat, and cdouble dtypes. Also supports batches of matrices, output has the same batch dimensions. Note that the computed A^k may be complex even if A is real, due to complex eigenvalues. Warning: If A is not diagonalizable, the result may not be accurate. Gradients might be numerically unstable if the distance between any two eigenvalues is close to zero.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "matrix_power_eig.py"
    },
    {
        "name": "log_tanh",
        "func_inputs": "def log_tanh(input, out=None) -> Tensor: input (Tensor): The input tensor. All elements must be positive for the log function. out (Tensor, optional): The output tensor.",
        "description": "Computes the natural logarithm of each element in the input tensor, then applies the hyperbolic tangent (tanh) function to the result. This involves applying the logarithm first, which is only defined for positive numbers, and then applying tanh to transform the result between -1 and 1.",
        "math": "\\text{out}_{i} = \\tanh(\\log(\\text{input}_{i}))",
        "example": ">>> import torch\n>>> a = torch.tensor([1.0, 2.0, 3.0, 4.0])\n>>> result = log_tanh(a)\n>>> result\ntensor([0.0000, 0.5493, 0.6931, 0.7616])\n\n>>> a = torch.tensor([0.5, 1.5, 2.5, 0.1])\n>>> result = log_tanh(a)\n>>> result\ntensor([-0.2554,  0.4051,  0.6194, -2.3026])",
        "torch_code": "torch.log(input)\ntorch.tanh(log_result, out=out)",
        "torch_cnt": 2,
        "other": "All input elements must be positive for the logarithm function to be defined.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "log_tanh.py"
    },
    {
        "name": "torch.exp",
        "func_inputs": "exp(input, *, out=None) -> Tensor\n    input (Tensor): the input tensor.\n    out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the exponential of the elements of the input tensor.",
        "math": "y_{i} = e^{x_{i}}",
        "example": ">>> torch.exp(torch.tensor([0, math.log(2.)])) tensor([ 1.,  2.])",
        "torch_code": "torch.exp(torch.tensor([0, math.log(2.)]))",
        "torch_cnt": 1,
        "other": "",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "exp.py"
    },
    {
        "name": "matrix_multiply_symmetric",
        "func_inputs": "matrix_multiply_symmetric(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, alpha: float, beta: float) -> torch.Tensor; Args: A (Tensor): The first input matrix of shape `(n, m)`. B (Tensor): The second input matrix of shape `(m, p)`. C (Tensor): The target matrix for the operations, shape `(n, p)`. alpha (float): Scalar multiplier for matrix products. beta (float): Scalar multiplier for adding to `C`. Example: A = torch.tensor([[1.0, 2.0], [3.0, 4.0]]), B = torch.tensor([[0.5, -1.0], [1.5, 2.0]]), C = torch.tensor([[1.0, 0.0], [0.0, 1.0]]), alpha, beta = 2.0, 0.5, result = matrix_multiply_symmetric(A, B, C, alpha, beta)",
        "description": "Computes two operations on matrix `C`: first, it performs the matrix-matrix product `C = alpha * torch.mm(A, B) + beta * C`, then updates `C` to be `C = alpha * torch.mm(C, C.T) + beta * C`. This function effectively performs two sequential matrix operations: a weighted sum of a matrix product and itself, followed by a weighted product of `C` and its transpose.",
        "math": "C = alpha * torch.mm(A, B) + beta * C\nC = alpha * torch.mm(C, C.T) + beta * C",
        "example": ">>> A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n>>> B = torch.tensor([[0.5, -1.0], [1.5, 2.0]])\n>>> C = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n>>> alpha, beta = 2.0, 0.5\n>>> result = matrix_multiply_symmetric(A, B, C, alpha, beta)\n>>> print(result)",
        "torch_code": "C = alpha * torch.mm(A, B) + beta * C\nC = alpha * torch.mm(C, C.T) + beta * C",
        "torch_cnt": 2,
        "other": "This function performs a fused operation of matrix multiplication and symmetric update.",
        "difficulty": 4,
        "params_cnt": 5,
        "file": "matrix_multiply_symmetric.py"
    },
    {
        "name": "fused_avg_pool2d_cosine_similarity",
        "func_inputs": "fused_avg_pool2d_cosine_similarity(x1: torch.Tensor, x2: torch.Tensor, kernel_size: int, stride: int = None, padding: int = 0, eps: float = 1e-8) -> torch.Tensor",
        "description": "Computes the cosine similarity between `x1` and `x2` along a specified dimension, adds a singleton dimension, and applies 2D average pooling. It first computes cosine similarity along dim=1 using `cosine_similarity`, then adds a singleton dimension using `unsqueeze`, and finally applies 2D average pooling using `avg_pool2d`.",
        "math": "",
        "example": ">>> x1 = torch.randn(10, 5, 32, 32)\n>>> x2 = torch.randn(10, 5, 32, 32)\n>>> output = fused_avg_pool2d_cosine_similarity(x1, x2, kernel_size=3, stride=2)\n>>> print(output.shape)\ntorch.Size([10, 1, 16, 16])",
        "torch_code": "cosine_sim = torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=eps)\ncosine_sim_expanded = cosine_sim.unsqueeze(1)\noutput = torch.nn.functional.avg_pool2d(cosine_sim_expanded, kernel_size=kernel_size, stride=stride, padding=padding)",
        "torch_cnt": 3,
        "other": "The function provides an optional `stride` parameter which defaults to the value of `kernel_size` if not provided. The `eps` parameter is used to prevent division by zero in cosine similarity.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "fused_avg_pool2d_cosine_similarity.py"
    },
    {
        "name": "fused_hardshrink_dropout",
        "func_inputs": "def fused_hardshrink_dropout(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False, lambd: float = 0.5) -> torch.Tensor\nArgs:\n  input (Tensor): The input tensor.\n  p (float, optional): Probability of an element to be zeroed in dropout. Default is 0.5.\n  training (bool, optional): Apply dropout if True. Default is True.\n  inplace (bool, optional): If set to True, dropout will be applied in-place. Default is False.\n  lambd (float, optional): The lambda parameter for the hard shrinkage function. Default is 0.5.\nReturns:\n  Tensor: Result after applying dropout and then hard shrinkage on the input.",
        "description": "Applies a fused operation consisting of dropout followed by hard shrinkage on the input tensor. The function first applies dropout to the input tensor, where each element is zeroed with a probability of p if training is True. The dropout can be applied in-place if specified. After dropout, a hard shrinkage operation is applied, which shrinks values towards zero based on the lambda parameter.",
        "math": "",
        "example": ">>> input = torch.randn(5, 5)  # Example input tensor\n>>> output = fused_hardshrink_dropout(input, p=0.5, lambd=0.5)\n>>> print(output)",
        "torch_code": "torch.nn.functional.dropout(input, p=p, training=training, inplace=inplace)\ntorch.nn.functional.hardshrink(dropout_output, lambd=lambd)",
        "torch_cnt": 2,
        "other": "The function combines dropout and hard shrinkage operations, which are typically used in neural network training to prevent overfitting and to enforce sparsity, respectively.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "fused_hardshrink_dropout.py"
    },
    {
        "name": "erfc_sqrt",
        "func_inputs": "def erfc_sqrt(input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: The input tensor for which the erfc and square root are computed.",
        "description": "Computes the complementary error function (erfc) and the square root of each element in the input tensor.",
        "math": "\\text{erfc}(x) = 1 - \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt \\text{out}_{i} = \\sqrt{\\text{input}_{i}}",
        "example": ">>> a = torch.tensor([0, -1., 10., 0.25]) >>> erfc_result, sqrt_result = erfc_sqrt(a) >>> erfc_result tensor([ 1.0000,  1.8427,  0.0000,  0.5205]) >>> sqrt_result tensor([0.0000,    nan, 3.1623, 0.5000])",
        "torch_code": "erfc_result = torch.special.erfc(input) sqrt_result = torch.sqrt(input)",
        "torch_cnt": 2,
        "other": "Returns a tuple containing the erfc result and the square root result for each element in the input tensor.",
        "difficulty": 2,
        "params_cnt": 1,
        "file": "erfc_sqrt.py"
    },
    {
        "name": "tensordot_rsqrt",
        "func_inputs": "def tensordot_rsqrt(a: torch.Tensor, b: torch.Tensor, dims) -> torch.Tensor: a (Tensor): Left tensor to contract. b (Tensor): Right tensor to contract. dims (int, Tuple[List[int], List[int]], or List[List[int]]): Dimensions for contraction, as per `torch.tensordot`.",
        "description": "Returns the reciprocal of the square root of the tensordot product of two tensors `a` and `b`. This function performs a tensor contraction of `a` and `b` over the specified dimensions using `torch.tensordot`, and then applies the element-wise reciprocal square root to the resulting tensor. The operation involves computing the tensordot product first and then applying the reciprocal of the square root element-wise to the result.",
        "math": "\\text{output} = \\frac{1}{\\sqrt{\\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}}}",
        "example": ">>> a = torch.arange(1, 13, dtype=torch.float32).reshape(3, 4)\n>>> b = torch.arange(1, 25, dtype=torch.float32).reshape(4, 6)\n>>> tensordot_rsqrt(a, b, dims=([1], [0]))\ntensor([[0.0141, 0.0126, 0.0115, 0.0108, 0.0102, 0.0097],\n        [0.0097, 0.0092, 0.0088, 0.0085, 0.0082, 0.0080],\n        [0.0085, 0.0082, 0.0080, 0.0077, 0.0075, 0.0073]])",
        "torch_code": "torch.tensordot(a, b, dims=dims)\ntorch.rsqrt(tensordot_result)",
        "torch_cnt": 2,
        "other": "The function applies the `torch.tensordot` and `torch.rsqrt` operations. The `dims` argument specifies the dimensions over which the contraction happens, similar to the `torch.tensordot` function.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "tensordot_rsqrt.py"
    },
    {
        "name": "softmax_log",
        "func_inputs": "def softmax_log(input, dim=-1, dtype=None) -> Tensor: \nArgs:\n    input (Tensor): The input tensor on which logarithm and softmax are applied.\n    dim (int): The dimension along which softmax will be computed. Default: -1.\n    dtype (:class:`torch.dtype`, optional): The desired data type of the returned tensor. If specified, the input tensor is cast to :attr:`dtype` before the operation is performed. Useful for preventing data type overflows. Default: None.\nExample:\n    >>> import torch\n    >>> import torch.nn.functional as F\n    >>> # Define input tensor\n    >>> input = torch.rand(3, 4) * 10\n    >>> # Apply softmax_log along the last dimension\n    >>> result = softmax_log(input, dim=1)\n    >>> result\n    tensor([[0.1829, 0.1782, 0.2783, 0.3606],\n            [0.3119, 0.1724, 0.3256, 0.1900],\n            [0.2057, 0.2166, 0.2991, 0.2786]])\n\n    >>> # Apply softmax_log along a different dimension\n    >>> result = softmax_log(input, dim=0)\n    >>> result\n    tensor([[0.3122, 0.4444, 0.2720, 0.2159],\n            [0.3879, 0.2167, 0.4226, 0.2165],\n            [0.2999, 0.3389, 0.3055, 0.5676]])",
        "description": "Applies the natural logarithm element-wise on the input tensor, followed by applying the softmax function along the specified dimension. This combined operation scales input values to a range between 0 and 1, summing to 1 after the logarithmic transformation. It allows transformation of the input tensor into a probability distribution.",
        "math": "out = Softmax(log(input))\n\nwhere:\n\ny_{i} = \\frac{\\exp(\\log(x_{i}))}{\\sum_j \\exp(\\log(x_{j}))} = \\frac{x_i}{\\sum_j x_j}",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define input tensor\n>>> input = torch.rand(3, 4) * 10\n>>> # Apply softmax_log along the last dimension\n>>> result = softmax_log(input, dim=1)\n>>> result\ntensor([[0.1829, 0.1782, 0.2783, 0.3606],\n        [0.3119, 0.1724, 0.3256, 0.1900],\n        [0.2057, 0.2166, 0.2991, 0.2786]])\n\n>>> # Apply softmax_log along a different dimension\n>>> result = softmax_log(input, dim=0)\n>>> result\ntensor([[0.3122, 0.4444, 0.2720, 0.2159],\n        [0.3879, 0.2167, 0.4226, 0.2165],\n        [0.2999, 0.3389, 0.3055, 0.5676]])",
        "torch_code": "log_result = torch.log(input)\nreturn torch.nn.functional.softmax(log_result, dim=dim, dtype=dtype)",
        "torch_cnt": 2,
        "other": "The function handles optional data type casting to prevent overflow and allows specifying the dimension for softmax application.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "softmax_log.py"
    },
    {
        "name": "dropout_sigmoid_linear",
        "func_inputs": "def dropout_sigmoid_linear(input: torch.Tensor, weight: torch.Tensor, bias=None, p=0.5, training=True, inplace=False) -> torch.Tensor: Input tensor of shape :math:`(*, \\text{in\\_features})`. Weight tensor of shape :math:`(\\text{out\\_features}, \\text{in\\_features})`. Bias tensor of shape :math:`(\\text{out\\_features})`. Default is `None`. Probability of an element to be zeroed in dropout. Default: 0.5 If `True`, applies dropout during training. Default: `True` If `True`, performs the operation in-place. Default: `False`",
        "description": "Applies a linear transformation followed by a sigmoid activation and dropout. This function sequentially applies a linear transformation to the input tensor, a sigmoid activation to scale the values between 0 and 1, and randomly zeroes some elements of the tensor with a specified probability during dropout.",
        "math": "",
        "example": ">>> input = torch.randn(10, 5)\n>>> weight = torch.randn(8, 5)\n>>> output = dropout_sigmoid_linear(input, weight, p=0.3, training=True)\n>>> output.shape\ntorch.Size([10, 8])",
        "torch_code": "x = torch.nn.functional.linear(input, weight, bias)\nx = torch.nn.functional.sigmoid(x)\nx = torch.nn.functional.dropout(x, p=p, training=training, inplace=inplace)",
        "torch_cnt": 3,
        "other": "The function applies dropout only if the `training` parameter is set to `True`. The `inplace` parameter allows for in-place operations to save memory.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "dropout_sigmoid_linear.py"
    },
    {
        "name": "torch.nn.functional.batch_norm",
        "func_inputs": "def batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05) -> Tensor",
        "description": "Applies Batch Normalization for each channel across a batch of data. Batch Normalization is a technique to improve the training of deep neural networks by ensuring that each layer receives whitened input, which helps to stabilize the learning process and reduce the number of training epochs needed to converge.",
        "math": "",
        "example": "import torch\nimport torch.nn.functional as F\n\n# Example input\ninput = torch.randn(20, 100, 35, 45)\n\n# Running mean and variance\nrunning_mean = torch.zeros(100)\nrunning_var = torch.ones(100)\n\n# Apply batch normalization\noutput = F.batch_norm(input, running_mean, running_var, training=True)",
        "torch_code": "F.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=True, momentum=0.1, eps=1e-05, cudnn_enabled=True)",
        "torch_cnt": 1,
        "other": "This function is related to the BatchNorm classes like BatchNorm1d, BatchNorm2d, and BatchNorm3d, which are layers that handle this operation with additional features.",
        "difficulty": 4,
        "params_cnt": 8,
        "file": "batch_norm.py"
    },
    {
        "name": "torch.special.gammaln",
        "func_inputs": "gammaln(input, *, out=None) -> Tensor\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.",
        "description": "Computes the natural logarithm of the absolute value of the gamma function on the input tensor.",
        "math": "\\text{out}_{i} = \\ln \\Gamma(|\\text{input}_{i}|)",
        "example": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])",
        "torch_code": "torch.special.gammaln(a)",
        "torch_cnt": 1,
        "other": "",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "gammaln.py"
    },
    {
        "name": "torch.bitwise_and",
        "func_inputs": "bitwise_and(input, other, *, out=None) -> Tensor; input: the first input tensor; other: the second input tensor; out (Tensor, optional): the output tensor.",
        "description": "Computes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.",
        "math": "",
        "example": ">>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0, 3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([False, True, False])",
        "torch_code": "torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)); torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))",
        "torch_cnt": 2,
        "other": "The input tensors must be of integral or Boolean types.",
        "difficulty": 2,
        "params_cnt": 3,
        "file": "bitwise_and.py"
    },
    {
        "name": "sub_gelu",
        "func_inputs": "def sub_gelu(input, other, alpha=1, approximate='none', out=None) -> Tensor: input (Tensor): The input tensor. other (Tensor or Number): The tensor or number to subtract from input. alpha (Number, optional): The multiplier for other. Default is 1. approximate (str, optional): The approximation method for GELU. Default is 'none'. out (Tensor, optional): The output tensor.",
        "description": "Subtracts 'other', scaled by 'alpha', from 'input', and then applies the Gaussian Error Linear Units (GELU) activation function to the result. The function supports two modes for GELU: exact and approximate using 'tanh'.",
        "math": "out_i = GELU(input_i - alpha * other_i)\n\nGELU(x) = x * Φ(x) when approximate is 'none'\nGELU(x) = 0.5 * x * (1 + Tanh(√(2/π) * (x + 0.044715 * x^3))) when approximate is 'tanh'",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.tensor([1.5, -2.0, 3.0])\n>>> b = torch.tensor([0.5, 1.0, -1.5])\n>>> result = sub_gelu(a, b, alpha=2)\n>>> result\ntensor([ 1.3998, -0.0000,  1.7744])\n\n>>> result = sub_gelu(a, b, alpha=2, approximate='tanh')\n>>> result\ntensor([ 1.3998, -0.0000,  1.7744])",
        "torch_code": "sub_result = torch.sub(input, other, alpha=alpha)\nreturn torch.nn.functional.gelu(sub_result, approximate=approximate, out=out)",
        "torch_cnt": 2,
        "other": "The function allows for an optional output tensor and supports both exact and approximate GELU calculations.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "sub_gelu.py"
    },
    {
        "name": "gelu_std",
        "func_inputs": "def gelu_std(input, dim=None, keepdim=False, correction=1, approximate='none', out=None) -> Tensor: input (Tensor): The input tensor. dim (int or tuple of ints, optional): The dimension or dimensions to reduce. If None, computes over all dimensions. keepdim (bool, optional): Whether to retain the dimension(s) with size 1 after reduction. Default is False. correction (int, optional): The correction factor for standard deviation. Default is 1. approximate (str, optional): The approximation method for GELU. Default is 'none'. out (Tensor, optional): The output tensor.",
        "description": "Applies the Gaussian Error Linear Units (GELU) activation function to the elements of input, then computes the standard deviation along the specified dimension(s). The GELU function is applied element-wise to the input tensor, with an option to use an approximation method. After activation, the standard deviation of the result is calculated over specified dimensions, with options to keep reduced dimensions and apply a correction factor.",
        "math": "GELU(x) = x * Φ(x) (when approximate is 'none')\nGELU(x) = 0.5 * x * (1 + Tanh(√(2/π) * (x + 0.044715 * x^3))) (when approximate is 'tanh')\nσ = √(1/(max(0, N - δN)) * Σ(x_i - x̄)^2)",
        "example": ">>> import torch\n>>> a = torch.tensor([[0.2035, 1.2959, 1.8101, -0.4644], [1.5027, -0.3270, 0.5905, 0.6538]])\n>>> result = gelu_std(a, dim=1, keepdim=True)\n>>> result\ntensor([[0.6067],\n        [0.6553]])\n\n>>> result = gelu_std(a, dim=1, keepdim=True, approximate='tanh')\n>>> result\ntensor([[0.6065],\n        [0.6550]])",
        "torch_code": "gelu_result = torch.nn.functional.gelu(input, approximate=approximate)\nreturn torch.std(gelu_result, dim=dim, keepdim=keepdim, correction=correction, out=out)",
        "torch_cnt": 2,
        "other": "The function allows the use of a correction factor in the standard deviation calculation. It supports two methods for computing GELU: exact using CDF or approximate using a tanh-based formula.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "gelu_std.py"
    },
    {
        "name": "torch.permute_copy",
        "func_inputs": "torch.permute_copy(input, dims) -> Tensor",
        "description": "Performs the same operation as torch.permute, which rearranges the dimensions of the input tensor according to the specified dims, but all output tensors are freshly created instead of aliasing the input.",
        "math": "",
        "example": "",
        "torch_code": "output = torch.permute_copy(input, dims)",
        "torch_cnt": 1,
        "other": "Freshly created output tensors mean that the function does not create views, so changes to the output will not affect the input.",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "permute_copy.py"
    },
    {
        "name": "torch.special.digamma",
        "func_inputs": "digamma(input, *, out=None) -> Tensor; Args: input (Tensor): the tensor to compute the digamma function on; Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the logarithmic derivative of the gamma function on input. This function is similar to SciPy's scipy.special.digamma. From PyTorch 1.8 onwards, the digamma function returns -Inf for 0, previously it returned NaN for 0.",
        "math": "\\digamma(x) = \\frac{d}{dx} \\ln\\left(\\Gamma\\left(x\\right)\\right) = \\frac{\\Gamma'(x)}{\\Gamma(x)}",
        "example": "Example::\n\n    >>> a = torch.tensor([1, 0.5])\n    >>> torch.special.digamma(a)\n    tensor([-0.5772, -1.9635])",
        "torch_code": "torch.special.digamma(a)",
        "torch_cnt": 1,
        "other": "This function is similar to SciPy's scipy.special.digamma. From PyTorch 1.8 onwards, the digamma function returns -Inf for 0, previously it returned NaN for 0.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "digamma.py"
    },
    {
        "name": "softmax_mul",
        "func_inputs": "def softmax_mul(input, other, dim, dtype=None, out=None) -> Tensor: Applies the softmax function to the input tensor along the specified dimension, and then multiplies the softmaxed values by other. Args: input (Tensor): The input tensor to apply softmax on. other (Tensor or Number): The tensor or number to multiply with the softmaxed values. dim (int): The dimension along which softmax will be computed. dtype (torch.dtype, optional): The desired data type of returned tensor. If specified, the input tensor is cast to dtype before the operation. Default is None. out (Tensor, optional): The output tensor.",
        "description": "Applies the softmax function to the input tensor along the specified dimension, and then multiplies the softmaxed values by another tensor or number. The softmax function re-scales the elements so that they lie in the range [0, 1] and sum to 1 along the specified dimension.",
        "math": "\\text{out}_i = \\text{Softmax}(\\text{input}_i) \\times \\text{other}_i\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]])\n>>> b = torch.tensor([[0.5, 1.0, 1.5], [1.0, 0.5, 2.0]])\n>>> # Apply softmax along dim=1, then multiply by b\n>>> result = softmax_mul(a, b, dim=1)\n>>> result\ntensor([[0.0900, 0.2447, 0.6652],\n        [0.1966, 0.0892, 0.7142]])\n\n>>> # Using a scalar for multiplication\n>>> result = softmax_mul(a, 2, dim=1)\n>>> result\ntensor([[0.2689, 0.7311, 2.0000],\n        [0.2689, 0.7311, 2.0000]])",
        "torch_code": "softmax_result = torch.nn.functional.softmax(input, dim=dim, dtype=dtype)\n    return torch.mul(softmax_result, other, out=out)",
        "torch_cnt": 2,
        "other": "Softmax re-scales the elements so that they lie in the range [0, 1] and sum to 1 along the specified dimension.",
        "difficulty": 4,
        "params_cnt": 5,
        "file": "softmax_mul.py"
    },
    {
        "name": "bitwise_and_binomial",
        "func_inputs": "def bitwise_and_binomial(input: torch.Tensor, other: torch.Tensor, total_count: torch.Tensor, probs: torch.Tensor = None, logits: torch.Tensor = None) -> torch.Tensor: input (Tensor): The first input tensor of integral or Boolean type. other (Tensor): The second input tensor of integral or Boolean type. total_count (Tensor): Number of Bernoulli trials, must be broadcastable with `probs` or `logits`. probs (Tensor, optional): Event probabilities. Only one of `probs` or `logits` should be provided. logits (Tensor, optional): Event log-odds.",
        "description": "Computes the bitwise AND operation between two tensors and then applies a Binomial distribution sampling based on the resulting tensor's values. First, it computes the bitwise AND of `input` and `other`. Then, the result is used as input for the Binomial distribution, with each element representing the number of trials with the probability specified in `probs` or `logits`.",
        "math": "\\text{output} = \\text{Binomial}( \\text{bitwise\\_and}(\\text{input}, \\text{other}))",
        "example": "input = torch.tensor([3, 7, 15], dtype=torch.int8)\nother = torch.tensor([1, 2, 8], dtype=torch.int8)\ntotal_count = torch.tensor([10, 20, 30])\nprobs = torch.tensor([0.2, 0.5, 0.8])\nbitwise_and_binomial(input, other, total_count, probs=probs)\ntensor([2., 10., 24.])",
        "torch_code": "and_result = torch.bitwise_and(input, other)\nbinomial_dist = torch.distributions.Binomial(total_count=total_count, probs=probs, logits=logits)\nreturn binomial_dist.sample() * and_result/n",
        "torch_cnt": 3,
        "other": "The function performs a bitwise AND on the input tensors and uses the result to conduct Binomial sampling with specified probabilities or logits. The `total_count` indicates the number of Bernoulli trials, and either `probs` or `logits` must be provided for the Binomial distribution.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "bitwise_and_binomial.py"
    },
    {
        "name": "rad2deg_sqrt",
        "func_inputs": "def rad2deg_sqrt(input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Args: input (Tensor): The input tensor with angles in radians.",
        "description": "This function computes the conversion of angles from radians to degrees and calculates the square root for each element in the input tensor. It returns a tuple where the first element is the converted degrees and the second is the square root of the input tensor elements.",
        "math": "\\text{out}_{i} = \\text{input}_{i} \\times (180.0 / \\pi) \\text{out}_{i} = \\sqrt{\\text{input}_{i}}",
        "example": ">>> a = torch.tensor([3.142, 1.570, 0.785, 0.0]) >>> deg_result, sqrt_result = rad2deg_sqrt(a) >>> deg_result tensor([180.0233,  89.9544,  45.0000,   0.0000]) >>> sqrt_result tensor([1.7725, 1.2533, 0.8862, 0.0000])",
        "torch_code": "deg_result = torch.rad2deg(input) sqrt_result = torch.sqrt(input)",
        "torch_cnt": 2,
        "other": "The function uses torch's rad2deg and sqrt functions to perform the operations.",
        "difficulty": 2,
        "params_cnt": 1,
        "file": "rad2deg_sqrt.py"
    },
    {
        "name": "torch.special.bessel_j1",
        "func_inputs": "bessel_j1(input, *, out=None) -> Tensor Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the Bessel function of the first kind of order 1 for each element of the input tensor.",
        "math": "Bessel function of the first kind of order :math:`1`.",
        "example": "",
        "torch_code": "torch.special.bessel_j1(input)",
        "torch_cnt": 1,
        "other": "The function supports an optional output tensor.",
        "difficulty": 3,
        "params_cnt": 2,
        "file": "bessel_j1.py"
    },
    {
        "name": "torch.linalg.lu",
        "func_inputs": "lu(A, *, pivot=True, out=None) -> (Tensor, Tensor, Tensor) Args: A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions. pivot (bool, optional): Controls whether to compute the LU decomposition with partial pivoting or no pivoting. Default: `True`. Keyword args: out (tuple, optional): output tuple of three tensors. Ignored if `None`. Default: `None`.",
        "description": "Computes the LU decomposition with partial pivoting of a matrix. If pivot=True, returns a permutation matrix P, a lower triangular matrix L, and an upper triangular matrix U such that A = PLU. If pivot=False and A is on GPU, computes the LU decomposition without pivoting, returning empty P, L and U such that A = LU. Supports float, double, cfloat, and cdouble dtypes, as well as batches of matrices. Outputs have the same batch dimensions as input.",
        "math": "A = PLU where P is a permutation matrix, L is lower triangular with ones on the diagonal, U is upper triangular. If pivot=False, A = LU.",
        "example": ">>> A = torch.randn(3, 2)\n>>> P, L, U = torch.linalg.lu(A)\n>>> P\ntensor([[0., 1., 0.],\n        [0., 0., 1.],\n        [1., 0., 0.]])\n>>> L\ntensor([[1.0000, 0.0000],\n        [0.5007, 1.0000],\n        [0.0633, 0.9755]])\n>>> U\ntensor([[0.3771, 0.0489],\n        [0.0000, 0.9644]])\n>>> torch.dist(A, P @ L @ U)\ntensor(5.9605e-08)\n\n>>> A = torch.randn(2, 5, 7, device=\"cuda\")\n>>> P, L, U = torch.linalg.lu(A, pivot=False)\n>>> P\ntensor([], device='cuda:0')\n>>> torch.dist(A, L @ U)\ntensor(1.0376e-06, device='cuda:0')",
        "torch_code": "torch.linalg.lu(A, *, pivot=True, out=None) -> (Tensor, Tensor, Tensor)",
        "torch_cnt": 2,
        "other": "LU decomposition is not unique; different platforms may yield different decompositions. Gradient computations are supported only if the matrix is full-rank.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "lu.py"
    },
    {
        "name": "gelu_min",
        "func_inputs": "gelu_min(input, approximate='none', dim=None, keepdim=False, out=None) -> Tensor or (Tensor, LongTensor)",
        "description": "Applies the Gaussian Error Linear Units (GELU) activation function to each element in the input tensor, followed by computing the minimum value along the specified dimension. If no dimension is specified, it computes the minimum over all elements. The function supports two methods for computing GELU: exact ('none') and an approximation using 'tanh'.",
        "math": "When approximate is 'none': GELU(x) = x * Φ(x), where Φ(x) is the Cumulative Distribution Function for Gaussian Distribution.\nWhen approximate is 'tanh': GELU(x) = 0.5 * x * (1 + Tanh(√(2/π) * (x + 0.044715 * x^3)))",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.tensor([[0.5, -1.2, 3.0], [1.5, -0.3, 2.1]])\n>>> # Apply GELU and then compute the minimum of the entire tensor\n>>> result = gelu_min(a)\n>>> result\ntensor(-0.1700)\n\n>>> # Apply GELU and then compute the minimum along dimension 1\n>>> result, indices = gelu_min(a, dim=1)\n>>> result\ntensor([-0.1700, -0.0708])\n>>> indices\ntensor([1, 1])",
        "torch_code": "gelu_result = torch.nn.functional.gelu(input, approximate=approximate)\nreturn torch.min(gelu_result, dim=dim, keepdim=keepdim, out=out)",
        "torch_cnt": 2,
        "other": "Returns a namedtuple (values, indices) if dim is specified, otherwise returns the minimum value tensor.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "gelu_min.py"
    },
    {
        "name": "grid_sample_with_affine",
        "func_inputs": "def grid_sample_with_affine(input: torch.Tensor, theta: torch.Tensor, size: torch.Size, mode: str = 'bilinear', padding_mode: str = 'zeros', align_corners: bool = False) -> torch.Tensor: Input tensor of shape (N, C, H_{in}, W_{in}) (4D). Affine transformation matrix of shape (N, 2, 3) for 2D transformations. Target output image size as a 4D size (N, C, H_{out}, W_{out}). Interpolation mode to calculate output values, 'bilinear', 'nearest', or 'bicubic'. Default is 'bilinear'. Defines how to handle grid values outside the input range. Options: 'zeros', 'border', 'reflection'. Default is 'zeros'. If True, aligns the grid to corner pixels for transformation consistency. Default is False.",
        "description": "This function applies an affine transformation to the input tensor followed by grid sampling. It first generates a 2D flow field (sampling grid) based on the input affine matrix `theta` using `affine_grid`. Then it uses the generated grid to sample from the input image using `grid_sample`. It supports multiple interpolation modes (such as 'bilinear', 'nearest', and 'bicubic'), different padding modes ('zeros', 'border', 'reflection'), and has an option to align corners for transformation consistency.",
        "math": "",
        "example": ">>> input = torch.randn(1, 3, 64, 64)  # (N, C, H_in, W_in)\n>>> theta = torch.tensor([[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]])  # Identity affine matrix\n>>> output_size = torch.Size([1, 3, 32, 32])  # Target output size\n>>> output = grid_sample_with_affine(input, theta, output_size, mode='bilinear', padding_mode='zeros', align_corners=False)\n>>> print(output.shape)\ntorch.Size([1, 3, 32, 32])",
        "torch_code": "torch.nn.functional.affine_grid, torch.nn.functional.grid_sample",
        "torch_cnt": 2,
        "other": "The function generates an affine transformation grid and applies grid sampling to the input tensor.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "grid_sample_with_affine.py"
    },
    {
        "name": "pseudoinverse_svd",
        "func_inputs": "def pseudoinverse_svd(A, *, full_matrices=True, rcond=1e-15, out=None) -> Tensor\n\nArgs:\n    A (Tensor): Input tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    full_matrices (bool, optional): If `True` (default), compute the full SVD. If `False`, compute the reduced SVD.\n    rcond (float, optional): Relative condition number threshold. Singular values smaller than `rcond * largest_singular_value` are set to zero. Default: `1e-15`.\n    out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Computes the Moore-Penrose pseudoinverse of a matrix using Singular Value Decomposition (SVD). It decomposes the input matrix A into its singular value components, inverts the non-zero singular values above a certain threshold to avoid numerical instability, and reconstructs the pseudoinverse using these components. Supports input of float, double, cfloat, and cdouble dtypes, and can handle batches of matrices.",
        "math": "A^{+} = V^{\\mathrm{H}} \\Sigma^{+} U^{\\mathrm{H}}; \\sigma_i^{+} = \\begin{cases} \\dfrac{1}{\\sigma_i}, & \\text{if } \\sigma_i > \\text{rcond} \\times \\sigma_{\\max} \\\\ 0, & \\text{otherwise} \\end{cases}",
        "example": ">>> import torch\n>>> def pseudoinverse_svd(A, *, full_matrices=True, rcond=1e-15, out=None):\n...     U, S, Vh = torch.linalg.svd(A, full_matrices=full_matrices)\n...     # Invert singular values larger than rcond * max(S)\n...     cutoff = rcond * S.max(dim=-1, keepdim=True).values\n...     S_inv = torch.where(S > cutoff, 1 / S, torch.zeros_like(S))\n...     # Create diagonal matrix of inverted singular values\n...     S_inv_mat = torch.diag_embed(S_inv)\n...     # Compute pseudoinverse\n...     A_pinv = Vh.transpose(-2, -1).conj() @ S_inv_mat @ U.transpose(-2, -1).conj()\n...     if out is not None:\n...         out.copy_(A_pinv)\n...         return out\n...     return A_pinv\n>>> # Example with a 2x3 matrix\n>>> A = torch.tensor([[1., 2., 3.],\n...                   [4., 5., 6.]])\n>>> A_pinv = pseudoinverse_svd(A)\n>>> A_pinv\ntensor([[-0.9444,  0.4444],\n        [-0.1111,  0.1111],\n        [ 0.7222, -0.2222]])\n>>> # Verify the pseudoinverse properties\n>>> # A @ A_pinv @ A ≈ A\n>>> torch.allclose(A @ A_pinv @ A, A)\nTrue\n>>> # A_pinv @ A @ A_pinv ≈ A_pinv\n>>> torch.allclose(A_pinv @ A @ A_pinv, A_pinv)\nTrue\n>>> # Example with a batch of matrices\n>>> A_batch = torch.randn(2, 4, 3)\n>>> A_pinv_batch = pseudoinverse_svd(A_batch)\n>>> A_pinv_batch.shape\ntorch.Size([2, 3, 4])\n>>> # Verify for the batch\n>>> torch.allclose(A_batch @ A_pinv_batch @ A_batch, A_batch)\nTrue",
        "torch_code": "U, S, Vh = torch.linalg.svd(A, full_matrices=full_matrices)\ncutoff = rcond * S.max(dim=-1, keepdim=True).values\nS_inv = torch.where(S > cutoff, 1 / S, torch.zeros_like(S))\nS_inv_mat = torch.diag_embed(S_inv)\nA_pinv = Vh.transpose(-2, -1).conj() @ S_inv_mat @ U.transpose(-2, -1).conj()\nif out is not None:\n    out.copy_(A_pinv)\n    return out\nreturn A_pinv",
        "torch_cnt": 6,
        "other": "Supports input of float, double, cfloat, and cdouble dtypes; Handles batches of matrices",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "pseudoinverse_svd.py"
    },
    {
        "name": "exp_mean",
        "func_inputs": "def exp_mean(input, dim=None, keepdim=False, dtype=None, out=None) -> Tensor",
        "description": "Applies the exponential function to each element in the input tensor and then computes the mean value of the result along the specified dimension or over all elements if no dimension is specified.",
        "math": "The combined operation is defined as: out = mean(e^{input}) where the exponential function is defined as: y_{i} = e^{x_{i}}",
        "example": ">>> import torch\n>>> a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n>>> # Compute the exponential of each element, then calculate the mean of all elements\n>>> result = exp_mean(a)\n>>> result\ntensor(90.2574)\n\n>>> # Compute the exponential of each element, then calculate the mean along dimension 1\n>>> result = exp_mean(a, dim=1)\n>>> result\ntensor([ 9.0797, 445.2395])\n\n>>> # Compute the exponential of each element, then calculate the mean along dimension 1 with keepdim=True\n>>> result = exp_mean(a, dim=1, keepdim=True)\n>>> result\ntensor([[  9.0797],\n        [445.2395]])",
        "torch_code": "exp_result = torch.exp(input)\nreturn torch.mean(exp_result, dim=dim, keepdim=keepdim, dtype=dtype, out=out)",
        "torch_cnt": 2,
        "other": "The function first applies the exponential function to each element of the input tensor and then computes the mean of these exponential values. The function allows specifying dimensions to reduce, whether to keep dimensions, and the data type of the output.",
        "difficulty": 3,
        "params_cnt": 5,
        "file": "exp_mean.py"
    },
    {
        "name": "low_rank_svd_approximation",
        "func_inputs": "def low_rank_svd_approximation(A, k, *, full_matrices=True, out=None) -> Tensor\n\nArgs:\n    A (Tensor): Tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    k (int): Rank of the approximation (must satisfy `1 <= k <= min(m, n)`).\n    full_matrices (bool, optional): Controls whether to compute the full or reduced SVD. Default: `True`.\n\nKeyword args:\n    out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Computes a rank-k approximation of a matrix using its Singular Value Decomposition (SVD). The function retains the top-k singular values and corresponding singular vectors from the SVD of A to form the approximation Ak. This low-rank approximation minimizes the Frobenius norm of the difference between A and Ak among all rank-k matrices. Supports input of float, double, cfloat, and cdouble dtypes, and batches of matrices.",
        "math": "A \\approx A_k = U_k \\Sigma_k V_k^{\\text{H}}; U_k \\in \\mathbb{K}^{m \\times k}; \\Sigma_k \\in \\mathbb{R}^{k \\times k}; V_k^{\\text{H}} \\in \\mathbb{K}^{k \\times n}",
        "example": ">>> import torch\n>>> def low_rank_svd_approximation(A, k, *, full_matrices=True, out=None):\n...     U, S, Vh = torch.linalg.svd(A, full_matrices=full_matrices)\n...     U_k = U[..., :k]\n...     S_k = S[..., :k]\n...     Vh_k = Vh[..., :k, :]\n...     S_k_diag = torch.diag_embed(S_k)\n...     A_k = U_k @ S_k_diag @ Vh_k\n...     if out is not None:\n...         out.copy_(A_k)\n...         return out\n...     return A_k\n>>> # Example with a 10x8 matrix\n>>> A = torch.randn(10, 8)\n>>> k = 5\n>>> A_k = low_rank_svd_approximation(A, k)\n>>> A_k.shape\ntorch.Size([10, 8])\n>>> # Check the approximation error\n>>> error = torch.norm(A - A_k)\n>>> print(\"Approximation error:\", error.item())\nApproximation error: ...\n>>> # Compare with full matrix\n>>> A_full = low_rank_svd_approximation(A, min(A.shape[-2], A.shape[-1]))\n>>> torch.allclose(A, A_full)\nTrue\n>>> # Example with a batch of matrices\n>>> A_batch = torch.randn(4, 10, 8)\n>>> k = 3\n>>> A_k_batch = low_rank_svd_approximation(A_batch, k)\n>>> A_k_batch.shape\ntorch.Size([4, 10, 8])\n>>> # Check approximation error for the batch\n>>> error_batch = torch.norm(A_batch - A_k_batch, dim=(1, 2))\n>>> print(\"Approximation errors:\", error_batch)\ntensor([...])",
        "torch_code": "U, S, Vh = torch.linalg.svd(A, full_matrices=full_matrices); U_k = U[..., :k]; S_k = S[..., :k]; Vh_k = Vh[..., :k, :]; S_k_diag = torch.diag_embed(S_k); A_k = U_k @ S_k_diag @ Vh_k; if out is not None: out.copy_(A_k); return out; return A_k",
        "torch_cnt": 6,
        "other": "Supports input of float, double, cfloat, and cdouble dtypes; Batches of matrices are supported.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "low_rank_svd_approximation.py"
    },
    {
        "name": "torch.min",
        "func_inputs": "min(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor) Args: input (Tensor): the input tensor. dim (int): the dimension to reduce. keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Keyword args: out (tuple, optional): the tuple of two output tensors (min, min_indices)",
        "description": "Returns the minimum value of each row of the input tensor in the given dimension dim, along with the index location of each minimum value found. If keepdim is True, the output tensors retain the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed, resulting in the output tensors having 1 fewer dimension than input. If there are multiple minimal values in a reduced row, the indices of the first minimal value are returned. The function can also compare two tensors element-wise and return a tensor with the minimum values.",
        "math": "",
        "example": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n        [-1.4644, -0.2635, -0.3651,  0.6134],\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\n>>> torch.min(a, 1)\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))",
        "torch_code": "torch.min(a, 1)",
        "torch_cnt": 1,
        "other": "If there are multiple minimal values in a reduced row, the indices of the first minimal value are returned.",
        "difficulty": 2,
        "params_cnt": 4,
        "file": "min.py"
    },
    {
        "name": "symmetric_mm_and_abs_sum",
        "func_inputs": "symmetric_mm_and_abs_sum(A: torch.Tensor, C: torch.Tensor, alpha: float, beta: float) -> torch.Tensor\nArgs:\n    A (Tensor): Input matrix of shape `(n, m)` for which the symmetric product with its transpose is calculated.\n    C (Tensor): Matrix of the same shape as `alpha * torch.mm(A, A.T)` to accumulate the scaled result.\n    alpha (float): Scaling factor for the matrix product.\n    beta (float): Scaling factor for matrix `C`.\nReturns:\n    Tensor: Scalar tensor representing the sum of absolute values of the resulting matrix `C`.",
        "description": "Performs a symmetric matrix multiplication by multiplying matrix `A` with its transpose, scales the result by `alpha`, adds it to matrix `C` scaled by `beta`, and returns the sum of the absolute values of the resulting matrix.",
        "math": "1. `C = alpha * torch.mm(A, A.T) + beta * C`; 2. `asum = torch.sum(torch.abs(C))`",
        "example": ">>> A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n>>> C = torch.zeros(2, 2)\n>>> alpha = 1.0\n>>> beta = 0.5\n>>> asum = symmetric_mm_and_abs_sum(A, C, alpha, beta)\n>>> print(asum)",
        "torch_code": "alpha * torch.mm(A, A.T) + beta * C; torch.sum(torch.abs(C))",
        "torch_cnt": 2,
        "other": "Returns a scalar tensor representing the sum of absolute values of the resulting matrix `C`.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "symmetric_mm_and_abs_sum.py"
    },
    {
        "name": "determinant_lu",
        "func_inputs": "determinant_lu(A, *, pivot=True, out=None) -> Tensor; A (Tensor): Tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions consisting of square matrices. pivot (bool, optional): Controls whether to compute the LU decomposition with partial pivoting (`True`) or without pivoting (`False`). Default: `True`. out (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Computes the determinant of a square matrix using LU decomposition. The function performs LU decomposition on a given square matrix A and calculates its determinant. It supports matrices over real or complex numbers and can handle batch dimensions. The determinant is computed as the product of the diagonal elements of the upper triangular matrix U from the LU decomposition, adjusted by the sign of the permutation matrix P if pivoting is used. The function assumes A is invertible and supports float, double, cfloat, and cdouble dtypes.",
        "math": "\\det(A) = \\det(P) \\cdot \\prod_{i=1}^{n} U_{ii}; When pivot=False: \\det(A) = \\prod_{i=1}^{n} U_{ii}",
        "example": ">>> import torch\n>>> def determinant_lu(A, *, pivot=True, out=None):\n...     P, L, U = torch.linalg.lu(A, pivot=pivot)\n...     diag_U = torch.diagonal(U, dim1=-2, dim2=-1)\n...     det_U = torch.prod(diag_U, dim=-1)\n...     if pivot:\n...         sign_P, _ = torch.linalg.slogdet(P)\n...         det = sign_P * det_U\n...     else:\n...         det = det_U\n...     if out is not None:\n...         out.copy_(det)\n...         return out\n...     return det\n>>> # Example with a 2x2 matrix\n>>> A = torch.tensor([[1., 2.], [3., 4.]])\n>>> determinant_lu(A)\ntensor(-2.)\n>>> # Compare with torch.det\n>>> torch.det(A)\ntensor(-2.)\n>>> # Example with a batch of matrices\n>>> A = torch.randn(2, 3, 3)\n>>> det = determinant_lu(A)\n>>> det.shape\ntorch.Size([2])\n>>> # Compare with torch.det\n>>> torch.allclose(det, torch.det(A))\nTrue\n>>> # Example without pivoting (on GPU)\n>>> if torch.cuda.is_available():\n...     A = torch.randn(3, 3, device='cuda')\n...     determinant_lu(A, pivot=False)\ntensor(-0.3558, device='cuda:0')",
        "torch_code": "P, L, U = torch.linalg.lu(A, pivot=pivot); diag_U = torch.diagonal(U, dim1=-2, dim2=-1); det_U = torch.prod(diag_U, dim=-1); sign_P, _ = torch.linalg.slogdet(P); out.copy_(det)",
        "torch_cnt": 5,
        "other": "This method assumes that A is invertible. If A is singular, the determinant will be zero, and the function may return `inf` or `nan` due to division by zero or numerical instability.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "determinant_lu.py"
    },
    {
        "name": "tanh_linear",
        "func_inputs": "def tanh_linear(input, weight, bias=None) -> Tensor: input (Tensor): The input tensor of shape `(*, in_features)`, where `*` represents any number of additional dimensions. weight (Tensor): The weight matrix of shape `(out_features, in_features)`. bias (Tensor, optional): The optional bias tensor of shape `(out_features)`. Default: None.",
        "description": "Applies a linear transformation to the input tensor followed by a Tanh activation function. This combined operation is useful for introducing non-linearity after a linear transformation, helping to capture complex relationships in the data.",
        "math": "The combined operation is defined as: out = tanh(linear(input, weight, bias)) where the linear transformation is applied as y = xA^T + b and Tanh activation is applied element-wise as: Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))",
        "example": "Example::\n\n    >>> import torch\n    >>> # Define input tensor, weights, and bias\n    >>> input = torch.randn(5, 3)       # (batch_size, in_features)\n    >>> weight = torch.randn(4, 3)      # (out_features, in_features)\n    >>> bias = torch.randn(4)           # (out_features)\n    >>> # Apply tanh_linear transformation\n    >>> result = tanh_linear(input, weight, bias)\n    >>> result.shape\n    torch.Size([5, 4])\n\n    >>> # Apply tanh_linear transformation without bias\n    >>> result = tanh_linear(input, weight)\n    >>> result.shape\n    torch.Size([5, 4])",
        "torch_code": "torch.nn.functional.linear(input, weight, bias)\ntorch.tanh(linear_result)",
        "torch_cnt": 2,
        "other": "A linear transformation followed by a Tanh activation helps capture complex relationships by introducing non-linearity.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "tanh_linear.py"
    },
    {
        "name": "torch.sum",
        "func_inputs": "def sum(input, dim, keepdim=False, *, dtype=None) -> Tensor; input (Tensor): the input tensor.; dim (int or tuple of ints, optional): the dimension or dimensions to reduce.; keepdim (bool): whether the output tensor has :attr:`dim` retained or not.; dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.",
        "description": "Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed, resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).",
        "math": "",
        "example": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n        [-0.2993,  0.9138,  0.9337, -1.6864],\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n>>> torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n>>> torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])",
        "torch_code": "torch.sum(a, 1); torch.sum(b, (2, 1))",
        "torch_cnt": 2,
        "other": "If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed.",
        "difficulty": 2,
        "params_cnt": 4,
        "file": "sum.py"
    },
    {
        "name": "torch.logspace",
        "func_inputs": "logspace(start, end, steps, base=10.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nArgs:\n    start (float or Tensor): the starting value for the set of points. If `Tensor`, it must be 0-dimensional\n    end (float or Tensor): the ending value for the set of points. If `Tensor`, it must be 0-dimensional\n    steps (int): size of the constructed tensor\n    base (float, optional): base of the logarithm function. Default: 10.0.\n\nKeyword arguments:\n    out (Tensor, optional): the output tensor.\n    dtype (torch.dtype, optional): the data type to perform the computation in. Default: if None, uses the global default dtype (see torch.get_default_dtype()) when both start and end are real, and corresponding complex dtype when either is complex.\n    layout (torch.layout, optional): the desired layout of returned Tensor. Default: torch.strided.\n    device (torch.device, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_device). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the returned tensor. Default: False.",
        "description": "Creates a one-dimensional tensor of size 'steps' whose values are evenly spaced from base^start to base^end, inclusive, on a logarithmic scale with a specified base. The tensor values are generated in a logarithmic progression from base^start to base^end using the specified number of steps.",
        "math": "(\text{base}^{\text{start}}, \text{base}^{(\text{start} + \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, \\ldots, \text{base}^{(\text{start} + (\text{steps} - 2) * \frac{\text{end} - \text{start}}{ \text{steps} - 1})}, \text{base}^{\text{end}})",
        "example": ">>> torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])",
        "torch_code": "torch.logspace(start, end, steps, base=10.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)",
        "torch_cnt": 1,
        "other": "From PyTorch 1.11, the 'steps' argument is required. Use steps=100 to restore the previous behavior. The function allows specifying various properties of the output tensor such as dtype, layout, and device.",
        "difficulty": 1,
        "params_cnt": 8,
        "file": "logspace.py"
    },
    {
        "name": "solve_and_add_scaled_vector",
        "func_inputs": "def solve_and_add_scaled_vector(A: torch.Tensor, b: torch.Tensor, y: torch.Tensor, alpha: float) -> torch.Tensor: A (Tensor): A triangular matrix of shape `(n, n)`. b (Tensor): Right-hand side vector or matrix of shape `(n,)` or `(n, k)`. y (Tensor): Vector to be scaled and added, must have shape `(n,)` or broadcastable to `(n,)`. alpha (float): Scaling factor for the vector y.",
        "description": "Solves the triangular system of linear equations Ax = b, where A is a triangular matrix. Then, adds a scaled version of the vector y to the solution x. The operations performed are: 1. Solve the triangular system Ax = b using torch.linalg.solve_triangular with A as an upper triangular matrix. 2. Add the scaled vector alpha * y to the solution x.",
        "math": "x = torch.linalg.solve_triangular(A, b, upper=True)\nx += alpha * y",
        "example": "Example::\n    >>> A = torch.tensor([[2.0, 1.0], [0.0, 3.0]])\n    >>> b = torch.tensor([5.0, 6.0])\n    >>> y = torch.tensor([1.0, 2.0])\n    >>> alpha = 0.5\n    >>> result = solve_and_add_scaled_vector(A, b, y, alpha)\n    >>> print(result)",
        "torch_code": "x = torch.linalg.solve_triangular(A, b, upper=True)\nx += alpha * y",
        "torch_cnt": 2,
        "other": "The function assumes A is an upper triangular matrix.",
        "difficulty": 1,
        "params_cnt": 4,
        "file": "solve_and_add_scaled_vector.py"
    },
    {
        "name": "pixel_shuffle_conv2d",
        "func_inputs": "def pixel_shuffle_conv2d(input: torch.Tensor, weight: torch.Tensor, bias=None, stride=1, padding=0, dilation=1, groups=1, upscale_factor=2) -> torch.Tensor: Input tensor of shape (minibatch, in_channels, iH, iW). Convolution filter tensor of shape (out_channels, in_channels/groups, kH, kW). Optional bias tensor of shape (out_channels). Stride of the convolving kernel. Padding added to all four sides of the input. Spacing between kernel elements. Number of blocked connections from input channels to output channels. Factor by which to increase spatial resolution.",
        "description": "Applies a 2D convolution followed by pixel shuffle upscaling to rearrange the spatial dimensions. This function sequentially applies a 2D convolution operation and then rearranges the elements of the convolution output to increase the spatial resolution by the upscale_factor.",
        "math": "",
        "example": ">>> input = torch.randn(1, 8, 16, 16)\n>>> weight = torch.randn(16, 8, 3, 3)\n>>> output = pixel_shuffle_conv2d(input, weight, stride=1, padding=1, upscale_factor=2)\n>>> output.shape\ntorch.Size([1, 4, 32, 32])",
        "torch_code": "torch.nn.functional.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups); torch.nn.functional.pixel_shuffle(x, upscale_factor)",
        "torch_cnt": 2,
        "other": "The function first applies a 2D convolution and then uses pixel shuffle to upscale the spatial dimensions by the given upscale_factor.",
        "difficulty": 4,
        "params_cnt": 8,
        "file": "pixel_shuffle_conv2d.py"
    },
    {
        "name": "matrix_vector_dot",
        "func_inputs": "def matrix_vector_dot(A: Tensor, x: Tensor, y: Tensor, alpha: float, beta: float) -> Tensor: \nArgs:\n    A (Tensor): The input matrix of shape `(n, m)`.\n    x (Tensor): The input vector of shape `(m,)`.\n    y (Tensor): The target vector to be modified, of shape `(n,)`.\n    alpha (float): Scalar multiplier for `torch.mv(A, x)`.\n    beta (float): Scalar multiplier for `y`.",
        "description": "Computes the matrix-vector product `y = alpha * torch.mv(A, x) + beta * y` and then returns the dot product `torch.dot(y, x)`. The function first computes a scaled matrix-vector product and updates `y`, then calculates the dot product of the updated `y` with `x`. It requires an input matrix `A` of shape `(n, m)`, an input vector `x` of shape `(m,)`, and a target vector `y` of shape `(n,)` that is modified in-place. The scalar `alpha` is a multiplier for `torch.mv(A, x)`, while `beta` is a multiplier for `y`.",
        "math": "y = alpha * torch.mv(A, x) + beta * y; result = torch.dot(y, x)",
        "example": ">>> A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n>>> x = torch.tensor([0.5, -1.0])\n>>> y = torch.tensor([1.0, 2.0])\n>>> alpha, beta = 2.0, 0.5\n>>> result = matrix_vector_dot(A, x, y, alpha, beta)\n>>> print(result)  # Outputs the dot product of the modified y and x",
        "torch_code": "y = alpha * torch.mv(A, x) + beta * y\nresult = torch.dot(y, x)",
        "torch_cnt": 2,
        "other": "The function modifies the `y` vector in-place and calculates a dot product after the update.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "matrix_vector_dot.py"
    },
    {
        "name": "min_gelu",
        "func_inputs": "min_gelu(input, dim=None, keepdim=False, approximate='none', out=None) -> Tensor: input (Tensor): The input tensor. dim (int, optional): The dimension to reduce. If ``None``, returns the minimum of all elements. keepdim (bool, optional): Whether the output tensor retains :attr:`dim` as size 1. Default is ``False``. approximate (str, optional): The approximation method for GELU. Default is 'none'. out (Tensor, optional): The output tensor.",
        "description": "Computes the Gaussian Error Linear Units (GELU) activation on the input tensor, then returns the minimum value along the specified dimension(s) or over all elements if no dimension is specified. The function supports two methods for computing GELU: exact and approximate using 'tanh'.",
        "math": "out = min(GELU(input))\n\nGELU(x) = x * Φ(x) if approximate is 'none'\nGELU(x) = 0.5 * x * (1 + Tanh(√(2/π) * (x + 0.044715 * x^3))) if approximate is 'tanh'",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.tensor([[0.5, -1.2, 3.0], [1.5, 0.0, -2.4]])\n>>> result = min_gelu(a)\n>>> result\ntensor(-0.0997)\n\n>>> values, indices = min_gelu(a, dim=1)\n>>> values\ntensor([-0.0997, -0.0952])\n>>> indices\ntensor([1, 2])\n\n>>> result = min_gelu(a, approximate='tanh')\n>>> result\ntensor(-0.0997)",
        "torch_code": "torch.nn.functional.gelu(input, approximate=approximate)\ntorch.min(gelu_result, dim=dim, keepdim=keepdim, out=out)",
        "torch_cnt": 2,
        "other": "Returns a namedtuple (values, indices) if dim is specified, otherwise returns the minimum value tensor.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "min_gelu.py"
    },
    {
        "name": "torch.pow",
        "func_inputs": "pow(input, exponent, *, out=None) -> Tensor; Args: input (Tensor): the input tensor. exponent (float or tensor): the exponent value; Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Takes the power of each element in input with exponent and returns a tensor with the result. exponent can be either a single float number or a Tensor with the same number of elements as input. If exponent is a scalar value, the operation applied is out_i = x_i ^ exponent. If exponent is a tensor, the operation applied is out_i = x_i ^ exponent_i. When exponent is a tensor, the shapes of input and exponent must be broadcastable.",
        "math": "out_i = x_i ^ exponent (for scalar exponent)\nout_i = x_i ^ exponent_i (for tensor exponent)",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n>>> torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n>>> exp = torch.arange(1., 5.)\n>>> a = torch.arange(1., 5.)\n>>> a\ntensor([ 1.,  2.,  3.,  4.])\n>>> exp\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])",
        "torch_code": "torch.pow(a, 2)\n torch.pow(a, exp)\n torch.pow(base, exp)",
        "torch_cnt": 3,
        "other": "The operation supports both scalar and tensor exponents. When exponent is a tensor, its shape must be broadcastable with the input tensor.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "pow.py"
    },
    {
        "name": "relu_max_pool2d_conv2d",
        "func_inputs": "relu_max_pool2d_conv2d(input, weight, bias=None, conv_stride=1, conv_padding=0, conv_dilation=1, conv_groups=1, pool_kernel_size=2, pool_stride=None, pool_padding=0, pool_dilation=1, pool_ceil_mode=False, inplace=False) -> Tensor: input (Tensor): The input tensor of shape `(minibatch, in_channels, iH, iW)`. weight (Tensor): The convolution filters of shape `(out_channels, in_channels / groups, kH, kW)`. bias (Tensor, optional): Optional bias tensor of shape `(out_channels)`. Default: None. conv_stride (int or tuple, optional): The stride of the convolution kernel. Default: 1. conv_padding (int, tuple, or string, optional): Padding added to all sides of the input in convolution. Default: 0. conv_dilation (int or tuple, optional): The spacing between kernel elements in convolution. Default: 1. conv_groups (int, optional): Number of blocked connections from input channels to output channels in convolution. Default: 1. pool_kernel_size (int or tuple): The size of the pooling region in max pooling. pool_stride (int or tuple, optional): The stride of the pooling operation. Default: `pool_kernel_size`. pool_padding (int or tuple, optional): Padding added to all sides of the input in max pooling. Default: 0. pool_dilation (int or tuple, optional): The stride between elements within a sliding window in max pooling. Default: 1. pool_ceil_mode (bool, optional): If True, uses `ceil` instead of `floor` to compute output shape. Default: False. inplace (bool, optional): If True, performs ReLU in-place. Default: False.",
        "description": "Applies a 2D convolution over the input tensor, followed by max pooling and then applies the ReLU activation function element-wise to the pooled result. This combined operation is often used in convolutional neural networks (CNNs) for feature extraction, downsampling, and adding non-linearity.",
        "math": "\n    \\text{out} = \\text{ReLU}(\\text{MaxPool2D}(\\text{conv2d}(\\text{input})))\n\nwhere the ReLU function is applied element-wise as:\n\n    \\text{ReLU}(x) = \\max(0, x)",
        "example": "\n>>> import torch\n>>> import torch.nn.functional as F\n>>> # Define input tensor and filters\n>>> inputs = torch.randn(1, 3, 32, 32)  # (batch, channels, height, width)\n>>> filters = torch.randn(6, 3, 5, 5)   # (out_channels, in_channels, kH, kW)\n>>> # Apply relu_max_pool2d_conv2d with conv_padding and pool_kernel_size\n>>> result = relu_max_pool2d_conv2d(inputs, filters, conv_padding=2, pool_kernel_size=2)\n>>> result.shape\ntorch.Size([1, 6, 16, 16])\n\n>>> # Apply relu_max_pool2d_conv2d with custom conv_stride and pool_stride\n>>> result = relu_max_pool2d_conv2d(inputs, filters, conv_stride=2, pool_kernel_size=3, pool_stride=2)\n>>> result.shape\ntorch.Size([1, 6, 8, 8])",
        "torch_code": "conv_result = torch.nn.functional.conv2d(input, weight, bias=bias, stride=conv_stride, padding=conv_padding, dilation=conv_dilation, groups=conv_groups)\npooled_result = torch.nn.functional.max_pool2d(conv_result, kernel_size=pool_kernel_size, stride=pool_stride, padding=pool_padding, dilation=pool_dilation, ceil_mode=pool_ceil_mode)\nreturn torch.nn.functional.relu(pooled_result, inplace=inplace)",
        "torch_cnt": 3,
        "other": "The function is typically used in CNNs.",
        "difficulty": 4,
        "params_cnt": 11,
        "file": "relu_max_pool2d_conv2d.py"
    },
    {
        "name": "torch.special.erf",
        "func_inputs": "erf(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the error function of the input tensor. The error function is used in probability, statistics, and partial differential equations describing diffusion.",
        "math": "\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt",
        "example": ">>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])",
        "torch_code": "torch.special.erf(torch.tensor([0, -1., 10.]))",
        "torch_cnt": 1,
        "other": "The function outputs a tensor with values representing the error function of each element in the input tensor.",
        "difficulty": 3,
        "params_cnt": 2,
        "file": "erf.py"
    },
    {
        "name": "torch.sigmoid",
        "func_inputs": "sigmoid(input, *, out=None) -> Tensor",
        "description": "This function computes the sigmoid of the input tensor element-wise. The sigmoid function is a common activation function used in neural networks, which maps any real-valued number into the range (0, 1).",
        "math": "The sigmoid function is defined as: sigmoid(x) = 1 / (1 + exp(-x))",
        "example": "",
        "torch_code": "torch.sigmoid(input)",
        "torch_cnt": 1,
        "other": "Alias for torch.special.expit.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "sigmoid.py"
    },
    {
        "name": "torch.nn.functional.gelu",
        "func_inputs": "gelu(input, approximate='none') -> Tensor",
        "description": "Applies the Gaussian Error Linear Unit (GELU) activation function element-wise to the input tensor. The function can be computed exactly or approximately using a tanh-based formula depending on the 'approximate' argument.",
        "math": "When approximate is 'none': GELU(x) = x * Φ(x), where Φ(x) is the Cumulative Distribution Function for Gaussian Distribution. When approximate is 'tanh': GELU(x) = 0.5 * x * (1 + Tanh(√(2/π) * (x + 0.044715 * x^3)))",
        "example": "",
        "torch_code": "torch.nn.functional.gelu(input, approximate='none')",
        "torch_cnt": 1,
        "other": "See Gaussian Error Linear Units (GELUs) https://arxiv.org/abs/1606.08415",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "gelu.py"
    },
    {
        "name": "torch.linalg.det",
        "func_inputs": "linalg.det(A, *, out=None) -> Tensor; A (Tensor): tensor of shape (*, n, n) where * is zero or more batch dimensions; out (Tensor, optional): output tensor. Ignored if None. Default: None.",
        "description": "Computes the determinant of a square matrix. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.",
        "math": "",
        "example": ">>> A = torch.randn(3, 3)\n>>> torch.linalg.det(A)\ntensor(0.0934)\n\n>>> A = torch.randn(3, 2, 2)\n>>> torch.linalg.det(A)\ntensor([1.1990, 0.4099, 0.7386])",
        "torch_code": "torch.linalg.det(A)",
        "torch_cnt": 1,
        "other": ":func:`torch.linalg.slogdet` computes the sign and natural logarithm of the absolute value of the determinant of square matrices.",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "det.py"
    },
    {
        "name": "fused_bmm_rmsnorm_gelu_dropout",
        "func_inputs": "fused_bmm_rmsnorm_gelu_dropout(input1, input2, normalized_shape, dropout_p=0.1, eps=1e-5, training=True, approximate='none', *, out=None) -> Tensor; input1 (Tensor): First input tensor for bmm, of shape (B, N, M), where B is the batch size; input2 (Tensor): Second input tensor for bmm, of shape (B, M, P); normalized_shape (int or list or torch.Size): Input shape from an expected input of size (B, N, P). This is the shape over which RMS normalization is applied; dropout_p (float, optional): Probability of an element to be zeroed in the dropout layer. Default: 0.1; eps (float, optional): A value added to the denominator for numerical stability in RMS normalization. Default: 1e-5; training (bool, optional): Apply dropout if True. Default: True; approximate (str, optional): Can be 'none' or 'tanh'. The approximation to use for GELU. Default: 'none'; out (Tensor, optional): Output tensor. Ignored if None. Default: None.",
        "description": "Performs a fused operation combining batch matrix multiplication, RMS normalization, GELU activation, and dropout.",
        "math": "Given two input tensors X and Y, this function computes: \\[ \\begin{align*} Z_1 &= \\text{bmm}(X, Y) \\\\ Z_2 &= \\text{RMSNorm}(Z_1, \\epsilon) \\\\ Z_3 &= \\text{GELU}(Z_2) \\\\ Z &= \\text{Dropout}(Z_3, p) \\end{align*} \\] where: \\- \\text{bmm}(X, Y) performs batch matrix multiplication. \\- \\text{RMSNorm}(Z_1, \\epsilon) = \\frac{Z_1}{\\sqrt{\\text{mean}(Z_1^2, \\text{dim}=\\text{last}) + \\epsilon}} \\times \\gamma, where \\gamma is a learnable parameter (if `elementwise_affine=True`). \\- \\text{GELU}(Z_2) applies the Gaussian Error Linear Unit activation function element-wise. \\- \\text{Dropout}(Z_3, p) randomly zeroes elements of Z_3 with probability p.",
        "example": "```python\nimport torch\nimport torch.nn.functional as F\n\ndef fused_bmm_rmsnorm_gelu_dropout(input1, input2, normalized_shape, dropout_p=0.1, eps=1e-5, training=True, approximate='none', *, out=None):\n    z1 = torch.bmm(input1, input2)\n    rms_norm = F.rms_norm(z1, normalized_shape, eps=eps)\n    gelu_out = F.gelu(rms_norm, approximate=approximate)\n    output = F.dropout(gelu_out, p=dropout_p, training=training)\n    if out is not None:\n        out.copy_(output)\n        return out\n    return output\n\n# Example usage\nB, N, M, P = 2, 3, 4, 5\ninput1 = torch.randn(B, N, M)\ninput2 = torch.randn(B, M, P)\nnormalized_shape = P  # Since the output of bmm has shape (B, N, P)\n\noutput = fused_bmm_rmsnorm_gelu_dropout(input1, input2, normalized_shape)\nprint(\"Output shape:\", output.shape)\n# Output shape: torch.Size([2, 3, 5])\n```",
        "torch_code": "z1 = torch.bmm(input1, input2)\nrms_norm = torch.nn.functional.rms_norm(z1, normalized_shape, eps=eps)\ngelu_out = torch.nn.functional.gelu(rms_norm, approximate=approximate)\noutput = torch.nn.functional.dropout(gelu_out, p=dropout_p, training=training)\nif out is not None:\n    out.copy_(output)\n    return out\nreturn output",
        "torch_cnt": 4,
        "other": "- The shapes of `input1` and `input2` must be compatible for batch matrix multiplication: `input1` of shape `(B, N, M)` and `input2` of shape `(B, M, P)` result in an output of shape `(B, N, P)`.\n- The `normalized_shape` argument for RMS normalization should match the shape of the last dimension(s) of the output tensor over which to compute the RMS.\n- The `GELU` activation is applied element-wise to the normalized output.\n- The `dropout` is applied during training when `training=True`. Set `training=False` to disable dropout during evaluation.\n- All operations are differentiable and support autograd.",
        "difficulty": 3,
        "params_cnt": 8,
        "file": "fused_bmm_rmsnorm_gelu_dropout.py"
    },
    {
        "name": "torch.floor",
        "func_inputs": "floor(input, *, out=None) -> Tensor\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the floor of the elements of the input, the largest integer less than or equal to each element. For integer inputs, follows the array-api convention of returning a copy of the input tensor.",
        "math": "\\text{out}_{i} = \\left\\lfloor \\text{input}_{i} \\right\\rfloor",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])",
        "torch_code": "torch.floor(a)",
        "torch_cnt": 1,
        "other": "For integer inputs, the function returns a copy of the input tensor.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "floor.py"
    },
    {
        "name": "torch.rand",
        "func_inputs": "rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\n\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor. Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor. Default: if ``None``, uses the current device for the default tensor type (see :func:`torch.set_default_device`). :attr:`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: ``False``.",
        "description": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1). The shape of the tensor is defined by the variable argument size.",
        "math": "",
        "example": ">>> torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n>>> torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])",
        "torch_code": "torch.rand(4); torch.rand(2, 3)",
        "torch_cnt": 2,
        "other": "The function can take a variable number of arguments to define the shape of the tensor. It supports optional parameters for generator, output tensor, data type, layout, device, autograd recording, and pinned memory.",
        "difficulty": 2,
        "params_cnt": 8,
        "file": "rand.py"
    },
    {
        "name": "torch.cholesky_solve",
        "func_inputs": "cholesky_solve(B, L, upper=False, *, out=None) -> Tensor; B (Tensor): right-hand side tensor of shape (*, n, k) where * is zero or more batch dimensions; L (Tensor): tensor of shape (*, n, n) where * is zero or more batch dimensions consisting of lower or upper triangular Cholesky decompositions of symmetric or Hermitian positive-definite matrices; upper (bool, optional): flag that indicates whether L is lower triangular or upper triangular. Default: False; out (Tensor, optional): output tensor. Ignored if None. Default: None",
        "description": "Computes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition. Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if :math:`A` or :math:`B` is a batch of matrices then the output has the same batch dimensions.",
        "math": "A = LL^{\\text{H}}; AX = B",
        "example": ">>> A = torch.randn(3, 3)\n>>> A = A @ A.T + torch.eye(3) * 1e-3 # Creates a symmetric positive-definite matrix\n>>> L = torch.linalg.cholesky(A) # Extract Cholesky decomposition\n>>> B = torch.randn(3, 2)\n>>> torch.cholesky_solve(B, L)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> A.inverse() @  B\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n\n>>> A = torch.randn(3, 2, 2, dtype=torch.complex64)\n>>> A = A @ A.mH + torch.eye(2) * 1e-3 # Batch of Hermitian positive-definite matrices\n>>> L = torch.linalg.cholesky(A)\n>>> B = torch.randn(2, 1, dtype=torch.complex64)\n>>> X = torch.cholesky_solve(B, L)\n>>> torch.dist(X, A.inverse() @ B)\ntensor(1.6881e-5)",
        "torch_code": "torch.cholesky_solve(B, L); torch.linalg.cholesky(A); A.inverse() @ B; torch.dist(X, A.inverse() @ B)",
        "torch_cnt": 4,
        "other": "Supports float, double, cfloat, cdouble dtypes; Handles batches of matrices; Uses Cholesky decomposition",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "cholesky_solve.py"
    },
    {
        "name": "mul_sub",
        "func_inputs": "def mul_sub(input, other_mul, other_sub, alpha=1, out=None) -> Tensor: input (Tensor): The input tensor to be multiplied. other_mul (Tensor or Number): The tensor or number to multiply with `input`. other_sub (Tensor or Number): The tensor or number to subtract from the multiplication result. alpha (Number, optional): The multiplier for :attr:`other_sub`. Default is 1. out (Tensor, optional): The output tensor.",
        "description": "Multiplies the input tensor by another tensor or number, then subtracts another tensor or number from the result, scaled by a given alpha. This operation is performed element-wise.",
        "math": "\\text{out}_i = (\\text{input}_i \\times \\text{other\\_mul}_i) - \\text{alpha} \\times \\text{other\\_sub}_i",
        "example": ">>> import torch\n>>> a = torch.tensor([2.0, 3.0, 4.0])\n>>> b = torch.tensor([0.5, 1.5, 2.0])\n>>> c = torch.tensor([1.0, 2.0, 3.0])\n>>> # Multiply a and b, then subtract c scaled by alpha\n>>> result = mul_sub(a, b, c, alpha=2)\n>>> result\ntensor([ 0.0000, -0.5000, -2.0000])\n\n>>> # Using scalars for multiplication and subtraction\n>>> result = mul_sub(a, 2, 1, alpha=1)\n>>> result\ntensor([3., 5., 7.])",
        "torch_code": "mul_result = torch.mul(input, other_mul)\nreturn torch.sub(mul_result, other_sub, alpha=alpha, out=out)",
        "torch_cnt": 2,
        "other": "The function allows for element-wise operations and supports both tensor and scalar inputs for multiplication and subtraction. The output can be stored in a specified tensor.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "mul_sub.py"
    },
    {
        "name": "torch.linalg.ldl_factor",
        "func_inputs": "linalg.ldl_factor(A, *, hermitian=False, out=None) -> (Tensor, Tensor)\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions consisting of symmetric or Hermitian matrices.\n\nKeyword args:\n    hermitian (bool, optional): whether to consider the input to be Hermitian or symmetric. For real-valued matrices, this switch has no effect. Default: `False`.\n    out (tuple, optional): tuple of two tensors to write the output to. Ignored if `None`. Default: `None`.\n\nReturns:\n    A named tuple `(LD, pivots)`.",
        "description": "Computes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions. When A is complex valued it can be Hermitian (hermitian=True) or symmetric (hermitian=False). The factorization is of the form A = L D L^T. If hermitian is True then transpose operation is the conjugate transpose. L (or U) and D are stored in compact form in LD. They follow the format specified by LAPACK's sytrf function. These tensors may be used in torch.linalg.ldl_solve to solve linear systems.",
        "math": "A = L D L^T",
        "example": "Examples::\n\n    >>> A = torch.randn(3, 3)\n    >>> A = A @ A.mT # make symmetric\n    >>> A\n    tensor([[7.2079, 4.2414, 1.9428],\n            [4.2414, 3.4554, 0.3264],\n            [1.9428, 0.3264, 1.3823]])\n    >>> LD, pivots = torch.linalg.ldl_factor(A)\n    >>> LD\n    tensor([[ 7.2079,  0.0000,  0.0000],\n            [ 0.5884,  0.9595,  0.0000],\n            [ 0.2695, -0.8513,  0.1633]])\n    >>> pivots\n    tensor([1, 2, 3], dtype=torch.int32)",
        "torch_code": "torch.linalg.ldl_factor(A)",
        "torch_cnt": 1,
        "other": "When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see torch.linalg.ldl_factor_ex.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "ldl_factor.py"
    },
    {
        "name": "torch.abs",
        "func_inputs": "abs(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the absolute value of each element in the input tensor.",
        "math": "\\text{out}_{i} = |\\text{input}_{i}|",
        "example": ">>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])",
        "torch_code": "torch.abs(torch.tensor([-1, -2, 3]))",
        "torch_cnt": 1,
        "other": "",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "abs.py"
    },
    {
        "name": "torch.mul",
        "func_inputs": "mul(input, other, *, out=None) -> Tensor\n    input (Tensor): the input tensor.\n    other (Tensor or Number) - the tensor or number to multiply input by.\n    out (Tensor, optional): the output tensor.",
        "description": "Multiplies the input tensor by another tensor or a number, supporting broadcasting to a common shape, type promotion, and integer, float, and complex inputs.",
        "math": "\\text{out}_i = \\text{input}_i \\times \\text{other}_i",
        "example": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])\n\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 1.1207],\n        [-0.3137],\n        [ 0.0700],\n        [ 0.8378]])\n>>> c = torch.randn(1, 4)\n>>> c\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n>>> torch.mul(b, c)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n        [-0.1614, -0.0382,  0.1645, -0.7021],\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])",
        "torch_code": "torch.mul(a, 100); torch.mul(b, c)",
        "torch_cnt": 2,
        "other": "Supports broadcasting and type promotion.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "mul.py"
    },
    {
        "name": "torch.nn.functional.softmax",
        "func_inputs": "def softmax(input, dim, dtype=None) -> Tensor: input (Tensor): input; dim (int): A dimension along which softmax will be computed.; dtype (torch.dtype, optional): the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.",
        "description": "Apply a softmax function to all slices along the specified dimension, re-scaling them so that the elements lie in the range [0, 1] and sum to 1.",
        "math": "Softmax(x_i) = exp(x_i) / sum_j exp(x_j)",
        "example": "",
        "torch_code": "torch.nn.functional.softmax(input, dim, dtype=None)",
        "torch_cnt": 1,
        "other": "This function doesn't work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it's faster and has better numerical properties).",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "softmax.py"
    },
    {
        "name": "torch.nn.functional.leaky_relu",
        "func_inputs": "leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor",
        "description": "Applies the Leaky ReLU activation function element-wise to the input tensor. The function is defined as LeakyReLU(x) = max(0, x) + negative_slope * min(0, x), where negative_slope is a small constant that allows a small, non-zero gradient when the unit is not active.",
        "math": "LeakyReLU(x) = max(0, x) + negative_slope * min(0, x)",
        "example": "",
        "torch_code": "torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False)",
        "torch_cnt": 1,
        "other": "See torch.nn.LeakyReLU for more details.",
        "difficulty": 2,
        "params_cnt": 3,
        "file": "leaky_relu.py"
    },
    {
        "name": "invert_matrix_lu",
        "func_inputs": "invert_matrix_lu(A, *, pivot=True, out=None) -> Tensor",
        "description": "Computes the inverse of a square matrix using LU decomposition. Given a square invertible matrix A, it computes the inverse A^{-1} by performing LU decomposition and solving linear systems involving triangular matrices. Supports inputs of 'float', 'double', 'cfloat', and 'cdouble' dtypes, as well as batches of matrices.",
        "math": "A = P L U\nA^{-1} = U^{-1} L^{-1} P\nY = L^{-1} P\nA^{-1} = U^{-1} Y",
        "example": ">>> import torch\n>>> def invert_matrix_lu(A, *, pivot=True, out=None):\n...     P, L, U = torch.linalg.lu(A, pivot=pivot)\n...     n = A.size(-1)\n...     if pivot:\n...         P_eye = torch.eye(n, device=A.device, dtype=A.dtype).expand_as(A)\n...         P_mat = P @ P_eye\n...     else:\n...         P_mat = torch.eye(n, device=A.device, dtype=A.dtype)\n...     Y = torch.linalg.solve(L, P_mat)\n...     A_inv = torch.linalg.solve(U, Y)\n...     if out is not None:\n...         out.copy_(A_inv)\n...         return out\n...     return A_inv\n>>> A = torch.tensor([[3., 0., 2.],\n...                   [2., 0., -2.],\n...                   [0., 1., 1.]])\n>>> A_inv = invert_matrix_lu(A)\n>>> A_inv\ntensor([[ 0.2000,  0.2000,  0.0000],\n        [-0.2000,  0.3000,  1.0000],\n        [ 0.2000, -0.3000, -0.0000]])\n>>> torch.allclose(A @ A_inv, torch.eye(3))\nTrue\n>>> A_batch = torch.randn(2, 3, 3)\n>>> A_inv_batch = invert_matrix_lu(A_batch)\n>>> A_inv_batch.shape\ntorch.Size([2, 3, 3])\n>>> torch.allclose(A_batch @ A_inv_batch, torch.eye(3).expand(2, 3, 3))\nTrue",
        "torch_code": "P, L, U = torch.linalg.lu(A, pivot=pivot)\nP_eye = torch.eye(n, device=A.device, dtype=A.dtype).expand_as(A)\nP_mat = P @ P_eye\nY = torch.linalg.solve(L, P_mat)\nA_inv = torch.linalg.solve(U, Y)\nout.copy_(A_inv)",
        "torch_cnt": 5,
        "other": "The function allows computing the inverse with or without pivoting (partial pivoting by default). It can handle batches of matrices, and an output tensor can be specified which will be ignored if set to None.",
        "difficulty": 3,
        "params_cnt": 3,
        "file": "invert_matrix_lu.py"
    },
    {
        "name": "torch.std",
        "func_inputs": "def std(input, dim=None, *, correction=1, keepdim=False, out=None) -> Tensor: input (Tensor): the input tensor. dim (int or tuple of ints): the dimension or dimensions to reduce. correction (int): difference between the sample size and sample degrees of freedom. Defaults to `Bessel's correction`, correction=1. keepdim (bool): whether the output tensor has dim retained or not. out (Tensor, optional): the output tensor.",
        "description": "Calculates the standard deviation over the specified dimensions of the input tensor. The dim argument can specify a single dimension, a list of dimensions, or None to reduce over all dimensions. If keepdim is set to True, the output tensor retains the reduced dimensions as size 1; otherwise, these dimensions are removed. The correction parameter adjusts the calculation for the difference between sample size and degrees of freedom, defaulting to Bessel's correction with correction=1.",
        "math": "\\sigma = \\sqrt{\\frac{1}{\\max(0,~N - \\delta N)}\\sum_{i=0}^{N-1}(x_i-\\bar{x})^2}",
        "example": ">>> a = torch.tensor(\n...     [[ 0.2035,  1.2959,  1.8101, -0.4644],\n...      [ 1.5027, -0.3270,  0.5905,  0.6538],\n...      [-1.5745,  1.3330, -0.5596, -0.6548],\n...      [ 0.1264, -0.5080,  1.6420,  0.1992]])\n>>> torch.std(a, dim=1, keepdim=True)\ntensor([[1.0311],\n        [0.7477],\n        [1.2204],\n        [0.9087]])",
        "torch_code": "torch.std(a, dim=1, keepdim=True)",
        "torch_cnt": 1,
        "other": "The standard deviation function has undergone a change in version 2.0, where the argument previously called unbiased has been renamed to correction. Bessel's correction link: https://en.wikipedia.org/wiki/Bessel%27s_correction",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "std.py"
    },
    {
        "name": "tril_mm_and_scale",
        "func_inputs": "def tril_mm_and_scale(A: torch.Tensor, B: torch.Tensor, alpha: float, beta: float) -> torch.Tensor: A (Tensor): A 2D matrix to be multiplied, of shape (n, n). B (Tensor): A matrix to be multiplied with the lower triangular part of A, of shape (n, p). alpha (float): Scaling factor for the initial matrix multiplication result. beta (float): Scaling factor for the final result.",
        "description": "Performs a matrix multiplication of the lower triangular part of matrix `A` with matrix `B`, scales the result by `alpha`, and then scales the final output by `beta`. The operations are as follows:\n1. Perform matrix multiplication between the lower triangular part of `A` (denoted as `torch.tril(A)`) and `B`, and scale the result by `alpha`.\n2. Scale the resulting matrix from step 1 by `beta` to obtain the final result.",
        "math": "B = alpha * torch.mm(torch.tril(A), B)\nC = beta * B",
        "example": ">>> A = torch.tensor([[2.0, 3.0], [1.0, 4.0]])\n>>> B = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n>>> alpha = 0.5\n>>> beta = 2.0\n>>> result = tril_mm_and_scale(A, B, alpha, beta)\n>>> print(result)",
        "torch_code": "torch.tril(A), torch.mm",
        "torch_cnt": 2,
        "other": "",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "tril_mm_and_scale.py"
    },
    {
        "name": "torch.linalg.solve",
        "func_inputs": "A (Tensor), B (Tensor), *, left (bool, optional), out (Tensor, optional)",
        "description": "Computes the solution of a square system of linear equations with a unique solution. Supports inputs of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if the inputs are batches of matrices then the output has the same batch dimensions. Assumes that matrix A is invertible.",
        "math": "AX = B; XA = B",
        "example": ">>> A = torch.randn(3, 3)\n>>> b = torch.randn(3)\n>>> x = torch.linalg.solve(A, b)\n>>> torch.allclose(A @ x, b)\nTrue\n>>> A = torch.randn(2, 3, 3)\n>>> B = torch.randn(2, 3, 4)\n>>> X = torch.linalg.solve(A, B)\n>>> X.shape\ntorch.Size([2, 3, 4])\n>>> torch.allclose(A @ X, B)\nTrue\n\n>>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(3, 1)\n>>> x = torch.linalg.solve(A, b) # b is broadcasted to size (2, 3, 1)\n>>> x.shape\ntorch.Size([2, 3, 1])\n>>> torch.allclose(A @ x, b)\nTrue\n>>> b = torch.randn(3)\n>>> x = torch.linalg.solve(A, b) # b is broadcasted to size (2, 3)\n>>> x.shape\ntorch.Size([2, 3])\n>>> Ax = A @ x.unsqueeze(-1)\n>>> torch.allclose(Ax, b.unsqueeze(-1).expand_as(Ax))\nTrue",
        "torch_code": "torch.linalg.solve(A, B)",
        "torch_cnt": 1,
        "other": "This function computes `X = A.inverse() @ B` in a faster and more numerically stable way than performing the computations separately. When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see `torch.linalg.solve_ex`.",
        "difficulty": 4,
        "params_cnt": 4,
        "file": "solve.py"
    },
    {
        "name": "torch.special.airy_ai",
        "func_inputs": "airy_ai(input, *, out=None) -> Tensor Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Computes the Airy function Ai for each element of the input tensor.",
        "math": "Airy function :math:`\\text{Ai}\\left(\\text{input}\\right)`.",
        "example": "",
        "torch_code": "torch.special.airy_ai(input)",
        "torch_cnt": 1,
        "other": "",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "airy_ai.py"
    },
    {
        "name": "torch.signbit",
        "func_inputs": "signbit(input, *, out=None) -> Tensor; Args: input (Tensor): the input tensor.; Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Tests if each element of the input tensor has its sign bit set or not. It handles signed zeros, so negative zero (-0) returns True.",
        "math": "",
        "example": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> torch.signbit(a)\ntensor([ False, True,  False,  False])\n>>> a = torch.tensor([-0.0, 0.0])\n>>> torch.signbit(a)\ntensor([ True,  False])",
        "torch_code": "torch.signbit(a)",
        "torch_cnt": 1,
        "other": "signbit handles signed zeros, so negative zero (-0) returns True.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "signbit.py"
    },
    {
        "name": "matrix_multiply_and_row_dot",
        "func_inputs": "def matrix_multiply_and_row_dot(A: torch.Tensor, B: torch.Tensor, alpha: float, beta: float, C: torch.Tensor) -> torch.Tensor: A (Tensor): First input matrix of shape `(n, m)`. B (Tensor): Second input matrix of shape `(m, p)`. alpha (float): Scalar multiplier for the matrix-matrix product. beta (float): Scalar multiplier for the input matrix `C`. C (Tensor): Output matrix of shape `(n, p)` where the results are added.",
        "description": "Computes a scaled matrix-matrix product, then calculates the dot product of the first two rows of the resulting matrix. First, it multiplies matrix A and B using the scalar alpha and then adds the scaled version of matrix C using scalar beta. Finally, it computes the dot product of the first two rows of the updated matrix C.",
        "math": "1. `C = alpha * torch.mm(A, B) + beta * C`; 2. `result = torch.dot(C[0], C[1])`",
        "example": ">>> A = torch.tensor([[1.0, 2.0], [3.0, 4.0]]); >>> B = torch.tensor([[5.0, 6.0], [7.0, 8.0]]); >>> C = torch.zeros(2, 2); >>> alpha, beta = 1.5, 0.5; >>> result = matrix_multiply_and_row_dot(A, B, alpha, beta, C); >>> print(result)",
        "torch_code": "C = alpha * torch.mm(A, B) + beta * C; result = torch.dot(C[0], C[1])",
        "torch_cnt": 3,
        "other": "Assumes `C` has at least two rows for the dot product to be computed.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "matrix_multiply_and_row_dot.py"
    },
    {
        "name": "torch.special.polygamma",
        "func_inputs": "def polygamma(n, input, *, out=None) -> Tensor: n (int): the order of the polygamma function; input (Tensor): the input tensor.; out (Tensor, optional): the output tensor.",
        "description": "Computes the n-th derivative of the digamma function on input. The function is implemented for nonnegative integers n >= 0.",
        "math": "\\psi^{(n)}(x) = \\frac{d^{(n)}}{dx^{(n)}} \\psi(x)",
        "example": ">>> a = torch.tensor([1, 0.5])\n>>> torch.special.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.special.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.special.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.special.polygamma(4, a)\ntensor([ -24.8863, -771.4742])",
        "torch_code": "torch.special.polygamma(1, a)\ntorch.special.polygamma(2, a)\ntorch.special.polygamma(3, a)\ntorch.special.polygamma(4, a)",
        "torch_cnt": 4,
        "other": "Implemented only for nonnegative integers n >= 0.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "polygamma.py"
    },
    {
        "name": "elu_linear",
        "func_inputs": "def elu_linear(input, weight, bias=None, alpha=1.0, inplace=False) -> Tensor: input (Tensor): The input tensor for the linear layer. weight (Tensor): The weight tensor for the linear transformation. bias (Tensor, optional): The bias tensor for the linear transformation. Default: None. alpha (float, optional): The \\(\\alpha\\) parameter for the ELU function. Default: 1.0. inplace (bool, optional): Whether to apply ELU in-place. Default: False.",
        "description": "Applies a linear transformation to the input tensor, followed by the Exponential Linear Unit (ELU) activation function applied element-wise. This combined operation first performs a linear transformation and then introduces non-linearity with ELU.",
        "math": "\\text{out} = \\text{ELU}(\\text{Linear}(x))\n\n\\text{ELU}(x) = \\begin{cases}\n    x, & \\text{ if } x > 0\\\\\n    \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0\n\\end{cases}",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define input tensor, weights, and bias\n>>> input = torch.randn(2, 3)   # (batch_size, in_features)\n>>> weight = torch.randn(4, 3)  # (out_features, in_features)\n>>> bias = torch.randn(4)       # (out_features)\n>>> # Apply elu_linear\n>>> result = elu_linear(input, weight, bias)\n>>> result.shape\ntorch.Size([2, 4])\n\n>>> # Apply elu_linear with custom alpha and inplace=True\n>>> result = elu_linear(input, weight, bias, alpha=2.0, inplace=True)\n>>> result.shape\ntorch.Size([2, 4])",
        "torch_code": "linear_result = torch.nn.functional.linear(input, weight, bias)\nreturn torch.nn.functional.elu(linear_result, alpha=alpha, inplace=inplace)",
        "torch_cnt": 2,
        "other": "The function integrates linear transformation and ELU activation. The ELU activation applies element-wise to incorporate non-linearity after linear mapping.",
        "difficulty": 4,
        "params_cnt": 5,
        "file": "elu_linear.py"
    },
    {
        "name": "fused_pairwise_distance_normalize",
        "func_inputs": "def fused_pairwise_distance_normalize(x1: torch.Tensor, x2: torch.Tensor, p_norm: float = 2.0, eps_norm: float = 1e-12, eps_distance: float = 1e-6, keepdim: bool = False) -> torch.Tensor\nArgs:\n    x1 (Tensor): First input tensor.\n    x2 (Tensor): Second input tensor.\n    p_norm (float, optional): The exponent value in the norm for normalization. Default: 2.\n    eps_norm (float, optional): Small value to avoid division by zero during normalization. Default: 1e-12.\n    eps_distance (float, optional): Small value to avoid division by zero in distance calculation. Default: 1e-6.\n    keepdim (bool, optional): If `True`, retains the last dimension in the output. Default: `False`.",
        "description": "Computes the pairwise distance between two input tensors `x1` and `x2` after normalizing both tensors. Normalization is performed along the specified dimension, followed by pairwise distance calculation.",
        "math": "",
        "example": ">>> x1 = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) >>> x2 = torch.tensor([[1.0, 0.5, 1.5], [4.0, 4.5, 5.5]]) >>> output = fused_pairwise_distance_normalize(x1, x2) >>> print(output)",
        "torch_code": "norm_x1 = torch.nn.functional.normalize(x1, p=p_norm, dim=1, eps=eps_norm) norm_x2 = torch.nn.functional.normalize(x2, p=p_norm, dim=1, eps=eps_norm) distance = torch.nn.functional.pairwise_distance(norm_x1, norm_x2, p=p_norm, eps=eps_distance, keepdim=keepdim)",
        "torch_cnt": 3,
        "other": "Normalization is performed along the specified dimension. Small values `eps_norm` and `eps_distance` are used to avoid division by zero during normalization and distance calculation, respectively.",
        "difficulty": 2,
        "params_cnt": 5,
        "file": "fused_pairwise_distance_normalize.py"
    },
    {
        "name": "torch.optim.Adam",
        "func_inputs": "def Adam(params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None) -> Optimizer",
        "description": "Implements the Adam optimization algorithm, which is an adaptive learning rate optimization algorithm designed for training deep neural networks. It computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. The algorithm can optionally use the AMSGrad variant, apply weight decay, and maximize the objective function. It supports various implementation optimizations like foreach and fused implementations for performance improvements on CUDA.",
        "math": "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t; v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t; \\widehat{m_t} = m_t/(1-\\beta_1^t); \\widehat{v_t} = v_t/(1-\\beta_2^t); \\theta_t = \\theta_{t-1} - \\gamma \\widehat{m_t}/(\\sqrt{\\widehat{v_t}} + \\epsilon)",
        "example": "",
        "torch_code": "torch.optim.Adam(params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None)",
        "torch_cnt": 1,
        "other": "The foreach and fused implementations are typically faster than the for-loop, single-tensor implementation. The algorithm is based on the paper 'Adam: A Method for Stochastic Optimization'.",
        "difficulty": 3,
        "params_cnt": 11,
        "file": "Adam.py"
    },
    {
        "name": "fused_hstack_div",
        "func_inputs": "fused_hstack_div(tensors, divisor, *, rounding_mode=None, out=None) -> Tensor\n\n- **tensors** (sequence of Tensors): Sequence of tensors to be horizontally stacked. The tensors must have compatible shapes for stacking.\n- **divisor** (Tensor or Number): The tensor or number to divide the stacked tensor by. Must be broadcastable to the shape of the stacked tensor.\n- **rounding_mode** (str, optional): Type of rounding applied to the result:\n  - `None`: Default behavior. Performs no rounding and, if both `input` and `divisor` are integer types, promotes the inputs to the default scalar type. Equivalent to true division in Python (`/` operator).\n  - `'trunc'`: Rounds the results of the division towards zero.\n  - `'floor'`: Rounds the results of the division down.\n\n  Default: `None`.\n\n- **out** (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Performs a fused operation combining horizontal stacking (hstack) and element-wise division. The function first horizontally stacks a sequence of tensors and then divides each element of the resulting tensor by the corresponding element of a divisor tensor, with optional rounding modes.",
        "math": "Given a sequence of tensors [X_1, X_2, \\dots, X_n] and a divisor tensor D, the function computes:\n\n1. **Horizontal Stacking:**\n\n\\[\nX = \\text{hstack}(X_1, X_2, \\dots, X_n)\n\\]\n\n2. **Element-wise Division:**\n\n\\[\nY = \\frac{X}{D}\n\\]",
        "example": "import torch\n\ndef fused_hstack_div(tensors, divisor, *, rounding_mode=None, out=None):\n    X = torch.hstack(tensors)\n    Y = torch.div(X, divisor, rounding_mode=rounding_mode)\n    if out is not None:\n        out.copy_(Y)\n        return out\n    return Y\n\n# Example usage\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[7, 8, 9], [10, 11, 12]])\ndivisor = torch.tensor([[2, 2, 2, 2, 2, 2]])\n\noutput = fused_hstack_div([a, b], divisor)\nprint(\"Output:\")\nprint(output)\n# Output:\n# tensor([[0.5000, 1.0000, 1.5000, 3.5000, 4.0000, 4.5000],\n#         [2.0000, 2.5000, 3.0000, 5.0000, 5.5000, 6.0000]])",
        "torch_code": "X = torch.hstack(tensors)\nY = torch.div(X, divisor, rounding_mode=rounding_mode)",
        "torch_cnt": 2,
        "other": "- The tensors in `tensors` must have shapes that are compatible for horizontal stacking, i.e., the dimensions except for the stacking dimension must be the same.\n- The `divisor` tensor must be broadcastable to the shape of the stacked tensor.\n- The function supports autograd for gradient computation.\n- All operations are differentiable and support backpropagation.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "fused_hstack_div.py"
    },
    {
        "name": "torch.broadcast_tensors",
        "func_inputs": "broadcast_tensors(*tensors) -> List of Tensors: *tensors (Args: any number of tensors of the same type) -> Example: x = torch.arange(3).view(1, 3), y = torch.arange(2).view(2, 1), a, b = torch.broadcast_tensors(x, y), a.size() == torch.Size([2, 3]), a == tensor([[0, 1, 2],[0, 1, 2]])",
        "description": "Broadcasts the given tensors according to broadcasting semantics. This function takes multiple tensors as input and broadcasts them to have the same shape. Broadcasting refers to expanding the dimensions of tensors as necessary to make them compatible for element-wise operations. The broadcasted tensors share the same memory location for their elements, leading to potential issues with in-place operations.",
        "math": "",
        "example": ">>> x = torch.arange(3).view(1, 3)\n>>> y = torch.arange(2).view(2, 1)\n>>> a, b = torch.broadcast_tensors(x, y)\n>>> a.size()\ntorch.Size([2, 3])\n>>> a\ntensor([[0, 1, 2],\n        [0, 1, 2]])",
        "torch_code": "torch.broadcast_tensors(x, y)",
        "torch_cnt": 1,
        "other": "More than one element of a broadcasted tensor may refer to a single memory location. In-place operations may result in incorrect behavior. If writing to tensors is needed, clone them first.",
        "difficulty": 2,
        "params_cnt": 1,
        "file": "broadcast_tensors.py"
    },
    {
        "name": "relu_conv2d",
        "func_inputs": "relu_conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, inplace=False) -> Tensor: input (Tensor): The input tensor of shape (minibatch, in_channels, iH, iW). weight (Tensor): The convolution filters of shape (out_channels, in_channels / groups, kH, kW). bias (Tensor, optional): Optional bias tensor of shape (out_channels). Default: None. stride (int or tuple, optional): The stride of the convolution kernel. Default: 1. padding (int, tuple, or string, optional): Padding added to all sides of the input. Default: 0. dilation (int or tuple, optional): The spacing between kernel elements. Default: 1. groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1. inplace (bool, optional): If True, will perform ReLU operation in-place. Default: False.",
        "description": "Applies a 2D convolution over an input tensor, followed by applying the rectified linear unit (ReLU) activation function element-wise on the result. This operation first applies a 2D convolution over the input tensor using the specified filters, and then applies ReLU activation to the convolution result, setting all negative values to zero.",
        "math": "The operation is defined as: \\text{out} = \\text{ReLU}(\\text{conv2d}(\\text{input})), where \\text{ReLU}(x) = \\max(0, x).",
        "example": "Example::\n\n    >>> import torch\n    >>> import torch.nn.functional as F\n    >>> # Define inputs and filters\n    >>> inputs = torch.randn(1, 3, 5, 5)\n    >>> filters = torch.randn(6, 3, 3, 3)\n    >>> bias = torch.randn(6)\n    >>> # Apply relu_conv2d with padding and bias\n    >>> result = relu_conv2d(inputs, filters, bias=bias, padding=1)\n    >>> result.shape\n    torch.Size([1, 6, 5, 5])\n\n    >>> # Apply relu_conv2d without bias and with stride 2\n    >>> result = relu_conv2d(inputs, filters, stride=2)\n    >>> result.shape\n    torch.Size([1, 6, 2, 2])",
        "torch_code": "conv_result = torch.nn.functional.conv2d(input, weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\nreturn torch.nn.functional.relu(conv_result, inplace=inplace)",
        "torch_cnt": 2,
        "other": "Returns:\n    Tensor: A tensor resulting from the 2D convolution followed by ReLU activation.",
        "difficulty": 4,
        "params_cnt": 8,
        "file": "relu_conv2d.py"
    },
    {
        "name": "torch.log",
        "func_inputs": "log(input, *, out=None) -> Tensor Args: input (Tensor): the input tensor. Keyword args: out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the natural logarithm of the elements of the input tensor.",
        "math": "y_{i} = \\log_{e} (x_{i})",
        "example": ">>> a = torch.rand(5) * 5\n>>> a\ntensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739])\n>>> torch.log(a)\ntensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204])",
        "torch_code": "torch.log(a)",
        "torch_cnt": 1,
        "other": "The function computes the natural logarithm (base e) of each element in the input tensor.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "log.py"
    },
    {
        "name": "torch.nn.functional.adaptive_avg_pool2d",
        "func_inputs": "def adaptive_avg_pool2d(output_size) -> Tensor\nArgs:\n    output_size: the target output size (single integer or double-integer tuple)\n\nShape:\n    - Input: (N, C, H_in, W_in) or (C, H_in, W_in)\n    - Output: (N, C, S_0, S_1) or (C, S_0, S_1), where S=output_size",
        "description": "Apply a 2D adaptive average pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes. The target output size of the image can be a tuple (H, W) or a single H for a square image H x H. H and W can be either an int, or None which means the size will be the same as that of the input.",
        "math": "",
        "example": ">>> # target output size of 5x7 >>> m = nn.AdaptiveAvgPool2d((5, 7)) >>> input = torch.randn(1, 64, 8, 9) >>> output = m(input) >>> # target output size of 7x7 (square) >>> m = nn.AdaptiveAvgPool2d(7) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input) >>> # target output size of 10x7 >>> m = nn.AdaptiveAvgPool2d((None, 7)) >>> input = torch.randn(1, 64, 10, 9) >>> output = m(input)",
        "torch_code": "nn.AdaptiveAvgPool2d",
        "torch_cnt": 1,
        "other": "The target output size can be a single integer for square images or a tuple for rectangular dimensions. H and W can be None to retain input dimensions.",
        "difficulty": 3,
        "params_cnt": 1,
        "file": "adaptive_avg_pool2d.py"
    },
    {
        "name": "torch.quantization.quantize_dynamic",
        "func_inputs": "quantize_dynamic(model, qconfig_spec=None, inplace=False, mapping=None) -> Model\nArgs:\n    model: input model\n    qconfig_spec: Either a dictionary mapping submodule names/types to quantization configurations or a set of types/names for dynamic quantization\n    inplace: carry out model transformations in-place, mutating the original module\n    mapping: maps submodule types to dynamically quantized versions",
        "description": "Converts a float model to a dynamic quantized model by replacing specified modules with their dynamic weight-only quantized versions. Provides simple usage with a dtype argument (either float16 or qint8), and fine-grained control with qconfig and mapping parameters. The process is performed in-place if specified, transforming the original model.",
        "math": "",
        "example": "",
        "torch_code": "torch.quantization.quantize_dynamic",
        "torch_cnt": 1,
        "other": "Dynamic quantization is typically performed on layers with large weight sizes such as Linear and RNN variants. The qconfig_spec can be a dictionary mapping submodule types or names to quantization configurations, or a set specifying which submodules to apply dynamic quantization to. If qconfig is provided, it overrides dtype.",
        "difficulty": 3,
        "params_cnt": 4,
        "file": "quantize_dynamic.py"
    },
    {
        "name": "conv2d_add",
        "func_inputs": "conv2d_add(input, weight, bias=None, other=None, stride=1, padding=0, dilation=1, groups=1, alpha=1, out=None) -> Tensor: input (Tensor): The input tensor of shape (minibatch, in_channels, iH, iW). weight (Tensor): The convolution filters of shape (out_channels, in_channels / groups, kH, kW). bias (Tensor, optional): Optional bias tensor of shape (out_channels). Default: None. other (Tensor or Number, optional): The tensor or number to add to the convolution result. Default: None. stride (int or tuple, optional): The stride of the convolution kernel. Can be a single number or a tuple (sH, sW). Default: 1. padding (int, tuple, or string, optional): Padding on both sides of the input. Can be 'valid', 'same', single number, or tuple (padH, padW). Default: 0. dilation (int or tuple, optional): The spacing between kernel elements. Default: 1. groups (int, optional): Number of groups to split the input into, must divide in_channels and out_channels. Default: 1. alpha (Number, optional): The multiplier for other. Default: 1. out (Tensor, optional): The output tensor.",
        "description": "Applies a 2D convolution over an input image using specified filters and an optional bias, then adds another tensor or scalar to the convolution result, scaled by alpha. The input tensor shape is (minibatch, in_channels, iH, iW), and the weight tensor shape is (out_channels, in_channels / groups, kH, kW). The function also allows for setting the stride, padding, dilation, groups, and an optional output tensor.",
        "math": "\\text{out} = \\text{conv2d}(\\text{input}, \\text{weight}) + \\alpha \\times \\text{other}",
        "example": ">>> import torch\n>>> import torch.nn.functional as F\n>>> # Define inputs and filters\n>>> inputs = torch.randn(1, 3, 5, 5)\n>>> filters = torch.randn(2, 3, 3, 3)\n>>> bias = torch.randn(2)\n>>> add_tensor = torch.randn(1, 2, 5, 5)\n>>> # Apply convolution with bias, then add another tensor\n>>> result = conv2d_add(inputs, filters, bias=bias, other=add_tensor, padding=1, alpha=0.5)\n>>> result.shape\ntorch.Size([1, 2, 5, 5])\n\n>>> # Using a scalar instead of a tensor for addition\n>>> result = conv2d_add(inputs, filters, bias=bias, other=3, padding=1)\n>>> result.shape\ntorch.Size([1, 2, 5, 5])",
        "torch_code": "torch.nn.functional.conv2d; torch.add",
        "torch_cnt": 2,
        "other": "The 'groups' argument must divide both in_channels and out_channels. Padding can be specified as 'valid', 'same', a single number, or a tuple. The output tensor shape depends on convolution parameters.",
        "difficulty": 4,
        "params_cnt": 9,
        "file": "conv2d_add.py"
    },
    {
        "name": "torch.fft.ifftshift",
        "func_inputs": "ifftshift(input, dim=None) -> Tensor\n\nArgs:\n    input (Tensor): the tensor in FFT order\n    dim (int, Tuple[int], optional): The dimensions to rearrange.\n        Only dimensions specified here will be rearranged, any other dimensions\n        will be left in their original order.\n        Default: All dimensions of input.",
        "description": "The function torch.fft.ifftshift is the inverse of torch.fft.fftshift. It rearranges the elements of the input tensor, which is in FFT order, such that the zero-frequency component is moved back to the original position. This is useful for preparing data for inverse FFT operations. The function can rearrange specified dimensions or all dimensions by default.",
        "math": "",
        "example": ">>> f = torch.fft.fftfreq(5)\n>>> f\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n\nA round-trip through :func:`~torch.fft.fftshift` and\n:func:`~torch.fft.ifftshift` gives the same result:\n\n>>> shifted = torch.fft.fftshift(f)\n>>> torch.fft.ifftshift(shifted)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])",
        "torch_code": "torch.fft.ifftshift(shifted)",
        "torch_cnt": 3,
        "other": "Inverse of torch.fft.fftshift.",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "ifftshift.py"
    },
    {
        "name": "signbit_bitwise_and",
        "func_inputs": "def signbit_bitwise_and(input: torch.Tensor, other: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    Args:\n        input (Tensor): The input tensor.\n        other (Tensor): The second tensor for bitwise AND, should be of integral or boolean types.\n    Example:\n        >>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n        >>> b = torch.tensor([1, 0, 1, 1], dtype=torch.int8)\n        >>> signbit_result, bitwise_and_result = signbit_bitwise_and(a, b)\n        >>> signbit_result\n        tensor([False, True, False, False])\n        >>> bitwise_and_result\n        tensor([0, 0, 0, 0], dtype=torch.int8)",
        "description": "Computes the sign bit check and the bitwise AND operation on the input tensors. `signbit` checks if the sign bit of each element in `input` is set, returning True for negative values, including -0. `bitwise_and` computes the bitwise AND between `input` and `other`, with the tensors needing to be of integral or boolean types.",
        "math": "",
        "example": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3]) >>> b = torch.tensor([1, 0, 1, 1], dtype=torch.int8) >>> signbit_result, bitwise_and_result = signbit_bitwise_and(a, b) >>> signbit_result tensor([False, True, False, False]) >>> bitwise_and_result tensor([0, 0, 0, 0], dtype=torch.int8)",
        "torch_code": "torch.signbit(input); torch.bitwise_and(input.to(other.dtype), other)",
        "torch_cnt": 2,
        "other": "The tensors need to be of integral or boolean types for the bitwise AND operation.",
        "difficulty": 1,
        "params_cnt": 2,
        "file": "signbit_bitwise_and.py"
    },
    {
        "name": "fused_repeat_interleave_log_softmax",
        "func_inputs": "fused_repeat_interleave_log_softmax(input, repeats, dim=None, *, output_size=None, dtype=None, out=None) -> Tensor",
        "description": "Performs a fused operation combining element-wise repeat interleave and log-softmax activation. First, the input tensor is repeated along the specified dimension according to the values in 'repeats'. Then, a log-softmax activation is applied to the repeated tensor along the specified dimension. This function is differentiable and supports autograd for gradient computation, making it useful for backpropagation in neural networks.",
        "math": "Given an input tensor X and repeats r, the function computes: 1. Repeat Interleave: The input tensor is repeated along the specified dimension: Y = repeat_interleave(X, r, dim). 2. Log-Softmax Activation: The log-softmax function is applied to the repeated tensor along the specified dimension: Z_i = log( exp(Y_i) / sum_j exp(Y_j) ) where the summation is over the specified dimension.",
        "example": "import torch\nimport torch.nn.functional as F\n\nx = torch.tensor([1.0, 2.0, 3.0])\nrepeats = 2\noutput = fused_repeat_interleave_log_softmax(x, repeats)\nprint('Output:')\nprint(output)\n# Output:\n# tensor([-2.4076, -2.4076, -1.4076, -1.4076, -0.4076, -0.4076])",
        "torch_code": "repeated = torch.repeat_interleave(input, repeats, dim=dim, output_size=output_size)\noutput = torch.nn.functional.log_softmax(repeated, dim=dim, dtype=dtype)\nif out is not None:\n    out.copy_(output)\n    return out\nreturn output",
        "torch_cnt": 2,
        "other": "The 'repeats' parameter controls how many times each element is repeated along the specified dimension. The 'dim' parameter specifies the dimension along which to repeat and apply log-softmax. If 'dim' is None, the input is flattened before repeating. All operations are differentiable and support backpropagation.",
        "difficulty": 3,
        "params_cnt": 6,
        "file": "fused_repeat_interleave_log_softmax.py"
    },
    {
        "name": "torch.linalg.cholesky",
        "func_inputs": "def linalg.cholesky(A, *, upper=False, out=None) -> Tensor\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions\n                consisting of symmetric or Hermitian positive-definite matrices.\n\nKeyword args:\n    upper (bool, optional): whether to return an upper triangular matrix.\n        The tensor returned with upper=True is the conjugate transpose of the tensor\n        returned with upper=False.\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.",
        "description": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if A is a batch of matrices then the output has the same batch dimensions.",
        "math": "A = LL^{\\text{H}} where L is a lower triangular matrix with real positive diagonal and L^{\\text{H}} is the conjugate transpose when L is complex, and the transpose when L is real-valued.",
        "example": ">>> A = torch.randn(2, 2, dtype=torch.complex128)\n>>> A = A @ A.T.conj() + torch.eye(2)\n>>> A\ntensor([[2.5266+0.0000j, 1.9586-2.0626j],\n        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)\n>>> L = torch.linalg.cholesky(A)\n>>> L\ntensor([[1.5895+0.0000j, 0.0000+0.0000j],\n        [1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)\n>>> torch.dist(L @ L.T.conj(), A)\ntensor(4.4692e-16, dtype=torch.float64)\n\n>>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> A = A @ A.mT + torch.eye(2)\n>>> L = torch.linalg.cholesky(A)\n>>> torch.dist(L @ L.mT, A)\ntensor(5.8747e-16, dtype=torch.float64)",
        "torch_code": "torch.linalg.cholesky(A)",
        "torch_cnt": 1,
        "other": "When inputs are on a CUDA device, this function synchronizes that device with the CPU. For a version of this function that does not synchronize, see torch.linalg.cholesky_ex. Raises RuntimeError if the A matrix or any matrix in a batched A is not Hermitian (resp. symmetric) positive-definite.",
        "difficulty": 4,
        "params_cnt": 3,
        "file": "cholesky.py"
    },
    {
        "name": "torch.ones_like",
        "func_inputs": "ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor; input (Tensor): the size of :attr:`input` will determine size of the output tensor.; dtype (torch.dtype, optional): the desired data type of returned Tensor. Default: if None, defaults to the dtype of :attr:`input`.; layout (torch.layout, optional): the desired layout of returned tensor. Default: if None, defaults to the layout of :attr:`input`.; device (torch.device, optional): the desired device of returned tensor. Default: if None, defaults to the device of :attr:`input`.; requires_grad (bool, optional): If autograd should record operations on the returned tensor. Default: False.; memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.",
        "description": "Returns a tensor filled with the scalar value 1, with the same size as the input tensor. It mirrors the properties of the input in terms of dtype, layout, device, and memory format unless specified otherwise. The function does not support the 'out' keyword as of version 0.4, and equivalent operation needs an alternative approach.",
        "math": "",
        "example": ">>> input = torch.empty(2, 3)\n>>> torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])",
        "torch_code": "torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)",
        "torch_cnt": 1,
        "other": "Function does not support an 'out' keyword as of version 0.4. Use torch.ones for similar functionality if 'out' keyword is needed.",
        "difficulty": 1,
        "params_cnt": 6,
        "file": "ones_like.py"
    },
    {
        "name": "torch.cuda.amp.autocast",
        "func_inputs": "autocast(device_type, enabled=True, dtype=None, cache_enabled=True) -> ContextManager",
        "description": "The function `torch.cuda.amp.autocast` is deprecated and replaced by `torch.amp.autocast(\"cuda\", args...)`. It allows scripts to run in mixed precision, improving performance while maintaining accuracy. `autocast` serves as a context manager or decorator, wrapping the forward pass(es) of a network and any related loss computations. Tensors can be any type when entering an autocast region, and it is not necessary to manually cast models or inputs to `half()` or `bfloat16()`. The function selects op-specific data types for operations within an autocast region. Backward operations should not be run under autocast, as they execute in the same data type chosen for the corresponding forward operations.",
        "math": "",
        "example": "CUDA Devices:\n\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n\nCUDA Example:\n\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    f_float16 = torch.mm(d_float32, e_float16)\n\ng_float32 = torch.mm(d_float32, f_float16.float())",
        "torch_code": "torch.autocast(device_type=\"cuda\")",
        "torch_cnt": 10,
        "other": "Deprecated in favor of torch.amp.autocast(\"cuda\"). Recommended to use for forward pass and loss computation only. Avoid using for backward passes. State is thread-local. Can be nested with `autocast(enabled=False)` to force a subregion to run in a specific dtype. The use of autocast in a new thread requires invoking the context manager or decorator in that thread.",
        "difficulty": 2,
        "params_cnt": 4,
        "file": "autocast.py"
    },
    {
        "name": "torch.reciprocal",
        "func_inputs": "reciprocal(input, *, out=None) -> Tensor; input (Tensor): the input tensor.; out (Tensor, optional): the output tensor.",
        "description": "Returns a new tensor with the reciprocal of the elements of the input. Unlike NumPy's reciprocal, this function supports integral inputs by promoting them to the default scalar type.",
        "math": "\\text{out}_{i} = \\frac{1}{\\text{input}_{i}}",
        "example": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n>>> torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])",
        "torch_code": "torch.reciprocal(a)",
        "torch_cnt": 1,
        "other": "Integral inputs to reciprocal are automatically promoted to the default scalar type.",
        "difficulty": 2,
        "params_cnt": 2,
        "file": "reciprocal.py"
    },
    {
        "name": "cos_signbit",
        "func_inputs": "def cos_signbit(input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: Args: input (Tensor): The input tensor for which the cosine and sign bit are computed.",
        "description": "Computes the cosine of each element in the input tensor, followed by determining the sign bit for each cosine result, indicating if it is positive or negative.",
        "math": "\\text{cos\\_result} = \\cos(\\text{input}) \\text{sign\\_bit} = \\text{signbit}(\\text{cos\\_result})",
        "example": ">>> a = torch.tensor([1.4309, 1.2706, -0.8562, 0.9796]) >>> cos_result, sign_bit = cos_signbit(a) >>> cos_result tensor([ 0.1395,  0.2957,  0.6553,  0.5574]) >>> sign_bit tensor([False, False, False, False])",
        "torch_code": "cos_result = torch.cos(input) sign_bit = torch.signbit(cos_result)",
        "torch_cnt": 2,
        "other": "Returns a tuple containing the cosine of each element and a boolean tensor indicating the sign bit of each cosine result.",
        "difficulty": 1,
        "params_cnt": 1,
        "file": "cos_signbit.py"
    },
    {
        "name": "spectral_norm_eig",
        "func_inputs": "spectral_norm_eig(A, *, out=None) -> Tensor\nA (Tensor): Tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions consisting of square matrices.\nout (Tensor, optional): Output tensor. Ignored if `None`. Default: `None`.",
        "description": "Computes the spectral norm (operator norm induced by the Euclidean vector norm) of a square matrix using its eigenvalues. The spectral norm is the largest absolute value among the eigenvalues of a matrix. It supports inputs of float, double, cfloat, and cdouble dtypes and handles batches of matrices.",
        "math": "\\|A\\|_2 = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}",
        "example": ">>> import torch\n>>> def spectral_norm_eig(A, *, out=None):\n...     eigenvalues, _ = torch.linalg.eig(A)\n...     abs_eigenvalues = torch.abs(eigenvalues)\n...     spectral_norm, _ = torch.max(abs_eigenvalues, dim=-1)\n...     if out is not None:\n...         out.copy_(spectral_norm)\n...         return out\n...     return spectral_norm\n>>> # Example with a symmetric matrix\n>>> A = torch.tensor([[1., 2.], [2., 1.]])\n>>> spectral_norm_eig(A)\ntensor(3.)\n>>> # Compare with torch.linalg.norm(A, 2)\n>>> torch.linalg.norm(A, 2)\ntensor(3.)\n>>> # Example with a general matrix\n>>> A = torch.tensor([[1., 2.], [3., 4.]])\n>>> spectral_norm_eig(A)\ntensor(5.3723)\n>>> # Compare with torch.linalg.norm(A, 2)\n>>> torch.linalg.norm(A, 2)\ntensor(5.4649)\n>>> # Note the difference due to non-normality of A\n>>> # Example with a batch of matrices\n>>> A_batch = torch.randn(4, 3, 3)\n>>> norms = spectral_norm_eig(A_batch)\n>>> norms.shape\ntorch.Size([4])",
        "torch_code": "eigenvalues, _ = torch.linalg.eig(A)\nabs_eigenvalues = torch.abs(eigenvalues)\nspectral_norm, _ = torch.max(abs_eigenvalues, dim=-1)\nif out is not None:\n    out.copy_(spectral_norm)\n    return out\nreturn spectral_norm",
        "torch_cnt": 3,
        "other": "For normal matrices (where A A^{H} = A^{H} A), the spectral norm equals the largest absolute eigenvalue.",
        "difficulty": 4,
        "params_cnt": 2,
        "file": "spectral_norm_eig.py"
    },
    {
        "name": "torch.fft.fftn",
        "func_inputs": "fftn(input, s=None, dim=None, norm=None, *, out=None) -> Tensor; input (Tensor): the input tensor; s (Tuple[int], optional): Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]; dim (Tuple[int], optional): Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given.; norm (str, optional): Normalization mode. For the forward transform (fftn), these correspond to: 'forward' - normalize by 1/n; 'backward' - no normalization; 'ortho' - normalize by 1/sqrt(n) (making the FFT orthonormal) Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn the exact inverse. Default is 'backward' (no normalization).; out (Tensor, optional): the output tensor.",
        "description": "Computes the N dimensional discrete Fourier transform of the input tensor. It returns all positive and negative frequency terms, even though for real inputs, half of these values are redundant. Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater, but only for powers of 2 signal length in every transformed dimension.",
        "math": "",
        "example": ">>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fftn = torch.fft.fftn(x)\n\nThe discrete Fourier transform is separable, so torch.fft.fftn here is equivalent to two one-dimensional torch.fft.fft calls:\n\n>>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.testing.assert_close(fftn, two_ffts, check_stride=False)",
        "torch_code": "torch.fft.fftn(x)",
        "torch_cnt": 4,
        "other": "The Fourier domain representation of any real signal satisfies the Hermitian property. torch.fft.rfftn returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.",
        "difficulty": 5,
        "params_cnt": 5,
        "file": "fftn.py"
    }
]